{"litestar-litestar/file_system.py-info": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/file_system.py:\n```\nfrom __future__ import annotations\n\nfrom stat import S_ISDIR\nfrom typing import TYPE_CHECKING, Any, AnyStr, cast\n\nfrom anyio import AsyncFile, Path, open_file\n\nfrom litestar.concurrency import sync_to_thread\nfrom litestar.exceptions import InternalServerException, NotAuthorizedException\nfrom litestar.types.file_types import FileSystemProtocol\nfrom litestar.utils.predicates import is_async_callable\n\n__all__ = (\"BaseLocalFileSystem\", \"FileSystemAdapter\")\n\n\nif TYPE_CHECKING:\n    from os import stat_result\n\n    from _typeshed import OpenBinaryMode\n\n    from litestar.types import PathType\n    from litestar.types.file_types import FileInfo\n\n\nclass BaseLocalFileSystem(FileSystemProtocol):\n    \"\"\"Base class for a local file system.\"\"\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def open(self, file: PathType, mode: str, buffering: int = -1) -> AsyncFile[AnyStr]:  # pyright: ignore\n        \"\"\"Return a file-like object from the filesystem.\n\n        Notes:\n            - The return value must be a context-manager\n\n        Args:\n            file: Path to the target file.\n            mode: Mode, similar to the built ``open``.\n            buffering: Buffer size.\n        \"\"\"\n        return await open_file(file=file, mode=mode, buffering=buffering)  # type: ignore[call-overload, no-any-return]\n\n\nclass FileSystemAdapter:\n    \"\"\"Wrapper around a ``FileSystemProtocol``, normalising its interface.\"\"\"\n\n    def __init__(self, file_system: FileSystemProtocol) -> None:\n        \"\"\"Initialize an adapter from a given ``file_system``\n\n        Args:\n            file_system: A filesystem class adhering to the :class:`FileSystemProtocol <litestar.types.FileSystemProtocol>`\n        \"\"\"\n        self.file_system = file_system\n\n    async def info(self, path: PathType) -> FileInfo:\n        \"\"\"Proxies the call to the underlying FS Spec's ``info`` method, ensuring it's done in an async fashion and with\n        strong typing.\n\n        Args:\n            path: A file path to load the info for.\n\n        Returns:\n            A dictionary of file info.\n        \"\"\"\n        try:\n            awaitable = (\n                self.file_system.info(str(path))\n                if is_async_callable(self.file_system.info)\n                else sync_to_thread(self.file_system.info, str(path))\n            )\n            return cast(\"FileInfo\", await awaitable)\n        except FileNotFoundError as e:\n            raise e\n        except PermissionError as e:\n            raise NotAuthorizedException(f\"failed to read {path} due to missing permissions\") from e\n        except OSError as e:  # pragma: no cover\n            raise InternalServerException from e\n\n    async def open(\n        self,\n        file: PathType,\n        mode: OpenBinaryMode = \"rb\",\n        buffering: int = -1,\n    ) -> AsyncFile[bytes]:\n        \"\"\"Return a file-like object from the filesystem.\n\n        Notes:\n            - The return value must function correctly in a context ``with`` block.\n\n        Args:\n            file: Path to the target file.\n            mode: Mode, similar to the built ``open``.\n            buffering: Buffer size.\n        \"\"\"\n        try:\n            if is_async_callable(self.file_system.open):  # pyright: ignore\n                return cast(\n                    \"AsyncFile[bytes]\",\n                    await self.file_system.open(\n                        file=file,\n                        mode=mode,\n                        buffering=buffering,\n                    ),\n                )\n            return AsyncFile(await sync_to_thread(self.file_system.open, file, mode, buffering))  # type: ignore[arg-type]\n        except PermissionError as e:\n            raise NotAuthorizedException(f\"failed to open {file} due to missing permissions\") from e\n        except OSError as e:\n            raise InternalServerException from e\n\n    @staticmethod\n    async def parse_stat_result(path: PathType, result: stat_result) -> FileInfo:\n        \"\"\"Convert a ``stat_result`` instance into a ``FileInfo``.\n\n        Args:\n            path: The file path for which the :func:`stat_result <os.stat_result>` is provided.\n            result: The :func:`stat_result <os.stat_result>` instance.\n\n        Returns:\n            A dictionary of file info.\n        \"\"\"\n        file_info: FileInfo = {\n            \"created\": result.st_ctime,\n            \"gid\": result.st_gid,\n            \"ino\": result.st_ino,\n            \"islink\": await Path(path).is_symlink(),\n            \"mode\": result.st_mode,\n            \"mtime\": result.st_mtime,\n            \"name\": str(path),\n            \"nlink\": result.st_nlink,\n            \"size\": result.st_size,\n            \"type\": \"directory\" if S_ISDIR(result.st_mode) else \"file\",\n            \"uid\": result.st_uid,\n        }\n\n        if file_info[\"islink\"]:\n            file_info[\"destination\"] = str(await Path(path).readlink()).encode(\"utf-8\")\n            try:\n                file_info[\"size\"] = (await Path(path).stat()).st_size\n            except OSError:  # pragma: no cover\n                file_info[\"size\"] = result.st_size\n\n        return file_info\n\n```\n# Function Signature: async def info(self, path: PathType, **kwargs: Any) -> FileInfo:\n# Natural Language Annotations: \"\"\"Retrieve information about a given file path.\n\n        Args:\n            path: A file path.\n            **kwargs: Any additional kwargs.\n\n        Returns:\n            A dictionary of file info.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def info(self, path: PathType, **kwargs", "  ```\nasync def info(self, path: PathType, **kwargs", "  ```\nasync def info(self, path: PathType, **kwargs"]}, "litestar-litestar/typing.py-has_inner_subclass_of": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/typing.py:\n```\nfrom __future__ import annotations\n\nimport warnings\nfrom collections import abc, deque\nfrom copy import deepcopy\nfrom dataclasses import dataclass, is_dataclass, replace\nfrom inspect import Parameter, Signature\nfrom typing import Any, AnyStr, Callable, Collection, ForwardRef, Literal, Mapping, Protocol, Sequence, TypeVar, cast\n\nfrom msgspec import UnsetType\nfrom typing_extensions import NewType, NotRequired, Required, Self, get_args, get_origin, get_type_hints, is_typeddict\n\nfrom litestar.exceptions import ImproperlyConfiguredException, LitestarWarning\nfrom litestar.openapi.spec import Example\nfrom litestar.params import BodyKwarg, DependencyKwarg, KwargDefinition, ParameterKwarg\nfrom litestar.types import Empty\nfrom litestar.types.builtin_types import NoneType, UnionTypes\nfrom litestar.utils.predicates import (\n    is_annotated_type,\n    is_any,\n    is_class_and_subclass,\n    is_generic,\n    is_non_string_iterable,\n    is_non_string_sequence,\n    is_union,\n)\nfrom litestar.utils.typing import (\n    get_instantiable_origin,\n    get_safe_generic_origin,\n    get_type_hints_with_generics_resolved,\n    make_non_optional_union,\n    unwrap_annotation,\n)\n\n__all__ = (\"FieldDefinition\",)\n\nT = TypeVar(\"T\", bound=KwargDefinition)\n\n\nclass _KwargMetaExtractor(Protocol):\n    @staticmethod\n    def matches(annotation: Any, name: str | None, default: Any) -> bool: ...\n\n    @staticmethod\n    def extract(annotation: Any, default: Any) -> Any: ...\n\n\n_KWARG_META_EXTRACTORS: set[_KwargMetaExtractor] = set()\n\n\ndef _unpack_predicate(value: Any) -> dict[str, Any]:\n    try:\n        from annotated_types import Predicate\n\n        if isinstance(value, Predicate):\n            if value.func == str.islower:\n                return {\"lower_case\": True}\n            if value.func == str.isupper:\n                return {\"upper_case\": True}\n            if value.func == str.isascii:\n                return {\"pattern\": \"[[:ascii:]]\"}\n            if value.func == str.isdigit:\n                return {\"pattern\": \"[[:digit:]]\"}\n    except ImportError:\n        pass\n\n    return {}\n\n\ndef _parse_metadata(value: Any, is_sequence_container: bool, extra: dict[str, Any] | None) -> dict[str, Any]:\n    \"\"\"Parse metadata from a value.\n\n    Args:\n        value: A metadata value from annotation, namely anything stored under Annotated[x, metadata...]\n        is_sequence_container: Whether the type is a sequence container (list, tuple etc...)\n        extra: Extra key values to parse.\n\n    Returns:\n        A dictionary of constraints, which fulfill the kwargs of a KwargDefinition class.\n    \"\"\"\n    extra = {\n        **cast(\"dict[str, Any]\", extra or getattr(value, \"extra\", None) or {}),\n        **(getattr(value, \"json_schema_extra\", None) or {}),\n    }\n    example_list: list[Any] | None\n    if example := extra.pop(\"example\", None):\n        example_list = [Example(value=example)]\n    elif examples := (extra.pop(\"examples\", None) or getattr(value, \"examples\", None)):\n        example_list = [Example(value=example) for example in cast(\"list[str]\", examples)]\n    else:\n        example_list = None\n\n    return {\n        k: v\n        for k, v in {\n            \"gt\": getattr(value, \"gt\", None),\n            \"ge\": getattr(value, \"ge\", None),\n            \"lt\": getattr(value, \"lt\", None),\n            \"le\": getattr(value, \"le\", None),\n            \"multiple_of\": getattr(value, \"multiple_of\", None),\n            \"min_length\": None if is_sequence_container else getattr(value, \"min_length\", None),\n            \"max_length\": None if is_sequence_container else getattr(value, \"max_length\", None),\n            \"description\": getattr(value, \"description\", None),\n            \"examples\": example_list,\n            \"title\": getattr(value, \"title\", None),\n            \"lower_case\": getattr(value, \"to_lower\", None),\n            \"upper_case\": getattr(value, \"to_upper\", None),\n            \"pattern\": getattr(value, \"regex\", getattr(value, \"pattern\", None)),\n            \"min_items\": getattr(value, \"min_items\", getattr(value, \"min_length\", None))\n            if is_sequence_container\n            else None,\n            \"max_items\": getattr(value, \"max_items\", getattr(value, \"max_length\", None))\n            if is_sequence_container\n            else None,\n            \"const\": getattr(value, \"const\", None) is not None,\n            **extra,\n        }.items()\n        if v is not None\n    }\n\n\ndef _traverse_metadata(\n    metadata: Sequence[Any], is_sequence_container: bool, extra: dict[str, Any] | None\n) -> dict[str, Any]:\n    \"\"\"Recursively traverse metadata from a value.\n\n    Args:\n        metadata: A list of metadata values from annotation, namely anything stored under Annotated[x, metadata...]\n        is_sequence_container: Whether the container is a sequence container (list, tuple etc...)\n        extra: Extra key values to parse.\n\n    Returns:\n        A dictionary of constraints, which fulfill the kwargs of a KwargDefinition class.\n    \"\"\"\n    constraints: dict[str, Any] = {}\n    for value in metadata:\n        if isinstance(value, (list, set, frozenset, deque)):\n            constraints.update(\n                _traverse_metadata(\n                    metadata=cast(\"Sequence[Any]\", value), is_sequence_container=is_sequence_container, extra=extra\n                )\n            )\n        elif is_annotated_type(value) and (type_args := [v for v in get_args(value) if v is not None]):\n            # annotated values can be nested inside other annotated values\n            # this behaviour is buggy in python 3.8, hence we need to guard here.\n            if len(type_args) > 1:\n                constraints.update(\n                    _traverse_metadata(metadata=type_args[1:], is_sequence_container=is_sequence_container, extra=extra)\n                )\n        elif unpacked_predicate := _unpack_predicate(value):\n            constraints.update(unpacked_predicate)\n        else:\n            constraints.update(_parse_metadata(value=value, is_sequence_container=is_sequence_container, extra=extra))\n    return constraints\n\n\ndef _create_metadata_from_type(\n    metadata: Sequence[Any], model: type[T], annotation: Any, extra: dict[str, Any] | None\n) -> tuple[T | None, dict[str, Any]]:\n    is_sequence_container = is_non_string_sequence(annotation)\n    result = _traverse_metadata(metadata=metadata, is_sequence_container=is_sequence_container, extra=extra)\n\n    constraints = {k: v for k, v in result.items() if k in dir(model)}\n    extra = {k: v for k, v in result.items() if k not in constraints}\n    return model(**constraints) if constraints else None, extra\n\n\n@dataclass(frozen=True)\nclass FieldDefinition:\n    \"\"\"Represents a function parameter or type annotation.\"\"\"\n\n    __slots__ = (\n        \"annotation\",\n        \"args\",\n        \"default\",\n        \"extra\",\n        \"inner_types\",\n        \"instantiable_origin\",\n        \"kwarg_definition\",\n        \"metadata\",\n        \"name\",\n        \"origin\",\n        \"raw\",\n        \"safe_generic_origin\",\n        \"type_wrappers\",\n    )\n\n    raw: Any\n    \"\"\"The annotation exactly as received.\"\"\"\n    annotation: Any\n    \"\"\"The annotation with any \"wrapper\" types removed, e.g. Annotated.\"\"\"\n    type_wrappers: tuple[type, ...]\n    \"\"\"A set of all \"wrapper\" types, e.g. Annotated.\"\"\"\n    origin: Any\n    \"\"\"The result of calling ``get_origin(annotation)`` after unwrapping Annotated, e.g. list.\"\"\"\n    args: tuple[Any, ...]\n    \"\"\"The result of calling ``get_args(annotation)`` after unwrapping Annotated, e.g. (int,).\"\"\"\n    metadata: tuple[Any, ...]\n    \"\"\"Any metadata associated with the annotation via ``Annotated``.\"\"\"\n    instantiable_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely instantiated. E.g., ``Sequence`` -> ``list``.\"\"\"\n    safe_generic_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely used as a generic type across all supported Python versions.\n\n    This is to serve safely rebuilding a generic outer type with different args at runtime.\n    \"\"\"\n    inner_types: tuple[FieldDefinition, ...]\n    \"\"\"The type's generic args parsed as ``FieldDefinition``, if applicable.\"\"\"\n    default: Any\n    \"\"\"Default value of the field.\"\"\"\n    extra: dict[str, Any]\n    \"\"\"A mapping of extra values.\"\"\"\n    kwarg_definition: KwargDefinition | DependencyKwarg | None\n    \"\"\"Kwarg Parameter.\"\"\"\n    name: str\n    \"\"\"Field name.\"\"\"\n\n    def __deepcopy__(self, memo: dict[str, Any]) -> Self:\n        return type(self)(**{attr: deepcopy(getattr(self, attr)) for attr in self.__slots__})\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, FieldDefinition):\n            return False\n\n        if self.origin:\n            return self.origin == other.origin and self.inner_types == other.inner_types\n\n        return self.annotation == other.annotation  # type: ignore[no-any-return]\n\n    def __hash__(self) -> int:\n        return hash((self.name, self.raw, self.annotation, self.origin, self.inner_types))\n\n    @classmethod\n    def _extract_metadata(\n        cls, annotation: Any, name: str | None, default: Any, metadata: tuple[Any, ...], extra: dict[str, Any] | None\n    ) -> tuple[KwargDefinition | None, dict[str, Any]]:\n        model = BodyKwarg if name == \"data\" else ParameterKwarg\n\n        for extractor in _KWARG_META_EXTRACTORS:\n            if extractor.matches(annotation=annotation, name=name, default=default):\n                return _create_metadata_from_type(\n                    extractor.extract(annotation=annotation, default=default),\n                    model=model,\n                    annotation=annotation,\n                    extra=extra,\n                )\n\n        if any(isinstance(arg, KwargDefinition) for arg in get_args(annotation)):\n            return next(arg for arg in get_args(annotation) if isinstance(arg, KwargDefinition)), extra or {}\n\n        if metadata:\n            return _create_metadata_from_type(metadata=metadata, model=model, annotation=annotation, extra=extra)\n\n        return None, {}\n\n    @property\n    def has_default(self) -> bool:\n        \"\"\"Check if the field has a default value.\n\n        Returns:\n            True if the default is not Empty or Ellipsis otherwise False.\n        \"\"\"\n        return self.default is not Empty and self.default is not Ellipsis\n\n    @property\n    def is_non_string_iterable(self) -> bool:\n        \"\"\"Check if the field type is an Iterable.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_iterable(annotation)\n\n    @property\n    def is_non_string_sequence(self) -> bool:\n        \"\"\"Check if the field type is a non-string Sequence.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_sequence(annotation)\n\n    @property\n    def is_any(self) -> bool:\n        \"\"\"Check if the field type is Any.\"\"\"\n        return is_any(self.annotation)\n\n    @property\n    def is_generic(self) -> bool:\n        \"\"\"Check if the field type is a custom class extending Generic.\"\"\"\n        return is_generic(self.annotation)\n\n    @property\n    def is_simple_type(self) -> bool:\n        \"\"\"Check if the field type is a singleton value (e.g. int, str etc.).\"\"\"\n        return not (\n            self.is_generic\n            or self.is_optional\n            or self.is_union\n            or self.is_mapping\n            or self.is_non_string_iterable\n            or self.is_new_type\n        )\n\n    @property\n    def is_parameter_field(self) -> bool:\n        \"\"\"Check if the field type is a parameter kwarg value.\"\"\"\n        return isinstance(self.kwarg_definition, ParameterKwarg)\n\n    @property\n    def is_const(self) -> bool:\n        \"\"\"Check if the field is defined as constant value.\"\"\"\n        return bool(self.kwarg_definition and getattr(self.kwarg_definition, \"const\", False))\n\n    @property\n    def is_required(self) -> bool:\n        \"\"\"Check if the field should be marked as a required parameter.\"\"\"\n        if Required in self.type_wrappers:  # type: ignore[comparison-overlap]\n            return True\n\n        if NotRequired in self.type_wrappers or UnsetType in self.args:  # type: ignore[comparison-overlap]\n            return False\n\n        if isinstance(self.kwarg_definition, ParameterKwarg) and self.kwarg_definition.required is not None:\n            return self.kwarg_definition.required\n\n        return not self.is_optional and not self.is_any and (not self.has_default or self.default is None)\n\n    @property\n    def is_annotated(self) -> bool:\n        \"\"\"Check if the field type is Annotated.\"\"\"\n        return bool(self.metadata)\n\n    @property\n    def is_literal(self) -> bool:\n        \"\"\"Check if the field type is Literal.\"\"\"\n        return self.origin is Literal\n\n    @property\n    def is_forward_ref(self) -> bool:\n        \"\"\"Whether the annotation is a forward reference or not.\"\"\"\n        return isinstance(self.annotation, (str, ForwardRef))\n\n    @property\n    def is_mapping(self) -> bool:\n        \"\"\"Whether the annotation is a mapping or not.\"\"\"\n        return self.is_subclass_of(Mapping)\n\n    @property\n    def is_tuple(self) -> bool:\n        \"\"\"Whether the annotation is a ``tuple`` or not.\"\"\"\n        return self.is_subclass_of(tuple)\n\n    @property\n    def is_new_type(self) -> bool:\n        return isinstance(self.annotation, NewType)\n\n    @property\n    def is_type_var(self) -> bool:\n        \"\"\"Whether the annotation is a TypeVar or not.\"\"\"\n        return isinstance(self.annotation, TypeVar)\n\n    @property\n    def is_union(self) -> bool:\n        \"\"\"Whether the annotation is a union type or not.\"\"\"\n        return self.origin in UnionTypes\n\n    @property\n    def is_optional(self) -> bool:\n        \"\"\"Whether the annotation is Optional or not.\"\"\"\n        return bool(self.is_union and NoneType in self.args)\n\n    @property\n    def is_none_type(self) -> bool:\n        \"\"\"Whether the annotation is NoneType or not.\"\"\"\n        return self.annotation is NoneType\n\n    @property\n    def is_collection(self) -> bool:\n        \"\"\"Whether the annotation is a collection type or not.\"\"\"\n        return self.is_subclass_of(Collection)\n\n    @property\n    def is_non_string_collection(self) -> bool:\n        \"\"\"Whether the annotation is a non-string collection type or not.\"\"\"\n        return self.is_collection and not self.is_subclass_of((str, bytes))\n\n    @property\n    def bound_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of bound types - if the annotation is a TypeVar with bound types, otherwise None.\"\"\"\n        if self.is_type_var and (bound := getattr(self.annotation, \"__bound__\", None)):\n            if is_union(bound):\n                return tuple(FieldDefinition.from_annotation(t) for t in get_args(bound))\n            return (FieldDefinition.from_annotation(bound),)\n        return None\n\n    @property\n    def generic_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of generic types passed into the annotation - if its generic.\"\"\"\n        if not (bases := getattr(self.annotation, \"__orig_bases__\", None)):\n            return None\n        args: list[FieldDefinition] = []\n        for base_args in [getattr(base, \"__args__\", ()) for base in bases]:\n            for arg in base_args:\n                field_definition = FieldDefinition.from_annotation(arg)\n                if field_definition.generic_types:\n                    args.extend(field_definition.generic_types)\n                else:\n                    args.append(field_definition)\n        return tuple(args)\n\n    @property\n    def is_dataclass_type(self) -> bool:\n        \"\"\"Whether the annotation is a dataclass type or not.\"\"\"\n\n        return is_dataclass(cast(\"type\", self.origin or self.annotation))\n\n    @property\n    def is_typeddict_type(self) -> bool:\n        \"\"\"Whether the type is TypedDict or not.\"\"\"\n\n        return is_typeddict(self.origin or self.annotation)\n\n    @property\n    def type_(self) -> Any:\n        \"\"\"The type of the annotation with all the wrappers removed, including the generic types.\"\"\"\n\n        return self.origin or self.annotation\n\n    def is_subclass_of(self, cl: type[Any] | tuple[type[Any], ...]) -> bool:\n        \"\"\"Whether the annotation is a subclass of the given type.\n\n        Where ``self.annotation`` is a union type, this method will return ``True`` when all members of the union are\n        a subtype of ``cl``, otherwise, ``False``.\n\n        Args:\n            cl: The type to check, or tuple of types. Passed as 2nd argument to ``issubclass()``.\n\n        Returns:\n            Whether the annotation is a subtype of the given type(s).\n        \"\"\"\n        if self.origin:\n            if self.origin in UnionTypes:\n                return all(t.is_subclass_of(cl) for t in self.inner_types)\n\n            return self.origin not in UnionTypes and is_class_and_subclass(self.origin, cl)\n\n        if self.annotation is AnyStr:\n            return is_class_and_subclass(str, cl) or is_class_and_subclass(bytes, cl)\n\n        return self.annotation is not Any and not self.is_type_var and is_class_and_subclass(self.annotation, cl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def from_annotation(cls, annotation: Any, **kwargs: Any) -> FieldDefinition:\n        \"\"\"Initialize FieldDefinition.\n\n        Args:\n            annotation: The type annotation. This should be extracted from the return of\n                ``get_type_hints(..., include_extras=True)`` so that forward references are resolved and recursive\n                ``Annotated`` types are flattened.\n            **kwargs: Additional keyword arguments to pass to the ``FieldDefinition`` constructor.\n\n        Returns:\n            FieldDefinition\n        \"\"\"\n\n        unwrapped, metadata, wrappers = unwrap_annotation(annotation if annotation is not Empty else Any)\n        origin = get_origin(unwrapped)\n\n        args = () if origin is abc.Callable else get_args(unwrapped)\n\n        if not kwargs.get(\"kwarg_definition\"):\n            if isinstance(kwargs.get(\"default\"), (KwargDefinition, DependencyKwarg)):\n                kwargs[\"kwarg_definition\"] = kwargs.pop(\"default\")\n            elif kwarg_definition := next(\n                (v for v in metadata if isinstance(v, (KwargDefinition, DependencyKwarg))), None\n            ):\n                kwargs[\"kwarg_definition\"] = kwarg_definition\n\n                if kwarg_definition.default is not Empty:\n                    warnings.warn(\n                        f\"Deprecated default value specification for annotation '{annotation}'. Setting defaults \"\n                        f\"inside 'typing.Annotated' is discouraged and support for this will be removed in a future \"\n                        f\"version. Defaults should be set with regular parameter default values. Use \"\n                        \"'param: Annotated[<type>, Parameter(...)] = <default>' instead of \"\n                        \"'param: Annotated[<type>, Parameter(..., default=<default>)].\",\n                        category=DeprecationWarning,\n                        stacklevel=2,\n                    )\n                    if kwargs.get(\"default\", Empty) is not Empty and kwarg_definition.default != kwargs[\"default\"]:\n                        warnings.warn(\n                            f\"Ambiguous default values for annotation '{annotation}'. The default value \"\n                            f\"'{kwarg_definition.default!r}' set inside the parameter annotation differs from the \"\n                            f\"parameter default value '{kwargs['default']!r}'\",\n                            category=LitestarWarning,\n                            stacklevel=2,\n                        )\n\n                metadata = tuple(v for v in metadata if not isinstance(v, (KwargDefinition, DependencyKwarg)))\n            elif (extra := kwargs.get(\"extra\", {})) and \"kwarg_definition\" in extra:\n                kwargs[\"kwarg_definition\"] = extra.pop(\"kwarg_definition\")\n            else:\n                kwargs[\"kwarg_definition\"], kwargs[\"extra\"] = cls._extract_metadata(\n                    annotation=annotation,\n                    name=kwargs.get(\"name\", \"\"),\n                    default=kwargs.get(\"default\", Empty),\n                    metadata=metadata,\n                    extra=kwargs.get(\"extra\"),\n                )\n\n        kwargs.setdefault(\"annotation\", unwrapped)\n        kwargs.setdefault(\"args\", args)\n        kwargs.setdefault(\"default\", Empty)\n        kwargs.setdefault(\"extra\", {})\n        kwargs.setdefault(\"inner_types\", tuple(FieldDefinition.from_annotation(arg) for arg in args))\n        kwargs.setdefault(\"instantiable_origin\", get_instantiable_origin(origin, unwrapped))\n        kwargs.setdefault(\"kwarg_definition\", None)\n        kwargs.setdefault(\"metadata\", metadata)\n        kwargs.setdefault(\"name\", \"\")\n        kwargs.setdefault(\"origin\", origin)\n        kwargs.setdefault(\"raw\", annotation)\n        kwargs.setdefault(\"safe_generic_origin\", get_safe_generic_origin(origin, unwrapped))\n        kwargs.setdefault(\"type_wrappers\", wrappers)\n\n        instance = FieldDefinition(**kwargs)\n        if not instance.has_default and instance.kwarg_definition:\n            return replace(instance, default=instance.kwarg_definition.default)\n\n        return instance\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def match_predicate_recursively(self, predicate: Callable[[FieldDefinition], bool]) -> bool:\n        \"\"\"Recursively test the passed in predicate against the field and any of its inner fields.\n\n        Args:\n            predicate: A callable that receives a field definition instance as an arg and returns a boolean.\n\n        Returns:\n            A boolean.\n        \"\"\"\n        return predicate(self) or any(t.match_predicate_recursively(predicate) for t in self.inner_types)\n\n```\n# Function Signature: def has_inner_subclass_of(self, cl: type[Any] | tuple[type[Any], ...]) -> bool:\n# Natural Language Annotations: \"\"\"Whether any generic args are a subclass of the given type.\n\n        Args:\n            cl: The type to check, or tuple of types. Passed as 2nd argument to ``issubclass()``.\n\n        Returns:\n            Whether any of the type's generic args are a subclass of the given type.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef has_inner_subclass_of(self, cl", "  ```\ndef has_inner_subclass_of(self, cl", "  ```\ndef has_inner_subclass_of(self, cl"]}, "litestar-litestar/typing.py-get_type_hints": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/typing.py:\n```\nfrom __future__ import annotations\n\nimport warnings\nfrom collections import abc, deque\nfrom copy import deepcopy\nfrom dataclasses import dataclass, is_dataclass, replace\nfrom inspect import Parameter, Signature\nfrom typing import Any, AnyStr, Callable, Collection, ForwardRef, Literal, Mapping, Protocol, Sequence, TypeVar, cast\n\nfrom msgspec import UnsetType\nfrom typing_extensions import NewType, NotRequired, Required, Self, get_args, get_origin, get_type_hints, is_typeddict\n\nfrom litestar.exceptions import ImproperlyConfiguredException, LitestarWarning\nfrom litestar.openapi.spec import Example\nfrom litestar.params import BodyKwarg, DependencyKwarg, KwargDefinition, ParameterKwarg\nfrom litestar.types import Empty\nfrom litestar.types.builtin_types import NoneType, UnionTypes\nfrom litestar.utils.predicates import (\n    is_annotated_type,\n    is_any,\n    is_class_and_subclass,\n    is_generic,\n    is_non_string_iterable,\n    is_non_string_sequence,\n    is_union,\n)\nfrom litestar.utils.typing import (\n    get_instantiable_origin,\n    get_safe_generic_origin,\n    get_type_hints_with_generics_resolved,\n    make_non_optional_union,\n    unwrap_annotation,\n)\n\n__all__ = (\"FieldDefinition\",)\n\nT = TypeVar(\"T\", bound=KwargDefinition)\n\n\nclass _KwargMetaExtractor(Protocol):\n    @staticmethod\n    def matches(annotation: Any, name: str | None, default: Any) -> bool: ...\n\n    @staticmethod\n    def extract(annotation: Any, default: Any) -> Any: ...\n\n\n_KWARG_META_EXTRACTORS: set[_KwargMetaExtractor] = set()\n\n\ndef _unpack_predicate(value: Any) -> dict[str, Any]:\n    try:\n        from annotated_types import Predicate\n\n        if isinstance(value, Predicate):\n            if value.func == str.islower:\n                return {\"lower_case\": True}\n            if value.func == str.isupper:\n                return {\"upper_case\": True}\n            if value.func == str.isascii:\n                return {\"pattern\": \"[[:ascii:]]\"}\n            if value.func == str.isdigit:\n                return {\"pattern\": \"[[:digit:]]\"}\n    except ImportError:\n        pass\n\n    return {}\n\n\ndef _parse_metadata(value: Any, is_sequence_container: bool, extra: dict[str, Any] | None) -> dict[str, Any]:\n    \"\"\"Parse metadata from a value.\n\n    Args:\n        value: A metadata value from annotation, namely anything stored under Annotated[x, metadata...]\n        is_sequence_container: Whether the type is a sequence container (list, tuple etc...)\n        extra: Extra key values to parse.\n\n    Returns:\n        A dictionary of constraints, which fulfill the kwargs of a KwargDefinition class.\n    \"\"\"\n    extra = {\n        **cast(\"dict[str, Any]\", extra or getattr(value, \"extra\", None) or {}),\n        **(getattr(value, \"json_schema_extra\", None) or {}),\n    }\n    example_list: list[Any] | None\n    if example := extra.pop(\"example\", None):\n        example_list = [Example(value=example)]\n    elif examples := (extra.pop(\"examples\", None) or getattr(value, \"examples\", None)):\n        example_list = [Example(value=example) for example in cast(\"list[str]\", examples)]\n    else:\n        example_list = None\n\n    return {\n        k: v\n        for k, v in {\n            \"gt\": getattr(value, \"gt\", None),\n            \"ge\": getattr(value, \"ge\", None),\n            \"lt\": getattr(value, \"lt\", None),\n            \"le\": getattr(value, \"le\", None),\n            \"multiple_of\": getattr(value, \"multiple_of\", None),\n            \"min_length\": None if is_sequence_container else getattr(value, \"min_length\", None),\n            \"max_length\": None if is_sequence_container else getattr(value, \"max_length\", None),\n            \"description\": getattr(value, \"description\", None),\n            \"examples\": example_list,\n            \"title\": getattr(value, \"title\", None),\n            \"lower_case\": getattr(value, \"to_lower\", None),\n            \"upper_case\": getattr(value, \"to_upper\", None),\n            \"pattern\": getattr(value, \"regex\", getattr(value, \"pattern\", None)),\n            \"min_items\": getattr(value, \"min_items\", getattr(value, \"min_length\", None))\n            if is_sequence_container\n            else None,\n            \"max_items\": getattr(value, \"max_items\", getattr(value, \"max_length\", None))\n            if is_sequence_container\n            else None,\n            \"const\": getattr(value, \"const\", None) is not None,\n            **extra,\n        }.items()\n        if v is not None\n    }\n\n\ndef _traverse_metadata(\n    metadata: Sequence[Any], is_sequence_container: bool, extra: dict[str, Any] | None\n) -> dict[str, Any]:\n    \"\"\"Recursively traverse metadata from a value.\n\n    Args:\n        metadata: A list of metadata values from annotation, namely anything stored under Annotated[x, metadata...]\n        is_sequence_container: Whether the container is a sequence container (list, tuple etc...)\n        extra: Extra key values to parse.\n\n    Returns:\n        A dictionary of constraints, which fulfill the kwargs of a KwargDefinition class.\n    \"\"\"\n    constraints: dict[str, Any] = {}\n    for value in metadata:\n        if isinstance(value, (list, set, frozenset, deque)):\n            constraints.update(\n                _traverse_metadata(\n                    metadata=cast(\"Sequence[Any]\", value), is_sequence_container=is_sequence_container, extra=extra\n                )\n            )\n        elif is_annotated_type(value) and (type_args := [v for v in get_args(value) if v is not None]):\n            # annotated values can be nested inside other annotated values\n            # this behaviour is buggy in python 3.8, hence we need to guard here.\n            if len(type_args) > 1:\n                constraints.update(\n                    _traverse_metadata(metadata=type_args[1:], is_sequence_container=is_sequence_container, extra=extra)\n                )\n        elif unpacked_predicate := _unpack_predicate(value):\n            constraints.update(unpacked_predicate)\n        else:\n            constraints.update(_parse_metadata(value=value, is_sequence_container=is_sequence_container, extra=extra))\n    return constraints\n\n\ndef _create_metadata_from_type(\n    metadata: Sequence[Any], model: type[T], annotation: Any, extra: dict[str, Any] | None\n) -> tuple[T | None, dict[str, Any]]:\n    is_sequence_container = is_non_string_sequence(annotation)\n    result = _traverse_metadata(metadata=metadata, is_sequence_container=is_sequence_container, extra=extra)\n\n    constraints = {k: v for k, v in result.items() if k in dir(model)}\n    extra = {k: v for k, v in result.items() if k not in constraints}\n    return model(**constraints) if constraints else None, extra\n\n\n@dataclass(frozen=True)\nclass FieldDefinition:\n    \"\"\"Represents a function parameter or type annotation.\"\"\"\n\n    __slots__ = (\n        \"annotation\",\n        \"args\",\n        \"default\",\n        \"extra\",\n        \"inner_types\",\n        \"instantiable_origin\",\n        \"kwarg_definition\",\n        \"metadata\",\n        \"name\",\n        \"origin\",\n        \"raw\",\n        \"safe_generic_origin\",\n        \"type_wrappers\",\n    )\n\n    raw: Any\n    \"\"\"The annotation exactly as received.\"\"\"\n    annotation: Any\n    \"\"\"The annotation with any \"wrapper\" types removed, e.g. Annotated.\"\"\"\n    type_wrappers: tuple[type, ...]\n    \"\"\"A set of all \"wrapper\" types, e.g. Annotated.\"\"\"\n    origin: Any\n    \"\"\"The result of calling ``get_origin(annotation)`` after unwrapping Annotated, e.g. list.\"\"\"\n    args: tuple[Any, ...]\n    \"\"\"The result of calling ``get_args(annotation)`` after unwrapping Annotated, e.g. (int,).\"\"\"\n    metadata: tuple[Any, ...]\n    \"\"\"Any metadata associated with the annotation via ``Annotated``.\"\"\"\n    instantiable_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely instantiated. E.g., ``Sequence`` -> ``list``.\"\"\"\n    safe_generic_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely used as a generic type across all supported Python versions.\n\n    This is to serve safely rebuilding a generic outer type with different args at runtime.\n    \"\"\"\n    inner_types: tuple[FieldDefinition, ...]\n    \"\"\"The type's generic args parsed as ``FieldDefinition``, if applicable.\"\"\"\n    default: Any\n    \"\"\"Default value of the field.\"\"\"\n    extra: dict[str, Any]\n    \"\"\"A mapping of extra values.\"\"\"\n    kwarg_definition: KwargDefinition | DependencyKwarg | None\n    \"\"\"Kwarg Parameter.\"\"\"\n    name: str\n    \"\"\"Field name.\"\"\"\n\n    def __deepcopy__(self, memo: dict[str, Any]) -> Self:\n        return type(self)(**{attr: deepcopy(getattr(self, attr)) for attr in self.__slots__})\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, FieldDefinition):\n            return False\n\n        if self.origin:\n            return self.origin == other.origin and self.inner_types == other.inner_types\n\n        return self.annotation == other.annotation  # type: ignore[no-any-return]\n\n    def __hash__(self) -> int:\n        return hash((self.name, self.raw, self.annotation, self.origin, self.inner_types))\n\n    @classmethod\n    def _extract_metadata(\n        cls, annotation: Any, name: str | None, default: Any, metadata: tuple[Any, ...], extra: dict[str, Any] | None\n    ) -> tuple[KwargDefinition | None, dict[str, Any]]:\n        model = BodyKwarg if name == \"data\" else ParameterKwarg\n\n        for extractor in _KWARG_META_EXTRACTORS:\n            if extractor.matches(annotation=annotation, name=name, default=default):\n                return _create_metadata_from_type(\n                    extractor.extract(annotation=annotation, default=default),\n                    model=model,\n                    annotation=annotation,\n                    extra=extra,\n                )\n\n        if any(isinstance(arg, KwargDefinition) for arg in get_args(annotation)):\n            return next(arg for arg in get_args(annotation) if isinstance(arg, KwargDefinition)), extra or {}\n\n        if metadata:\n            return _create_metadata_from_type(metadata=metadata, model=model, annotation=annotation, extra=extra)\n\n        return None, {}\n\n    @property\n    def has_default(self) -> bool:\n        \"\"\"Check if the field has a default value.\n\n        Returns:\n            True if the default is not Empty or Ellipsis otherwise False.\n        \"\"\"\n        return self.default is not Empty and self.default is not Ellipsis\n\n    @property\n    def is_non_string_iterable(self) -> bool:\n        \"\"\"Check if the field type is an Iterable.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_iterable(annotation)\n\n    @property\n    def is_non_string_sequence(self) -> bool:\n        \"\"\"Check if the field type is a non-string Sequence.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_sequence(annotation)\n\n    @property\n    def is_any(self) -> bool:\n        \"\"\"Check if the field type is Any.\"\"\"\n        return is_any(self.annotation)\n\n    @property\n    def is_generic(self) -> bool:\n        \"\"\"Check if the field type is a custom class extending Generic.\"\"\"\n        return is_generic(self.annotation)\n\n    @property\n    def is_simple_type(self) -> bool:\n        \"\"\"Check if the field type is a singleton value (e.g. int, str etc.).\"\"\"\n        return not (\n            self.is_generic\n            or self.is_optional\n            or self.is_union\n            or self.is_mapping\n            or self.is_non_string_iterable\n            or self.is_new_type\n        )\n\n    @property\n    def is_parameter_field(self) -> bool:\n        \"\"\"Check if the field type is a parameter kwarg value.\"\"\"\n        return isinstance(self.kwarg_definition, ParameterKwarg)\n\n    @property\n    def is_const(self) -> bool:\n        \"\"\"Check if the field is defined as constant value.\"\"\"\n        return bool(self.kwarg_definition and getattr(self.kwarg_definition, \"const\", False))\n\n    @property\n    def is_required(self) -> bool:\n        \"\"\"Check if the field should be marked as a required parameter.\"\"\"\n        if Required in self.type_wrappers:  # type: ignore[comparison-overlap]\n            return True\n\n        if NotRequired in self.type_wrappers or UnsetType in self.args:  # type: ignore[comparison-overlap]\n            return False\n\n        if isinstance(self.kwarg_definition, ParameterKwarg) and self.kwarg_definition.required is not None:\n            return self.kwarg_definition.required\n\n        return not self.is_optional and not self.is_any and (not self.has_default or self.default is None)\n\n    @property\n    def is_annotated(self) -> bool:\n        \"\"\"Check if the field type is Annotated.\"\"\"\n        return bool(self.metadata)\n\n    @property\n    def is_literal(self) -> bool:\n        \"\"\"Check if the field type is Literal.\"\"\"\n        return self.origin is Literal\n\n    @property\n    def is_forward_ref(self) -> bool:\n        \"\"\"Whether the annotation is a forward reference or not.\"\"\"\n        return isinstance(self.annotation, (str, ForwardRef))\n\n    @property\n    def is_mapping(self) -> bool:\n        \"\"\"Whether the annotation is a mapping or not.\"\"\"\n        return self.is_subclass_of(Mapping)\n\n    @property\n    def is_tuple(self) -> bool:\n        \"\"\"Whether the annotation is a ``tuple`` or not.\"\"\"\n        return self.is_subclass_of(tuple)\n\n    @property\n    def is_new_type(self) -> bool:\n        return isinstance(self.annotation, NewType)\n\n    @property\n    def is_type_var(self) -> bool:\n        \"\"\"Whether the annotation is a TypeVar or not.\"\"\"\n        return isinstance(self.annotation, TypeVar)\n\n    @property\n    def is_union(self) -> bool:\n        \"\"\"Whether the annotation is a union type or not.\"\"\"\n        return self.origin in UnionTypes\n\n    @property\n    def is_optional(self) -> bool:\n        \"\"\"Whether the annotation is Optional or not.\"\"\"\n        return bool(self.is_union and NoneType in self.args)\n\n    @property\n    def is_none_type(self) -> bool:\n        \"\"\"Whether the annotation is NoneType or not.\"\"\"\n        return self.annotation is NoneType\n\n    @property\n    def is_collection(self) -> bool:\n        \"\"\"Whether the annotation is a collection type or not.\"\"\"\n        return self.is_subclass_of(Collection)\n\n    @property\n    def is_non_string_collection(self) -> bool:\n        \"\"\"Whether the annotation is a non-string collection type or not.\"\"\"\n        return self.is_collection and not self.is_subclass_of((str, bytes))\n\n    @property\n    def bound_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of bound types - if the annotation is a TypeVar with bound types, otherwise None.\"\"\"\n        if self.is_type_var and (bound := getattr(self.annotation, \"__bound__\", None)):\n            if is_union(bound):\n                return tuple(FieldDefinition.from_annotation(t) for t in get_args(bound))\n            return (FieldDefinition.from_annotation(bound),)\n        return None\n\n    @property\n    def generic_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of generic types passed into the annotation - if its generic.\"\"\"\n        if not (bases := getattr(self.annotation, \"__orig_bases__\", None)):\n            return None\n        args: list[FieldDefinition] = []\n        for base_args in [getattr(base, \"__args__\", ()) for base in bases]:\n            for arg in base_args:\n                field_definition = FieldDefinition.from_annotation(arg)\n                if field_definition.generic_types:\n                    args.extend(field_definition.generic_types)\n                else:\n                    args.append(field_definition)\n        return tuple(args)\n\n    @property\n    def is_dataclass_type(self) -> bool:\n        \"\"\"Whether the annotation is a dataclass type or not.\"\"\"\n\n        return is_dataclass(cast(\"type\", self.origin or self.annotation))\n\n    @property\n    def is_typeddict_type(self) -> bool:\n        \"\"\"Whether the type is TypedDict or not.\"\"\"\n\n        return is_typeddict(self.origin or self.annotation)\n\n    @property\n    def type_(self) -> Any:\n        \"\"\"The type of the annotation with all the wrappers removed, including the generic types.\"\"\"\n\n        return self.origin or self.annotation\n\n    def is_subclass_of(self, cl: type[Any] | tuple[type[Any], ...]) -> bool:\n        \"\"\"Whether the annotation is a subclass of the given type.\n\n        Where ``self.annotation`` is a union type, this method will return ``True`` when all members of the union are\n        a subtype of ``cl``, otherwise, ``False``.\n\n        Args:\n            cl: The type to check, or tuple of types. Passed as 2nd argument to ``issubclass()``.\n\n        Returns:\n            Whether the annotation is a subtype of the given type(s).\n        \"\"\"\n        if self.origin:\n            if self.origin in UnionTypes:\n                return all(t.is_subclass_of(cl) for t in self.inner_types)\n\n            return self.origin not in UnionTypes and is_class_and_subclass(self.origin, cl)\n\n        if self.annotation is AnyStr:\n            return is_class_and_subclass(str, cl) or is_class_and_subclass(bytes, cl)\n\n        return self.annotation is not Any and not self.is_type_var and is_class_and_subclass(self.annotation, cl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def from_annotation(cls, annotation: Any, **kwargs: Any) -> FieldDefinition:\n        \"\"\"Initialize FieldDefinition.\n\n        Args:\n            annotation: The type annotation. This should be extracted from the return of\n                ``get_type_hints(..., include_extras=True)`` so that forward references are resolved and recursive\n                ``Annotated`` types are flattened.\n            **kwargs: Additional keyword arguments to pass to the ``FieldDefinition`` constructor.\n\n        Returns:\n            FieldDefinition\n        \"\"\"\n\n        unwrapped, metadata, wrappers = unwrap_annotation(annotation if annotation is not Empty else Any)\n        origin = get_origin(unwrapped)\n\n        args = () if origin is abc.Callable else get_args(unwrapped)\n\n        if not kwargs.get(\"kwarg_definition\"):\n            if isinstance(kwargs.get(\"default\"), (KwargDefinition, DependencyKwarg)):\n                kwargs[\"kwarg_definition\"] = kwargs.pop(\"default\")\n            elif kwarg_definition := next(\n                (v for v in metadata if isinstance(v, (KwargDefinition, DependencyKwarg))), None\n            ):\n                kwargs[\"kwarg_definition\"] = kwarg_definition\n\n                if kwarg_definition.default is not Empty:\n                    warnings.warn(\n                        f\"Deprecated default value specification for annotation '{annotation}'. Setting defaults \"\n                        f\"inside 'typing.Annotated' is discouraged and support for this will be removed in a future \"\n                        f\"version. Defaults should be set with regular parameter default values. Use \"\n                        \"'param: Annotated[<type>, Parameter(...)] = <default>' instead of \"\n                        \"'param: Annotated[<type>, Parameter(..., default=<default>)].\",\n                        category=DeprecationWarning,\n                        stacklevel=2,\n                    )\n                    if kwargs.get(\"default\", Empty) is not Empty and kwarg_definition.default != kwargs[\"default\"]:\n                        warnings.warn(\n                            f\"Ambiguous default values for annotation '{annotation}'. The default value \"\n                            f\"'{kwarg_definition.default!r}' set inside the parameter annotation differs from the \"\n                            f\"parameter default value '{kwargs['default']!r}'\",\n                            category=LitestarWarning,\n                            stacklevel=2,\n                        )\n\n                metadata = tuple(v for v in metadata if not isinstance(v, (KwargDefinition, DependencyKwarg)))\n            elif (extra := kwargs.get(\"extra\", {})) and \"kwarg_definition\" in extra:\n                kwargs[\"kwarg_definition\"] = extra.pop(\"kwarg_definition\")\n            else:\n                kwargs[\"kwarg_definition\"], kwargs[\"extra\"] = cls._extract_metadata(\n                    annotation=annotation,\n                    name=kwargs.get(\"name\", \"\"),\n                    default=kwargs.get(\"default\", Empty),\n                    metadata=metadata,\n                    extra=kwargs.get(\"extra\"),\n                )\n\n        kwargs.setdefault(\"annotation\", unwrapped)\n        kwargs.setdefault(\"args\", args)\n        kwargs.setdefault(\"default\", Empty)\n        kwargs.setdefault(\"extra\", {})\n        kwargs.setdefault(\"inner_types\", tuple(FieldDefinition.from_annotation(arg) for arg in args))\n        kwargs.setdefault(\"instantiable_origin\", get_instantiable_origin(origin, unwrapped))\n        kwargs.setdefault(\"kwarg_definition\", None)\n        kwargs.setdefault(\"metadata\", metadata)\n        kwargs.setdefault(\"name\", \"\")\n        kwargs.setdefault(\"origin\", origin)\n        kwargs.setdefault(\"raw\", annotation)\n        kwargs.setdefault(\"safe_generic_origin\", get_safe_generic_origin(origin, unwrapped))\n        kwargs.setdefault(\"type_wrappers\", wrappers)\n\n        instance = FieldDefinition(**kwargs)\n        if not instance.has_default and instance.kwarg_definition:\n            return replace(instance, default=instance.kwarg_definition.default)\n\n        return instance\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def match_predicate_recursively(self, predicate: Callable[[FieldDefinition], bool]) -> bool:\n        \"\"\"Recursively test the passed in predicate against the field and any of its inner fields.\n\n        Args:\n            predicate: A callable that receives a field definition instance as an arg and returns a boolean.\n\n        Returns:\n            A boolean.\n        \"\"\"\n        return predicate(self) or any(t.match_predicate_recursively(predicate) for t in self.inner_types)\n\n```\n# Function Signature: def get_type_hints(self, *, include_extras: bool = False, resolve_generics: bool = False) -> dict[str, Any]:\n# Natural Language Annotations: \"\"\"Get the type hints for the annotation.\n\n        Args:\n            include_extras: Flag to indicate whether to include ``Annotated[T, ...]`` or not.\n            resolve_generics: Flag to indicate whether to resolve the generic types in the type hints or not.\n\n        Returns:\n            The type hints.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_type_hints(self, *, include", "  ```\ndef get_type_hints(self, *, include", "  ```\ndef get_type_hints(self, *, include"]}, "litestar-litestar/typing.py-from_kwarg": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/typing.py:\n```\nfrom __future__ import annotations\n\nimport warnings\nfrom collections import abc, deque\nfrom copy import deepcopy\nfrom dataclasses import dataclass, is_dataclass, replace\nfrom inspect import Parameter, Signature\nfrom typing import Any, AnyStr, Callable, Collection, ForwardRef, Literal, Mapping, Protocol, Sequence, TypeVar, cast\n\nfrom msgspec import UnsetType\nfrom typing_extensions import NewType, NotRequired, Required, Self, get_args, get_origin, get_type_hints, is_typeddict\n\nfrom litestar.exceptions import ImproperlyConfiguredException, LitestarWarning\nfrom litestar.openapi.spec import Example\nfrom litestar.params import BodyKwarg, DependencyKwarg, KwargDefinition, ParameterKwarg\nfrom litestar.types import Empty\nfrom litestar.types.builtin_types import NoneType, UnionTypes\nfrom litestar.utils.predicates import (\n    is_annotated_type,\n    is_any,\n    is_class_and_subclass,\n    is_generic,\n    is_non_string_iterable,\n    is_non_string_sequence,\n    is_union,\n)\nfrom litestar.utils.typing import (\n    get_instantiable_origin,\n    get_safe_generic_origin,\n    get_type_hints_with_generics_resolved,\n    make_non_optional_union,\n    unwrap_annotation,\n)\n\n__all__ = (\"FieldDefinition\",)\n\nT = TypeVar(\"T\", bound=KwargDefinition)\n\n\nclass _KwargMetaExtractor(Protocol):\n    @staticmethod\n    def matches(annotation: Any, name: str | None, default: Any) -> bool: ...\n\n    @staticmethod\n    def extract(annotation: Any, default: Any) -> Any: ...\n\n\n_KWARG_META_EXTRACTORS: set[_KwargMetaExtractor] = set()\n\n\ndef _unpack_predicate(value: Any) -> dict[str, Any]:\n    try:\n        from annotated_types import Predicate\n\n        if isinstance(value, Predicate):\n            if value.func == str.islower:\n                return {\"lower_case\": True}\n            if value.func == str.isupper:\n                return {\"upper_case\": True}\n            if value.func == str.isascii:\n                return {\"pattern\": \"[[:ascii:]]\"}\n            if value.func == str.isdigit:\n                return {\"pattern\": \"[[:digit:]]\"}\n    except ImportError:\n        pass\n\n    return {}\n\n\ndef _parse_metadata(value: Any, is_sequence_container: bool, extra: dict[str, Any] | None) -> dict[str, Any]:\n    \"\"\"Parse metadata from a value.\n\n    Args:\n        value: A metadata value from annotation, namely anything stored under Annotated[x, metadata...]\n        is_sequence_container: Whether the type is a sequence container (list, tuple etc...)\n        extra: Extra key values to parse.\n\n    Returns:\n        A dictionary of constraints, which fulfill the kwargs of a KwargDefinition class.\n    \"\"\"\n    extra = {\n        **cast(\"dict[str, Any]\", extra or getattr(value, \"extra\", None) or {}),\n        **(getattr(value, \"json_schema_extra\", None) or {}),\n    }\n    example_list: list[Any] | None\n    if example := extra.pop(\"example\", None):\n        example_list = [Example(value=example)]\n    elif examples := (extra.pop(\"examples\", None) or getattr(value, \"examples\", None)):\n        example_list = [Example(value=example) for example in cast(\"list[str]\", examples)]\n    else:\n        example_list = None\n\n    return {\n        k: v\n        for k, v in {\n            \"gt\": getattr(value, \"gt\", None),\n            \"ge\": getattr(value, \"ge\", None),\n            \"lt\": getattr(value, \"lt\", None),\n            \"le\": getattr(value, \"le\", None),\n            \"multiple_of\": getattr(value, \"multiple_of\", None),\n            \"min_length\": None if is_sequence_container else getattr(value, \"min_length\", None),\n            \"max_length\": None if is_sequence_container else getattr(value, \"max_length\", None),\n            \"description\": getattr(value, \"description\", None),\n            \"examples\": example_list,\n            \"title\": getattr(value, \"title\", None),\n            \"lower_case\": getattr(value, \"to_lower\", None),\n            \"upper_case\": getattr(value, \"to_upper\", None),\n            \"pattern\": getattr(value, \"regex\", getattr(value, \"pattern\", None)),\n            \"min_items\": getattr(value, \"min_items\", getattr(value, \"min_length\", None))\n            if is_sequence_container\n            else None,\n            \"max_items\": getattr(value, \"max_items\", getattr(value, \"max_length\", None))\n            if is_sequence_container\n            else None,\n            \"const\": getattr(value, \"const\", None) is not None,\n            **extra,\n        }.items()\n        if v is not None\n    }\n\n\ndef _traverse_metadata(\n    metadata: Sequence[Any], is_sequence_container: bool, extra: dict[str, Any] | None\n) -> dict[str, Any]:\n    \"\"\"Recursively traverse metadata from a value.\n\n    Args:\n        metadata: A list of metadata values from annotation, namely anything stored under Annotated[x, metadata...]\n        is_sequence_container: Whether the container is a sequence container (list, tuple etc...)\n        extra: Extra key values to parse.\n\n    Returns:\n        A dictionary of constraints, which fulfill the kwargs of a KwargDefinition class.\n    \"\"\"\n    constraints: dict[str, Any] = {}\n    for value in metadata:\n        if isinstance(value, (list, set, frozenset, deque)):\n            constraints.update(\n                _traverse_metadata(\n                    metadata=cast(\"Sequence[Any]\", value), is_sequence_container=is_sequence_container, extra=extra\n                )\n            )\n        elif is_annotated_type(value) and (type_args := [v for v in get_args(value) if v is not None]):\n            # annotated values can be nested inside other annotated values\n            # this behaviour is buggy in python 3.8, hence we need to guard here.\n            if len(type_args) > 1:\n                constraints.update(\n                    _traverse_metadata(metadata=type_args[1:], is_sequence_container=is_sequence_container, extra=extra)\n                )\n        elif unpacked_predicate := _unpack_predicate(value):\n            constraints.update(unpacked_predicate)\n        else:\n            constraints.update(_parse_metadata(value=value, is_sequence_container=is_sequence_container, extra=extra))\n    return constraints\n\n\ndef _create_metadata_from_type(\n    metadata: Sequence[Any], model: type[T], annotation: Any, extra: dict[str, Any] | None\n) -> tuple[T | None, dict[str, Any]]:\n    is_sequence_container = is_non_string_sequence(annotation)\n    result = _traverse_metadata(metadata=metadata, is_sequence_container=is_sequence_container, extra=extra)\n\n    constraints = {k: v for k, v in result.items() if k in dir(model)}\n    extra = {k: v for k, v in result.items() if k not in constraints}\n    return model(**constraints) if constraints else None, extra\n\n\n@dataclass(frozen=True)\nclass FieldDefinition:\n    \"\"\"Represents a function parameter or type annotation.\"\"\"\n\n    __slots__ = (\n        \"annotation\",\n        \"args\",\n        \"default\",\n        \"extra\",\n        \"inner_types\",\n        \"instantiable_origin\",\n        \"kwarg_definition\",\n        \"metadata\",\n        \"name\",\n        \"origin\",\n        \"raw\",\n        \"safe_generic_origin\",\n        \"type_wrappers\",\n    )\n\n    raw: Any\n    \"\"\"The annotation exactly as received.\"\"\"\n    annotation: Any\n    \"\"\"The annotation with any \"wrapper\" types removed, e.g. Annotated.\"\"\"\n    type_wrappers: tuple[type, ...]\n    \"\"\"A set of all \"wrapper\" types, e.g. Annotated.\"\"\"\n    origin: Any\n    \"\"\"The result of calling ``get_origin(annotation)`` after unwrapping Annotated, e.g. list.\"\"\"\n    args: tuple[Any, ...]\n    \"\"\"The result of calling ``get_args(annotation)`` after unwrapping Annotated, e.g. (int,).\"\"\"\n    metadata: tuple[Any, ...]\n    \"\"\"Any metadata associated with the annotation via ``Annotated``.\"\"\"\n    instantiable_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely instantiated. E.g., ``Sequence`` -> ``list``.\"\"\"\n    safe_generic_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely used as a generic type across all supported Python versions.\n\n    This is to serve safely rebuilding a generic outer type with different args at runtime.\n    \"\"\"\n    inner_types: tuple[FieldDefinition, ...]\n    \"\"\"The type's generic args parsed as ``FieldDefinition``, if applicable.\"\"\"\n    default: Any\n    \"\"\"Default value of the field.\"\"\"\n    extra: dict[str, Any]\n    \"\"\"A mapping of extra values.\"\"\"\n    kwarg_definition: KwargDefinition | DependencyKwarg | None\n    \"\"\"Kwarg Parameter.\"\"\"\n    name: str\n    \"\"\"Field name.\"\"\"\n\n    def __deepcopy__(self, memo: dict[str, Any]) -> Self:\n        return type(self)(**{attr: deepcopy(getattr(self, attr)) for attr in self.__slots__})\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, FieldDefinition):\n            return False\n\n        if self.origin:\n            return self.origin == other.origin and self.inner_types == other.inner_types\n\n        return self.annotation == other.annotation  # type: ignore[no-any-return]\n\n    def __hash__(self) -> int:\n        return hash((self.name, self.raw, self.annotation, self.origin, self.inner_types))\n\n    @classmethod\n    def _extract_metadata(\n        cls, annotation: Any, name: str | None, default: Any, metadata: tuple[Any, ...], extra: dict[str, Any] | None\n    ) -> tuple[KwargDefinition | None, dict[str, Any]]:\n        model = BodyKwarg if name == \"data\" else ParameterKwarg\n\n        for extractor in _KWARG_META_EXTRACTORS:\n            if extractor.matches(annotation=annotation, name=name, default=default):\n                return _create_metadata_from_type(\n                    extractor.extract(annotation=annotation, default=default),\n                    model=model,\n                    annotation=annotation,\n                    extra=extra,\n                )\n\n        if any(isinstance(arg, KwargDefinition) for arg in get_args(annotation)):\n            return next(arg for arg in get_args(annotation) if isinstance(arg, KwargDefinition)), extra or {}\n\n        if metadata:\n            return _create_metadata_from_type(metadata=metadata, model=model, annotation=annotation, extra=extra)\n\n        return None, {}\n\n    @property\n    def has_default(self) -> bool:\n        \"\"\"Check if the field has a default value.\n\n        Returns:\n            True if the default is not Empty or Ellipsis otherwise False.\n        \"\"\"\n        return self.default is not Empty and self.default is not Ellipsis\n\n    @property\n    def is_non_string_iterable(self) -> bool:\n        \"\"\"Check if the field type is an Iterable.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_iterable(annotation)\n\n    @property\n    def is_non_string_sequence(self) -> bool:\n        \"\"\"Check if the field type is a non-string Sequence.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_sequence(annotation)\n\n    @property\n    def is_any(self) -> bool:\n        \"\"\"Check if the field type is Any.\"\"\"\n        return is_any(self.annotation)\n\n    @property\n    def is_generic(self) -> bool:\n        \"\"\"Check if the field type is a custom class extending Generic.\"\"\"\n        return is_generic(self.annotation)\n\n    @property\n    def is_simple_type(self) -> bool:\n        \"\"\"Check if the field type is a singleton value (e.g. int, str etc.).\"\"\"\n        return not (\n            self.is_generic\n            or self.is_optional\n            or self.is_union\n            or self.is_mapping\n            or self.is_non_string_iterable\n            or self.is_new_type\n        )\n\n    @property\n    def is_parameter_field(self) -> bool:\n        \"\"\"Check if the field type is a parameter kwarg value.\"\"\"\n        return isinstance(self.kwarg_definition, ParameterKwarg)\n\n    @property\n    def is_const(self) -> bool:\n        \"\"\"Check if the field is defined as constant value.\"\"\"\n        return bool(self.kwarg_definition and getattr(self.kwarg_definition, \"const\", False))\n\n    @property\n    def is_required(self) -> bool:\n        \"\"\"Check if the field should be marked as a required parameter.\"\"\"\n        if Required in self.type_wrappers:  # type: ignore[comparison-overlap]\n            return True\n\n        if NotRequired in self.type_wrappers or UnsetType in self.args:  # type: ignore[comparison-overlap]\n            return False\n\n        if isinstance(self.kwarg_definition, ParameterKwarg) and self.kwarg_definition.required is not None:\n            return self.kwarg_definition.required\n\n        return not self.is_optional and not self.is_any and (not self.has_default or self.default is None)\n\n    @property\n    def is_annotated(self) -> bool:\n        \"\"\"Check if the field type is Annotated.\"\"\"\n        return bool(self.metadata)\n\n    @property\n    def is_literal(self) -> bool:\n        \"\"\"Check if the field type is Literal.\"\"\"\n        return self.origin is Literal\n\n    @property\n    def is_forward_ref(self) -> bool:\n        \"\"\"Whether the annotation is a forward reference or not.\"\"\"\n        return isinstance(self.annotation, (str, ForwardRef))\n\n    @property\n    def is_mapping(self) -> bool:\n        \"\"\"Whether the annotation is a mapping or not.\"\"\"\n        return self.is_subclass_of(Mapping)\n\n    @property\n    def is_tuple(self) -> bool:\n        \"\"\"Whether the annotation is a ``tuple`` or not.\"\"\"\n        return self.is_subclass_of(tuple)\n\n    @property\n    def is_new_type(self) -> bool:\n        return isinstance(self.annotation, NewType)\n\n    @property\n    def is_type_var(self) -> bool:\n        \"\"\"Whether the annotation is a TypeVar or not.\"\"\"\n        return isinstance(self.annotation, TypeVar)\n\n    @property\n    def is_union(self) -> bool:\n        \"\"\"Whether the annotation is a union type or not.\"\"\"\n        return self.origin in UnionTypes\n\n    @property\n    def is_optional(self) -> bool:\n        \"\"\"Whether the annotation is Optional or not.\"\"\"\n        return bool(self.is_union and NoneType in self.args)\n\n    @property\n    def is_none_type(self) -> bool:\n        \"\"\"Whether the annotation is NoneType or not.\"\"\"\n        return self.annotation is NoneType\n\n    @property\n    def is_collection(self) -> bool:\n        \"\"\"Whether the annotation is a collection type or not.\"\"\"\n        return self.is_subclass_of(Collection)\n\n    @property\n    def is_non_string_collection(self) -> bool:\n        \"\"\"Whether the annotation is a non-string collection type or not.\"\"\"\n        return self.is_collection and not self.is_subclass_of((str, bytes))\n\n    @property\n    def bound_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of bound types - if the annotation is a TypeVar with bound types, otherwise None.\"\"\"\n        if self.is_type_var and (bound := getattr(self.annotation, \"__bound__\", None)):\n            if is_union(bound):\n                return tuple(FieldDefinition.from_annotation(t) for t in get_args(bound))\n            return (FieldDefinition.from_annotation(bound),)\n        return None\n\n    @property\n    def generic_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of generic types passed into the annotation - if its generic.\"\"\"\n        if not (bases := getattr(self.annotation, \"__orig_bases__\", None)):\n            return None\n        args: list[FieldDefinition] = []\n        for base_args in [getattr(base, \"__args__\", ()) for base in bases]:\n            for arg in base_args:\n                field_definition = FieldDefinition.from_annotation(arg)\n                if field_definition.generic_types:\n                    args.extend(field_definition.generic_types)\n                else:\n                    args.append(field_definition)\n        return tuple(args)\n\n    @property\n    def is_dataclass_type(self) -> bool:\n        \"\"\"Whether the annotation is a dataclass type or not.\"\"\"\n\n        return is_dataclass(cast(\"type\", self.origin or self.annotation))\n\n    @property\n    def is_typeddict_type(self) -> bool:\n        \"\"\"Whether the type is TypedDict or not.\"\"\"\n\n        return is_typeddict(self.origin or self.annotation)\n\n    @property\n    def type_(self) -> Any:\n        \"\"\"The type of the annotation with all the wrappers removed, including the generic types.\"\"\"\n\n        return self.origin or self.annotation\n\n    def is_subclass_of(self, cl: type[Any] | tuple[type[Any], ...]) -> bool:\n        \"\"\"Whether the annotation is a subclass of the given type.\n\n        Where ``self.annotation`` is a union type, this method will return ``True`` when all members of the union are\n        a subtype of ``cl``, otherwise, ``False``.\n\n        Args:\n            cl: The type to check, or tuple of types. Passed as 2nd argument to ``issubclass()``.\n\n        Returns:\n            Whether the annotation is a subtype of the given type(s).\n        \"\"\"\n        if self.origin:\n            if self.origin in UnionTypes:\n                return all(t.is_subclass_of(cl) for t in self.inner_types)\n\n            return self.origin not in UnionTypes and is_class_and_subclass(self.origin, cl)\n\n        if self.annotation is AnyStr:\n            return is_class_and_subclass(str, cl) or is_class_and_subclass(bytes, cl)\n\n        return self.annotation is not Any and not self.is_type_var and is_class_and_subclass(self.annotation, cl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def from_annotation(cls, annotation: Any, **kwargs: Any) -> FieldDefinition:\n        \"\"\"Initialize FieldDefinition.\n\n        Args:\n            annotation: The type annotation. This should be extracted from the return of\n                ``get_type_hints(..., include_extras=True)`` so that forward references are resolved and recursive\n                ``Annotated`` types are flattened.\n            **kwargs: Additional keyword arguments to pass to the ``FieldDefinition`` constructor.\n\n        Returns:\n            FieldDefinition\n        \"\"\"\n\n        unwrapped, metadata, wrappers = unwrap_annotation(annotation if annotation is not Empty else Any)\n        origin = get_origin(unwrapped)\n\n        args = () if origin is abc.Callable else get_args(unwrapped)\n\n        if not kwargs.get(\"kwarg_definition\"):\n            if isinstance(kwargs.get(\"default\"), (KwargDefinition, DependencyKwarg)):\n                kwargs[\"kwarg_definition\"] = kwargs.pop(\"default\")\n            elif kwarg_definition := next(\n                (v for v in metadata if isinstance(v, (KwargDefinition, DependencyKwarg))), None\n            ):\n                kwargs[\"kwarg_definition\"] = kwarg_definition\n\n                if kwarg_definition.default is not Empty:\n                    warnings.warn(\n                        f\"Deprecated default value specification for annotation '{annotation}'. Setting defaults \"\n                        f\"inside 'typing.Annotated' is discouraged and support for this will be removed in a future \"\n                        f\"version. Defaults should be set with regular parameter default values. Use \"\n                        \"'param: Annotated[<type>, Parameter(...)] = <default>' instead of \"\n                        \"'param: Annotated[<type>, Parameter(..., default=<default>)].\",\n                        category=DeprecationWarning,\n                        stacklevel=2,\n                    )\n                    if kwargs.get(\"default\", Empty) is not Empty and kwarg_definition.default != kwargs[\"default\"]:\n                        warnings.warn(\n                            f\"Ambiguous default values for annotation '{annotation}'. The default value \"\n                            f\"'{kwarg_definition.default!r}' set inside the parameter annotation differs from the \"\n                            f\"parameter default value '{kwargs['default']!r}'\",\n                            category=LitestarWarning,\n                            stacklevel=2,\n                        )\n\n                metadata = tuple(v for v in metadata if not isinstance(v, (KwargDefinition, DependencyKwarg)))\n            elif (extra := kwargs.get(\"extra\", {})) and \"kwarg_definition\" in extra:\n                kwargs[\"kwarg_definition\"] = extra.pop(\"kwarg_definition\")\n            else:\n                kwargs[\"kwarg_definition\"], kwargs[\"extra\"] = cls._extract_metadata(\n                    annotation=annotation,\n                    name=kwargs.get(\"name\", \"\"),\n                    default=kwargs.get(\"default\", Empty),\n                    metadata=metadata,\n                    extra=kwargs.get(\"extra\"),\n                )\n\n        kwargs.setdefault(\"annotation\", unwrapped)\n        kwargs.setdefault(\"args\", args)\n        kwargs.setdefault(\"default\", Empty)\n        kwargs.setdefault(\"extra\", {})\n        kwargs.setdefault(\"inner_types\", tuple(FieldDefinition.from_annotation(arg) for arg in args))\n        kwargs.setdefault(\"instantiable_origin\", get_instantiable_origin(origin, unwrapped))\n        kwargs.setdefault(\"kwarg_definition\", None)\n        kwargs.setdefault(\"metadata\", metadata)\n        kwargs.setdefault(\"name\", \"\")\n        kwargs.setdefault(\"origin\", origin)\n        kwargs.setdefault(\"raw\", annotation)\n        kwargs.setdefault(\"safe_generic_origin\", get_safe_generic_origin(origin, unwrapped))\n        kwargs.setdefault(\"type_wrappers\", wrappers)\n\n        instance = FieldDefinition(**kwargs)\n        if not instance.has_default and instance.kwarg_definition:\n            return replace(instance, default=instance.kwarg_definition.default)\n\n        return instance\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def match_predicate_recursively(self, predicate: Callable[[FieldDefinition], bool]) -> bool:\n        \"\"\"Recursively test the passed in predicate against the field and any of its inner fields.\n\n        Args:\n            predicate: A callable that receives a field definition instance as an arg and returns a boolean.\n\n        Returns:\n            A boolean.\n        \"\"\"\n        return predicate(self) or any(t.match_predicate_recursively(predicate) for t in self.inner_types)\n\n```\n# Function Signature: def from_kwarg(\n# Natural Language Annotations: \"\"\"Create a new FieldDefinition instance.\n\n        Args:\n            annotation: The type of the kwarg.\n            name: Field name.\n            default: A default value.\n            inner_types: A tuple of FieldDefinition instances representing the inner types, if any.\n            kwarg_definition: Kwarg Parameter.\n            extra: A mapping of extra values.\n\n        Returns:\n            FieldDefinition instance.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfrom litestar.typing import FieldDefinition\n\ndef", "  ```\ndef from_kwarg(\n    annotation: Any,\n", "  ```\n@dataclass(frozen=True)\nclass Field"]}, "litestar-litestar/typing.py-from_parameter": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/typing.py:\n```\nfrom __future__ import annotations\n\nimport warnings\nfrom collections import abc, deque\nfrom copy import deepcopy\nfrom dataclasses import dataclass, is_dataclass, replace\nfrom inspect import Parameter, Signature\nfrom typing import Any, AnyStr, Callable, Collection, ForwardRef, Literal, Mapping, Protocol, Sequence, TypeVar, cast\n\nfrom msgspec import UnsetType\nfrom typing_extensions import NewType, NotRequired, Required, Self, get_args, get_origin, get_type_hints, is_typeddict\n\nfrom litestar.exceptions import ImproperlyConfiguredException, LitestarWarning\nfrom litestar.openapi.spec import Example\nfrom litestar.params import BodyKwarg, DependencyKwarg, KwargDefinition, ParameterKwarg\nfrom litestar.types import Empty\nfrom litestar.types.builtin_types import NoneType, UnionTypes\nfrom litestar.utils.predicates import (\n    is_annotated_type,\n    is_any,\n    is_class_and_subclass,\n    is_generic,\n    is_non_string_iterable,\n    is_non_string_sequence,\n    is_union,\n)\nfrom litestar.utils.typing import (\n    get_instantiable_origin,\n    get_safe_generic_origin,\n    get_type_hints_with_generics_resolved,\n    make_non_optional_union,\n    unwrap_annotation,\n)\n\n__all__ = (\"FieldDefinition\",)\n\nT = TypeVar(\"T\", bound=KwargDefinition)\n\n\nclass _KwargMetaExtractor(Protocol):\n    @staticmethod\n    def matches(annotation: Any, name: str | None, default: Any) -> bool: ...\n\n    @staticmethod\n    def extract(annotation: Any, default: Any) -> Any: ...\n\n\n_KWARG_META_EXTRACTORS: set[_KwargMetaExtractor] = set()\n\n\ndef _unpack_predicate(value: Any) -> dict[str, Any]:\n    try:\n        from annotated_types import Predicate\n\n        if isinstance(value, Predicate):\n            if value.func == str.islower:\n                return {\"lower_case\": True}\n            if value.func == str.isupper:\n                return {\"upper_case\": True}\n            if value.func == str.isascii:\n                return {\"pattern\": \"[[:ascii:]]\"}\n            if value.func == str.isdigit:\n                return {\"pattern\": \"[[:digit:]]\"}\n    except ImportError:\n        pass\n\n    return {}\n\n\ndef _parse_metadata(value: Any, is_sequence_container: bool, extra: dict[str, Any] | None) -> dict[str, Any]:\n    \"\"\"Parse metadata from a value.\n\n    Args:\n        value: A metadata value from annotation, namely anything stored under Annotated[x, metadata...]\n        is_sequence_container: Whether the type is a sequence container (list, tuple etc...)\n        extra: Extra key values to parse.\n\n    Returns:\n        A dictionary of constraints, which fulfill the kwargs of a KwargDefinition class.\n    \"\"\"\n    extra = {\n        **cast(\"dict[str, Any]\", extra or getattr(value, \"extra\", None) or {}),\n        **(getattr(value, \"json_schema_extra\", None) or {}),\n    }\n    example_list: list[Any] | None\n    if example := extra.pop(\"example\", None):\n        example_list = [Example(value=example)]\n    elif examples := (extra.pop(\"examples\", None) or getattr(value, \"examples\", None)):\n        example_list = [Example(value=example) for example in cast(\"list[str]\", examples)]\n    else:\n        example_list = None\n\n    return {\n        k: v\n        for k, v in {\n            \"gt\": getattr(value, \"gt\", None),\n            \"ge\": getattr(value, \"ge\", None),\n            \"lt\": getattr(value, \"lt\", None),\n            \"le\": getattr(value, \"le\", None),\n            \"multiple_of\": getattr(value, \"multiple_of\", None),\n            \"min_length\": None if is_sequence_container else getattr(value, \"min_length\", None),\n            \"max_length\": None if is_sequence_container else getattr(value, \"max_length\", None),\n            \"description\": getattr(value, \"description\", None),\n            \"examples\": example_list,\n            \"title\": getattr(value, \"title\", None),\n            \"lower_case\": getattr(value, \"to_lower\", None),\n            \"upper_case\": getattr(value, \"to_upper\", None),\n            \"pattern\": getattr(value, \"regex\", getattr(value, \"pattern\", None)),\n            \"min_items\": getattr(value, \"min_items\", getattr(value, \"min_length\", None))\n            if is_sequence_container\n            else None,\n            \"max_items\": getattr(value, \"max_items\", getattr(value, \"max_length\", None))\n            if is_sequence_container\n            else None,\n            \"const\": getattr(value, \"const\", None) is not None,\n            **extra,\n        }.items()\n        if v is not None\n    }\n\n\ndef _traverse_metadata(\n    metadata: Sequence[Any], is_sequence_container: bool, extra: dict[str, Any] | None\n) -> dict[str, Any]:\n    \"\"\"Recursively traverse metadata from a value.\n\n    Args:\n        metadata: A list of metadata values from annotation, namely anything stored under Annotated[x, metadata...]\n        is_sequence_container: Whether the container is a sequence container (list, tuple etc...)\n        extra: Extra key values to parse.\n\n    Returns:\n        A dictionary of constraints, which fulfill the kwargs of a KwargDefinition class.\n    \"\"\"\n    constraints: dict[str, Any] = {}\n    for value in metadata:\n        if isinstance(value, (list, set, frozenset, deque)):\n            constraints.update(\n                _traverse_metadata(\n                    metadata=cast(\"Sequence[Any]\", value), is_sequence_container=is_sequence_container, extra=extra\n                )\n            )\n        elif is_annotated_type(value) and (type_args := [v for v in get_args(value) if v is not None]):\n            # annotated values can be nested inside other annotated values\n            # this behaviour is buggy in python 3.8, hence we need to guard here.\n            if len(type_args) > 1:\n                constraints.update(\n                    _traverse_metadata(metadata=type_args[1:], is_sequence_container=is_sequence_container, extra=extra)\n                )\n        elif unpacked_predicate := _unpack_predicate(value):\n            constraints.update(unpacked_predicate)\n        else:\n            constraints.update(_parse_metadata(value=value, is_sequence_container=is_sequence_container, extra=extra))\n    return constraints\n\n\ndef _create_metadata_from_type(\n    metadata: Sequence[Any], model: type[T], annotation: Any, extra: dict[str, Any] | None\n) -> tuple[T | None, dict[str, Any]]:\n    is_sequence_container = is_non_string_sequence(annotation)\n    result = _traverse_metadata(metadata=metadata, is_sequence_container=is_sequence_container, extra=extra)\n\n    constraints = {k: v for k, v in result.items() if k in dir(model)}\n    extra = {k: v for k, v in result.items() if k not in constraints}\n    return model(**constraints) if constraints else None, extra\n\n\n@dataclass(frozen=True)\nclass FieldDefinition:\n    \"\"\"Represents a function parameter or type annotation.\"\"\"\n\n    __slots__ = (\n        \"annotation\",\n        \"args\",\n        \"default\",\n        \"extra\",\n        \"inner_types\",\n        \"instantiable_origin\",\n        \"kwarg_definition\",\n        \"metadata\",\n        \"name\",\n        \"origin\",\n        \"raw\",\n        \"safe_generic_origin\",\n        \"type_wrappers\",\n    )\n\n    raw: Any\n    \"\"\"The annotation exactly as received.\"\"\"\n    annotation: Any\n    \"\"\"The annotation with any \"wrapper\" types removed, e.g. Annotated.\"\"\"\n    type_wrappers: tuple[type, ...]\n    \"\"\"A set of all \"wrapper\" types, e.g. Annotated.\"\"\"\n    origin: Any\n    \"\"\"The result of calling ``get_origin(annotation)`` after unwrapping Annotated, e.g. list.\"\"\"\n    args: tuple[Any, ...]\n    \"\"\"The result of calling ``get_args(annotation)`` after unwrapping Annotated, e.g. (int,).\"\"\"\n    metadata: tuple[Any, ...]\n    \"\"\"Any metadata associated with the annotation via ``Annotated``.\"\"\"\n    instantiable_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely instantiated. E.g., ``Sequence`` -> ``list``.\"\"\"\n    safe_generic_origin: Any\n    \"\"\"An equivalent type to ``origin`` that can be safely used as a generic type across all supported Python versions.\n\n    This is to serve safely rebuilding a generic outer type with different args at runtime.\n    \"\"\"\n    inner_types: tuple[FieldDefinition, ...]\n    \"\"\"The type's generic args parsed as ``FieldDefinition``, if applicable.\"\"\"\n    default: Any\n    \"\"\"Default value of the field.\"\"\"\n    extra: dict[str, Any]\n    \"\"\"A mapping of extra values.\"\"\"\n    kwarg_definition: KwargDefinition | DependencyKwarg | None\n    \"\"\"Kwarg Parameter.\"\"\"\n    name: str\n    \"\"\"Field name.\"\"\"\n\n    def __deepcopy__(self, memo: dict[str, Any]) -> Self:\n        return type(self)(**{attr: deepcopy(getattr(self, attr)) for attr in self.__slots__})\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, FieldDefinition):\n            return False\n\n        if self.origin:\n            return self.origin == other.origin and self.inner_types == other.inner_types\n\n        return self.annotation == other.annotation  # type: ignore[no-any-return]\n\n    def __hash__(self) -> int:\n        return hash((self.name, self.raw, self.annotation, self.origin, self.inner_types))\n\n    @classmethod\n    def _extract_metadata(\n        cls, annotation: Any, name: str | None, default: Any, metadata: tuple[Any, ...], extra: dict[str, Any] | None\n    ) -> tuple[KwargDefinition | None, dict[str, Any]]:\n        model = BodyKwarg if name == \"data\" else ParameterKwarg\n\n        for extractor in _KWARG_META_EXTRACTORS:\n            if extractor.matches(annotation=annotation, name=name, default=default):\n                return _create_metadata_from_type(\n                    extractor.extract(annotation=annotation, default=default),\n                    model=model,\n                    annotation=annotation,\n                    extra=extra,\n                )\n\n        if any(isinstance(arg, KwargDefinition) for arg in get_args(annotation)):\n            return next(arg for arg in get_args(annotation) if isinstance(arg, KwargDefinition)), extra or {}\n\n        if metadata:\n            return _create_metadata_from_type(metadata=metadata, model=model, annotation=annotation, extra=extra)\n\n        return None, {}\n\n    @property\n    def has_default(self) -> bool:\n        \"\"\"Check if the field has a default value.\n\n        Returns:\n            True if the default is not Empty or Ellipsis otherwise False.\n        \"\"\"\n        return self.default is not Empty and self.default is not Ellipsis\n\n    @property\n    def is_non_string_iterable(self) -> bool:\n        \"\"\"Check if the field type is an Iterable.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_iterable(annotation)\n\n    @property\n    def is_non_string_sequence(self) -> bool:\n        \"\"\"Check if the field type is a non-string Sequence.\n\n        If ``self.annotation`` is an optional union, only the non-optional members of the union are evaluated.\n\n        See: https://github.com/litestar-org/litestar/issues/1106\n        \"\"\"\n        annotation = self.annotation\n        if self.is_optional:\n            annotation = make_non_optional_union(annotation)\n        return is_non_string_sequence(annotation)\n\n    @property\n    def is_any(self) -> bool:\n        \"\"\"Check if the field type is Any.\"\"\"\n        return is_any(self.annotation)\n\n    @property\n    def is_generic(self) -> bool:\n        \"\"\"Check if the field type is a custom class extending Generic.\"\"\"\n        return is_generic(self.annotation)\n\n    @property\n    def is_simple_type(self) -> bool:\n        \"\"\"Check if the field type is a singleton value (e.g. int, str etc.).\"\"\"\n        return not (\n            self.is_generic\n            or self.is_optional\n            or self.is_union\n            or self.is_mapping\n            or self.is_non_string_iterable\n            or self.is_new_type\n        )\n\n    @property\n    def is_parameter_field(self) -> bool:\n        \"\"\"Check if the field type is a parameter kwarg value.\"\"\"\n        return isinstance(self.kwarg_definition, ParameterKwarg)\n\n    @property\n    def is_const(self) -> bool:\n        \"\"\"Check if the field is defined as constant value.\"\"\"\n        return bool(self.kwarg_definition and getattr(self.kwarg_definition, \"const\", False))\n\n    @property\n    def is_required(self) -> bool:\n        \"\"\"Check if the field should be marked as a required parameter.\"\"\"\n        if Required in self.type_wrappers:  # type: ignore[comparison-overlap]\n            return True\n\n        if NotRequired in self.type_wrappers or UnsetType in self.args:  # type: ignore[comparison-overlap]\n            return False\n\n        if isinstance(self.kwarg_definition, ParameterKwarg) and self.kwarg_definition.required is not None:\n            return self.kwarg_definition.required\n\n        return not self.is_optional and not self.is_any and (not self.has_default or self.default is None)\n\n    @property\n    def is_annotated(self) -> bool:\n        \"\"\"Check if the field type is Annotated.\"\"\"\n        return bool(self.metadata)\n\n    @property\n    def is_literal(self) -> bool:\n        \"\"\"Check if the field type is Literal.\"\"\"\n        return self.origin is Literal\n\n    @property\n    def is_forward_ref(self) -> bool:\n        \"\"\"Whether the annotation is a forward reference or not.\"\"\"\n        return isinstance(self.annotation, (str, ForwardRef))\n\n    @property\n    def is_mapping(self) -> bool:\n        \"\"\"Whether the annotation is a mapping or not.\"\"\"\n        return self.is_subclass_of(Mapping)\n\n    @property\n    def is_tuple(self) -> bool:\n        \"\"\"Whether the annotation is a ``tuple`` or not.\"\"\"\n        return self.is_subclass_of(tuple)\n\n    @property\n    def is_new_type(self) -> bool:\n        return isinstance(self.annotation, NewType)\n\n    @property\n    def is_type_var(self) -> bool:\n        \"\"\"Whether the annotation is a TypeVar or not.\"\"\"\n        return isinstance(self.annotation, TypeVar)\n\n    @property\n    def is_union(self) -> bool:\n        \"\"\"Whether the annotation is a union type or not.\"\"\"\n        return self.origin in UnionTypes\n\n    @property\n    def is_optional(self) -> bool:\n        \"\"\"Whether the annotation is Optional or not.\"\"\"\n        return bool(self.is_union and NoneType in self.args)\n\n    @property\n    def is_none_type(self) -> bool:\n        \"\"\"Whether the annotation is NoneType or not.\"\"\"\n        return self.annotation is NoneType\n\n    @property\n    def is_collection(self) -> bool:\n        \"\"\"Whether the annotation is a collection type or not.\"\"\"\n        return self.is_subclass_of(Collection)\n\n    @property\n    def is_non_string_collection(self) -> bool:\n        \"\"\"Whether the annotation is a non-string collection type or not.\"\"\"\n        return self.is_collection and not self.is_subclass_of((str, bytes))\n\n    @property\n    def bound_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of bound types - if the annotation is a TypeVar with bound types, otherwise None.\"\"\"\n        if self.is_type_var and (bound := getattr(self.annotation, \"__bound__\", None)):\n            if is_union(bound):\n                return tuple(FieldDefinition.from_annotation(t) for t in get_args(bound))\n            return (FieldDefinition.from_annotation(bound),)\n        return None\n\n    @property\n    def generic_types(self) -> tuple[FieldDefinition, ...] | None:\n        \"\"\"A tuple of generic types passed into the annotation - if its generic.\"\"\"\n        if not (bases := getattr(self.annotation, \"__orig_bases__\", None)):\n            return None\n        args: list[FieldDefinition] = []\n        for base_args in [getattr(base, \"__args__\", ()) for base in bases]:\n            for arg in base_args:\n                field_definition = FieldDefinition.from_annotation(arg)\n                if field_definition.generic_types:\n                    args.extend(field_definition.generic_types)\n                else:\n                    args.append(field_definition)\n        return tuple(args)\n\n    @property\n    def is_dataclass_type(self) -> bool:\n        \"\"\"Whether the annotation is a dataclass type or not.\"\"\"\n\n        return is_dataclass(cast(\"type\", self.origin or self.annotation))\n\n    @property\n    def is_typeddict_type(self) -> bool:\n        \"\"\"Whether the type is TypedDict or not.\"\"\"\n\n        return is_typeddict(self.origin or self.annotation)\n\n    @property\n    def type_(self) -> Any:\n        \"\"\"The type of the annotation with all the wrappers removed, including the generic types.\"\"\"\n\n        return self.origin or self.annotation\n\n    def is_subclass_of(self, cl: type[Any] | tuple[type[Any], ...]) -> bool:\n        \"\"\"Whether the annotation is a subclass of the given type.\n\n        Where ``self.annotation`` is a union type, this method will return ``True`` when all members of the union are\n        a subtype of ``cl``, otherwise, ``False``.\n\n        Args:\n            cl: The type to check, or tuple of types. Passed as 2nd argument to ``issubclass()``.\n\n        Returns:\n            Whether the annotation is a subtype of the given type(s).\n        \"\"\"\n        if self.origin:\n            if self.origin in UnionTypes:\n                return all(t.is_subclass_of(cl) for t in self.inner_types)\n\n            return self.origin not in UnionTypes and is_class_and_subclass(self.origin, cl)\n\n        if self.annotation is AnyStr:\n            return is_class_and_subclass(str, cl) or is_class_and_subclass(bytes, cl)\n\n        return self.annotation is not Any and not self.is_type_var and is_class_and_subclass(self.annotation, cl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def from_annotation(cls, annotation: Any, **kwargs: Any) -> FieldDefinition:\n        \"\"\"Initialize FieldDefinition.\n\n        Args:\n            annotation: The type annotation. This should be extracted from the return of\n                ``get_type_hints(..., include_extras=True)`` so that forward references are resolved and recursive\n                ``Annotated`` types are flattened.\n            **kwargs: Additional keyword arguments to pass to the ``FieldDefinition`` constructor.\n\n        Returns:\n            FieldDefinition\n        \"\"\"\n\n        unwrapped, metadata, wrappers = unwrap_annotation(annotation if annotation is not Empty else Any)\n        origin = get_origin(unwrapped)\n\n        args = () if origin is abc.Callable else get_args(unwrapped)\n\n        if not kwargs.get(\"kwarg_definition\"):\n            if isinstance(kwargs.get(\"default\"), (KwargDefinition, DependencyKwarg)):\n                kwargs[\"kwarg_definition\"] = kwargs.pop(\"default\")\n            elif kwarg_definition := next(\n                (v for v in metadata if isinstance(v, (KwargDefinition, DependencyKwarg))), None\n            ):\n                kwargs[\"kwarg_definition\"] = kwarg_definition\n\n                if kwarg_definition.default is not Empty:\n                    warnings.warn(\n                        f\"Deprecated default value specification for annotation '{annotation}'. Setting defaults \"\n                        f\"inside 'typing.Annotated' is discouraged and support for this will be removed in a future \"\n                        f\"version. Defaults should be set with regular parameter default values. Use \"\n                        \"'param: Annotated[<type>, Parameter(...)] = <default>' instead of \"\n                        \"'param: Annotated[<type>, Parameter(..., default=<default>)].\",\n                        category=DeprecationWarning,\n                        stacklevel=2,\n                    )\n                    if kwargs.get(\"default\", Empty) is not Empty and kwarg_definition.default != kwargs[\"default\"]:\n                        warnings.warn(\n                            f\"Ambiguous default values for annotation '{annotation}'. The default value \"\n                            f\"'{kwarg_definition.default!r}' set inside the parameter annotation differs from the \"\n                            f\"parameter default value '{kwargs['default']!r}'\",\n                            category=LitestarWarning,\n                            stacklevel=2,\n                        )\n\n                metadata = tuple(v for v in metadata if not isinstance(v, (KwargDefinition, DependencyKwarg)))\n            elif (extra := kwargs.get(\"extra\", {})) and \"kwarg_definition\" in extra:\n                kwargs[\"kwarg_definition\"] = extra.pop(\"kwarg_definition\")\n            else:\n                kwargs[\"kwarg_definition\"], kwargs[\"extra\"] = cls._extract_metadata(\n                    annotation=annotation,\n                    name=kwargs.get(\"name\", \"\"),\n                    default=kwargs.get(\"default\", Empty),\n                    metadata=metadata,\n                    extra=kwargs.get(\"extra\"),\n                )\n\n        kwargs.setdefault(\"annotation\", unwrapped)\n        kwargs.setdefault(\"args\", args)\n        kwargs.setdefault(\"default\", Empty)\n        kwargs.setdefault(\"extra\", {})\n        kwargs.setdefault(\"inner_types\", tuple(FieldDefinition.from_annotation(arg) for arg in args))\n        kwargs.setdefault(\"instantiable_origin\", get_instantiable_origin(origin, unwrapped))\n        kwargs.setdefault(\"kwarg_definition\", None)\n        kwargs.setdefault(\"metadata\", metadata)\n        kwargs.setdefault(\"name\", \"\")\n        kwargs.setdefault(\"origin\", origin)\n        kwargs.setdefault(\"raw\", annotation)\n        kwargs.setdefault(\"safe_generic_origin\", get_safe_generic_origin(origin, unwrapped))\n        kwargs.setdefault(\"type_wrappers\", wrappers)\n\n        instance = FieldDefinition(**kwargs)\n        if not instance.has_default and instance.kwarg_definition:\n            return replace(instance, default=instance.kwarg_definition.default)\n\n        return instance\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def match_predicate_recursively(self, predicate: Callable[[FieldDefinition], bool]) -> bool:\n        \"\"\"Recursively test the passed in predicate against the field and any of its inner fields.\n\n        Args:\n            predicate: A callable that receives a field definition instance as an arg and returns a boolean.\n\n        Returns:\n            A boolean.\n        \"\"\"\n        return predicate(self) or any(t.match_predicate_recursively(predicate) for t in self.inner_types)\n\n```\n# Function Signature: def from_parameter(cls, parameter: Parameter, fn_type_hints: dict[str, Any]) -> FieldDefinition:\n# Natural Language Annotations: \"\"\"Initialize ParsedSignatureParameter.\n\n        Args:\n            parameter: inspect.Parameter\n            fn_type_hints: mapping of names to types. Should be result of ``get_type_hints()``, preferably via the\n                :attr:``get_fn_type_hints() <.utils.signature_parsing.get_fn_type_hints>`` helper.\n\n        Returns:\n            ParsedSignatureParameter.\n\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef from_parameter(cls, parameter: Parameter, fn_", "  ```\n@classmethod\ndef from_parameter(cls, parameter:", "  ```\nfrom __future__ import annotations\n\nfrom typing import Any"]}, "litestar-litestar/_openapi/responses.py-create_success_response": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/_openapi/responses.py:\n```\nfrom __future__ import annotations\n\nimport contextlib\nimport re\nfrom copy import copy\nfrom dataclasses import asdict\nfrom http import HTTPStatus\nfrom operator import attrgetter\nfrom typing import TYPE_CHECKING, Any, Iterator\n\nfrom litestar._openapi.schema_generation import SchemaCreator\nfrom litestar._openapi.schema_generation.utils import get_formatted_examples\nfrom litestar.enums import MediaType\nfrom litestar.exceptions import HTTPException, ValidationException\nfrom litestar.openapi.spec import Example, OpenAPIResponse, Reference\nfrom litestar.openapi.spec.enums import OpenAPIFormat, OpenAPIType\nfrom litestar.openapi.spec.header import OpenAPIHeader\nfrom litestar.openapi.spec.media_type import OpenAPIMediaType\nfrom litestar.openapi.spec.schema import Schema\nfrom litestar.response import (\n    File,\n    Redirect,\n    Stream,\n    Template,\n)\nfrom litestar.response import (\n    Response as LitestarResponse,\n)\nfrom litestar.response.base import ASGIResponse\nfrom litestar.types.builtin_types import NoneType\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils import get_enum_string_value, get_name\n\nif TYPE_CHECKING:\n    from litestar._openapi.datastructures import OpenAPIContext\n    from litestar.datastructures.cookie import Cookie\n    from litestar.handlers.http_handlers import HTTPRouteHandler\n    from litestar.openapi.spec.responses import Responses\n\n\n__all__ = (\"create_responses_for_handler\",)\n\nCAPITAL_LETTERS_PATTERN = re.compile(r\"(?=[A-Z])\")\n\n\ndef pascal_case_to_text(string: str) -> str:\n    \"\"\"Given a 'PascalCased' string, return its split form- 'Pascal Cased'.\"\"\"\n    return \" \".join(re.split(CAPITAL_LETTERS_PATTERN, string)).strip()\n\n\ndef create_cookie_schema(cookie: Cookie) -> Schema:\n    \"\"\"Given a Cookie instance, return its corresponding OpenAPI schema.\n\n    Args:\n        cookie: Cookie\n\n    Returns:\n        Schema\n    \"\"\"\n    cookie_copy = copy(cookie)\n    cookie_copy.value = \"<string>\"\n    value = cookie_copy.to_header(header=\"\")\n    return Schema(description=cookie.description or \"\", example=value)\n\n\nclass ResponseFactory:\n    \"\"\"Factory for creating a Response instance for a given route handler.\"\"\"\n\n    def __init__(self, context: OpenAPIContext, route_handler: HTTPRouteHandler) -> None:\n        \"\"\"Initialize the factory.\n\n        Args:\n            context: An OpenAPIContext instance.\n            route_handler: An HTTPRouteHandler instance.\n        \"\"\"\n        self.context = context\n        self.route_handler = route_handler\n        self.field_definition = route_handler.parsed_fn_signature.return_type\n        self.schema_creator = SchemaCreator.from_openapi_context(context, prefer_alias=False)\n\n    def create_responses(self, raises_validation_error: bool) -> Responses | None:\n        \"\"\"Create the schema for responses, if any.\n\n        Args:\n            raises_validation_error: Boolean flag indicating whether the handler raises a ValidationException.\n\n        Returns:\n            Responses\n        \"\"\"\n        responses: Responses = {\n            str(self.route_handler.status_code): self.create_success_response(),\n        }\n\n        exceptions = list(self.route_handler.raises or [])\n        if raises_validation_error and ValidationException not in exceptions:\n            exceptions.append(ValidationException)\n\n        for status_code, response in create_error_responses(exceptions=exceptions):\n            responses[status_code] = response\n\n        for status_code, response in self.create_additional_responses():\n            responses[status_code] = response\n\n        return responses or None\n\n    def create_description(self) -> str:\n        \"\"\"Create the description for a success response.\"\"\"\n        default_descriptions: dict[Any, str] = {\n            Stream: \"Stream Response\",\n            Redirect: \"Redirect Response\",\n            File: \"File Download\",\n        }\n        return (\n            self.route_handler.response_description\n            or default_descriptions.get(self.field_definition.annotation)\n            or HTTPStatus(self.route_handler.status_code).description\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def create_redirect_response(self) -> OpenAPIResponse:\n        \"\"\"Create the schema for a redirect response.\"\"\"\n        return OpenAPIResponse(\n            content=None,\n            description=self.create_description(),\n            headers={\n                \"location\": OpenAPIHeader(\n                    schema=Schema(type=OpenAPIType.STRING), description=\"target path for the redirect\"\n                )\n            },\n        )\n\n    def create_file_response(self) -> OpenAPIResponse:\n        \"\"\"Create the schema for a file/stream response.\"\"\"\n        return OpenAPIResponse(\n            content={\n                self.route_handler.media_type: OpenAPIMediaType(\n                    schema=Schema(\n                        type=OpenAPIType.STRING,\n                        content_encoding=self.route_handler.content_encoding,\n                        content_media_type=self.route_handler.content_media_type or \"application/octet-stream\",\n                    ),\n                )\n            },\n            description=self.create_description(),\n            headers={\n                \"content-length\": OpenAPIHeader(\n                    schema=Schema(type=OpenAPIType.STRING), description=\"File size in bytes\"\n                ),\n                \"last-modified\": OpenAPIHeader(\n                    schema=Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.DATE_TIME),\n                    description=\"Last modified data-time in RFC 2822 format\",\n                ),\n                \"etag\": OpenAPIHeader(schema=Schema(type=OpenAPIType.STRING), description=\"Entity tag\"),\n            },\n        )\n\n    def set_success_response_headers(self, response: OpenAPIResponse) -> None:\n        \"\"\"Set the schema for success response headers, if any.\"\"\"\n\n        if response.headers is None:\n            response.headers = {}\n\n        if not self.schema_creator.generate_examples:\n            schema_creator = self.schema_creator\n        else:\n            schema_creator = SchemaCreator.from_openapi_context(self.context, generate_examples=False)\n\n        for response_header in self.route_handler.resolve_response_headers():\n            header = OpenAPIHeader()\n            for attribute_name, attribute_value in (\n                (k, v) for k, v in asdict(response_header).items() if v is not None\n            ):\n                if attribute_name == \"value\":\n                    header.schema = schema_creator.for_field_definition(\n                        FieldDefinition.from_annotation(type(attribute_value))\n                    )\n                elif attribute_name != \"documentation_only\":\n                    setattr(header, attribute_name, attribute_value)\n\n            response.headers[response_header.name] = header\n\n        if cookies := self.route_handler.resolve_response_cookies():\n            response.headers[\"Set-Cookie\"] = OpenAPIHeader(\n                schema=Schema(\n                    all_of=[create_cookie_schema(cookie=cookie) for cookie in sorted(cookies, key=attrgetter(\"key\"))]\n                )\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef create_error_responses(exceptions: list[type[HTTPException]]) -> Iterator[tuple[str, OpenAPIResponse]]:\n    \"\"\"Create the schema for error responses, if any.\"\"\"\n    grouped_exceptions: dict[int, list[type[HTTPException]]] = {}\n    for exc in exceptions:\n        if not grouped_exceptions.get(exc.status_code):\n            grouped_exceptions[exc.status_code] = []\n        grouped_exceptions[exc.status_code].append(exc)\n    for status_code, exception_group in grouped_exceptions.items():\n        exceptions_schemas = []\n        group_description: str = \"\"\n        for exc in exception_group:\n            example_detail = \"\"\n            if hasattr(exc, \"detail\") and exc.detail:\n                group_description = exc.detail\n                example_detail = exc.detail\n\n            if not example_detail:\n                with contextlib.suppress(Exception):\n                    example_detail = HTTPStatus(status_code).phrase\n\n            exceptions_schemas.append(\n                Schema(\n                    type=OpenAPIType.OBJECT,\n                    required=[\"detail\", \"status_code\"],\n                    properties={\n                        \"status_code\": Schema(type=OpenAPIType.INTEGER),\n                        \"detail\": Schema(type=OpenAPIType.STRING),\n                        \"extra\": Schema(\n                            type=[OpenAPIType.NULL, OpenAPIType.OBJECT, OpenAPIType.ARRAY],\n                            additional_properties=Schema(),\n                        ),\n                    },\n                    description=pascal_case_to_text(get_name(exc)),\n                    examples=[{\"status_code\": status_code, \"detail\": example_detail, \"extra\": {}}],\n                )\n            )\n        if len(exceptions_schemas) > 1:  # noqa: SIM108\n            schema = Schema(one_of=exceptions_schemas)\n        else:\n            schema = exceptions_schemas[0]\n\n        if not group_description:\n            with contextlib.suppress(Exception):\n                group_description = HTTPStatus(status_code).description\n\n        yield (\n            str(status_code),\n            OpenAPIResponse(\n                description=group_description,\n                content={MediaType.JSON: OpenAPIMediaType(schema=schema)},\n            ),\n        )\n\n\ndef create_responses_for_handler(\n    context: OpenAPIContext, route_handler: HTTPRouteHandler, raises_validation_error: bool\n) -> Responses | None:\n    \"\"\"Create the schema for responses, if any.\n\n    Args:\n        context: An OpenAPIContext instance.\n        route_handler: An HTTPRouteHandler instance.\n        raises_validation_error: Boolean flag indicating whether the handler raises a ValidationException.\n\n    Returns:\n        Responses\n    \"\"\"\n    return ResponseFactory(context, route_handler).create_responses(raises_validation_error=raises_validation_error)\n\n```\n# Function Signature: def create_success_response(self) -> OpenAPIResponse:\n# Natural Language Annotations: \"\"\"Create the schema for a success response.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef create_success_response(self) -> OpenAPIResponse", "  ```\ndef create_success_response(self) -> OpenAPIResponse", "  ```\ndef create_success_response(self) -> OpenAPIResponse"]}, "litestar-litestar/_openapi/responses.py-create_additional_responses": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/_openapi/responses.py:\n```\nfrom __future__ import annotations\n\nimport contextlib\nimport re\nfrom copy import copy\nfrom dataclasses import asdict\nfrom http import HTTPStatus\nfrom operator import attrgetter\nfrom typing import TYPE_CHECKING, Any, Iterator\n\nfrom litestar._openapi.schema_generation import SchemaCreator\nfrom litestar._openapi.schema_generation.utils import get_formatted_examples\nfrom litestar.enums import MediaType\nfrom litestar.exceptions import HTTPException, ValidationException\nfrom litestar.openapi.spec import Example, OpenAPIResponse, Reference\nfrom litestar.openapi.spec.enums import OpenAPIFormat, OpenAPIType\nfrom litestar.openapi.spec.header import OpenAPIHeader\nfrom litestar.openapi.spec.media_type import OpenAPIMediaType\nfrom litestar.openapi.spec.schema import Schema\nfrom litestar.response import (\n    File,\n    Redirect,\n    Stream,\n    Template,\n)\nfrom litestar.response import (\n    Response as LitestarResponse,\n)\nfrom litestar.response.base import ASGIResponse\nfrom litestar.types.builtin_types import NoneType\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils import get_enum_string_value, get_name\n\nif TYPE_CHECKING:\n    from litestar._openapi.datastructures import OpenAPIContext\n    from litestar.datastructures.cookie import Cookie\n    from litestar.handlers.http_handlers import HTTPRouteHandler\n    from litestar.openapi.spec.responses import Responses\n\n\n__all__ = (\"create_responses_for_handler\",)\n\nCAPITAL_LETTERS_PATTERN = re.compile(r\"(?=[A-Z])\")\n\n\ndef pascal_case_to_text(string: str) -> str:\n    \"\"\"Given a 'PascalCased' string, return its split form- 'Pascal Cased'.\"\"\"\n    return \" \".join(re.split(CAPITAL_LETTERS_PATTERN, string)).strip()\n\n\ndef create_cookie_schema(cookie: Cookie) -> Schema:\n    \"\"\"Given a Cookie instance, return its corresponding OpenAPI schema.\n\n    Args:\n        cookie: Cookie\n\n    Returns:\n        Schema\n    \"\"\"\n    cookie_copy = copy(cookie)\n    cookie_copy.value = \"<string>\"\n    value = cookie_copy.to_header(header=\"\")\n    return Schema(description=cookie.description or \"\", example=value)\n\n\nclass ResponseFactory:\n    \"\"\"Factory for creating a Response instance for a given route handler.\"\"\"\n\n    def __init__(self, context: OpenAPIContext, route_handler: HTTPRouteHandler) -> None:\n        \"\"\"Initialize the factory.\n\n        Args:\n            context: An OpenAPIContext instance.\n            route_handler: An HTTPRouteHandler instance.\n        \"\"\"\n        self.context = context\n        self.route_handler = route_handler\n        self.field_definition = route_handler.parsed_fn_signature.return_type\n        self.schema_creator = SchemaCreator.from_openapi_context(context, prefer_alias=False)\n\n    def create_responses(self, raises_validation_error: bool) -> Responses | None:\n        \"\"\"Create the schema for responses, if any.\n\n        Args:\n            raises_validation_error: Boolean flag indicating whether the handler raises a ValidationException.\n\n        Returns:\n            Responses\n        \"\"\"\n        responses: Responses = {\n            str(self.route_handler.status_code): self.create_success_response(),\n        }\n\n        exceptions = list(self.route_handler.raises or [])\n        if raises_validation_error and ValidationException not in exceptions:\n            exceptions.append(ValidationException)\n\n        for status_code, response in create_error_responses(exceptions=exceptions):\n            responses[status_code] = response\n\n        for status_code, response in self.create_additional_responses():\n            responses[status_code] = response\n\n        return responses or None\n\n    def create_description(self) -> str:\n        \"\"\"Create the description for a success response.\"\"\"\n        default_descriptions: dict[Any, str] = {\n            Stream: \"Stream Response\",\n            Redirect: \"Redirect Response\",\n            File: \"File Download\",\n        }\n        return (\n            self.route_handler.response_description\n            or default_descriptions.get(self.field_definition.annotation)\n            or HTTPStatus(self.route_handler.status_code).description\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def create_redirect_response(self) -> OpenAPIResponse:\n        \"\"\"Create the schema for a redirect response.\"\"\"\n        return OpenAPIResponse(\n            content=None,\n            description=self.create_description(),\n            headers={\n                \"location\": OpenAPIHeader(\n                    schema=Schema(type=OpenAPIType.STRING), description=\"target path for the redirect\"\n                )\n            },\n        )\n\n    def create_file_response(self) -> OpenAPIResponse:\n        \"\"\"Create the schema for a file/stream response.\"\"\"\n        return OpenAPIResponse(\n            content={\n                self.route_handler.media_type: OpenAPIMediaType(\n                    schema=Schema(\n                        type=OpenAPIType.STRING,\n                        content_encoding=self.route_handler.content_encoding,\n                        content_media_type=self.route_handler.content_media_type or \"application/octet-stream\",\n                    ),\n                )\n            },\n            description=self.create_description(),\n            headers={\n                \"content-length\": OpenAPIHeader(\n                    schema=Schema(type=OpenAPIType.STRING), description=\"File size in bytes\"\n                ),\n                \"last-modified\": OpenAPIHeader(\n                    schema=Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.DATE_TIME),\n                    description=\"Last modified data-time in RFC 2822 format\",\n                ),\n                \"etag\": OpenAPIHeader(schema=Schema(type=OpenAPIType.STRING), description=\"Entity tag\"),\n            },\n        )\n\n    def set_success_response_headers(self, response: OpenAPIResponse) -> None:\n        \"\"\"Set the schema for success response headers, if any.\"\"\"\n\n        if response.headers is None:\n            response.headers = {}\n\n        if not self.schema_creator.generate_examples:\n            schema_creator = self.schema_creator\n        else:\n            schema_creator = SchemaCreator.from_openapi_context(self.context, generate_examples=False)\n\n        for response_header in self.route_handler.resolve_response_headers():\n            header = OpenAPIHeader()\n            for attribute_name, attribute_value in (\n                (k, v) for k, v in asdict(response_header).items() if v is not None\n            ):\n                if attribute_name == \"value\":\n                    header.schema = schema_creator.for_field_definition(\n                        FieldDefinition.from_annotation(type(attribute_value))\n                    )\n                elif attribute_name != \"documentation_only\":\n                    setattr(header, attribute_name, attribute_value)\n\n            response.headers[response_header.name] = header\n\n        if cookies := self.route_handler.resolve_response_cookies():\n            response.headers[\"Set-Cookie\"] = OpenAPIHeader(\n                schema=Schema(\n                    all_of=[create_cookie_schema(cookie=cookie) for cookie in sorted(cookies, key=attrgetter(\"key\"))]\n                )\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef create_error_responses(exceptions: list[type[HTTPException]]) -> Iterator[tuple[str, OpenAPIResponse]]:\n    \"\"\"Create the schema for error responses, if any.\"\"\"\n    grouped_exceptions: dict[int, list[type[HTTPException]]] = {}\n    for exc in exceptions:\n        if not grouped_exceptions.get(exc.status_code):\n            grouped_exceptions[exc.status_code] = []\n        grouped_exceptions[exc.status_code].append(exc)\n    for status_code, exception_group in grouped_exceptions.items():\n        exceptions_schemas = []\n        group_description: str = \"\"\n        for exc in exception_group:\n            example_detail = \"\"\n            if hasattr(exc, \"detail\") and exc.detail:\n                group_description = exc.detail\n                example_detail = exc.detail\n\n            if not example_detail:\n                with contextlib.suppress(Exception):\n                    example_detail = HTTPStatus(status_code).phrase\n\n            exceptions_schemas.append(\n                Schema(\n                    type=OpenAPIType.OBJECT,\n                    required=[\"detail\", \"status_code\"],\n                    properties={\n                        \"status_code\": Schema(type=OpenAPIType.INTEGER),\n                        \"detail\": Schema(type=OpenAPIType.STRING),\n                        \"extra\": Schema(\n                            type=[OpenAPIType.NULL, OpenAPIType.OBJECT, OpenAPIType.ARRAY],\n                            additional_properties=Schema(),\n                        ),\n                    },\n                    description=pascal_case_to_text(get_name(exc)),\n                    examples=[{\"status_code\": status_code, \"detail\": example_detail, \"extra\": {}}],\n                )\n            )\n        if len(exceptions_schemas) > 1:  # noqa: SIM108\n            schema = Schema(one_of=exceptions_schemas)\n        else:\n            schema = exceptions_schemas[0]\n\n        if not group_description:\n            with contextlib.suppress(Exception):\n                group_description = HTTPStatus(status_code).description\n\n        yield (\n            str(status_code),\n            OpenAPIResponse(\n                description=group_description,\n                content={MediaType.JSON: OpenAPIMediaType(schema=schema)},\n            ),\n        )\n\n\ndef create_responses_for_handler(\n    context: OpenAPIContext, route_handler: HTTPRouteHandler, raises_validation_error: bool\n) -> Responses | None:\n    \"\"\"Create the schema for responses, if any.\n\n    Args:\n        context: An OpenAPIContext instance.\n        route_handler: An HTTPRouteHandler instance.\n        raises_validation_error: Boolean flag indicating whether the handler raises a ValidationException.\n\n    Returns:\n        Responses\n    \"\"\"\n    return ResponseFactory(context, route_handler).create_responses(raises_validation_error=raises_validation_error)\n\n```\n# Function Signature: def create_additional_responses(self) -> Iterator[tuple[str, OpenAPIResponse]]:\n# Natural Language Annotations: \"\"\"Create the schema for additional responses, if any.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef create_additional_responses(self) -> Iter", "  ```\ndef create_additional_responses(self) -> Iter", "  ```\ndef create_additional_responses(self) -> Iter"]}, "litestar-litestar/_openapi/path_item.py-create_path_item": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/_openapi/path_item.py:\n```\nfrom __future__ import annotations\n\nimport dataclasses\nfrom inspect import cleandoc\nfrom typing import TYPE_CHECKING\n\nfrom litestar._openapi.parameters import create_parameters_for_handler\nfrom litestar._openapi.request_body import create_request_body\nfrom litestar._openapi.responses import create_responses_for_handler\nfrom litestar._openapi.utils import SEPARATORS_CLEANUP_PATTERN\nfrom litestar.enums import HttpMethod\nfrom litestar.exceptions import ImproperlyConfiguredException\nfrom litestar.openapi.spec import Operation, PathItem\nfrom litestar.utils.helpers import unwrap_partial\n\nif TYPE_CHECKING:\n    from litestar._openapi.datastructures import OpenAPIContext\n    from litestar.handlers.http_handlers import HTTPRouteHandler\n    from litestar.routes import HTTPRoute\n\n__all__ = (\"create_path_item_for_route\", \"merge_path_item_operations\")\n\n\nclass PathItemFactory:\n    \"\"\"Factory for creating a PathItem instance for a given route.\"\"\"\n\n    def __init__(self, openapi_context: OpenAPIContext, route: HTTPRoute) -> None:\n        self.context = openapi_context\n        self.route = route\n        self._path_item = PathItem()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def create_operation_for_handler_method(\n        self, route_handler: HTTPRouteHandler, http_method: HttpMethod\n    ) -> Operation:\n        \"\"\"Create an Operation instance for a given route handler and http method.\n\n        Args:\n            route_handler: A route handler instance.\n            http_method: An HttpMethod enum value.\n\n        Returns:\n            An Operation instance.\n        \"\"\"\n        operation_id = self.create_operation_id(route_handler, http_method)\n        parameters = create_parameters_for_handler(self.context, route_handler, self.route.path_parameters)\n        signature_fields = route_handler.parsed_fn_signature.parameters\n\n        request_body = None\n        if data_field := signature_fields.get(\"data\"):\n            request_body = create_request_body(\n                self.context, route_handler.handler_id, route_handler.resolve_data_dto(), data_field\n            )\n\n        raises_validation_error = bool(data_field or self._path_item.parameters or parameters)\n        responses = create_responses_for_handler(\n            self.context, route_handler, raises_validation_error=raises_validation_error\n        )\n\n        return route_handler.operation_class(\n            operation_id=operation_id,\n            tags=route_handler.resolve_tags() or None,\n            summary=route_handler.summary or SEPARATORS_CLEANUP_PATTERN.sub(\"\", route_handler.handler_name.title()),\n            description=self.create_description_for_handler(route_handler),\n            deprecated=route_handler.deprecated,\n            responses=responses,\n            request_body=request_body,\n            parameters=parameters or None,  # type: ignore[arg-type]\n            security=route_handler.resolve_security() or None,\n        )\n\n    def create_operation_id(self, route_handler: HTTPRouteHandler, http_method: HttpMethod) -> str:\n        \"\"\"Create an operation id for a given route handler and http method.\n\n        Adds the operation id to the context's operation id set, where it is checked for uniqueness.\n\n        Args:\n            route_handler: A route handler instance.\n            http_method: An HttpMethod enum value.\n\n        Returns:\n            An operation id string.\n        \"\"\"\n        if isinstance(route_handler.operation_id, str):\n            operation_id = route_handler.operation_id\n        elif callable(route_handler.operation_id):\n            operation_id = route_handler.operation_id(route_handler, http_method, self.route.path_components)\n        else:\n            operation_id = self.context.openapi_config.operation_id_creator(\n                route_handler, http_method, self.route.path_components\n            )\n        self.context.add_operation_id(operation_id)\n        return operation_id\n\n    def create_description_for_handler(self, route_handler: HTTPRouteHandler) -> str | None:\n        \"\"\"Produce the operation description for a route handler.\n\n        Args:\n            route_handler: A route handler instance.\n\n        Returns:\n            An optional description string\n        \"\"\"\n        handler_description = route_handler.description\n        if handler_description is None and self.context.openapi_config.use_handler_docstrings:\n            fn = unwrap_partial(route_handler.fn)\n            return cleandoc(fn.__doc__) if fn.__doc__ else None\n        return handler_description\n\n\ndef create_path_item_for_route(openapi_context: OpenAPIContext, route: HTTPRoute) -> PathItem:\n    \"\"\"Create a PathItem for the given route parsing all http_methods into Operation Models.\n\n    Args:\n        openapi_context: The OpenAPIContext instance.\n        route: The route to create a PathItem for.\n\n    Returns:\n        A PathItem instance.\n    \"\"\"\n    path_item_factory = PathItemFactory(openapi_context, route)\n    return path_item_factory.create_path_item()\n\n\ndef merge_path_item_operations(source: PathItem, other: PathItem, for_path: str) -> PathItem:\n    \"\"\"Merge operations from path items, creating a new path item that includes\n    operations from both.\n    \"\"\"\n    attrs_to_merge = {\"get\", \"put\", \"post\", \"delete\", \"options\", \"head\", \"patch\", \"trace\"}\n    fields = {f.name for f in dataclasses.fields(PathItem)} - attrs_to_merge\n    if any(getattr(source, attr) and getattr(other, attr) for attr in attrs_to_merge):\n        raise ValueError(\"Cannot merge operation for PathItem if operation is set on both items\")\n\n    if differing_values := [\n        (value_a, value_b) for attr in fields if (value_a := getattr(source, attr)) != (value_b := getattr(other, attr))\n    ]:\n        raise ImproperlyConfiguredException(\n            f\"Conflicting OpenAPI path configuration for {for_path!r}. \"\n            f\"{', '.join(f'{a} != {b}' for a, b in differing_values)}\"\n        )\n\n    return dataclasses.replace(\n        source,\n        get=source.get or other.get,\n        post=source.post or other.post,\n        patch=source.patch or other.patch,\n        put=source.put or other.put,\n        delete=source.delete or other.delete,\n        options=source.options or other.options,\n        trace=source.trace or other.trace,\n    )\n\n```\n# Function Signature: def create_path_item(self) -> PathItem:\n# Natural Language Annotations: \"\"\"Create a PathItem for the given route parsing all http_methods into Operation Models.\n\n        Returns:\n            A PathItem instance.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef create_path_item(self) -> PathItem:", "  ```\ndef create_path_item(self):\n    \"\"\"Create", "  ```\ndef create_path_item(self) -> PathItem:"]}, "litestar-litestar/_openapi/schema_generation/schema.py-for_field_definition": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/_openapi/schema_generation/schema.py:\n```\nfrom __future__ import annotations\n\nfrom collections import deque\nfrom copy import copy\nfrom datetime import date, datetime, time, timedelta\nfrom decimal import Decimal\nfrom enum import Enum, EnumMeta\nfrom ipaddress import IPv4Address, IPv4Interface, IPv4Network, IPv6Address, IPv6Interface, IPv6Network\nfrom pathlib import Path\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    DefaultDict,\n    Deque,\n    Dict,\n    FrozenSet,\n    Hashable,\n    Iterable,\n    List,\n    Literal,\n    Mapping,\n    MutableMapping,\n    MutableSequence,\n    OrderedDict,\n    Pattern,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\nfrom uuid import UUID\n\nfrom typing_extensions import Self, get_args\n\nfrom litestar._openapi.datastructures import SchemaRegistry\nfrom litestar._openapi.schema_generation.constrained_fields import (\n    create_date_constrained_field_schema,\n    create_numerical_constrained_field_schema,\n    create_string_constrained_field_schema,\n)\nfrom litestar._openapi.schema_generation.utils import (\n    _get_normalized_schema_key,\n    _should_create_enum_schema,\n    _should_create_literal_schema,\n    _type_or_first_not_none_inner_type,\n    get_json_schema_formatted_examples,\n)\nfrom litestar.datastructures import SecretBytes, SecretString, UploadFile\nfrom litestar.exceptions import ImproperlyConfiguredException\nfrom litestar.openapi.spec.enums import OpenAPIFormat, OpenAPIType\nfrom litestar.openapi.spec.schema import Schema, SchemaDataContainer\nfrom litestar.params import BodyKwarg, KwargDefinition, ParameterKwarg\nfrom litestar.plugins import OpenAPISchemaPlugin\nfrom litestar.types import Empty\nfrom litestar.types.builtin_types import NoneType\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils.helpers import get_name\nfrom litestar.utils.predicates import (\n    is_class_and_subclass,\n    is_undefined_sentinel,\n)\nfrom litestar.utils.typing import (\n    get_origin_or_inner_type,\n    make_non_optional_union,\n    unwrap_new_type,\n)\n\nif TYPE_CHECKING:\n    from litestar._openapi.datastructures import OpenAPIContext\n    from litestar.openapi.spec import Example, Reference\n    from litestar.plugins import OpenAPISchemaPluginProtocol\n\nKWARG_DEFINITION_ATTRIBUTE_TO_OPENAPI_PROPERTY_MAP: dict[str, str] = {\n    \"content_encoding\": \"content_encoding\",\n    \"default\": \"default\",\n    \"description\": \"description\",\n    \"enum\": \"enum\",\n    \"examples\": \"examples\",\n    \"external_docs\": \"external_docs\",\n    \"format\": \"format\",\n    \"ge\": \"minimum\",\n    \"gt\": \"exclusive_minimum\",\n    \"le\": \"maximum\",\n    \"lt\": \"exclusive_maximum\",\n    \"max_items\": \"max_items\",\n    \"max_length\": \"max_length\",\n    \"min_items\": \"min_items\",\n    \"min_length\": \"min_length\",\n    \"multiple_of\": \"multiple_of\",\n    \"pattern\": \"pattern\",\n    \"title\": \"title\",\n    \"read_only\": \"read_only\",\n}\n\nTYPE_MAP: dict[type[Any] | None | Any, Schema] = {\n    Decimal: Schema(type=OpenAPIType.NUMBER),\n    DefaultDict: Schema(type=OpenAPIType.OBJECT),\n    Deque: Schema(type=OpenAPIType.ARRAY),\n    Dict: Schema(type=OpenAPIType.OBJECT),\n    FrozenSet: Schema(type=OpenAPIType.ARRAY),\n    IPv4Address: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.IPV4),\n    IPv4Interface: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.IPV4),\n    IPv4Network: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.IPV4),\n    IPv6Address: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.IPV6),\n    IPv6Interface: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.IPV6),\n    IPv6Network: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.IPV6),\n    Iterable: Schema(type=OpenAPIType.ARRAY),\n    List: Schema(type=OpenAPIType.ARRAY),\n    Mapping: Schema(type=OpenAPIType.OBJECT),\n    MutableMapping: Schema(type=OpenAPIType.OBJECT),\n    MutableSequence: Schema(type=OpenAPIType.ARRAY),\n    None: Schema(type=OpenAPIType.NULL),\n    NoneType: Schema(type=OpenAPIType.NULL),\n    OrderedDict: Schema(type=OpenAPIType.OBJECT),\n    Path: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.URI),\n    Pattern: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.REGEX),\n    SecretBytes: Schema(type=OpenAPIType.STRING),\n    SecretString: Schema(type=OpenAPIType.STRING),\n    Sequence: Schema(type=OpenAPIType.ARRAY),\n    Set: Schema(type=OpenAPIType.ARRAY),\n    Tuple: Schema(type=OpenAPIType.ARRAY),\n    UUID: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.UUID),\n    bool: Schema(type=OpenAPIType.BOOLEAN),\n    bytearray: Schema(type=OpenAPIType.STRING),\n    bytes: Schema(type=OpenAPIType.STRING),\n    date: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.DATE),\n    datetime: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.DATE_TIME),\n    deque: Schema(type=OpenAPIType.ARRAY),\n    dict: Schema(type=OpenAPIType.OBJECT),\n    float: Schema(type=OpenAPIType.NUMBER),\n    frozenset: Schema(type=OpenAPIType.ARRAY),\n    int: Schema(type=OpenAPIType.INTEGER),\n    list: Schema(type=OpenAPIType.ARRAY),\n    set: Schema(type=OpenAPIType.ARRAY),\n    str: Schema(type=OpenAPIType.STRING),\n    time: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.DURATION),\n    timedelta: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.DURATION),\n    tuple: Schema(type=OpenAPIType.ARRAY),\n}\n\n\ndef _types_in_list(lst: list[Any]) -> list[OpenAPIType] | OpenAPIType:\n    \"\"\"Extract unique OpenAPITypes present in the values of a list.\n\n    Args:\n        lst: A list of values\n\n    Returns:\n        OpenAPIType in the given list. If more then one exists, return\n        a list of OpenAPITypes.\n    \"\"\"\n    schema_types: list[OpenAPIType] = []\n    for item in lst:\n        schema_type = TYPE_MAP[type(item)].type\n        if isinstance(schema_type, OpenAPIType):\n            schema_types.append(schema_type)\n        else:\n            raise RuntimeError(\"Unexpected type for schema item\")  # pragma: no cover\n    schema_types = list(set(schema_types))\n    return schema_types[0] if len(schema_types) == 1 else schema_types\n\n\ndef _get_type_schema_name(field_definition: FieldDefinition) -> str:\n    \"\"\"Extract the schema name from a data container.\n\n    Args:\n        field_definition: A field definition instance.\n\n    Returns:\n        A string\n    \"\"\"\n\n    if name := getattr(field_definition.annotation, \"__schema_name__\", None):\n        return cast(\"str\", name)\n\n    name = get_name(field_definition.annotation)\n    if field_definition.inner_types:\n        inner_parts = \", \".join(_get_type_schema_name(t) for t in field_definition.inner_types)\n        return f\"{name}[{inner_parts}]\"\n\n    return name\n\n\ndef create_enum_schema(annotation: EnumMeta, include_null: bool = False) -> Schema:\n    \"\"\"Create a schema instance for an enum.\n\n    Args:\n        annotation: An enum.\n        include_null: Whether to include null as a possible value.\n\n    Returns:\n        A schema instance.\n    \"\"\"\n    enum_values: list[str | int | None] = [v.value for v in annotation]  # type: ignore[var-annotated]\n    if include_null and None not in enum_values:\n        enum_values.append(None)\n    return Schema(type=_types_in_list(enum_values), enum=enum_values)\n\n\ndef _iter_flat_literal_args(annotation: Any) -> Iterable[Any]:\n    \"\"\"Iterate over the flattened arguments of a Literal.\n\n    Args:\n        annotation: An Literal annotation.\n\n    Yields:\n        The flattened arguments of the Literal.\n    \"\"\"\n    for arg in get_args(annotation):\n        if get_origin_or_inner_type(arg) is Literal:\n            yield from _iter_flat_literal_args(arg)\n        else:\n            yield arg.value if isinstance(arg, Enum) else arg\n\n\ndef create_literal_schema(annotation: Any, include_null: bool = False) -> Schema:\n    \"\"\"Create a schema instance for a Literal.\n\n    Args:\n        annotation: An Literal annotation.\n        include_null: Whether to include null as a possible value.\n\n    Returns:\n        A schema instance.\n    \"\"\"\n    args = list(_iter_flat_literal_args(annotation))\n    if include_null and None not in args:\n        args.append(None)\n    schema = Schema(type=_types_in_list(args))\n    if len(args) > 1:\n        schema.enum = args\n    else:\n        schema.const = args[0]\n    return schema\n\n\ndef create_schema_for_annotation(annotation: Any) -> Schema:\n    \"\"\"Get a schema from the type mapping - if possible.\n\n    Args:\n        annotation: A type annotation.\n\n    Returns:\n        A schema instance or None.\n    \"\"\"\n\n    return copy(TYPE_MAP[annotation]) if annotation in TYPE_MAP else Schema()\n\n\nclass SchemaCreator:\n    __slots__ = (\"generate_examples\", \"plugins\", \"prefer_alias\", \"schema_registry\")\n\n    def __init__(\n        self,\n        generate_examples: bool = False,\n        plugins: Iterable[OpenAPISchemaPluginProtocol] | None = None,\n        prefer_alias: bool = True,\n        schema_registry: SchemaRegistry | None = None,\n    ) -> None:\n        \"\"\"Instantiate a SchemaCreator.\n\n        Args:\n            generate_examples: Whether to generate examples if none are given.\n            plugins: A list of plugins.\n            prefer_alias: Whether to prefer the alias name for the schema.\n            schema_registry: A SchemaRegistry instance.\n        \"\"\"\n        self.generate_examples = generate_examples\n        self.plugins = plugins if plugins is not None else []\n        self.prefer_alias = prefer_alias\n        self.schema_registry = schema_registry or SchemaRegistry()\n\n    @classmethod\n    def from_openapi_context(cls, context: OpenAPIContext, prefer_alias: bool = True, **kwargs: Any) -> Self:\n        kwargs.setdefault(\"generate_examples\", context.openapi_config.create_examples)\n        kwargs.setdefault(\"plugins\", context.plugins)\n        kwargs.setdefault(\"schema_registry\", context.schema_registry)\n        return cls(**kwargs, prefer_alias=prefer_alias)\n\n    @property\n    def not_generating_examples(self) -> SchemaCreator:\n        \"\"\"Return a SchemaCreator with generate_examples set to False.\"\"\"\n        if not self.generate_examples:\n            return self\n        return type(self)(generate_examples=False, plugins=self.plugins, prefer_alias=False)\n\n    @staticmethod\n    def plugin_supports_field(plugin: OpenAPISchemaPluginProtocol, field: FieldDefinition) -> bool:\n        if predicate := getattr(plugin, \"is_plugin_supported_field\", None):\n            return predicate(field)  # type: ignore[no-any-return]\n        return plugin.is_plugin_supported_type(field.annotation)\n\n    def get_plugin_for(self, field_definition: FieldDefinition) -> OpenAPISchemaPluginProtocol | None:\n        return next(\n            (plugin for plugin in self.plugins if self.plugin_supports_field(plugin, field_definition)),\n            None,\n        )\n\n    def is_constrained_field(self, field_definition: FieldDefinition) -> bool:\n        \"\"\"Return if the field is constrained, taking into account constraints defined by plugins\"\"\"\n        return (\n            isinstance(field_definition.kwarg_definition, (ParameterKwarg, BodyKwarg))\n            and field_definition.kwarg_definition.is_constrained\n        ) or any(\n            p.is_constrained_field(field_definition)\n            for p in self.plugins\n            if isinstance(p, OpenAPISchemaPlugin) and p.is_plugin_supported_field(field_definition)\n        )\n\n    def is_undefined(self, value: Any) -> bool:\n        \"\"\"Return if the field is undefined, taking into account undefined types defined by plugins\"\"\"\n        return is_undefined_sentinel(value) or any(\n            p.is_undefined_sentinel(value) for p in self.plugins if isinstance(p, OpenAPISchemaPlugin)\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def for_new_type(self, field_definition: FieldDefinition) -> Schema | Reference:\n        return self.for_field_definition(\n            FieldDefinition.from_kwarg(\n                annotation=unwrap_new_type(field_definition.annotation),\n                name=field_definition.name,\n                default=field_definition.default,\n            )\n        )\n\n    @staticmethod\n    def for_upload_file(field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create schema for UploadFile.\n\n        Args:\n            field_definition: A field definition instance.\n\n        Returns:\n            A Schema instance.\n        \"\"\"\n\n        property_key = \"file\"\n        schema = Schema(\n            type=OpenAPIType.STRING,\n            content_media_type=\"application/octet-stream\",\n            format=OpenAPIFormat.BINARY,\n        )\n\n        # If the type is `dict[str, UploadFile]`, then it's the same as a `list[UploadFile]`\n        # but we will internally convert that into a `dict[str, UploadFile]`.\n        if field_definition.is_non_string_sequence or field_definition.is_mapping:\n            property_key = \"files\"\n            schema = Schema(type=OpenAPIType.ARRAY, items=schema)\n\n        # If the uploadfile is annotated directly on the handler, then the\n        # 'properties' needs to be created. Else, the 'properties' will be\n        # created by the corresponding plugin.\n        is_defined_on_handler = field_definition.name == \"data\" and isinstance(\n            field_definition.kwarg_definition, BodyKwarg\n        )\n        if is_defined_on_handler:\n            return Schema(type=OpenAPIType.OBJECT, properties={property_key: schema})\n\n        return schema\n\n    @staticmethod\n    def for_typevar() -> Schema:\n        \"\"\"Create a schema for a TypeVar.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n\n        return Schema(type=OpenAPIType.OBJECT)\n\n    def for_optional_field(self, field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create a Schema for an optional FieldDefinition.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        schema_or_reference = self.for_field_definition(\n            FieldDefinition.from_kwarg(\n                annotation=make_non_optional_union(field_definition.annotation),\n                name=field_definition.name,\n                default=field_definition.default,\n            )\n        )\n        if isinstance(schema_or_reference, Schema) and isinstance(schema_or_reference.one_of, list):\n            result = schema_or_reference.one_of\n        else:\n            result = [schema_or_reference]\n\n        return Schema(one_of=[Schema(type=OpenAPIType.NULL), *result])\n\n    def for_union_field(self, field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create a Schema for a union FieldDefinition.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        inner_types = (f for f in (field_definition.inner_types or []) if not self.is_undefined(f.annotation))\n        values = list(map(self.for_field_definition, inner_types))\n        return Schema(one_of=values)\n\n    def for_object_type(self, field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create schema for object types (dict, Mapping, list, Sequence etc.) types.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        if field_definition.has_inner_subclass_of(UploadFile):\n            return self.for_upload_file(field_definition)\n\n        if field_definition.is_mapping:\n            return Schema(\n                type=OpenAPIType.OBJECT,\n                additional_properties=(\n                    self.for_field_definition(field_definition.inner_types[1])\n                    if field_definition.inner_types and len(field_definition.inner_types) == 2\n                    else None\n                ),\n            )\n\n        if field_definition.is_non_string_sequence or field_definition.is_non_string_iterable:\n            # filters out ellipsis from tuple[int, ...] type annotations\n            inner_types = (f for f in field_definition.inner_types if f.annotation is not Ellipsis)\n            items = list(map(self.for_field_definition, inner_types or ()))\n\n            return Schema(\n                type=OpenAPIType.ARRAY,\n                items=Schema(one_of=items) if len(items) > 1 else items[0],\n            )\n\n        raise ImproperlyConfiguredException(  # pragma: no cover\n            f\"Parameter '{field_definition.name}' with type '{field_definition.annotation}' could not be mapped to an Open API type. \"\n            f\"This can occur if a user-defined generic type is resolved as a parameter. If '{field_definition.name}' should \"\n            \"not be documented as a parameter, annotate it using the `Dependency` function, e.g., \"\n            f\"`{field_definition.name}: ... = Dependency(...)`.\"\n        )\n\n    def for_plugin(self, field_definition: FieldDefinition, plugin: OpenAPISchemaPluginProtocol) -> Schema | Reference:\n        \"\"\"Create a schema using a plugin.\n\n        Args:\n            field_definition: A signature field instance.\n            plugin: A plugin for the field type.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        key = _get_normalized_schema_key(field_definition.annotation)\n        if (ref := self.schema_registry.get_reference_for_key(key)) is not None:\n            return ref\n\n        schema = plugin.to_openapi_schema(field_definition=field_definition, schema_creator=self)\n        if isinstance(schema, SchemaDataContainer):  # pragma: no cover\n            return self.for_field_definition(\n                FieldDefinition.from_kwarg(\n                    annotation=schema.data_container,\n                    name=field_definition.name,\n                    default=field_definition.default,\n                    extra=field_definition.extra,\n                    kwarg_definition=field_definition.kwarg_definition,\n                )\n            )\n        return schema\n\n    def for_constrained_field(self, field: FieldDefinition) -> Schema:\n        \"\"\"Create Schema for Pydantic Constrained fields (created using constr(), conint() and so forth, or by subclassing\n        Constrained*)\n\n        Args:\n            field: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        kwarg_definition = cast(Union[ParameterKwarg, BodyKwarg], field.kwarg_definition)\n        if any(is_class_and_subclass(field.annotation, t) for t in (int, float, Decimal)):\n            return create_numerical_constrained_field_schema(field.annotation, kwarg_definition)\n        if any(is_class_and_subclass(field.annotation, t) for t in (str, bytes)):  # type: ignore[arg-type]\n            return create_string_constrained_field_schema(field.annotation, kwarg_definition)\n        if any(is_class_and_subclass(field.annotation, t) for t in (date, datetime)):\n            return create_date_constrained_field_schema(field.annotation, kwarg_definition)\n        return self.for_collection_constrained_field(field)\n\n    def for_collection_constrained_field(self, field_definition: FieldDefinition) -> Schema:\n        \"\"\"Create Schema from Constrained List/Set field.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        schema = Schema(type=OpenAPIType.ARRAY)\n        kwarg_definition = cast(Union[ParameterKwarg, BodyKwarg], field_definition.kwarg_definition)\n        if kwarg_definition.min_items:\n            schema.min_items = kwarg_definition.min_items\n        if kwarg_definition.max_items:\n            schema.max_items = kwarg_definition.max_items\n        if any(is_class_and_subclass(field_definition.annotation, t) for t in (set, frozenset)):  # type: ignore[arg-type]\n            schema.unique_items = True\n\n        item_creator = self.not_generating_examples\n        if field_definition.inner_types:\n            items = list(map(item_creator.for_field_definition, field_definition.inner_types))\n            schema.items = Schema(one_of=items) if len(items) > 1 else items[0]\n        else:\n            schema.items = item_creator.for_field_definition(\n                FieldDefinition.from_kwarg(\n                    field_definition.annotation.item_type, f\"{field_definition.annotation.__name__}Field\"\n                )\n            )\n        return schema\n\n    def process_schema_result(self, field: FieldDefinition, schema: Schema) -> Schema | Reference:\n        if field.kwarg_definition and field.is_const and field.has_default and schema.const is None:\n            schema.const = field.default\n\n        if field.kwarg_definition:\n            for kwarg_definition_key, schema_key in KWARG_DEFINITION_ATTRIBUTE_TO_OPENAPI_PROPERTY_MAP.items():\n                if (value := getattr(field.kwarg_definition, kwarg_definition_key, Empty)) and (\n                    not isinstance(value, Hashable) or not self.is_undefined(value)\n                ):\n                    if schema_key == \"examples\":\n                        value = get_json_schema_formatted_examples(cast(\"list[Example]\", value))\n\n                    # we only want to transfer values from the `KwargDefinition` to `Schema` if the schema object\n                    # doesn't already have a value for that property. For example, if a field is a constrained date,\n                    # by this point, we have already set the `exclusive_minimum` and/or `exclusive_maximum` fields\n                    # to floating point timestamp values on the schema object. However, the original `date` objects\n                    # that define those constraints on `KwargDefinition` are still `date` objects. We don't want to\n                    # overwrite them here.\n                    if getattr(schema, schema_key, None) is None:\n                        setattr(schema, schema_key, value)\n\n            if isinstance(field.kwarg_definition, KwargDefinition) and (extra := field.kwarg_definition.schema_extra):\n                for schema_key, value in extra.items():\n                    if not hasattr(schema, schema_key):\n                        raise ValueError(\n                            f\"`schema_extra` declares key `{schema_key}` which does not exist in `Schema` object\"\n                        )\n                    setattr(schema, schema_key, value)\n\n        if schema.default is None and field.default is not Empty:\n            schema.default = field.default\n\n        if not schema.examples and self.generate_examples:\n            from litestar._openapi.schema_generation.examples import create_examples_for_field\n\n            schema.examples = get_json_schema_formatted_examples(create_examples_for_field(field))\n\n        if schema.title and schema.type == OpenAPIType.OBJECT:\n            key = _get_normalized_schema_key(field.annotation)\n            return self.schema_registry.get_reference_for_key(key) or schema\n        return schema\n\n    def create_component_schema(\n        self,\n        type_: FieldDefinition,\n        /,\n        required: list[str],\n        property_fields: Mapping[str, FieldDefinition],\n        openapi_type: OpenAPIType = OpenAPIType.OBJECT,\n        title: str | None = None,\n        examples: list[Any] | None = None,\n    ) -> Schema:\n        \"\"\"Create a schema for the components/schemas section of the OpenAPI spec.\n\n        These are schemas that can be referenced by other schemas in the document, including self references.\n\n        To support self referencing schemas, the schema is added to the registry before schemas for its properties\n        are created. This allows the schema to be referenced by its properties.\n\n        Args:\n            type_: ``FieldDefinition`` instance of the type to create a schema for.\n            required: A list of required fields.\n            property_fields: Mapping of name to ``FieldDefinition`` instances for the properties of the schema.\n            openapi_type: The OpenAPI type, defaults to ``OpenAPIType.OBJECT``.\n            title: The schema title, generated if not provided.\n            examples: A mapping of example names to ``Example`` instances, not required.\n\n        Returns:\n            A schema instance.\n        \"\"\"\n        schema = self\n```\n# Function Signature: def for_field_definition(self, field_definition: FieldDefinition) -> Schema | Reference:\n# Natural Language Annotations: \"\"\"Create a Schema for a given FieldDefinition.\n\n        Args:\n            field_definition: A signature field instance.\n\n        Returns:\n            A schema instance.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef for_field_definition(self, field_definition:", "  \n[PYTHON]\ndef for_field_definition(self", "  ```\ndef for_field_definition(self, field_definition:"]}, "litestar-litestar/_openapi/typescript_converter/converter.py-convert_openapi_to_typescript": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/_openapi/typescript_converter/converter.py:\n```\nfrom __future__ import annotations\n\nfrom copy import copy\nfrom dataclasses import fields\nfrom typing import Any, TypeVar, cast\n\nfrom litestar._openapi.typescript_converter.schema_parsing import (\n    normalize_typescript_namespace,\n    parse_schema,\n)\nfrom litestar._openapi.typescript_converter.types import (\n    TypeScriptInterface,\n    TypeScriptNamespace,\n    TypeScriptPrimitive,\n    TypeScriptProperty,\n    TypeScriptType,\n    TypeScriptUnion,\n)\nfrom litestar.enums import HttpMethod, ParamType\nfrom litestar.openapi.spec import (\n    Components,\n    OpenAPI,\n    Operation,\n    Parameter,\n    Reference,\n    RequestBody,\n    Responses,\n    Schema,\n)\n\n__all__ = (\n    \"convert_openapi_to_typescript\",\n    \"deref_container\",\n    \"get_openapi_type\",\n    \"parse_params\",\n    \"parse_request_body\",\n    \"parse_responses\",\n    \"resolve_ref\",\n)\n\nfrom litestar.openapi.spec.base import BaseSchemaObject\n\nT = TypeVar(\"T\")\n\n\ndef _deref_schema_object(value: BaseSchemaObject, components: Components) -> BaseSchemaObject:\n    for field in fields(value):\n        if field_value := getattr(value, field.name, None):\n            if isinstance(field_value, Reference):\n                setattr(\n                    value,\n                    field.name,\n                    deref_container(resolve_ref(field_value, components=components), components=components),\n                )\n            elif isinstance(field_value, (Schema, dict, list)):\n                setattr(value, field.name, deref_container(field_value, components=components))\n    return value\n\n\ndef _deref_dict(value: dict[str, Any], components: Components) -> dict[str, Any]:\n    for k, v in value.items():\n        if isinstance(v, Reference):\n            value[k] = deref_container(resolve_ref(v, components=components), components=components)\n        elif isinstance(v, (Schema, dict, list)):\n            value[k] = deref_container(v, components=components)\n    return value\n\n\ndef _deref_list(values: list[Any], components: Components) -> list[Any]:\n    for i, value in enumerate(values):\n        if isinstance(value, Reference):\n            values[i] = deref_container(resolve_ref(value, components=components), components=components)\n        elif isinstance(value, (Schema, (dict, list))):\n            values[i] = deref_container(value, components=components)\n    return values\n\n\ndef deref_container(open_api_container: T, components: Components) -> T:\n    \"\"\"Dereference an object that may contain Reference instances.\n\n    Args:\n        open_api_container: Either an OpenAPI content, a dict or a list.\n        components: The OpenAPI schema Components section.\n\n    Returns:\n        A dereferenced object.\n    \"\"\"\n    if isinstance(open_api_container, BaseSchemaObject):\n        return cast(\"T\", _deref_schema_object(open_api_container, components))\n\n    if isinstance(open_api_container, dict):\n        return cast(\"T\", _deref_dict(copy(open_api_container), components))\n\n    if isinstance(open_api_container, list):\n        return cast(\"T\", _deref_list(copy(open_api_container), components))\n    raise ValueError(f\"unexpected container type {type(open_api_container).__name__}\")  # pragma: no cover\n\n\ndef resolve_ref(ref: Reference, components: Components) -> Schema:\n    \"\"\"Resolve a reference object into the actual value it points at.\n\n    Args:\n        ref: A Reference instance.\n        components: The OpenAPI schema Components section.\n\n    Returns:\n        An OpenAPI schema instance.\n    \"\"\"\n    current: Any = components\n    for path in [p for p in ref.ref.split(\"/\") if p not in {\"#\", \"components\"}]:\n        current = current[path] if isinstance(current, dict) else getattr(current, path, None)\n\n    if not isinstance(current, Schema):  # pragma: no cover\n        raise ValueError(\n            f\"unexpected value type, expected schema but received {type(current).__name__ if current is not None else 'None'}\"\n        )\n\n    return current\n\n\ndef get_openapi_type(value: Reference | T, components: Components) -> T:\n    \"\"\"Extract or dereference an OpenAPI container type.\n\n    Args:\n        value: Either a reference or a container type.\n        components: The OpenAPI schema Components section.\n\n    Returns:\n        The extracted container.\n    \"\"\"\n    if isinstance(value, Reference):\n        resolved_ref = resolve_ref(value, components=components)\n        return cast(\"T\", deref_container(open_api_container=resolved_ref, components=components))\n\n    return deref_container(open_api_container=value, components=components)\n\n\ndef parse_params(\n    params: list[Parameter],\n    components: Components,\n) -> tuple[TypeScriptInterface, ...]:\n    \"\"\"Parse request parameters.\n\n    Args:\n        params: An OpenAPI Operation parameters.\n        components: The OpenAPI schema Components section.\n\n    Returns:\n        A tuple of resolved interfaces.\n    \"\"\"\n    cookie_params: list[TypeScriptProperty] = []\n    header_params: list[TypeScriptProperty] = []\n    path_params: list[TypeScriptProperty] = []\n    query_params: list[TypeScriptProperty] = []\n\n    for param in params:\n        if param.schema:\n            schema = get_openapi_type(param.schema, components)\n            ts_prop = TypeScriptProperty(\n                key=normalize_typescript_namespace(param.name, allow_quoted=True),\n                required=param.required,\n                value=parse_schema(schema),\n            )\n            if param.param_in == ParamType.COOKIE:\n                cookie_params.append(ts_prop)\n            elif param.param_in == ParamType.HEADER:\n                header_params.append(ts_prop)\n            elif param.param_in == ParamType.PATH:\n                path_params.append(ts_prop)\n            else:\n                query_params.append(ts_prop)\n\n    result: list[TypeScriptInterface] = []\n\n    if cookie_params:\n        result.append(TypeScriptInterface(\"CookieParameters\", tuple(cookie_params)))\n    if header_params:\n        result.append(TypeScriptInterface(\"HeaderParameters\", tuple(header_params)))\n    if path_params:\n        result.append(TypeScriptInterface(\"PathParameters\", tuple(path_params)))\n    if query_params:\n        result.append(TypeScriptInterface(\"QueryParameters\", tuple(query_params)))\n\n    return tuple(result)\n\n\ndef parse_request_body(body: RequestBody, components: Components) -> TypeScriptType:\n    \"\"\"Parse the schema request body.\n\n    Args:\n        body: An OpenAPI RequestBody instance.\n        components: The OpenAPI schema Components section.\n\n    Returns:\n        A TypeScript type.\n    \"\"\"\n    undefined = TypeScriptPrimitive(\"undefined\")\n    if not body.content:\n        return TypeScriptType(\"RequestBody\", undefined)\n\n    if content := [get_openapi_type(v.schema, components) for v in body.content.values() if v.schema]:\n        schema = content[0]\n        return TypeScriptType(\n            \"RequestBody\",\n            parse_schema(schema) if body.required else TypeScriptUnion((parse_schema(schema), undefined)),\n        )\n\n    return TypeScriptType(\"RequestBody\", undefined)\n\n\ndef parse_responses(responses: Responses, components: Components) -> tuple[TypeScriptNamespace, ...]:\n    \"\"\"Parse a given Operation's Responses object.\n\n    Args:\n        responses: An OpenAPI Responses object.\n        components: The OpenAPI schema Components section.\n\n    Returns:\n        A tuple of namespaces, mapping response codes to data.\n    \"\"\"\n    result: list[TypeScriptNamespace] = []\n    for http_status, response in [\n        (status, get_openapi_type(res, components=components)) for status, res in responses.items()\n    ]:\n        if response.content and (\n            content := [get_openapi_type(v.schema, components) for v in response.content.values() if v.schema]\n        ):\n            ts_type = parse_schema(content[0])\n        else:\n            ts_type = TypeScriptPrimitive(\"undefined\")\n\n        containers = [\n            TypeScriptType(\"ResponseBody\", ts_type),\n            TypeScriptInterface(\n                \"ResponseHeaders\",\n                tuple(\n                    TypeScriptProperty(\n                        required=get_openapi_type(header, components=components).required,\n                        key=normalize_typescript_namespace(key, allow_quoted=True),\n                        value=TypeScriptPrimitive(\"string\"),\n                    )\n                    for key, header in response.headers.items()\n                ),\n            )\n            if response.headers\n            else None,\n        ]\n\n        result.append(TypeScriptNamespace(f\"Http{http_status}\", tuple(c for c in containers if c)))\n\n    return tuple(result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def convert_openapi_to_typescript(openapi_schema: OpenAPI, namespace: str = \"API\") -> TypeScriptNamespace:\n# Natural Language Annotations: \"\"\"Convert an OpenAPI Schema instance to a TypeScript namespace. This function is the main entry point for the\n    TypeScript converter.\n\n    Args:\n        openapi_schema: An OpenAPI Schema instance.\n        namespace: The namespace to use.\n\n    Returns:\n        A string representing the generated types.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef convert_openapi_to_typescript(openapi", "  ```\ndef convert_openapi_to_typescript(openapi", "  ```\ndef convert_openapi_to_typescript(openapi"]}, "litestar-litestar/_openapi/typescript_converter/types.py-write": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/_openapi/typescript_converter/types.py:\n```\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Literal\n\n__all__ = (\n    \"TypeScriptAnonymousInterface\",\n    \"TypeScriptArray\",\n    \"TypeScriptConst\",\n    \"TypeScriptContainer\",\n    \"TypeScriptElement\",\n    \"TypeScriptEnum\",\n    \"TypeScriptInterface\",\n    \"TypeScriptIntersection\",\n    \"TypeScriptLiteral\",\n    \"TypeScriptNamespace\",\n    \"TypeScriptPrimitive\",\n    \"TypeScriptProperty\",\n    \"TypeScriptType\",\n    \"TypeScriptUnion\",\n)\n\n\ndef _as_string(value: Any) -> str:\n    if isinstance(value, str):\n        return f'\"{value}\"'\n\n    if isinstance(value, bool):\n        return \"true\" if value else \"false\"\n\n    return \"null\" if value is None else str(value)\n\n\nclass TypeScriptElement(ABC):\n    \"\"\"A class representing a TypeScript type element.\"\"\"\n\n    @abstractmethod\n    def write(self) -> str:\n        \"\"\"Write a typescript value corresponding to the given typescript element.\n\n        Returns:\n            A typescript string\n        \"\"\"\n        raise NotImplementedError(\"\")\n\n\nclass TypeScriptContainer(TypeScriptElement):\n    \"\"\"A class representing a TypeScript type container.\"\"\"\n\n    name: str\n\n    @abstractmethod\n    def write(self) -> str:\n        \"\"\"Write a typescript value corresponding to the given typescript container.\n\n        Returns:\n            A typescript string\n        \"\"\"\n        raise NotImplementedError(\"\")\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptIntersection(TypeScriptElement):\n    \"\"\"A class representing a TypeScript intersection type.\"\"\"\n\n    types: tuple[TypeScriptElement, ...]\n\n\n\n\n\n\n\n\n\n\n\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptUnion(TypeScriptElement):\n    \"\"\"A class representing a TypeScript union type.\"\"\"\n\n    types: tuple[TypeScriptElement, ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript union value.\n\n        Example:\n            string | number\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return \" | \".join(sorted(t.write() for t in self.types))\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptPrimitive(TypeScriptElement):\n    \"\"\"A class representing a TypeScript primitive type.\"\"\"\n\n    type: Literal[\n        \"string\", \"number\", \"boolean\", \"any\", \"null\", \"undefined\", \"symbol\", \"Record<string, unknown>\", \"unknown[]\"\n    ]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript primitive type.\n\n        Example:\n            null\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return self.type\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptLiteral(TypeScriptElement):\n    \"\"\"A class representing a TypeScript literal type.\"\"\"\n\n    value: str | int | float | bool | None\n\n    def write(self) -> str:\n        \"\"\"Write a typescript literal type.\n\n        Example:\n            \"someValue\"\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return _as_string(self.value)\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptArray(TypeScriptElement):\n    \"\"\"A class representing a TypeScript array type.\"\"\"\n\n    item_type: TypeScriptElement\n\n    def write(self) -> str:\n        \"\"\"Write a typescript array type.\n\n        Example:\n            number[]\n\n        Returns:\n            A typescript string\n        \"\"\"\n        value = (\n            f\"({self.item_type.write()})\"\n            if isinstance(self.item_type, (TypeScriptUnion, TypeScriptIntersection))\n            else self.item_type.write()\n        )\n        return f\"{value}[]\"\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptProperty(TypeScriptElement):\n    \"\"\"A class representing a TypeScript interface property.\"\"\"\n\n    required: bool\n    key: str\n    value: TypeScriptElement\n\n    def write(self) -> str:\n        \"\"\"Write a typescript property. This class is used exclusively inside interfaces.\n\n        Example:\n            key: string;\n            optional?: number;\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return f\"{self.key}{':' if self.required else '?:'} {self.value.write()};\"\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptAnonymousInterface(TypeScriptElement):\n    \"\"\"A class representing a TypeScript anonymous interface.\"\"\"\n\n    properties: tuple[TypeScriptProperty, ...]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptInterface(TypeScriptContainer):\n    \"\"\"A class representing a TypeScript interface.\"\"\"\n\n    name: str\n    properties: tuple[TypeScriptProperty, ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript interface.\n\n        Example:\n            export interface MyInterface {\n                key: string;\n                optional?: number;\n            };\n\n        Returns:\n            A typescript string\n        \"\"\"\n        interface = TypeScriptAnonymousInterface(properties=self.properties)\n        return f\"export interface {self.name} {interface.write()};\"\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptEnum(TypeScriptContainer):\n    \"\"\"A class representing a TypeScript enum.\"\"\"\n\n    name: str\n    values: tuple[tuple[str, str], ...] | tuple[tuple[str, int | float], ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript enum.\n\n        Example:\n            export enum MyEnum {\n                DOG = \"canine\",\n                CAT = \"feline\",\n            };\n\n        Returns:\n            A typescript string\n        \"\"\"\n        members = \"\\t\" + \"\\n\\t\".join(\n            [f\"{key} = {_as_string(value)},\" for key, value in sorted(self.values, key=lambda member: member[0])]\n        )\n        return f\"export enum {self.name} {{\\n{members}\\n}};\"\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptType(TypeScriptContainer):\n    \"\"\"A class representing a TypeScript type.\"\"\"\n\n    name: str\n    value: TypeScriptElement\n\n    def write(self) -> str:\n        \"\"\"Write a typescript type.\n\n        Example:\n            export type MyType = number | \"42\";\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return f\"export type {self.name} = {self.value.write()};\"\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptConst(TypeScriptContainer):\n    \"\"\"A class representing a TypeScript const.\"\"\"\n\n    name: str\n    value: TypeScriptPrimitive | TypeScriptLiteral\n\n    def write(self) -> str:\n        \"\"\"Write a typescript const.\n\n        Example:\n            export const MyConst: number;\n\n        Returns:\n            A typescript string\n        \"\"\"\n        return f\"export const {self.name}: {self.value.write()};\"\n\n\n@dataclass(unsafe_hash=True)\nclass TypeScriptNamespace(TypeScriptContainer):\n    \"\"\"A class representing a TypeScript namespace.\"\"\"\n\n    name: str\n    values: tuple[TypeScriptContainer, ...]\n\n    def write(self) -> str:\n        \"\"\"Write a typescript namespace.\n\n        Example:\n            export MyNamespace {\n                export const MyConst: number;\n            }\n\n        Returns:\n            A typescript string\n        \"\"\"\n        members = \"\\t\" + \"\\n\\n\\t\".join([value.write() for value in sorted(self.values, key=lambda el: el.name)])\n        return f\"export namespace {self.name} {{\\n{members}\\n}};\"\n\n```\n# Function Signature: def write(self) -> str:\n# Natural Language Annotations: \"\"\"Write a typescript interface object, without a name.\n\n        Example:\n            {\n                key: string;\n                optional?: number;\n            }\n\n        Returns:\n            A typescript string\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef write(self) -> str:\n        \"\"\"Write a", "  ```\nexport interface {\n    key: string;\n    optional?:", "  ```\n    export interface {\n        key: string;\n        optional"]}, "litestar-litestar/testing/helpers.py-create_test_client": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/testing/helpers.py:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Callable, Literal, Mapping, Sequence\n\nfrom litestar.app import DEFAULT_OPENAPI_CONFIG, Litestar\nfrom litestar.controller import Controller\nfrom litestar.events import SimpleEventEmitter\nfrom litestar.testing.client import AsyncTestClient, TestClient\nfrom litestar.types import Empty\nfrom litestar.utils.predicates import is_class_and_subclass\n\nif TYPE_CHECKING:\n    from contextlib import AbstractAsyncContextManager\n\n    from litestar import Request, Response, WebSocket\n    from litestar.config.allowed_hosts import AllowedHostsConfig\n    from litestar.config.app import ExperimentalFeatures\n    from litestar.config.compression import CompressionConfig\n    from litestar.config.cors import CORSConfig\n    from litestar.config.csrf import CSRFConfig\n    from litestar.config.response_cache import ResponseCacheConfig\n    from litestar.datastructures import CacheControlHeader, ETag, State\n    from litestar.dto import AbstractDTO\n    from litestar.events import BaseEventEmitterBackend, EventListener\n    from litestar.logging.config import BaseLoggingConfig\n    from litestar.middleware.session.base import BaseBackendConfig\n    from litestar.openapi.config import OpenAPIConfig\n    from litestar.openapi.spec import SecurityRequirement\n    from litestar.plugins import PluginProtocol\n    from litestar.static_files.config import StaticFilesConfig\n    from litestar.stores.base import Store\n    from litestar.stores.registry import StoreRegistry\n    from litestar.template.config import TemplateConfig\n    from litestar.types import (\n        AfterExceptionHookHandler,\n        AfterRequestHookHandler,\n        AfterResponseHookHandler,\n        BeforeMessageSendHookHandler,\n        BeforeRequestHookHandler,\n        ControllerRouterHandler,\n        Dependencies,\n        EmptyType,\n        ExceptionHandlersMap,\n        Guard,\n        LifespanHook,\n        Middleware,\n        OnAppInitHandler,\n        ParametersMap,\n        ResponseCookies,\n        ResponseHeaders,\n        TypeEncodersMap,\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef create_async_test_client(\n    route_handlers: ControllerRouterHandler | Sequence[ControllerRouterHandler] | None = None,\n    *,\n    after_exception: Sequence[AfterExceptionHookHandler] | None = None,\n    after_request: AfterRequestHookHandler | None = None,\n    after_response: AfterResponseHookHandler | None = None,\n    allowed_hosts: Sequence[str] | AllowedHostsConfig | None = None,\n    backend: Literal[\"asyncio\", \"trio\"] = \"asyncio\",\n    backend_options: Mapping[str, Any] | None = None,\n    base_url: str = \"http://testserver.local\",\n    before_request: BeforeRequestHookHandler | None = None,\n    before_send: Sequence[BeforeMessageSendHookHandler] | None = None,\n    cache_control: CacheControlHeader | None = None,\n    compression_config: CompressionConfig | None = None,\n    cors_config: CORSConfig | None = None,\n    csrf_config: CSRFConfig | None = None,\n    debug: bool = True,\n    dependencies: Dependencies | None = None,\n    dto: type[AbstractDTO] | None | EmptyType = Empty,\n    etag: ETag | None = None,\n    event_emitter_backend: type[BaseEventEmitterBackend] = SimpleEventEmitter,\n    exception_handlers: ExceptionHandlersMap | None = None,\n    guards: Sequence[Guard] | None = None,\n    include_in_schema: bool | EmptyType = Empty,\n    lifespan: list[Callable[[Litestar], AbstractAsyncContextManager] | AbstractAsyncContextManager] | None = None,\n    listeners: Sequence[EventListener] | None = None,\n    logging_config: BaseLoggingConfig | EmptyType | None = Empty,\n    middleware: Sequence[Middleware] | None = None,\n    multipart_form_part_limit: int = 1000,\n    on_app_init: Sequence[OnAppInitHandler] | None = None,\n    on_shutdown: Sequence[LifespanHook] | None = None,\n    on_startup: Sequence[LifespanHook] | None = None,\n    openapi_config: OpenAPIConfig | None = DEFAULT_OPENAPI_CONFIG,\n    opt: Mapping[str, Any] | None = None,\n    parameters: ParametersMap | None = None,\n    pdb_on_exception: bool | None = None,\n    path: str | None = None,\n    plugins: Sequence[PluginProtocol] | None = None,\n    raise_server_exceptions: bool = True,\n    request_class: type[Request] | None = None,\n    response_cache_config: ResponseCacheConfig | None = None,\n    response_class: type[Response] | None = None,\n    response_cookies: ResponseCookies | None = None,\n    response_headers: ResponseHeaders | None = None,\n    return_dto: type[AbstractDTO] | None | EmptyType = Empty,\n    root_path: str = \"\",\n    security: Sequence[SecurityRequirement] | None = None,\n    session_config: BaseBackendConfig | None = None,\n    signature_namespace: Mapping[str, Any] | None = None,\n    signature_types: Sequence[Any] | None = None,\n    state: State | None = None,\n    static_files_config: Sequence[StaticFilesConfig] | None = None,\n    stores: StoreRegistry | dict[str, Store] | None = None,\n    tags: Sequence[str] | None = None,\n    template_config: TemplateConfig | None = None,\n    timeout: float | None = None,\n    type_encoders: TypeEncodersMap | None = None,\n    websocket_class: type[WebSocket] | None = None,\n    experimental_features: list[ExperimentalFeatures] | None = None,\n) -> AsyncTestClient[Litestar]:\n    \"\"\"Create a Litestar app instance and initializes it.\n\n    :class:`AsyncTestClient <litestar.testing.AsyncTestClient>` with it.\n\n    Notes:\n        - This function should be called as a context manager to ensure async startup and shutdown are\n            handled correctly.\n\n    Examples:\n        .. code-block:: python\n\n            from litestar import get\n            from litestar.testing import create_async_test_client\n\n\n            @get(\"/some-path\")\n            def my_handler() -> dict[str, str]:\n                return {\"hello\": \"world\"}\n\n\n            async def test_my_handler() -> None:\n                async with create_async_test_client(my_handler) as client:\n                    response = await client.get(\"/some-path\")\n                    assert response.json() == {\"hello\": \"world\"}\n\n    Args:\n        route_handlers: A single handler or a sequence of route handlers, which can include instances of\n            :class:`Router <litestar.router.Router>`, subclasses of :class:`Controller <.controller.Controller>` or\n            any function decorated by the route handler decorators.\n        backend: The async backend to use, options are \"asyncio\" or \"trio\".\n        backend_options: ``anyio`` options.\n        base_url: URL scheme and domain for test request paths, e.g. ``http://testserver``.\n        raise_server_exceptions: Flag for underlying the test client to raise server exceptions instead of wrapping them\n            in an HTTP response.\n        root_path: Path prefix for requests.\n        session_config: Configuration for Session Middleware class to create raw session cookies for request to the\n            route handlers.\n        after_exception: A sequence of :class:`exception hook handlers <.types.AfterExceptionHookHandler>`. This\n            hook is called after an exception occurs. In difference to exception handlers, it is not meant to\n            return a response - only to process the exception (e.g. log it, send it to Sentry etc.).\n        after_request: A sync or async function executed after the route handler function returned and the response\n            object has been resolved. Receives the response object.\n        after_response: A sync or async function called after the response has been awaited. It receives the\n            :class:`Request <.connection.Request>` object and should not return any values.\n        allowed_hosts: A sequence of allowed hosts, or an\n            :class:`AllowedHostsConfig <.config.allowed_hosts.AllowedHostsConfig>` instance. Enables the builtin\n            allowed hosts middleware.\n        before_request: A sync or async function called immediately before calling the route handler. Receives the\n            :class:`Request <.connection.Request>` instance and any non-``None`` return value is used for the\n            response, bypassing the route handler.\n        before_send: A sequence of :class:`before send hook handlers <.types.BeforeMessageSendHookHandler>`. Called\n            when the ASGI send function is called.\n        cache_control: A ``cache-control`` header of type\n            :class:`CacheControlHeader <litestar.datastructures.CacheControlHeader>` to add to route handlers of\n            this app. Can be overridden by route handlers.\n        compression_config: Configures compression behaviour of the application, this enabled a builtin or user\n            defined Compression middleware.\n        cors_config: If set, configures CORS handling for the application.\n        csrf_config: If set, configures :class:`CSRFMiddleware <.middleware.csrf.CSRFMiddleware>`.\n        debug: If ``True``, app errors rendered as HTML with a stack trace.\n        dependencies: A string keyed mapping of dependency :class:`Providers <.di.Provide>`.\n        dto: :class:`AbstractDTO <.dto.base_dto.AbstractDTO>` to use for (de)serializing and\n            validation of request data.\n        etag: An ``etag`` header of type :class:`ETag <.datastructures.ETag>` to add to route handlers of this app.\n            Can be overridden by route handlers.\n        event_emitter_backend: A subclass of\n            :class:`BaseEventEmitterBackend <.events.emitter.BaseEventEmitterBackend>`.\n        exception_handlers: A mapping of status codes and/or exception types to handler functions.\n        guards: A sequence of :class:`Guard <.types.Guard>` callables.\n        include_in_schema: A boolean flag dictating whether  the route handler should be documented in the OpenAPI schema.\n        lifespan: A list of callables returning async context managers, wrapping the lifespan of the ASGI application\n        listeners: A sequence of :class:`EventListener <.events.listener.EventListener>`.\n        logging_config: A subclass of :class:`BaseLoggingConfig <.logging.config.BaseLoggingConfig>`.\n        middleware: A sequence of :class:`Middleware <.types.Middleware>`.\n        multipart_form_part_limit: The maximal number of allowed parts in a multipart/formdata request. This limit\n            is intended to protect from DoS attacks.\n        on_app_init: A sequence of :class:`OnAppInitHandler <.types.OnAppInitHandler>` instances. Handlers receive\n            an instance of :class:`AppConfig <.config.app.AppConfig>` that will have been initially populated with\n            the parameters passed to :class:`Litestar <litestar.app.Litestar>`, and must return an instance of same.\n            If more than one handler is registered they are called in the order they are provided.\n        on_shutdown: A sequence of :class:`LifespanHook <.types.LifespanHook>` called during application\n            shutdown.\n        on_startup: A sequence of :class:`LifespanHook <litestar.types.LifespanHook>` called during\n            application startup.\n        openapi_config: Defaults to :attr:`DEFAULT_OPENAPI_CONFIG`\n        opt: A string keyed mapping of arbitrary values that can be accessed in :class:`Guards <.types.Guard>` or\n            wherever you have access to :class:`Request <litestar.connection.request.Request>` or\n            :class:`ASGI Scope <.types.Scope>`.\n        parameters: A mapping of :class:`Parameter <.params.Parameter>` definitions available to all application\n            paths.\n        path: A path fragment that is prefixed to all route handlers, controllers and routers associated\n            with the application instance.\n\n            .. versionadded:: 2.8.0\n        pdb_on_exception: Drop into the PDB when an exception occurs.\n        plugins: Sequence of plugins.\n        request_class: An optional subclass of :class:`Request <.connection.Request>` to use for http connections.\n        response_class: A custom subclass of :class:`Response <.response.Response>` to be used as the app's default\n            response.\n        response_cookies: A sequence of :class:`Cookie <.datastructures.Cookie>`.\n        response_headers: A string keyed mapping of :class:`ResponseHeader <.datastructures.ResponseHeader>`\n        response_cache_config: Configures caching behavior of the application.\n        return_dto: :class:`AbstractDTO <.dto.base_dto.AbstractDTO>` to use for serializing\n            outbound response data.\n        route_handlers: A sequence of route handlers, which can include instances of\n            :class:`Router <.router.Router>`, subclasses of :class:`Controller <.controller.Controller>` or any\n            callable decorated by the route handler decorators.\n        security: A sequence of dicts that will be added to the schema of all route handlers in the application.\n            See\n            :data:`SecurityRequirement <.openapi.spec.SecurityRequirement>` for details.\n        signature_namespace: A mapping of names to types for use in forward reference resolution during signature modeling.\n        signature_types: A sequence of types for use in forward reference resolution during signature modeling.\n            These types will be added to the signature namespace using their ``__name__`` attribute.\n        state: An optional :class:`State <.datastructures.State>` for application state.\n        static_files_config: A sequence of :class:`StaticFilesConfig <.static_files.StaticFilesConfig>`\n        stores: Central registry of :class:`Store <.stores.base.Store>` that will be available throughout the\n            application. If this is a dictionary to it will be passed to a\n            :class:`StoreRegistry <.stores.registry.StoreRegistry>`. If it is a\n            :class:`StoreRegistry <.stores.registry.StoreRegistry>`, this instance will be used directly.\n        tags: A sequence of string tags that will be appended to the schema of all route handlers under the\n            application.\n        template_config: An instance of :class:`TemplateConfig <.template.TemplateConfig>`\n        timeout: Request timeout\n        type_encoders: A mapping of types to callables that transform them into types supported for serialization.\n        websocket_class: An optional subclass of :class:`WebSocket <.connection.WebSocket>` to use for websocket\n            connections.\n        experimental_features: An iterable of experimental features to enable\n\n    Returns:\n        An instance of :class:`AsyncTestClient <litestar.testing.AsyncTestClient>` with a created app instance.\n    \"\"\"\n    route_handlers = () if route_handlers is None else route_handlers\n    if is_class_and_subclass(route_handlers, Controller) or not isinstance(route_handlers, Sequence):\n        route_handlers = (route_handlers,)\n\n    app = Litestar(\n        after_exception=after_exception,\n        after_request=after_request,\n        after_response=after_response,\n        allowed_hosts=allowed_hosts,\n        before_request=before_request,\n        before_send=before_send,\n        cache_control=cache_control,\n        compression_config=compression_config,\n        cors_config=cors_config,\n        csrf_config=csrf_config,\n        debug=debug,\n        dependencies=dependencies,\n        dto=dto,\n        etag=etag,\n        event_emitter_backend=event_emitter_backend,\n        exception_handlers=exception_handlers,\n        guards=guards,\n        include_in_schema=include_in_schema,\n        lifespan=lifespan,\n        listeners=listeners,\n        logging_config=logging_config,\n        middleware=middleware,\n        multipart_form_part_limit=multipart_form_part_limit,\n        on_app_init=on_app_init,\n        on_shutdown=on_shutdown,\n        on_startup=on_startup,\n        openapi_config=openapi_config,\n        opt=opt,\n        parameters=parameters,\n        path=path,\n        pdb_on_exception=pdb_on_exception,\n        plugins=plugins,\n        request_class=request_class,\n        response_cache_config=response_cache_config,\n        response_class=response_class,\n        response_cookies=response_cookies,\n        response_headers=response_headers,\n        return_dto=return_dto,\n        route_handlers=route_handlers,\n        security=security,\n        signature_namespace=signature_namespace,\n        signature_types=signature_types,\n        state=state,\n        static_files_config=static_files_config,\n        stores=stores,\n        tags=tags,\n        template_config=template_config,\n        type_encoders=type_encoders,\n        websocket_class=websocket_class,\n        experimental_features=experimental_features,\n    )\n\n    return AsyncTestClient[Litestar](\n        app=app,\n        backend=backend,\n        backend_options=backend_options,\n        base_url=base_url,\n        raise_server_exceptions=raise_server_exceptions,\n        root_path=root_path,\n        session_config=session_config,\n        timeout=timeout,\n    )\n\n```\n# Function Signature: def create_test_client(\n# Natural Language Annotations: \"\"\"Create a Litestar app instance and initializes it.\n\n    :class:`TestClient <litestar.testing.TestClient>` with it.\n\n    Notes:\n        - This function should be called as a context manager to ensure async startup and shutdown are\n            handled correctly.\n\n    Examples:\n        .. code-block:: python\n\n            from litestar import get\n            from litestar.testing import create_test_client\n\n\n            @get(\"/some-path\")\n            def my_handler() -> dict[str, str]:\n                return {\"hello\": \"world\"}\n\n\n            def test_my_handler() -> None:\n                with create_test_client(my_handler) as client:\n                    response = client.get(\"/some-path\")\n                    assert response.json() == {\"hello\": \"world\"}\n\n    Args:\n        route_handlers: A single handler or a sequence of route handlers, which can include instances of\n            :class:`Router <litestar.router.Router>`, subclasses of :class:`Controller <.controller.Controller>` or\n            any function decorated by the route handler decorators.\n        backend: The async backend to use, options are \"asyncio\" or \"trio\".\n        backend_options: ``anyio`` options.\n        base_url: URL scheme and domain for test request paths, e.g. ``http://testserver``.\n        raise_server_exceptions: Flag for underlying the test client to raise server exceptions instead of wrapping them\n            in an HTTP response.\n        root_path: Path prefix for requests.\n        session_config: Configuration for Session Middleware class to create raw session cookies for request to the\n            route handlers.\n        after_exception: A sequence of :class:`exception hook handlers <.types.AfterExceptionHookHandler>`. This\n            hook is called after an exception occurs. In difference to exception handlers, it is not meant to\n            return a response - only to process the exception (e.g. log it, send it to Sentry etc.).\n        after_request: A sync or async function executed after the route handler function returned and the response\n            object has been resolved. Receives the response object.\n        after_response: A sync or async function called after the response has been awaited. It receives the\n            :class:`Request <.connection.Request>` object and should not return any values.\n        allowed_hosts: A sequence of allowed hosts, or an\n            :class:`AllowedHostsConfig <.config.allowed_hosts.AllowedHostsConfig>` instance. Enables the builtin\n            allowed hosts middleware.\n        before_request: A sync or async function called immediately before calling the route handler. Receives the\n            :class:`Request <.connection.Request>` instance and any non-``None`` return value is used for the\n            response, bypassing the route handler.\n        before_send: A sequence of :class:`before send hook handlers <.types.BeforeMessageSendHookHandler>`. Called\n            when the ASGI send function is called.\n        cache_control: A ``cache-control`` header of type\n            :class:`CacheControlHeader <litestar.datastructures.CacheControlHeader>` to add to route handlers of\n            this app. Can be overridden by route handlers.\n        compression_config: Configures compression behaviour of the application, this enabled a builtin or user\n            defined Compression middleware.\n        cors_config: If set, configures CORS handling for the application.\n        csrf_config: If set, configures :class:`CSRFMiddleware <.middleware.csrf.CSRFMiddleware>`.\n        debug: If ``True``, app errors rendered as HTML with a stack trace.\n        dependencies: A string keyed mapping of dependency :class:`Providers <.di.Provide>`.\n        dto: :class:`AbstractDTO <.dto.base_dto.AbstractDTO>` to use for (de)serializing and\n            validation of request data.\n        etag: An ``etag`` header of type :class:`ETag <.datastructures.ETag>` to add to route handlers of this app.\n            Can be overridden by route handlers.\n        event_emitter_backend: A subclass of\n            :class:`BaseEventEmitterBackend <.events.emitter.BaseEventEmitterBackend>`.\n        exception_handlers: A mapping of status codes and/or exception types to handler functions.\n        guards: A sequence of :class:`Guard <.types.Guard>` callables.\n        include_in_schema: A boolean flag dictating whether  the route handler should be documented in the OpenAPI schema.\n        lifespan: A list of callables returning async context managers, wrapping the lifespan of the ASGI application\n        listeners: A sequence of :class:`EventListener <.events.listener.EventListener>`.\n        logging_config: A subclass of :class:`BaseLoggingConfig <.logging.config.BaseLoggingConfig>`.\n        middleware: A sequence of :class:`Middleware <.types.Middleware>`.\n        multipart_form_part_limit: The maximal number of allowed parts in a multipart/formdata request. This limit\n            is intended to protect from DoS attacks.\n        on_app_init: A sequence of :class:`OnAppInitHandler <.types.OnAppInitHandler>` instances. Handlers receive\n            an instance of :class:`AppConfig <.config.app.AppConfig>` that will have been initially populated with\n            the parameters passed to :class:`Litestar <litestar.app.Litestar>`, and must return an instance of same.\n            If more than one handler is registered they are called in the order they are provided.\n        on_shutdown: A sequence of :class:`LifespanHook <.types.LifespanHook>` called during application\n            shutdown.\n        on_startup: A sequence of :class:`LifespanHook <litestar.types.LifespanHook>` called during\n            application startup.\n        openapi_config: Defaults to :attr:`DEFAULT_OPENAPI_CONFIG`\n        opt: A string keyed mapping of arbitrary values that can be accessed in :class:`Guards <.types.Guard>` or\n            wherever you have access to :class:`Request <litestar.connection.request.Request>` or\n            :class:`ASGI Scope <.types.Scope>`.\n        parameters: A mapping of :class:`Parameter <.params.Parameter>` definitions available to all application\n            paths.\n        path: A path fragment that is prefixed to all route handlers, controllers and routers associated\n            with the application instance.\n\n            .. versionadded:: 2.8.0\n        pdb_on_exception: Drop into the PDB when an exception occurs.\n        plugins: Sequence of plugins.\n        request_class: An optional subclass of :class:`Request <.connection.Request>` to use for http connections.\n        response_class: A custom subclass of :class:`Response <.response.Response>` to be used as the app's default\n            response.\n        response_cookies: A sequence of :class:`Cookie <.datastructures.Cookie>`.\n        response_headers: A string keyed mapping of :class:`ResponseHeader <.datastructures.ResponseHeader>`\n        response_cache_config: Configures caching behavior of the application.\n        return_dto: :class:`AbstractDTO <.dto.base_dto.AbstractDTO>` to use for serializing\n            outbound response data.\n        route_handlers: A sequence of route handlers, which can include instances of\n            :class:`Router <.router.Router>`, subclasses of :class:`Controller <.controller.Controller>` or any\n            callable decorated by the route handler decorators.\n        security: A sequence of dicts that will be added to the schema of all route handlers in the application.\n            See\n            :data:`SecurityRequirement <.openapi.spec.SecurityRequirement>` for details.\n        signature_namespace: A mapping of names to types for use in forward reference resolution during signature modeling.\n        signature_types: A sequence of types for use in forward reference resolution during signature modeling.\n            These types will be added to the signature namespace using their ``__name__`` attribute.\n        state: An optional :class:`State <.datastructures.State>` for application state.\n        static_files_config: A sequence of :class:`StaticFilesConfig <.static_files.StaticFilesConfig>`\n        stores: Central registry of :class:`Store <.stores.base.Store>` that will be available throughout the\n            application. If this is a dictionary to it will be passed to a\n            :class:`StoreRegistry <.stores.registry.StoreRegistry>`. If it is a\n            :class:`StoreRegistry <.stores.registry.StoreRegistry>`, this instance will be used directly.\n        tags: A sequence of string tags that will be appended to the schema of all route handlers under the\n            application.\n        template_config: An instance of :class:`TemplateConfig <.template.TemplateConfig>`\n        timeout: Request timeout\n        type_encoders: A mapping of types to callables that transform them into types supported for serialization.\n        websocket_class: An optional subclass of :class:`WebSocket <.connection.WebSocket>` to use for websocket\n            connections.\n        experimental_features: An iterable of experimental features to enable\n\n\n    Returns:\n        An instance of :class:`TestClient <.testing.TestClient>` with a created app instance.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nclass LitestarTestClient:\n    def __init__(", "  ```\nasync def create_test_client(\n    route_hand", "  ```\ndef create_test_client(route_handlers, *"]}, "litestar-litestar/testing/client/sync_client.py-websocket_connect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/testing/client/sync_client.py:\n```\nfrom __future__ import annotations\n\nfrom contextlib import ExitStack\nfrom typing import TYPE_CHECKING, Any, Generic, Mapping, Sequence, TypeVar\n\nfrom httpx import USE_CLIENT_DEFAULT, Client\n\nfrom litestar.testing.client.base import BaseTestClient\nfrom litestar.testing.life_span_handler import LifeSpanHandler\nfrom litestar.testing.transport import ConnectionUpgradeExceptionError, TestClientTransport\nfrom litestar.types import AnyIOBackend, ASGIApp\n\nif TYPE_CHECKING:\n    from httpx._client import UseClientDefault\n    from httpx._types import (\n        AuthTypes,\n        CookieTypes,\n        HeaderTypes,\n        QueryParamTypes,\n        TimeoutTypes,\n    )\n    from typing_extensions import Self\n\n    from litestar.middleware.session.base import BaseBackendConfig\n    from litestar.testing.websocket_test_session import WebSocketTestSession\n\n\nT = TypeVar(\"T\", bound=ASGIApp)\n\n\nclass TestClient(Client, BaseTestClient, Generic[T]):  # type: ignore[misc]\n    lifespan_handler: LifeSpanHandler[Any]\n    exit_stack: ExitStack\n\n    def __init__(\n        self,\n        app: T,\n        base_url: str = \"http://testserver.local\",\n        raise_server_exceptions: bool = True,\n        root_path: str = \"\",\n        backend: AnyIOBackend = \"asyncio\",\n        backend_options: Mapping[str, Any] | None = None,\n        session_config: BaseBackendConfig | None = None,\n        timeout: float | None = None,\n        cookies: CookieTypes | None = None,\n    ) -> None:\n        \"\"\"A client implementation providing a context manager for testing applications.\n\n        Args:\n            app: The instance of :class:`Litestar <litestar.app.Litestar>` under test.\n            base_url: URL scheme and domain for test request paths, e.g. ``http://testserver``.\n            raise_server_exceptions: Flag for the underlying test client to raise server exceptions instead of\n                wrapping them in an HTTP response.\n            root_path: Path prefix for requests.\n            backend: The async backend to use, options are \"asyncio\" or \"trio\".\n            backend_options: ``anyio`` options.\n            session_config: Configuration for Session Middleware class to create raw session cookies for request to the\n                route handlers.\n            timeout: Request timeout\n            cookies: Cookies to set on the client.\n        \"\"\"\n        BaseTestClient.__init__(\n            self,\n            app=app,\n            base_url=base_url,\n            backend=backend,\n            backend_options=backend_options,\n            session_config=session_config,\n            cookies=cookies,\n        )\n\n        Client.__init__(\n            self,\n            base_url=base_url,\n            headers={\"user-agent\": \"testclient\"},\n            follow_redirects=True,\n            cookies=cookies,\n            transport=TestClientTransport(  # type: ignore[arg-type]\n                client=self,\n                raise_server_exceptions=raise_server_exceptions,\n                root_path=root_path,\n            ),\n            timeout=timeout,\n        )\n\n    def __enter__(self) -> Self:\n        with ExitStack() as stack:\n            self.blocking_portal = portal = stack.enter_context(self.portal())\n            self.lifespan_handler = LifeSpanHandler(client=self)\n\n            @stack.callback\n            def reset_portal() -> None:\n                delattr(self, \"blocking_portal\")\n\n            @stack.callback\n            def wait_shutdown() -> None:\n                portal.call(self.lifespan_handler.wait_shutdown)\n\n            self.exit_stack = stack.pop_all()\n\n        return self\n\n    def __exit__(self, *args: Any) -> None:\n        self.exit_stack.close()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def set_session_data(self, data: dict[str, Any]) -> None:\n        \"\"\"Set session data.\n\n        Args:\n            data: Session data\n\n        Returns:\n            None\n\n        Examples:\n            .. code-block:: python\n\n                from litestar import Litestar, get\n                from litestar.middleware.session.memory_backend import MemoryBackendConfig\n\n                session_config = MemoryBackendConfig()\n\n\n                @get(path=\"/test\")\n                def get_session_data(request: Request) -> Dict[str, Any]:\n                    return request.session\n\n\n                app = Litestar(\n                    route_handlers=[get_session_data], middleware=[session_config.middleware]\n                )\n\n                with TestClient(app=app, session_config=session_config) as client:\n                    client.set_session_data({\"foo\": \"bar\"})\n                    assert client.get(\"/test\").json() == {\"foo\": \"bar\"}\n\n        \"\"\"\n        with self.portal() as portal:\n            portal.call(self._set_session_data, data)\n\n    def get_session_data(self) -> dict[str, Any]:\n        \"\"\"Get session data.\n\n        Returns:\n            A dictionary containing session data.\n\n        Examples:\n            .. code-block:: python\n\n                from litestar import Litestar, post\n                from litestar.middleware.session.memory_backend import MemoryBackendConfig\n\n                session_config = MemoryBackendConfig()\n\n\n                @post(path=\"/test\")\n                def set_session_data(request: Request) -> None:\n                    request.session[\"foo\"] == \"bar\"\n\n\n                app = Litestar(\n                    route_handlers=[set_session_data], middleware=[session_config.middleware]\n                )\n\n                with TestClient(app=app, session_config=session_config) as client:\n                    client.post(\"/test\")\n                    assert client.get_session_data() == {\"foo\": \"bar\"}\n\n        \"\"\"\n        with self.portal() as portal:\n            return portal.call(self._get_session_data)\n\n```\n# Function Signature: def websocket_connect(\n# Natural Language Annotations: \"\"\"Sends a GET request to establish a websocket connection.\n\n        Args:\n            url: Request URL.\n            subprotocols: Websocket subprotocols.\n            params: Query parameters.\n            headers: Request headers.\n            cookies: Request cookies.\n            auth: Auth headers.\n            follow_redirects: Whether to follow redirects.\n            timeout: Request timeout.\n            extensions: Dictionary of ASGI extensions.\n\n        Returns:\n            A `WebSocketTestSession <litestar.testing.WebSocketTestSession>` instance.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef websocket_connect(\n    self,\n    url", "  ```\ndef websocket_connect(\n        self,\n        url", "  ```\ndef websocket_connect(\n    url: str,\n"]}, "litestar-litestar/contrib/pydantic/pydantic_schema_plugin.py-for_pydantic_model": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/contrib/pydantic/pydantic_schema_plugin.py:\n```\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Optional\n\nfrom typing_extensions import Annotated\n\nfrom litestar.contrib.pydantic.utils import (\n    create_field_definitions_for_computed_fields,\n    is_pydantic_2_model,\n    is_pydantic_constrained_field,\n    is_pydantic_model_class,\n    is_pydantic_undefined,\n    is_pydantic_v2,\n    pydantic_get_type_hints_with_generics_resolved,\n    pydantic_unwrap_and_get_origin,\n)\nfrom litestar.exceptions import MissingDependencyException\nfrom litestar.openapi.spec import OpenAPIFormat, OpenAPIType, Schema\nfrom litestar.plugins import OpenAPISchemaPlugin\nfrom litestar.types import Empty\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils import is_class_and_subclass, is_generic\n\ntry:\n    import pydantic as _  # noqa: F401\nexcept ImportError as e:\n    raise MissingDependencyException(\"pydantic\") from e\n\ntry:\n    import pydantic as pydantic_v2\n\n    if not is_pydantic_v2(pydantic_v2):\n        raise ImportError\n\n    from pydantic import v1 as pydantic_v1\nexcept ImportError:\n    import pydantic as pydantic_v1  # type: ignore[no-redef]\n\n    pydantic_v2 = None  # type: ignore[assignment]\n\nif TYPE_CHECKING:\n    from litestar._openapi.schema_generation.schema import SchemaCreator\n\nPYDANTIC_TYPE_MAP: dict[type[Any] | None | Any, Schema] = {\n    pydantic_v1.ByteSize: Schema(type=OpenAPIType.INTEGER),\n    pydantic_v1.EmailStr: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.EMAIL),\n    pydantic_v1.IPvAnyAddress: Schema(\n        one_of=[\n            Schema(\n                type=OpenAPIType.STRING,\n                format=OpenAPIFormat.IPV4,\n                description=\"IPv4 address\",\n            ),\n            Schema(\n                type=OpenAPIType.STRING,\n                format=OpenAPIFormat.IPV6,\n                description=\"IPv6 address\",\n            ),\n        ]\n    ),\n    pydantic_v1.IPvAnyInterface: Schema(\n        one_of=[\n            Schema(\n                type=OpenAPIType.STRING,\n                format=OpenAPIFormat.IPV4,\n                description=\"IPv4 interface\",\n            ),\n            Schema(\n                type=OpenAPIType.STRING,\n                format=OpenAPIFormat.IPV6,\n                description=\"IPv6 interface\",\n            ),\n        ]\n    ),\n    pydantic_v1.IPvAnyNetwork: Schema(\n        one_of=[\n            Schema(\n                type=OpenAPIType.STRING,\n                format=OpenAPIFormat.IPV4,\n                description=\"IPv4 network\",\n            ),\n            Schema(\n                type=OpenAPIType.STRING,\n                format=OpenAPIFormat.IPV6,\n                description=\"IPv6 network\",\n            ),\n        ]\n    ),\n    pydantic_v1.Json: Schema(type=OpenAPIType.OBJECT, format=OpenAPIFormat.JSON_POINTER),\n    pydantic_v1.NameEmail: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.EMAIL, description=\"Name and email\"),\n    # removed in v2\n    pydantic_v1.PyObject: Schema(\n        type=OpenAPIType.STRING,\n        description=\"dot separated path identifying a python object, e.g. 'decimal.Decimal'\",\n    ),\n    # annotated in v2\n    pydantic_v1.UUID1: Schema(\n        type=OpenAPIType.STRING,\n        format=OpenAPIFormat.UUID,\n        description=\"UUID1 string\",\n    ),\n    pydantic_v1.UUID3: Schema(\n        type=OpenAPIType.STRING,\n        format=OpenAPIFormat.UUID,\n        description=\"UUID3 string\",\n    ),\n    pydantic_v1.UUID4: Schema(\n        type=OpenAPIType.STRING,\n        format=OpenAPIFormat.UUID,\n        description=\"UUID4 string\",\n    ),\n    pydantic_v1.UUID5: Schema(\n        type=OpenAPIType.STRING,\n        format=OpenAPIFormat.UUID,\n        description=\"UUID5 string\",\n    ),\n    pydantic_v1.DirectoryPath: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.URI_REFERENCE),\n    pydantic_v1.AnyUrl: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.URL),\n    pydantic_v1.AnyHttpUrl: Schema(\n        type=OpenAPIType.STRING, format=OpenAPIFormat.URL, description=\"must be a valid HTTP based URL\"\n    ),\n    pydantic_v1.FilePath: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.URI_REFERENCE),\n    pydantic_v1.HttpUrl: Schema(\n        type=OpenAPIType.STRING,\n        format=OpenAPIFormat.URL,\n        description=\"must be a valid HTTP based URL\",\n        max_length=2083,\n    ),\n    pydantic_v1.RedisDsn: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.URI, description=\"redis DSN\"),\n    pydantic_v1.PostgresDsn: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.URI, description=\"postgres DSN\"),\n    pydantic_v1.SecretBytes: Schema(type=OpenAPIType.STRING),\n    pydantic_v1.SecretStr: Schema(type=OpenAPIType.STRING),\n    pydantic_v1.StrictBool: Schema(type=OpenAPIType.BOOLEAN),\n    pydantic_v1.StrictBytes: Schema(type=OpenAPIType.STRING),\n    pydantic_v1.StrictFloat: Schema(type=OpenAPIType.NUMBER),\n    pydantic_v1.StrictInt: Schema(type=OpenAPIType.INTEGER),\n    pydantic_v1.StrictStr: Schema(type=OpenAPIType.STRING),\n    pydantic_v1.NegativeFloat: Schema(type=OpenAPIType.NUMBER, exclusive_maximum=0.0),\n    pydantic_v1.NegativeInt: Schema(type=OpenAPIType.INTEGER, exclusive_maximum=0),\n    pydantic_v1.NonNegativeInt: Schema(type=OpenAPIType.INTEGER, minimum=0),\n    pydantic_v1.NonPositiveFloat: Schema(type=OpenAPIType.NUMBER, maximum=0.0),\n    pydantic_v1.PaymentCardNumber: Schema(type=OpenAPIType.STRING, min_length=12, max_length=19),\n    pydantic_v1.PositiveFloat: Schema(type=OpenAPIType.NUMBER, exclusive_minimum=0.0),\n    pydantic_v1.PositiveInt: Schema(type=OpenAPIType.INTEGER, exclusive_minimum=0),\n}\n\nif pydantic_v2 is not None:  # pragma: no cover\n    PYDANTIC_TYPE_MAP.update(\n        {\n            pydantic_v2.SecretStr: Schema(type=OpenAPIType.STRING),\n            pydantic_v2.SecretBytes: Schema(type=OpenAPIType.STRING),\n            pydantic_v2.ByteSize: Schema(type=OpenAPIType.INTEGER),\n            pydantic_v2.EmailStr: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.EMAIL),\n            pydantic_v2.IPvAnyAddress: Schema(\n                one_of=[\n                    Schema(\n                        type=OpenAPIType.STRING,\n                        format=OpenAPIFormat.IPV4,\n                        description=\"IPv4 address\",\n                    ),\n                    Schema(\n                        type=OpenAPIType.STRING,\n                        format=OpenAPIFormat.IPV6,\n                        description=\"IPv6 address\",\n                    ),\n                ]\n            ),\n            pydantic_v2.IPvAnyInterface: Schema(\n                one_of=[\n                    Schema(\n                        type=OpenAPIType.STRING,\n                        format=OpenAPIFormat.IPV4,\n                        description=\"IPv4 interface\",\n                    ),\n                    Schema(\n                        type=OpenAPIType.STRING,\n                        format=OpenAPIFormat.IPV6,\n                        description=\"IPv6 interface\",\n                    ),\n                ]\n            ),\n            pydantic_v2.IPvAnyNetwork: Schema(\n                one_of=[\n                    Schema(\n                        type=OpenAPIType.STRING,\n                        format=OpenAPIFormat.IPV4,\n                        description=\"IPv4 network\",\n                    ),\n                    Schema(\n                        type=OpenAPIType.STRING,\n                        format=OpenAPIFormat.IPV6,\n                        description=\"IPv6 network\",\n                    ),\n                ]\n            ),\n            pydantic_v2.Json: Schema(type=OpenAPIType.OBJECT, format=OpenAPIFormat.JSON_POINTER),\n            pydantic_v2.NameEmail: Schema(\n                type=OpenAPIType.STRING, format=OpenAPIFormat.EMAIL, description=\"Name and email\"\n            ),\n            pydantic_v2.AnyUrl: Schema(type=OpenAPIType.STRING, format=OpenAPIFormat.URL),\n        }\n    )\n\n\n_supported_types = (pydantic_v1.BaseModel, *PYDANTIC_TYPE_MAP.keys())\nif pydantic_v2 is not None:  # pragma: no cover\n    _supported_types = (pydantic_v2.BaseModel, *_supported_types)\n\n\nclass PydanticSchemaPlugin(OpenAPISchemaPlugin):\n    __slots__ = (\"prefer_alias\",)\n\n    def __init__(self, prefer_alias: bool = False) -> None:\n        self.prefer_alias = prefer_alias\n\n    @staticmethod\n    def is_plugin_supported_type(value: Any) -> bool:\n        return isinstance(value, _supported_types) or is_class_and_subclass(value, _supported_types)  # type: ignore[arg-type]\n\n    @staticmethod\n    def is_undefined_sentinel(value: Any) -> bool:\n        return is_pydantic_undefined(value)\n\n    @staticmethod\n    def is_constrained_field(field_definition: FieldDefinition) -> bool:\n        return is_pydantic_constrained_field(field_definition.annotation)\n\n    def to_openapi_schema(self, field_definition: FieldDefinition, schema_creator: SchemaCreator) -> Schema:\n        \"\"\"Given a type annotation, transform it into an OpenAPI schema class.\n\n        Args:\n            field_definition: FieldDefinition instance.\n            schema_creator: An instance of the schema creator class\n\n        Returns:\n            An :class:`OpenAPI <litestar.openapi.spec.schema.Schema>` instance.\n        \"\"\"\n        if schema_creator.prefer_alias != self.prefer_alias:\n            schema_creator.prefer_alias = True\n        if is_pydantic_model_class(field_definition.annotation):\n            return self.for_pydantic_model(field_definition=field_definition, schema_creator=schema_creator)\n        return PYDANTIC_TYPE_MAP[field_definition.annotation]  # pragma: no cover\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def for_pydantic_model(cls, field_definition: FieldDefinition, schema_creator: SchemaCreator) -> Schema:  # pyright: ignore\n# Natural Language Annotations: \"\"\"Create a schema object for a given pydantic model class.\n\n        Args:\n            field_definition: FieldDefinition instance.\n            schema_creator: An instance of the schema creator class\n\n        Returns:\n            A schema instance.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef for_pydantic_model(cls, field_", "  ```\ndef for_pydantic_model(cls, field_", "  ```\ndef for_pydantic_model(cls, field_"]}, "litestar-litestar/_kwargs/cleanup.py-cleanup": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/_kwargs/cleanup.py:\n```\nfrom __future__ import annotations\n\nfrom inspect import Traceback, isasyncgen\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, Awaitable, Callable, Generator\n\nfrom anyio import create_task_group\n\nfrom litestar.utils import ensure_async_callable\nfrom litestar.utils.compat import async_next\n\n__all__ = (\"DependencyCleanupGroup\",)\n\n\nif TYPE_CHECKING:\n    from litestar.types import AnyGenerator\n\n\nclass DependencyCleanupGroup:\n    \"\"\"Wrapper for generator based dependencies.\n\n    Simplify cleanup by wrapping :func:`next` / :func:`anext` calls and providing facilities to\n    :meth:`throw <generator.throw>` / :meth:`athrow <agen.athrow>` into all generators consecutively. An instance of\n    this class can be used as a contextmanager, which will automatically throw any exceptions into its generators. All\n    exceptions caught in this manner will be re-raised after they have been thrown in the generators.\n    \"\"\"\n\n    __slots__ = (\"_generators\", \"_closed\")\n\n    def __init__(self, generators: list[AnyGenerator] | None = None) -> None:\n        \"\"\"Initialize ``DependencyCleanupGroup``.\n\n        Args:\n            generators: An optional list of generators to be called at cleanup\n        \"\"\"\n        self._generators = generators or []\n        self._closed = False\n\n    def add(self, generator: Generator[Any, None, None] | AsyncGenerator[Any, None]) -> None:\n        \"\"\"Add a new generator to the group.\n\n        Args:\n            generator: The generator to add\n\n        Returns:\n            None\n        \"\"\"\n        if self._closed:\n            raise RuntimeError(\"Cannot call cleanup on a closed DependencyCleanupGroup\")\n        self._generators.append(generator)\n\n    @staticmethod\n    def _wrap_next(generator: AnyGenerator) -> Callable[[], Awaitable[None]]:\n        if isasyncgen(generator):\n\n            async def wrapped_async() -> None:\n                await async_next(generator, None)\n\n            return wrapped_async\n\n        def wrapped() -> None:\n            next(generator, None)  # type: ignore[arg-type]\n\n        return ensure_async_callable(wrapped)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def __aenter__(self) -> None:\n        \"\"\"Support the async contextmanager protocol to allow for easier catching and throwing of exceptions into the\n        generators.\n        \"\"\"\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: Traceback | None,\n    ) -> None:\n        \"\"\"If an exception was raised within the contextmanager block, throw it into all generators.\"\"\"\n        if exc_val:\n            await self.throw(exc_val)\n\n    async def throw(self, exc: BaseException) -> None:\n        \"\"\"Throw an exception in all generators sequentially.\n\n        Args:\n            exc: Exception to throw\n        \"\"\"\n        for gen in self._generators:\n            try:\n                if isasyncgen(gen):\n                    await gen.athrow(exc)\n                else:\n                    gen.throw(exc)  # type: ignore[union-attr]\n            except (StopIteration, StopAsyncIteration):\n                continue\n\n```\n# Function Signature: async def cleanup(self) -> None:\n# Natural Language Annotations: \"\"\"Execute cleanup by calling :func:`next` / :func:`anext` on all generators.\n\n        If there are multiple generators to be called, they will be executed in a :class:`anyio.TaskGroup`.\n\n        Returns:\n            None\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def cleanup(self) -> None:\n    \"\"\"", "  ```\nasync def cleanup(self):\n    \"\"\"Execute cleanup", "  ```\nasync def cleanup(self) -> None:\n    if"]}, "litestar-litestar/_asgi/routing_trie/mapping.py-build_route_middleware_stack": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/_asgi/routing_trie/mapping.py:\n```\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, cast\n\nfrom litestar._asgi.routing_trie.types import (\n    ASGIHandlerTuple,\n    PathParameterSentinel,\n    create_node,\n)\nfrom litestar._asgi.utils import wrap_in_exception_handler\nfrom litestar.types.internal_types import PathParameterDefinition\n\n__all__ = (\"add_mount_route\", \"add_route_to_trie\", \"build_route_middleware_stack\", \"configure_node\")\n\n\nif TYPE_CHECKING:\n    from litestar._asgi.routing_trie.types import RouteTrieNode\n    from litestar.app import Litestar\n    from litestar.routes import ASGIRoute, HTTPRoute, WebSocketRoute\n    from litestar.types import ASGIApp, RouteHandlerType\n\n\ndef add_mount_route(\n    current_node: RouteTrieNode,\n    mount_routes: dict[str, RouteTrieNode],\n    root_node: RouteTrieNode,\n    route: ASGIRoute,\n) -> RouteTrieNode:\n    \"\"\"Add a node for a mount route.\n\n    Args:\n        current_node: The current trie node that is being mapped.\n        mount_routes: A dictionary mapping static routes to trie nodes.\n        root_node: The root trie node.\n        route: The route that is being added.\n\n    Returns:\n        A trie node.\n    \"\"\"\n\n    # we need to ensure that we can traverse the map both through the full path key, e.g. \"/my-route/sub-path\" and\n    # via the components keys [\"my-route, \"sub-path\"]\n    if route.path not in current_node.children:\n        root_node = current_node\n        for component in route.path_components:\n            if component not in current_node.children:\n                current_node.children[component] = create_node()  # type: ignore[index]\n            current_node = current_node.children[component]  # type: ignore[index]\n\n    current_node.is_mount = True\n    current_node.is_static = route.route_handler.is_static\n\n    if route.path != \"/\":\n        mount_routes[route.path] = root_node.children[route.path] = current_node\n    else:\n        mount_routes[route.path] = current_node\n\n    return current_node\n\n\ndef add_route_to_trie(\n    app: Litestar,\n    mount_routes: dict[str, RouteTrieNode],\n    plain_routes: set[str],\n    root_node: RouteTrieNode,\n    route: HTTPRoute | WebSocketRoute | ASGIRoute,\n) -> RouteTrieNode:\n    \"\"\"Add a new route path (e.g. '/foo/bar/{param:int}') into the route_map tree.\n\n    Inserts non-parameter paths ('plain routes') off the tree's root\n    node. For paths containing parameters, splits the path on '/' and\n    nests each path segment under the previous segment's node (see\n    prefix tree / trie).\n\n    Args:\n        app: The Litestar app instance.\n        mount_routes: A dictionary mapping static routes to trie nodes.\n        plain_routes: A set of routes that do not have path parameters.\n        root_node: The root trie node.\n        route: The route that is being added.\n\n    Returns:\n        A RouteTrieNode instance.\n    \"\"\"\n    current_node = root_node\n\n    has_path_parameters = bool(route.path_parameters)\n\n    if (route_handler := getattr(route, \"route_handler\", None)) and getattr(route_handler, \"is_mount\", False):\n        current_node = add_mount_route(\n            current_node=current_node,\n            mount_routes=mount_routes,\n            root_node=root_node,\n            route=cast(\"ASGIRoute\", route),\n        )\n\n    elif not has_path_parameters:\n        plain_routes.add(route.path)\n        if route.path not in root_node.children:\n            current_node.children[route.path] = create_node()\n        current_node = root_node.children[route.path]\n\n    else:\n        for component in route.path_components:\n            if isinstance(component, PathParameterDefinition):\n                current_node.is_path_param_node = True\n                next_node_key: type[PathParameterSentinel] | str = PathParameterSentinel\n\n            else:\n                next_node_key = component\n\n            if next_node_key not in current_node.children:\n                current_node.children[next_node_key] = create_node()\n\n            current_node.child_keys = set(current_node.children.keys())\n            current_node = current_node.children[next_node_key]\n\n            if isinstance(component, PathParameterDefinition) and component.type is Path:\n                current_node.is_path_type = True\n\n    configure_node(route=route, app=app, node=current_node)\n    return current_node\n\n\ndef configure_node(\n    app: Litestar,\n    route: HTTPRoute | WebSocketRoute | ASGIRoute,\n    node: RouteTrieNode,\n) -> None:\n    \"\"\"Set required attributes and route handlers on route_map tree node.\n\n    Args:\n        app: The Litestar app instance.\n        route: The route that is being added.\n        node: The trie node being configured.\n\n    Returns:\n        None\n    \"\"\"\n    from litestar.routes import HTTPRoute, WebSocketRoute\n\n    if not node.path_parameters:\n        node.path_parameters = {}\n\n    if isinstance(route, HTTPRoute):\n        for method, handler_mapping in route.route_handler_map.items():\n            handler, _ = handler_mapping\n            node.asgi_handlers[method] = ASGIHandlerTuple(\n                asgi_app=build_route_middleware_stack(app=app, route=route, route_handler=handler),\n                handler=handler,\n            )\n            node.path_parameters[method] = tuple(route.path_parameters.values())\n\n    elif isinstance(route, WebSocketRoute):\n        node.asgi_handlers[\"websocket\"] = ASGIHandlerTuple(\n            asgi_app=build_route_middleware_stack(app=app, route=route, route_handler=route.route_handler),\n            handler=route.route_handler,\n        )\n        node.path_parameters[\"websocket\"] = tuple(route.path_parameters.values())\n\n    else:\n        node.asgi_handlers[\"asgi\"] = ASGIHandlerTuple(\n            asgi_app=build_route_middleware_stack(app=app, route=route, route_handler=route.route_handler),\n            handler=route.route_handler,\n        )\n        node.path_parameters[\"asgi\"] = tuple(route.path_parameters.values())\n        node.is_asgi = True\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def build_route_middleware_stack(\n# Natural Language Annotations: \"\"\"Construct a middleware stack that serves as the point of entry for each route.\n\n    Args:\n        app: The Litestar app instance.\n        route: The route that is being added.\n        route_handler: The route handler that is being wrapped.\n\n    Returns:\n        An ASGIApp that is composed of a \"stack\" of middlewares.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef build_route_middleware_stack(app: Lit", "  ```\ndef build_route_middleware_stack(app: Lit", "  ```\ndef build_route_middleware_stack(app, route"]}, "litestar-litestar/utils/signature.py-from_fn": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/signature.py:\n```\nfrom __future__ import annotations\n\nimport sys\nimport typing\nfrom copy import deepcopy\nfrom dataclasses import dataclass, replace\nfrom inspect import Signature, getmembers, isclass, ismethod\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Any, Union\n\nfrom typing_extensions import Annotated, Self, get_args, get_origin, get_type_hints\n\nfrom litestar import connection, datastructures, types\nfrom litestar.exceptions import ImproperlyConfiguredException\nfrom litestar.types import Empty\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils.typing import expand_type_var_in_type_hint, unwrap_annotation\n\nif TYPE_CHECKING:\n    from typing import Sequence\n\n    from litestar.types import AnyCallable\n\nif sys.version_info < (3, 11):\n    from typing import _get_defaults  # type: ignore[attr-defined]\nelse:\n\n    def _get_defaults(_: Any) -> Any: ...\n\n\n__all__ = (\n    \"add_types_to_signature_namespace\",\n    \"get_fn_type_hints\",\n    \"ParsedSignature\",\n)\n\n_GLOBAL_NAMES = {\n    namespace: export\n    for namespace, export in chain(\n        tuple(getmembers(types)), tuple(getmembers(connection)), tuple(getmembers(datastructures))\n    )\n    if namespace[0].isupper() and namespace in chain(types.__all__, connection.__all__, datastructures.__all__)  # pyright: ignore\n}\n\"\"\"A mapping of names used for handler signature forward-ref resolution.\n\nThis allows users to include these names within an `if TYPE_CHECKING:` block in their handler module.\n\"\"\"\n\n\ndef _unwrap_implicit_optional_hints(defaults: dict[str, Any], hints: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Unwrap implicit optional hints.\n\n    On Python<3.11, if a function parameter annotation has a ``None`` default, it is unconditionally wrapped in an\n    ``Optional`` type.\n\n    If the annotation is not annotated, then any nested unions are flattened, e.g.,:\n\n    .. code-block:: python\n\n        def foo(a: Optional[Union[str, int]] = None): ...\n\n    ...will become `Union[str, int, NoneType]`.\n\n    However, if the annotation is annotated, then we end up with an optional union around the annotated type, e.g.,:\n\n    .. code-block:: python\n\n        def foo(a: Annotated[Optional[Union[str, int]], ...] = None): ...\n\n    ... becomes `Union[Annotated[Union[str, int, NoneType], ...], NoneType]`\n\n    This function makes the latter case consistent with the former by either removing the outer union if it is redundant\n    or flattening the union if it is not. The latter case would become `Annotated[Union[str, int, NoneType], ...]`.\n\n    Args:\n        defaults: Mapping of names to default values.\n        hints: Mapping of names to types.\n\n    Returns:\n        Mapping of names to types.\n    \"\"\"\n\n    def _is_two_arg_optional(origin_: Any, args_: Any) -> bool:\n        \"\"\"Check if a type is a two-argument optional type.\n\n        If the type has been wrapped in `Optional` by `get_type_hints()` it will always be a union of a type and\n        `NoneType`.\n\n        See: https://github.com/litestar-org/litestar/pull/2516\n        \"\"\"\n        return origin_ is Union and len(args_) == 2 and args_[1] is type(None)\n\n    def _is_any_optional(origin_: Any, args_: tuple[Any, ...]) -> bool:\n        \"\"\"Detect if a type is a union with `NoneType`.\n\n        After detecting that a type is a two-argument optional type, this function can be used to detect if the\n        inner type is a union with `NoneType` at all.\n\n        We only want to perform the unwrapping of the optional union if the inner type is optional as well.\n        \"\"\"\n        return origin_ is Union and any(arg is type(None) for arg in args_)\n\n    for name, default in defaults.items():\n        if default is not None:\n            continue\n\n        hint = hints[name]\n        origin = get_origin(hint)\n        args = get_args(hint)\n\n        if _is_two_arg_optional(origin, args):\n            unwrapped_inner, meta, wrappers = unwrap_annotation(args[0])\n\n            if Annotated not in wrappers:\n                continue\n\n            inner_args = get_args(unwrapped_inner)\n\n            if not _is_any_optional(get_origin(unwrapped_inner), inner_args):\n                # this is where hint is like `Union[Annotated[Union[str, int], ...], NoneType]`, we add the outer union\n                # into the inner one, and re-wrap with Annotated\n                union_args = (*(inner_args or (unwrapped_inner,)), type(None))\n                # calling `__class_getitem__` directly as in earlier py vers it is a syntax error to unpack into\n                # the getitem brackets, e.g., Annotated[T, *meta].\n                hints[name] = Annotated.__class_getitem__((Union[union_args], *meta))  # type: ignore[attr-defined]\n                continue\n\n            # this is where hint is like `Union[Annotated[Union[str, NoneType], ...], NoneType]`, we remove the\n            # redundant outer union\n            hints[name] = args[0]\n    return hints\n\n\ndef get_fn_type_hints(fn: Any, namespace: dict[str, Any] | None = None) -> dict[str, Any]:\n    \"\"\"Resolve type hints for ``fn``.\n\n    Args:\n        fn: Callable that is being inspected\n        namespace: Extra names for resolution of forward references.\n\n    Returns:\n        Mapping of names to types.\n    \"\"\"\n    fn_to_inspect: Any = fn\n\n    module_name = fn_to_inspect.__module__\n\n    if isclass(fn_to_inspect):\n        fn_to_inspect = fn_to_inspect.__init__\n\n    # detect objects that are not functions and that have a `__call__` method\n    if callable(fn_to_inspect) and ismethod(fn_to_inspect.__call__):\n        fn_to_inspect = fn_to_inspect.__call__\n\n    # inspect the underlying function for methods\n    if hasattr(fn_to_inspect, \"__func__\"):\n        fn_to_inspect = fn_to_inspect.__func__\n\n    # Order important. If a litestar name has been overridden in the function module, we want\n    # to use that instead of the litestar one.\n    namespace = {\n        **_GLOBAL_NAMES,\n        **vars(typing),\n        **vars(sys.modules[module_name]),\n        **(namespace or {}),\n    }\n    hints = get_type_hints(fn_to_inspect, globalns=namespace, include_extras=True)\n\n    if sys.version_info < (3, 11):\n        # see https://github.com/litestar-org/litestar/pull/2516\n        defaults = _get_defaults(fn_to_inspect)\n        hints = _unwrap_implicit_optional_hints(defaults, hints)\n\n    return hints\n\n\n@dataclass(frozen=True)\nclass ParsedSignature:\n    \"\"\"Parsed signature.\n\n    This object is the primary source of handler/dependency signature information.\n\n    The only post-processing that occurs is the conversion of any forward referenced type annotations.\n    \"\"\"\n\n    __slots__ = (\"parameters\", \"return_type\", \"original_signature\")\n\n    parameters: dict[str, FieldDefinition]\n    \"\"\"A mapping of parameter names to ParsedSignatureParameter instances.\"\"\"\n    return_type: FieldDefinition\n    \"\"\"The return annotation of the callable.\"\"\"\n    original_signature: Signature\n    \"\"\"The raw signature as returned by :func:`inspect.signature`\"\"\"\n\n    def __deepcopy__(self, memo: dict[str, Any]) -> Self:\n        return type(self)(\n            parameters={k: deepcopy(v) for k, v in self.parameters.items()},\n            return_type=deepcopy(self.return_type),\n            original_signature=deepcopy(self.original_signature),\n        )\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def from_signature(cls, signature: Signature, fn_type_hints: dict[str, type]) -> Self:\n        \"\"\"Parse an :class:`inspect.Signature` instance.\n\n        Args:\n            signature: An :class:`inspect.Signature` instance.\n            fn_type_hints: mapping of types\n\n        Returns:\n            ParsedSignature\n        \"\"\"\n\n        parameters = tuple(\n            FieldDefinition.from_parameter(parameter=parameter, fn_type_hints=fn_type_hints)\n            for name, parameter in signature.parameters.items()\n            if name not in (\"self\", \"cls\")\n        )\n\n        return_type = FieldDefinition.from_annotation(fn_type_hints.get(\"return\", Any))\n\n        return cls(\n            parameters={p.name: p for p in parameters},\n            return_type=return_type if \"return\" in fn_type_hints else replace(return_type, annotation=Empty),\n            original_signature=signature,\n        )\n\n\ndef add_types_to_signature_namespace(\n    signature_types: Sequence[Any], signature_namespace: dict[str, Any]\n) -> dict[str, Any]:\n    \"\"\"Add types to ith signature namespace mapping.\n\n    Types are added mapped to their `__name__` attribute.\n\n    Args:\n        signature_types: A list of types to add to the signature namespace.\n        signature_namespace: The signature namespace to add types to.\n\n    Raises:\n        ImproperlyConfiguredException: If a type is already defined in the signature namespace.\n        AttributeError: If a type does not have a `__name__` attribute.\n\n    Returns:\n        The updated signature namespace.\n    \"\"\"\n    for typ in signature_types:\n        if (name := typ.__name__) in signature_namespace:\n            raise ImproperlyConfiguredException(f\"Type '{name}' is already defined in the signature namespace\")\n        signature_namespace[name] = typ\n    return signature_namespace\n\n```\n# Function Signature: def from_fn(cls, fn: AnyCallable, signature_namespace: dict[str, Any]) -> Self:\n# Natural Language Annotations: \"\"\"Parse a function signature.\n\n        Args:\n            fn: Any callable.\n            signature_namespace: mapping of names to types for forward reference resolution\n\n        Returns:\n            ParsedSignature\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef from_fn(cls, fn: AnyCallable,", "  ```\ndef from_fn(cls, fn: AnyCallable,", "  ```\n@classmethod\ndef from_fn(cls, fn:"]}, "litestar-litestar/utils/deprecation.py-deprecated": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/deprecation.py:\n```\nfrom __future__ import annotations\n\nimport inspect\nfrom functools import wraps\nfrom typing import Callable, Literal, TypeVar\nfrom warnings import warn\n\nfrom typing_extensions import ParamSpec\n\n__all__ = (\"deprecated\", \"warn_deprecation\")\n\n\nT = TypeVar(\"T\")\nP = ParamSpec(\"P\")\nDeprecatedKind = Literal[\"function\", \"method\", \"classmethod\", \"attribute\", \"property\", \"class\", \"parameter\", \"import\"]\n\n\ndef warn_deprecation(\n    version: str,\n    deprecated_name: str,\n    kind: DeprecatedKind,\n    *,\n    removal_in: str | None = None,\n    alternative: str | None = None,\n    info: str | None = None,\n    pending: bool = False,\n) -> None:\n    \"\"\"Warn about a call to a (soon to be) deprecated function.\n\n    Args:\n        version: Litestar version where the deprecation will occur\n        deprecated_name: Name of the deprecated function\n        removal_in: Litestar version where the deprecated function will be removed\n        alternative: Name of a function that should be used instead\n        info: Additional information\n        pending: Use ``PendingDeprecationWarning`` instead of ``DeprecationWarning``\n        kind: Type of the deprecated thing\n    \"\"\"\n    parts = []\n\n    if kind == \"import\":\n        access_type = \"Import of\"\n    elif kind in {\"function\", \"method\"}:\n        access_type = \"Call to\"\n    else:\n        access_type = \"Use of\"\n\n    if pending:\n        parts.append(f\"{access_type} {kind} awaiting deprecation {deprecated_name!r}\")\n    else:\n        parts.append(f\"{access_type} deprecated {kind} {deprecated_name!r}\")\n\n    parts.extend(\n        (\n            f\"Deprecated in litestar {version}\",\n            f\"This {kind} will be removed in {removal_in or 'the next major version'}\",\n        )\n    )\n    if alternative:\n        parts.append(f\"Use {alternative!r} instead\")\n\n    if info:\n        parts.append(info)\n\n    text = \". \".join(parts)\n    warning_class = PendingDeprecationWarning if pending else DeprecationWarning\n\n    warn(text, warning_class, stacklevel=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def deprecated(\n# Natural Language Annotations: \"\"\"Create a decorator wrapping a function, method or property with a warning call about a (pending) deprecation.\n\n    Args:\n        version: Litestar version where the deprecation will occur\n        removal_in: Litestar version where the deprecated function will be removed\n        alternative: Name of a function that should be used instead\n        info: Additional information\n        pending: Use ``PendingDeprecationWarning`` instead of ``DeprecationWarning``\n        kind: Type of the deprecated callable. If ``None``, will use ``inspect`` to figure\n            out if it's a function or method\n\n    Returns:\n        A decorator wrapping the function call with a warning\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef deprecated(\n    version: str,\n    removal_", "  ```\ndef deprecated(\n    version: str,\n    removal_", "  ```\ndef deprecated(\n    version,\n    removal_in="]}, "litestar-litestar/utils/sync.py-ensure_async_callable": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/sync.py:\n```\nfrom __future__ import annotations\n\nfrom typing import (\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    Generic,\n    Iterable,\n    Iterator,\n    TypeVar,\n)\n\nfrom typing_extensions import ParamSpec\n\nfrom litestar.concurrency import sync_to_thread\nfrom litestar.utils.predicates import is_async_callable\n\n__all__ = (\"ensure_async_callable\", \"AsyncIteratorWrapper\", \"AsyncCallable\", \"is_async_callable\")\n\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\n\n\n\n\n\n\n\n\n\n\nclass AsyncCallable:\n    \"\"\"Wrap a given callable to be called in a thread pool using\n    ``anyio.to_thread.run_sync``, keeping a reference to the original callable as\n    :attr:`func`\n    \"\"\"\n\n    def __init__(self, fn: Callable[P, T]) -> None:  # pyright: ignore\n        self.func = fn\n\n    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> Awaitable[T]:  # pyright: ignore\n        return sync_to_thread(self.func, *args, **kwargs)  # pyright: ignore\n\n\nclass AsyncIteratorWrapper(Generic[T]):\n    \"\"\"Asynchronous generator, wrapping an iterable or iterator.\"\"\"\n\n    __slots__ = (\"iterator\", \"generator\")\n\n    def __init__(self, iterator: Iterator[T] | Iterable[T]) -> None:\n        \"\"\"Take a sync iterator or iterable and yields values from it asynchronously.\n\n        Args:\n            iterator: A sync iterator or iterable.\n        \"\"\"\n        self.iterator = iterator if isinstance(iterator, Iterator) else iter(iterator)\n        self.generator = self._async_generator()\n\n    def _call_next(self) -> T:\n        try:\n            return next(self.iterator)\n        except StopIteration as e:\n            raise ValueError from e\n\n    async def _async_generator(self) -> AsyncGenerator[T, None]:\n        while True:\n            try:\n                yield await sync_to_thread(self._call_next)\n            except ValueError:\n                return\n\n    def __aiter__(self) -> AsyncIteratorWrapper[T]:\n        return self\n\n    async def __anext__(self) -> T:\n        return await self.generator.__anext__()\n\n```\n# Function Signature: def ensure_async_callable(fn: Callable[P, T]) -> Callable[P, Awaitable[T]]:\n# Natural Language Annotations: \"\"\"Ensure that ``fn`` is an asynchronous callable.\n    If it is an asynchronous, return the original object, else wrap it in an\n    ``AsyncCallable``\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef ensure_async_callable(fn: Callable[", "  ```\ndef ensure_async_callable(fn: Callable[", "  ```\ndef ensure_async_callable(fn: Callable["]}, "litestar-litestar/utils/typing.py-expand_type_var_in_type_hint": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/typing.py:\n```\nfrom __future__ import annotations\n\nimport re\nfrom collections import abc, defaultdict, deque\nfrom typing import (\n    AbstractSet,\n    Any,\n    AsyncGenerator,\n    AsyncIterable,\n    AsyncIterator,\n    Awaitable,\n    Collection,\n    Container,\n    Coroutine,\n    DefaultDict,\n    Deque,\n    Dict,\n    FrozenSet,\n    Generator,\n    ItemsView,\n    Iterable,\n    Iterator,\n    KeysView,\n    List,\n    Mapping,\n    MappingView,\n    MutableMapping,\n    MutableSequence,\n    MutableSet,\n    Reversible,\n    Sequence,\n    Set,\n    Tuple,\n    TypeVar,\n    Union,\n    ValuesView,\n    cast,\n)\n\nfrom typing_extensions import Annotated, NewType, NotRequired, Required, get_args, get_origin, get_type_hints\n\nfrom litestar.types.builtin_types import NoneType, UnionTypes\n\n__all__ = (\n    \"get_instantiable_origin\",\n    \"get_origin_or_inner_type\",\n    \"get_safe_generic_origin\",\n    \"instantiable_type_mapping\",\n    \"make_non_optional_union\",\n    \"safe_generic_origin_map\",\n    \"unwrap_annotation\",\n)\n\n\nT = TypeVar(\"T\")\nUnionT = TypeVar(\"UnionT\", bound=\"Union\")\n\ntuple_types_regex = re.compile(\n    \"^\"\n    + \"|\".join(\n        [*[repr(x) for x in (List, Sequence, Iterable, Iterator, Tuple, Deque)], \"tuple\", \"list\", \"collections.deque\"]\n    )\n)\n\ninstantiable_type_mapping = {\n    AbstractSet: set,\n    DefaultDict: defaultdict,\n    Deque: deque,\n    Dict: dict,\n    FrozenSet: frozenset,\n    List: list,\n    Mapping: dict,\n    MutableMapping: dict,\n    MutableSequence: list,\n    MutableSet: set,\n    Sequence: list,\n    Set: set,\n    Tuple: tuple,\n    abc.Mapping: dict,\n    abc.MutableMapping: dict,\n    abc.MutableSequence: list,\n    abc.MutableSet: set,\n    abc.Sequence: list,\n    abc.Set: set,\n    defaultdict: defaultdict,\n    deque: deque,\n    dict: dict,\n    frozenset: frozenset,\n    list: list,\n    set: set,\n    tuple: tuple,\n}\n\nsafe_generic_origin_map = {\n    set: AbstractSet,\n    defaultdict: DefaultDict,\n    deque: Deque,\n    dict: Dict,\n    frozenset: FrozenSet,\n    list: List,\n    tuple: Tuple,\n    abc.Mapping: Mapping,\n    abc.MutableMapping: MutableMapping,\n    abc.MutableSequence: MutableSequence,\n    abc.MutableSet: MutableSet,\n    abc.Sequence: Sequence,\n    abc.Set: AbstractSet,\n    abc.Collection: Collection,\n    abc.Container: Container,\n    abc.ItemsView: ItemsView,\n    abc.KeysView: KeysView,\n    abc.MappingView: MappingView,\n    abc.ValuesView: ValuesView,\n    abc.Iterable: Iterable,\n    abc.Iterator: Iterator,\n    abc.Generator: Generator,\n    abc.Reversible: Reversible,\n    abc.Coroutine: Coroutine,\n    abc.AsyncGenerator: AsyncGenerator,\n    abc.AsyncIterable: AsyncIterable,\n    abc.AsyncIterator: AsyncIterator,\n    abc.Awaitable: Awaitable,\n    **{union_t: Union for union_t in UnionTypes},\n}\n\"\"\"A mapping of types to equivalent types that are safe to be used as generics across all Python versions.\n\nThis is necessary because occasionally we want to rebuild a generic outer type with different args, and types such as\n``collections.abc.Mapping``, are not valid generic types in Python 3.8.\n\"\"\"\n\nwrapper_type_set = {Annotated, Required, NotRequired}\n\"\"\"Types that always contain a wrapped type annotation as their first arg.\"\"\"\n\n\ndef normalize_type_annotation(annotation: Any) -> Any:\n    \"\"\"Normalize a type annotation to a standard form.\"\"\"\n    return instantiable_type_mapping.get(annotation, annotation)\n\n\ndef make_non_optional_union(annotation: UnionT | None) -> UnionT:\n    \"\"\"Make a :data:`Union <typing.Union>` type that excludes ``NoneType``.\n\n    Args:\n        annotation: A type annotation.\n\n    Returns:\n        The union with all original members, except ``NoneType``.\n    \"\"\"\n    args = tuple(tp for tp in get_args(annotation) if tp is not NoneType)\n    return cast(\"UnionT\", Union[args])  # pyright: ignore\n\n\ndef unwrap_annotation(annotation: Any) -> tuple[Any, tuple[Any, ...], set[Any]]:\n    \"\"\"Remove \"wrapper\" annotation types, such as ``Annotated``, ``Required``, and ``NotRequired``.\n\n    Note:\n        ``annotation`` should have been retrieved from :func:`get_type_hints()` with ``include_extras=True``. This\n        ensures that any nested ``Annotated`` types are flattened according to the PEP 593 specification.\n\n    Args:\n        annotation: A type annotation.\n\n    Returns:\n        A tuple of the unwrapped annotation and any ``Annotated`` metadata, and a set of any wrapper types encountered.\n    \"\"\"\n    origin = get_origin(annotation)\n    wrappers = set()\n    metadata = []\n    while origin in wrapper_type_set:\n        wrappers.add(origin)\n        annotation, *meta = get_args(annotation)\n        metadata.extend(meta)\n        origin = get_origin(annotation)\n    return annotation, tuple(metadata), wrappers\n\n\ndef unwrap_new_type(new_type: Any) -> Any:\n    \"\"\"Unwrap a (nested) ``typing.NewType``\"\"\"\n    inner = new_type\n    while isinstance(inner, NewType):\n        inner = inner.__supertype__\n    return inner\n\n\ndef get_origin_or_inner_type(annotation: Any) -> Any:\n    \"\"\"Get origin or unwrap it. Returns None for non-generic types.\n\n    Args:\n        annotation: A type annotation.\n\n    Returns:\n        Any type.\n    \"\"\"\n    origin = get_origin(annotation)\n    if origin in wrapper_type_set:\n        inner, _, _ = unwrap_annotation(annotation)\n        # we need to recursively call here 'get_origin_or_inner_type' because we might be dealing\n        # with a generic type alias e.g. Annotated[dict[str, list[int]]\n        origin = get_origin_or_inner_type(inner)\n    return instantiable_type_mapping.get(origin, origin)\n\n\ndef get_safe_generic_origin(origin_type: Any, annotation: Any) -> Any:\n    \"\"\"Get a type that is safe to use as a generic type across all supported Python versions.\n\n    If a builtin collection type is annotated without generic args, e.g, ``a: dict``, then the origin type will be\n    ``None``. In this case, we can use the annotation to determine the correct generic type, if one exists.\n\n    Args:\n        origin_type: A type - would be the return value of :func:`get_origin()`.\n        annotation: Type annotation associated with the origin type. Should be unwrapped from any wrapper types, such\n            as ``Annotated``.\n\n    Returns:\n        The ``typing`` module equivalent of the given type, if it exists. Otherwise, the original type is returned.\n    \"\"\"\n    if origin_type is None:\n        return safe_generic_origin_map.get(annotation)\n    return safe_generic_origin_map.get(origin_type, origin_type)\n\n\ndef get_instantiable_origin(origin_type: Any, annotation: Any) -> Any:\n    \"\"\"Get a type that is safe to instantiate for the given origin type.\n\n    If a builtin collection type is annotated without generic args, e.g, ``a: dict``, then the origin type will be\n    ``None``. In this case, we can use the annotation to determine the correct instantiable type, if one exists.\n\n    Args:\n        origin_type: A type - would be the return value of :func:`get_origin()`.\n        annotation: Type annotation associated with the origin type. Should be unwrapped from any wrapper types, such\n            as ``Annotated``.\n\n    Returns:\n        A builtin type that is safe to instantiate for the given origin type.\n    \"\"\"\n    if origin_type is None:\n        return instantiable_type_mapping.get(annotation)\n    return instantiable_type_mapping.get(origin_type, origin_type)\n\n\ndef get_type_hints_with_generics_resolved(\n    annotation: Any,\n    globalns: dict[str, Any] | None = None,\n    localns: dict[str, Any] | None = None,\n    include_extras: bool = False,\n    type_hints: dict[str, Any] | None = None,\n) -> dict[str, Any]:\n    \"\"\"Get the type hints for the given object after resolving the generic types as much as possible.\n\n    Args:\n        annotation: A type annotation.\n        globalns: The global namespace.\n        localns: The local namespace.\n        include_extras: A flag indicating whether to include the ``Annotated[T, ...]`` or not.\n        type_hints: Already resolved type hints\n    \"\"\"\n    origin = get_origin(annotation)\n\n    if origin is None:\n        # Implies the generic types have not been specified in the annotation\n        if type_hints is None:  # pragma: no cover\n            type_hints = get_type_hints(annotation, globalns=globalns, localns=localns, include_extras=include_extras)\n        typevar_map = {p: p for p in annotation.__parameters__}\n    else:\n        if type_hints is None:  # pragma: no cover\n            type_hints = get_type_hints(origin, globalns=globalns, localns=localns, include_extras=include_extras)\n        # the __parameters__ is only available on the origin itself and not the annotation\n        typevar_map = dict(zip(origin.__parameters__, get_args(annotation)))\n\n    return {n: _substitute_typevars(type_, typevar_map) for n, type_ in type_hints.items()}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef _substitute_typevars(obj: Any, typevar_map: Mapping[Any, Any]) -> Any:\n    if params := getattr(obj, \"__parameters__\", None):\n        args = tuple(_substitute_typevars(typevar_map.get(p, p), typevar_map) for p in params)\n        return obj[args]\n\n    if isinstance(obj, TypeVar):\n        # If there's a mapped type for the TypeVar already, then it should be returned instead\n        # of considering __constraints__ or __bound__. For a generic `Foo[T]`, if Foo[int] is given\n        # then int should be returned and if `Foo` is given then the __bounds__ and __constraints__\n        # should be considered.\n        if (type_ := typevar_map.get(obj, None)) is not None and not isinstance(type_, TypeVar):\n            return type_\n\n        if obj.__bound__ is not None:\n            return obj.__bound__\n\n        if obj.__constraints__:\n            return Union[obj.__constraints__]  # pyright: ignore\n\n    return obj\n\n```\n# Function Signature: def expand_type_var_in_type_hint(type_hint: dict[str, Any], namespace: dict[str, Any] | None) -> dict[str, Any]:\n# Natural Language Annotations: \"\"\"Expand TypeVar for any parameters in type_hint\n\n    Args:\n        type_hint: mapping of parameter to type obtained from calling `get_type_hints` or `get_fn_type_hints`\n        namespace: mapping of TypeVar to concrete type\n\n    Returns:\n        type_hint with any TypeVar parameter expanded\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef expand_type_var_in_type_hint", "  ```\ndef expand_type_var_in_type_hint", "  ```\ndef expand_type_var_in_type_hint"]}, "litestar-litestar/utils/dataclass.py-simple_asdict": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/dataclass.py:\n```\nfrom __future__ import annotations\n\nfrom dataclasses import Field, fields\nfrom typing import TYPE_CHECKING\n\nfrom litestar.types import Empty\nfrom litestar.utils.predicates import is_dataclass_instance\n\nif TYPE_CHECKING:\n    from typing import AbstractSet, Any, Iterable\n\n    from litestar.types.protocols import DataclassProtocol\n\n__all__ = (\n    \"extract_dataclass_fields\",\n    \"extract_dataclass_items\",\n    \"simple_asdict\",\n)\n\n\ndef extract_dataclass_fields(\n    dt: DataclassProtocol,\n    exclude_none: bool = False,\n    exclude_empty: bool = False,\n    include: AbstractSet[str] | None = None,\n    exclude: AbstractSet[str] | None = None,\n) -> tuple[Field[Any], ...]:\n    \"\"\"Extract dataclass fields.\n\n    Args:\n        dt: A dataclass instance.\n        exclude_none: Whether to exclude None values.\n        exclude_empty: Whether to exclude Empty values.\n        include: An iterable of fields to include.\n        exclude: An iterable of fields to exclude.\n\n\n    Returns:\n        A tuple of dataclass fields.\n    \"\"\"\n    include = include or set()\n    exclude = exclude or set()\n\n    if common := (include & exclude):\n        raise ValueError(f\"Fields {common} are both included and excluded.\")\n\n    dataclass_fields: Iterable[Field[Any]] = fields(dt)\n    if exclude_none:\n        dataclass_fields = (field for field in dataclass_fields if getattr(dt, field.name) is not None)\n    if exclude_empty:\n        dataclass_fields = (field for field in dataclass_fields if getattr(dt, field.name) is not Empty)\n    if include:\n        dataclass_fields = (field for field in dataclass_fields if field.name in include)\n    if exclude:\n        dataclass_fields = (field for field in dataclass_fields if field.name not in exclude)\n\n    return tuple(dataclass_fields)\n\n\ndef extract_dataclass_items(\n    dt: DataclassProtocol,\n    exclude_none: bool = False,\n    exclude_empty: bool = False,\n    include: AbstractSet[str] | None = None,\n    exclude: AbstractSet[str] | None = None,\n) -> tuple[tuple[str, Any], ...]:\n    \"\"\"Extract dataclass name, value pairs.\n\n    Unlike the 'asdict' method exports by the stlib, this function does not pickle values.\n\n    Args:\n        dt: A dataclass instance.\n        exclude_none: Whether to exclude None values.\n        exclude_empty: Whether to exclude Empty values.\n        include: An iterable of fields to include.\n        exclude: An iterable of fields to exclude.\n\n    Returns:\n        A tuple of key/value pairs.\n    \"\"\"\n    dataclass_fields = extract_dataclass_fields(dt, exclude_none, exclude_empty, include, exclude)\n    return tuple((field.name, getattr(dt, field.name)) for field in dataclass_fields)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def simple_asdict(\n# Natural Language Annotations: \"\"\"Convert a dataclass to a dictionary.\n\n    This method has important differences to the standard library version:\n    - it does not deepcopy values\n    - it does not recurse into collections\n\n    Args:\n        obj: A dataclass instance.\n        exclude_none: Whether to exclude None values.\n        exclude_empty: Whether to exclude Empty values.\n        convert_nested: Whether to recursively convert nested dataclasses.\n        exclude: An iterable of fields to exclude.\n\n    Returns:\n        A dictionary of key/value pairs.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef simple_asdict(obj, exclude_none=False", "  ```\ndef simple_asdict(\n    obj: DataclassProtocol", "  ```\ndef simple_asdict(\n    obj: DataclassProtocol"]}, "litestar-litestar/utils/path.py-join_paths": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/path.py:\n```\nfrom __future__ import annotations\n\nimport re\nfrom typing import Iterable\n\n__all__ = (\"join_paths\", \"normalize_path\")\n\n\nmulti_slash_pattern = re.compile(\"//+\")\n\n\ndef normalize_path(path: str) -> str:\n    \"\"\"Normalize a given path by ensuring it starts with a slash and does not end with a slash.\n\n    Args:\n        path: Path string\n\n    Returns:\n        Path string\n    \"\"\"\n    path = path.strip(\"/\")\n    path = f\"/{path}\"\n    return multi_slash_pattern.sub(\"/\", path)\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def join_paths(paths: Iterable[str]) -> str:\n# Natural Language Annotations: \"\"\"Normalize and joins path fragments.\n\n    Args:\n        paths: An iterable of path fragments.\n\n    Returns:\n        A normalized joined path string.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef join_paths(paths: Iterable[str]) ->", "  ```\ndef join_paths(paths: Iterable[str]) ->", "  ```\ndef join_paths(paths: Iterable[str]) ->"]}, "litestar-litestar/utils/helpers.py-get_name": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/helpers.py:\n```\nfrom __future__ import annotations\n\nfrom enum import Enum\nfrom functools import partial\nfrom typing import TYPE_CHECKING, TypeVar, cast\nfrom urllib.parse import quote\n\nfrom litestar.utils.typing import get_origin_or_inner_type\n\nif TYPE_CHECKING:\n    from collections.abc import Container\n\n    from litestar.types import MaybePartial\n\n__all__ = (\n    \"get_enum_string_value\",\n    \"get_name\",\n    \"unwrap_partial\",\n    \"url_quote\",\n    \"unique_name_for_scope\",\n)\n\nT = TypeVar(\"T\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef get_enum_string_value(value: Enum | str) -> str:\n    \"\"\"Return the string value of a string enum.\n\n    See: https://github.com/litestar-org/litestar/pull/633#issuecomment-1286519267\n\n    Args:\n        value: An enum or string.\n\n    Returns:\n        A string.\n    \"\"\"\n    return value.value if isinstance(value, Enum) else value  # type: ignore[no-any-return]\n\n\ndef unwrap_partial(value: MaybePartial[T]) -> T:\n    \"\"\"Unwraps a partial, returning the underlying callable.\n\n    Args:\n        value: A partial function.\n\n    Returns:\n        Callable\n    \"\"\"\n    from litestar.utils.sync import AsyncCallable\n\n    return cast(\"T\", value.func if isinstance(value, (partial, AsyncCallable)) else value)\n\n\ndef url_quote(value: str | bytes) -> str:\n    \"\"\"Quote a URL.\n\n    Args:\n        value: A URL.\n\n    Returns:\n        A quoted URL.\n    \"\"\"\n    return quote(value, safe=\"/#%[]=:;$&()+,!?*@'~\")\n\n\ndef unique_name_for_scope(base_name: str, scope: Container[str]) -> str:\n    \"\"\"Create a name derived from ``base_name`` that's unique within ``scope``\"\"\"\n    i = 0\n    while True:\n        if (unique_name := f\"{base_name}_{i}\") not in scope:\n            return unique_name\n        i += 1\n\n\ndef get_exception_group() -> type[BaseException]:\n    \"\"\"Get the exception group class with version compatibility.\"\"\"\n    try:\n        return cast(\"type[BaseException]\", ExceptionGroup)  # type:ignore[name-defined]\n    except NameError:\n        from exceptiongroup import ExceptionGroup as _ExceptionGroup  # pyright: ignore\n\n        return cast(\"type[BaseException]\", _ExceptionGroup)\n\n```\n# Function Signature: def get_name(value: object) -> str:\n# Natural Language Annotations: \"\"\"Get the ``__name__`` of an object.\n\n    Args:\n        value: An arbitrary object.\n\n    Returns:\n        A name string.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_name(value: object) -> str:\n", "  ```\ndef get_name(value: object) -> str:\n", "  ```\ndef get_name(value: object) -> str:\n"]}, "litestar-litestar/utils/predicates.py-is_dataclass_class": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/predicates.py:\n```\nfrom __future__ import annotations\n\nfrom asyncio import iscoroutinefunction\nfrom collections import defaultdict, deque\nfrom collections.abc import Iterable as CollectionsIterable\nfrom dataclasses import is_dataclass\nfrom inspect import isasyncgenfunction, isclass, isgeneratorfunction\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    ClassVar,\n    DefaultDict,\n    Deque,\n    Dict,\n    FrozenSet,\n    Generic,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    TypeVar,\n)\n\nfrom typing_extensions import (\n    ParamSpec,\n    TypeGuard,\n    _AnnotatedAlias,\n    get_args,\n)\n\nfrom litestar.constants import UNDEFINED_SENTINELS\nfrom litestar.types.builtin_types import NoneType, UnionTypes\nfrom litestar.utils.deprecation import warn_deprecation\nfrom litestar.utils.helpers import unwrap_partial\nfrom litestar.utils.typing import get_origin_or_inner_type\n\nif TYPE_CHECKING:\n    from litestar.types.callable_types import AnyGenerator\n    from litestar.types.protocols import DataclassProtocol\n\n\n__all__ = (\n    \"is_annotated_type\",\n    \"is_any\",\n    \"is_async_callable\",\n    \"is_class_and_subclass\",\n    \"is_class_var\",\n    \"is_dataclass_class\",\n    \"is_dataclass_instance\",\n    \"is_generic\",\n    \"is_mapping\",\n    \"is_non_string_iterable\",\n    \"is_non_string_sequence\",\n    \"is_optional_union\",\n    \"is_undefined_sentinel\",\n    \"is_union\",\n)\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\ndef is_async_callable(value: Callable[P, T]) -> TypeGuard[Callable[P, Awaitable[T]]]:\n    \"\"\"Extend :func:`asyncio.iscoroutinefunction` to additionally detect async :func:`functools.partial` objects and\n    class instances with ``async def __call__()`` defined.\n\n    Args:\n        value: Any\n\n    Returns:\n        Bool determining if type of ``value`` is an awaitable.\n    \"\"\"\n    value = unwrap_partial(value)\n\n    return iscoroutinefunction(value) or (\n        callable(value) and iscoroutinefunction(value.__call__)  # type: ignore[operator]\n    )\n\n\ndef is_dataclass_instance(obj: Any) -> TypeGuard[DataclassProtocol]:\n    \"\"\"Check if an object is a dataclass instance.\n\n    Args:\n        obj: An object to check.\n\n    Returns:\n        True if the object is a dataclass instance.\n    \"\"\"\n    return hasattr(type(obj), \"__dataclass_fields__\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef is_class_and_subclass(annotation: Any, type_or_type_tuple: type[T] | tuple[type[T], ...]) -> TypeGuard[type[T]]:\n    \"\"\"Return ``True`` if ``value`` is a ``class`` and is a subtype of ``t_type``.\n\n    See https://github.com/litestar-org/litestar/issues/367\n\n    Args:\n        annotation: The value to check if is class and subclass of ``t_type``.\n        type_or_type_tuple: Type used for :func:`issubclass` check of ``value``\n\n    Returns:\n        bool\n    \"\"\"\n    origin = get_origin_or_inner_type(annotation)\n    if not origin and not isclass(annotation):\n        return False\n    try:\n        return issubclass(origin or annotation, type_or_type_tuple)\n    except TypeError:  # pragma: no cover\n        return False\n\n\ndef is_generic(annotation: Any) -> bool:\n    \"\"\"Given a type annotation determine if the annotation is a generic class.\n\n    Args:\n    annotation: A type.\n\n    Returns:\n        True if the annotation is a subclass of :data:`Generic <typing.Generic>` otherwise ``False``.\n    \"\"\"\n    return is_class_and_subclass(annotation, Generic)  # type: ignore[arg-type]\n\n\ndef is_mapping(annotation: Any) -> TypeGuard[Mapping[Any, Any]]:\n    \"\"\"Given a type annotation determine if the annotation is a mapping type.\n\n    Args:\n    annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type can be cast as :class:`Mapping <typing.Mapping>`.\n    \"\"\"\n    _type = get_origin_or_inner_type(annotation) or annotation\n    return isclass(_type) and issubclass(_type, (dict, defaultdict, DefaultDict, Mapping))\n\n\ndef is_non_string_iterable(annotation: Any) -> TypeGuard[Iterable[Any]]:\n    \"\"\"Given a type annotation determine if the annotation is an iterable.\n\n    Args:\n    annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type can be cast as :class:`Iterable <typing.Iterable>` that is not a string.\n    \"\"\"\n    origin = get_origin_or_inner_type(annotation)\n    if not origin and not isclass(annotation):\n        return False\n    try:\n        return not issubclass(origin or annotation, (str, bytes)) and (\n            issubclass(origin or annotation, (Iterable, CollectionsIterable, Dict, dict, Mapping))\n            or is_non_string_sequence(annotation)\n        )\n    except TypeError:  # pragma: no cover\n        return False\n\n\ndef is_non_string_sequence(annotation: Any) -> TypeGuard[Sequence[Any]]:\n    \"\"\"Given a type annotation determine if the annotation is a sequence.\n\n    Args:\n    annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type can be cast as :class`Sequence <typing.Sequence>` that is not a string.\n    \"\"\"\n    origin = get_origin_or_inner_type(annotation)\n    if not origin and not isclass(annotation):\n        return False\n    try:\n        return not issubclass(origin or annotation, (str, bytes)) and issubclass(\n            origin or annotation,\n            (  # type: ignore[arg-type]\n                Tuple,\n                List,\n                Set,\n                FrozenSet,\n                Deque,\n                Sequence,\n                list,\n                tuple,\n                deque,\n                set,\n                frozenset,\n            ),\n        )\n    except TypeError:  # pragma: no cover\n        return False\n\n\ndef is_any(annotation: Any) -> TypeGuard[Any]:\n    \"\"\"Given a type annotation determine if the annotation is Any.\n\n    Args:\n        annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type is :data:`Any <typing.Any>`.\n    \"\"\"\n    return (\n        annotation is Any\n        or getattr(annotation, \"_name\", \"\") == \"typing.Any\"\n        or (get_origin_or_inner_type(annotation) in UnionTypes and Any in get_args(annotation))\n    )\n\n\ndef is_union(annotation: Any) -> bool:\n    \"\"\"Given a type annotation determine if the annotation infers an optional union.\n\n    Args:\n        annotation: A type.\n\n    Returns:\n        A boolean determining whether the type is :data:`Union typing.Union>`.\n    \"\"\"\n    return get_origin_or_inner_type(annotation) in UnionTypes\n\n\ndef is_optional_union(annotation: Any) -> TypeGuard[Any | None]:\n    \"\"\"Given a type annotation determine if the annotation infers an optional union.\n\n    Args:\n        annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type is :data:`Union typing.Union>` with a\n            None value or :data:`Optional <typing.Optional>` which is equivalent.\n    \"\"\"\n    origin = get_origin_or_inner_type(annotation)\n    return origin is Optional or (\n        get_origin_or_inner_type(annotation) in UnionTypes and NoneType in get_args(annotation)\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef _is_sync_or_async_generator(obj: Any) -> TypeGuard[AnyGenerator]:\n    \"\"\"Check if the given annotation is a sync or async generator.\n\n    Args:\n        obj: type to be tested for sync or async generator.\n\n    Returns:\n        A boolean.\n    \"\"\"\n    return isgeneratorfunction(obj) or isasyncgenfunction(obj)\n\n\ndef is_annotated_type(annotation: Any) -> bool:\n    \"\"\"Check if the given annotation is an Annotated.\n\n    Args:\n        annotation: A type annotation\n\n    Returns:\n        A boolean.\n    \"\"\"\n    return isinstance(annotation, _AnnotatedAlias) and getattr(annotation, \"__args__\", None) is not None\n\n\ndef is_undefined_sentinel(value: Any) -> bool:\n    \"\"\"Check if the given value is the undefined sentinel.\n\n    Args:\n        value: A value to be tested for undefined sentinel.\n\n    Returns:\n        A boolean.\n    \"\"\"\n    return any(v is value for v in UNDEFINED_SENTINELS)\n\n\n_deprecated_names = {\"is_sync_or_async_generator\": _is_sync_or_async_generator}\n\n\ndef __getattr__(name: str) -> Any:\n    if name in _deprecated_names:\n        warn_deprecation(\n            deprecated_name=f\"litestar.utils.scope.{name}\",\n            version=\"2.4\",\n            kind=\"import\",\n            removal_in=\"3.0\",\n            info=f\"'litestar.utils.predicates.{name}' is deprecated.\",\n        )\n        return globals()[\"_deprecated_names\"][name]\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")  # pragma: no cover\n\n```\n# Function Signature: def is_dataclass_class(annotation: Any) -> TypeGuard[type[DataclassProtocol]]:\n# Natural Language Annotations: \"\"\"Wrap :func:`is_dataclass <dataclasses.is_dataclass>` in a :data:`typing.TypeGuard`.\n\n    Args:\n        annotation: tested to determine if instance or type of :class:`dataclasses.dataclass`.\n\n    Returns:\n        ``True`` if instance or type of ``dataclass``.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef is_dataclass_class(annotation: Any) ->", "  ```\ndef is_dataclass_class(annotation: Any) ->", "  ```\ndef is_dataclass_class(annotation: Any) ->"]}, "litestar-litestar/utils/predicates.py-is_class_var": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/predicates.py:\n```\nfrom __future__ import annotations\n\nfrom asyncio import iscoroutinefunction\nfrom collections import defaultdict, deque\nfrom collections.abc import Iterable as CollectionsIterable\nfrom dataclasses import is_dataclass\nfrom inspect import isasyncgenfunction, isclass, isgeneratorfunction\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    ClassVar,\n    DefaultDict,\n    Deque,\n    Dict,\n    FrozenSet,\n    Generic,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    TypeVar,\n)\n\nfrom typing_extensions import (\n    ParamSpec,\n    TypeGuard,\n    _AnnotatedAlias,\n    get_args,\n)\n\nfrom litestar.constants import UNDEFINED_SENTINELS\nfrom litestar.types.builtin_types import NoneType, UnionTypes\nfrom litestar.utils.deprecation import warn_deprecation\nfrom litestar.utils.helpers import unwrap_partial\nfrom litestar.utils.typing import get_origin_or_inner_type\n\nif TYPE_CHECKING:\n    from litestar.types.callable_types import AnyGenerator\n    from litestar.types.protocols import DataclassProtocol\n\n\n__all__ = (\n    \"is_annotated_type\",\n    \"is_any\",\n    \"is_async_callable\",\n    \"is_class_and_subclass\",\n    \"is_class_var\",\n    \"is_dataclass_class\",\n    \"is_dataclass_instance\",\n    \"is_generic\",\n    \"is_mapping\",\n    \"is_non_string_iterable\",\n    \"is_non_string_sequence\",\n    \"is_optional_union\",\n    \"is_undefined_sentinel\",\n    \"is_union\",\n)\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\ndef is_async_callable(value: Callable[P, T]) -> TypeGuard[Callable[P, Awaitable[T]]]:\n    \"\"\"Extend :func:`asyncio.iscoroutinefunction` to additionally detect async :func:`functools.partial` objects and\n    class instances with ``async def __call__()`` defined.\n\n    Args:\n        value: Any\n\n    Returns:\n        Bool determining if type of ``value`` is an awaitable.\n    \"\"\"\n    value = unwrap_partial(value)\n\n    return iscoroutinefunction(value) or (\n        callable(value) and iscoroutinefunction(value.__call__)  # type: ignore[operator]\n    )\n\n\ndef is_dataclass_instance(obj: Any) -> TypeGuard[DataclassProtocol]:\n    \"\"\"Check if an object is a dataclass instance.\n\n    Args:\n        obj: An object to check.\n\n    Returns:\n        True if the object is a dataclass instance.\n    \"\"\"\n    return hasattr(type(obj), \"__dataclass_fields__\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef is_class_and_subclass(annotation: Any, type_or_type_tuple: type[T] | tuple[type[T], ...]) -> TypeGuard[type[T]]:\n    \"\"\"Return ``True`` if ``value`` is a ``class`` and is a subtype of ``t_type``.\n\n    See https://github.com/litestar-org/litestar/issues/367\n\n    Args:\n        annotation: The value to check if is class and subclass of ``t_type``.\n        type_or_type_tuple: Type used for :func:`issubclass` check of ``value``\n\n    Returns:\n        bool\n    \"\"\"\n    origin = get_origin_or_inner_type(annotation)\n    if not origin and not isclass(annotation):\n        return False\n    try:\n        return issubclass(origin or annotation, type_or_type_tuple)\n    except TypeError:  # pragma: no cover\n        return False\n\n\ndef is_generic(annotation: Any) -> bool:\n    \"\"\"Given a type annotation determine if the annotation is a generic class.\n\n    Args:\n    annotation: A type.\n\n    Returns:\n        True if the annotation is a subclass of :data:`Generic <typing.Generic>` otherwise ``False``.\n    \"\"\"\n    return is_class_and_subclass(annotation, Generic)  # type: ignore[arg-type]\n\n\ndef is_mapping(annotation: Any) -> TypeGuard[Mapping[Any, Any]]:\n    \"\"\"Given a type annotation determine if the annotation is a mapping type.\n\n    Args:\n    annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type can be cast as :class:`Mapping <typing.Mapping>`.\n    \"\"\"\n    _type = get_origin_or_inner_type(annotation) or annotation\n    return isclass(_type) and issubclass(_type, (dict, defaultdict, DefaultDict, Mapping))\n\n\ndef is_non_string_iterable(annotation: Any) -> TypeGuard[Iterable[Any]]:\n    \"\"\"Given a type annotation determine if the annotation is an iterable.\n\n    Args:\n    annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type can be cast as :class:`Iterable <typing.Iterable>` that is not a string.\n    \"\"\"\n    origin = get_origin_or_inner_type(annotation)\n    if not origin and not isclass(annotation):\n        return False\n    try:\n        return not issubclass(origin or annotation, (str, bytes)) and (\n            issubclass(origin or annotation, (Iterable, CollectionsIterable, Dict, dict, Mapping))\n            or is_non_string_sequence(annotation)\n        )\n    except TypeError:  # pragma: no cover\n        return False\n\n\ndef is_non_string_sequence(annotation: Any) -> TypeGuard[Sequence[Any]]:\n    \"\"\"Given a type annotation determine if the annotation is a sequence.\n\n    Args:\n    annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type can be cast as :class`Sequence <typing.Sequence>` that is not a string.\n    \"\"\"\n    origin = get_origin_or_inner_type(annotation)\n    if not origin and not isclass(annotation):\n        return False\n    try:\n        return not issubclass(origin or annotation, (str, bytes)) and issubclass(\n            origin or annotation,\n            (  # type: ignore[arg-type]\n                Tuple,\n                List,\n                Set,\n                FrozenSet,\n                Deque,\n                Sequence,\n                list,\n                tuple,\n                deque,\n                set,\n                frozenset,\n            ),\n        )\n    except TypeError:  # pragma: no cover\n        return False\n\n\ndef is_any(annotation: Any) -> TypeGuard[Any]:\n    \"\"\"Given a type annotation determine if the annotation is Any.\n\n    Args:\n        annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type is :data:`Any <typing.Any>`.\n    \"\"\"\n    return (\n        annotation is Any\n        or getattr(annotation, \"_name\", \"\") == \"typing.Any\"\n        or (get_origin_or_inner_type(annotation) in UnionTypes and Any in get_args(annotation))\n    )\n\n\ndef is_union(annotation: Any) -> bool:\n    \"\"\"Given a type annotation determine if the annotation infers an optional union.\n\n    Args:\n        annotation: A type.\n\n    Returns:\n        A boolean determining whether the type is :data:`Union typing.Union>`.\n    \"\"\"\n    return get_origin_or_inner_type(annotation) in UnionTypes\n\n\ndef is_optional_union(annotation: Any) -> TypeGuard[Any | None]:\n    \"\"\"Given a type annotation determine if the annotation infers an optional union.\n\n    Args:\n        annotation: A type.\n\n    Returns:\n        A typeguard determining whether the type is :data:`Union typing.Union>` with a\n            None value or :data:`Optional <typing.Optional>` which is equivalent.\n    \"\"\"\n    origin = get_origin_or_inner_type(annotation)\n    return origin is Optional or (\n        get_origin_or_inner_type(annotation) in UnionTypes and NoneType in get_args(annotation)\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef _is_sync_or_async_generator(obj: Any) -> TypeGuard[AnyGenerator]:\n    \"\"\"Check if the given annotation is a sync or async generator.\n\n    Args:\n        obj: type to be tested for sync or async generator.\n\n    Returns:\n        A boolean.\n    \"\"\"\n    return isgeneratorfunction(obj) or isasyncgenfunction(obj)\n\n\ndef is_annotated_type(annotation: Any) -> bool:\n    \"\"\"Check if the given annotation is an Annotated.\n\n    Args:\n        annotation: A type annotation\n\n    Returns:\n        A boolean.\n    \"\"\"\n    return isinstance(annotation, _AnnotatedAlias) and getattr(annotation, \"__args__\", None) is not None\n\n\ndef is_undefined_sentinel(value: Any) -> bool:\n    \"\"\"Check if the given value is the undefined sentinel.\n\n    Args:\n        value: A value to be tested for undefined sentinel.\n\n    Returns:\n        A boolean.\n    \"\"\"\n    return any(v is value for v in UNDEFINED_SENTINELS)\n\n\n_deprecated_names = {\"is_sync_or_async_generator\": _is_sync_or_async_generator}\n\n\ndef __getattr__(name: str) -> Any:\n    if name in _deprecated_names:\n        warn_deprecation(\n            deprecated_name=f\"litestar.utils.scope.{name}\",\n            version=\"2.4\",\n            kind=\"import\",\n            removal_in=\"3.0\",\n            info=f\"'litestar.utils.predicates.{name}' is deprecated.\",\n        )\n        return globals()[\"_deprecated_names\"][name]\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")  # pragma: no cover\n\n```\n# Function Signature: def is_class_var(annotation: Any) -> bool:\n# Natural Language Annotations: \"\"\"Check if the given annotation is a ClassVar.\n\n    Args:\n        annotation: A type annotation\n\n    Returns:\n        A boolean.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef is_class_var(annotation: Any) -> bool", "  ```\ndef is_class_var(annotation: Any) -> bool", "  ```\ndef is_class_var(annotation: Any) -> bool"]}, "litestar-litestar/utils/module_loader.py-import_string": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/utils/module_loader.py:\n```\n\"\"\"General utility functions.\"\"\"\n\nfrom __future__ import annotations\n\nimport os.path\nimport sys\nfrom importlib import import_module\nfrom importlib.util import find_spec\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\n\nif TYPE_CHECKING:\n    from types import ModuleType\n\n__all__ = (\n    \"import_string\",\n    \"module_to_os_path\",\n)\n\n\ndef module_to_os_path(dotted_path: str = \"app\") -> Path:\n    \"\"\"Find Module to OS Path.\n\n    Return a path to the base directory of the project or the module\n    specified by `dotted_path`.\n\n    Args:\n        dotted_path: The path to the module. Defaults to \"app\".\n\n    Raises:\n        TypeError: The module could not be found.\n\n    Returns:\n        Path: The path to the module.\n    \"\"\"\n    try:\n        if (src := find_spec(dotted_path)) is None:  # pragma: no cover\n            raise TypeError(f\"Couldn't find the path for {dotted_path}\")\n    except ModuleNotFoundError as e:\n        raise TypeError(f\"Couldn't find the path for {dotted_path}\") from e\n\n    return Path(str(src.origin).rsplit(os.path.sep + \"__init__.py\", maxsplit=1)[0])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def import_string(dotted_path: str) -> Any:\n# Natural Language Annotations: \"\"\"Dotted Path Import.\n\n    Import a dotted module path and return the attribute/class designated by the\n    last name in the path. Raise ImportError if the import failed.\n\n    Args:\n        dotted_path: The path of the module to import.\n\n    Raises:\n        ImportError: Could not import the module.\n\n    Returns:\n        object: The imported object.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef import_string(dotted_path: str) ->", "  ```\ndef import_string(dotted_path: str) ->", "  ```\ndef import_string(dotted_path: str) ->"]}, "litestar-litestar/channels/backends/redis.py-unsubscribe": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/channels/backends/redis.py:\n```\nfrom __future__ import annotations\n\nimport asyncio\nimport sys\n\nif sys.version_info < (3, 9):\n    import importlib_resources  # pyright: ignore\nelse:\n    import importlib.resources as importlib_resources\nfrom abc import ABC\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, Iterable, cast\n\nfrom litestar.channels.backends.base import ChannelsBackend\n\nif TYPE_CHECKING:\n    from redis.asyncio import Redis\n    from redis.asyncio.client import PubSub\n\n_resource_path = importlib_resources.files(\"litestar.channels.backends\")\n_PUBSUB_PUBLISH_SCRIPT = (_resource_path / \"_redis_pubsub_publish.lua\").read_text()\n_FLUSHALL_STREAMS_SCRIPT = (_resource_path / \"_redis_flushall_streams.lua\").read_text()\n_XADD_EXPIRE_SCRIPT = (_resource_path / \"_redis_xadd_expire.lua\").read_text()\n\n\nclass _LazyEvent:\n    \"\"\"A lazy proxy to asyncio.Event that only creates the event once it's accessed.\n\n    It ensures that the Event is created within a running event loop. If it's not, there can be an issue where a future\n    within the event itself is attached to a different loop.\n\n    This happens in our tests and could also happen when a user creates an instance of the backend outside an event loop\n    in their application.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.__event: asyncio.Event | None = None\n\n    @property\n    def _event(self) -> asyncio.Event:\n        if self.__event is None:\n            self.__event = asyncio.Event()\n        return self.__event\n\n    def set(self) -> None:\n        self._event.set()\n\n    def clear(self) -> None:\n        self._event.clear()\n\n    async def wait(self) -> None:\n        await self._event.wait()\n\n\nclass RedisChannelsBackend(ChannelsBackend, ABC):\n    def __init__(self, *, redis: Redis, key_prefix: str, stream_sleep_no_subscriptions: int) -> None:\n        \"\"\"Base redis channels backend.\n\n        Args:\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for storing data in redis\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n        \"\"\"\n        self._redis = redis\n        self._key_prefix = key_prefix\n        self._stream_sleep_no_subscriptions = stream_sleep_no_subscriptions\n\n    def _make_key(self, channel: str) -> str:\n        return f\"{self._key_prefix}_{channel.upper()}\"\n\n\nclass RedisChannelsPubSubBackend(RedisChannelsBackend):\n    def __init__(\n        self, *, redis: Redis, stream_sleep_no_subscriptions: int = 1, key_prefix: str = \"LITESTAR_CHANNELS\"\n    ) -> None:\n        \"\"\"Redis channels backend, `Pub/Sub <https://redis.io/docs/manual/pubsub/>`_.\n\n        This backend provides low overhead and resource usage but no support for history.\n\n        Args:\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for storing data in redis\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n        \"\"\"\n        super().__init__(\n            redis=redis, stream_sleep_no_subscriptions=stream_sleep_no_subscriptions, key_prefix=key_prefix\n        )\n        self.__pub_sub: PubSub | None = None\n        self._publish_script = self._redis.register_script(_PUBSUB_PUBLISH_SCRIPT)\n        self._has_subscribed = _LazyEvent()\n\n    @property\n    def _pub_sub(self) -> PubSub:\n        if self.__pub_sub is None:\n            self.__pub_sub = self._redis.pubsub()\n        return self.__pub_sub\n\n    async def on_startup(self) -> None:\n        # this method should not do anything in this case\n        pass\n\n    async def on_shutdown(self) -> None:\n        await self._pub_sub.reset()\n\n    async def subscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Subscribe to ``channels``, and enable publishing to them\"\"\"\n        await self._pub_sub.subscribe(*channels)\n        self._has_subscribed.set()\n\n    async def unsubscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Stop listening for events on ``channels``\"\"\"\n        await self._pub_sub.unsubscribe(*channels)\n        # if we have no active subscriptions, or only subscriptions which are pending\n        # to be unsubscribed we consider the backend to be unsubscribed from all\n        # channels, so we reset the event\n        if not self._pub_sub.channels.keys() - self._pub_sub.pending_unsubscribe_channels:\n            self._has_subscribed.clear()\n\n    async def publish(self, data: bytes, channels: Iterable[str]) -> None:\n        \"\"\"Publish ``data`` to ``channels``\n\n        .. note::\n            This operation is performed atomically, using a lua script\n        \"\"\"\n        await self._publish_script(keys=list(set(channels)), args=[data])\n\n    async def stream_events(self) -> AsyncGenerator[tuple[str, Any], None]:\n        \"\"\"Return a generator, iterating over events of subscribed channels as they become available.\n\n        If no channels have been subscribed to yet via :meth:`subscribe`, sleep for ``stream_sleep_no_subscriptions``\n        milliseconds.\n        \"\"\"\n\n        while True:\n            await self._has_subscribed.wait()\n            message = await self._pub_sub.get_message(\n                ignore_subscribe_messages=True, timeout=self._stream_sleep_no_subscriptions\n            )\n            if message is None:\n                continue\n\n            channel: str = message[\"channel\"].decode()\n            data: bytes = message[\"data\"]\n            # redis handles the unsubscibes with a queue; Unsubscribing doesn't mean the\n            # unsubscribe will happen immediately after requesting it, so we could\n            # receive a message on a channel that, from a client's perspective, it's not\n            # subscribed to anymore\n            if channel.encode() in self._pub_sub.channels.keys() - self._pub_sub.pending_unsubscribe_channels:\n                yield channel, data\n\n    async def get_history(self, channel: str, limit: int | None = None) -> list[bytes]:\n        \"\"\"Not implemented\"\"\"\n        raise NotImplementedError()\n\n\nclass RedisChannelsStreamBackend(RedisChannelsBackend):\n    def __init__(\n        self,\n        history: int,\n        *,\n        redis: Redis,\n        stream_sleep_no_subscriptions: int = 1,\n        cap_streams_approximate: bool = True,\n        stream_ttl: int | timedelta = timedelta(seconds=60),\n        key_prefix: str = \"LITESTAR_CHANNELS\",\n    ) -> None:\n        \"\"\"Redis channels backend, `streams <https://redis.io/docs/data-types/streams/>`_.\n\n        Args:\n            history: Amount of messages to keep. This will set a ``MAXLEN`` to the streams\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for streams\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n            cap_streams_approximate: Set the streams ``MAXLEN`` using the ``~`` approximation\n                operator for improved performance\n            stream_ttl: TTL of a stream in milliseconds or as a timedelta. A streams TTL will be set on each publishing\n                operation using ``PEXPIRE``\n        \"\"\"\n        super().__init__(\n            redis=redis, stream_sleep_no_subscriptions=stream_sleep_no_subscriptions, key_prefix=key_prefix\n        )\n\n        self._history_limit = history\n        self._subscribed_channels: set[str] = set()\n        self._cap_streams_approximate = cap_streams_approximate\n        self._stream_ttl = stream_ttl if isinstance(stream_ttl, int) else int(stream_ttl.total_seconds() * 1000)\n        self._flush_all_streams_script = self._redis.register_script(_FLUSHALL_STREAMS_SCRIPT)\n        self._publish_script = self._redis.register_script(_XADD_EXPIRE_SCRIPT)\n        self._has_subscribed_channels = _LazyEvent()\n\n    async def on_startup(self) -> None:\n        \"\"\"Called on application startup\"\"\"\n\n    async def on_shutdown(self) -> None:\n        \"\"\"Called on application shutdown\"\"\"\n\n    async def subscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Subscribe to ``channels``\"\"\"\n        self._subscribed_channels.update(channels)\n        self._has_subscribed_channels.set()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def _get_subscribed_channels(self) -> set[str]:\n        \"\"\"Get subscribed channels. If no channels are currently subscribed, wait\"\"\"\n        await self._has_subscribed_channels.wait()\n        return self._subscribed_channels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def flush_all(self) -> int:\n        \"\"\"Delete all stream keys with the ``key_prefix``.\n\n        .. important::\n            This method is incompatible with redis clusters\n        \"\"\"\n        deleted_streams = await self._flush_all_streams_script(keys=[], args=[f\"{self._key_prefix}*\"])\n        return cast(\"int\", deleted_streams)\n\n```\n# Function Signature: async def unsubscribe(self, channels: Iterable[str]) -> None:\n# Natural Language Annotations: \"\"\"Unsubscribe from ``channels``\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def unsubscribe(self, channels: Iterable[str", "  ```\nasync def unsubscribe(self, channels: Iterable[str", "  ```\nasync def unsubscribe(self, channels: Iterable[str"]}, "litestar-litestar/channels/backends/redis.py-publish": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/channels/backends/redis.py:\n```\nfrom __future__ import annotations\n\nimport asyncio\nimport sys\n\nif sys.version_info < (3, 9):\n    import importlib_resources  # pyright: ignore\nelse:\n    import importlib.resources as importlib_resources\nfrom abc import ABC\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, Iterable, cast\n\nfrom litestar.channels.backends.base import ChannelsBackend\n\nif TYPE_CHECKING:\n    from redis.asyncio import Redis\n    from redis.asyncio.client import PubSub\n\n_resource_path = importlib_resources.files(\"litestar.channels.backends\")\n_PUBSUB_PUBLISH_SCRIPT = (_resource_path / \"_redis_pubsub_publish.lua\").read_text()\n_FLUSHALL_STREAMS_SCRIPT = (_resource_path / \"_redis_flushall_streams.lua\").read_text()\n_XADD_EXPIRE_SCRIPT = (_resource_path / \"_redis_xadd_expire.lua\").read_text()\n\n\nclass _LazyEvent:\n    \"\"\"A lazy proxy to asyncio.Event that only creates the event once it's accessed.\n\n    It ensures that the Event is created within a running event loop. If it's not, there can be an issue where a future\n    within the event itself is attached to a different loop.\n\n    This happens in our tests and could also happen when a user creates an instance of the backend outside an event loop\n    in their application.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.__event: asyncio.Event | None = None\n\n    @property\n    def _event(self) -> asyncio.Event:\n        if self.__event is None:\n            self.__event = asyncio.Event()\n        return self.__event\n\n    def set(self) -> None:\n        self._event.set()\n\n    def clear(self) -> None:\n        self._event.clear()\n\n    async def wait(self) -> None:\n        await self._event.wait()\n\n\nclass RedisChannelsBackend(ChannelsBackend, ABC):\n    def __init__(self, *, redis: Redis, key_prefix: str, stream_sleep_no_subscriptions: int) -> None:\n        \"\"\"Base redis channels backend.\n\n        Args:\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for storing data in redis\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n        \"\"\"\n        self._redis = redis\n        self._key_prefix = key_prefix\n        self._stream_sleep_no_subscriptions = stream_sleep_no_subscriptions\n\n    def _make_key(self, channel: str) -> str:\n        return f\"{self._key_prefix}_{channel.upper()}\"\n\n\nclass RedisChannelsPubSubBackend(RedisChannelsBackend):\n    def __init__(\n        self, *, redis: Redis, stream_sleep_no_subscriptions: int = 1, key_prefix: str = \"LITESTAR_CHANNELS\"\n    ) -> None:\n        \"\"\"Redis channels backend, `Pub/Sub <https://redis.io/docs/manual/pubsub/>`_.\n\n        This backend provides low overhead and resource usage but no support for history.\n\n        Args:\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for storing data in redis\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n        \"\"\"\n        super().__init__(\n            redis=redis, stream_sleep_no_subscriptions=stream_sleep_no_subscriptions, key_prefix=key_prefix\n        )\n        self.__pub_sub: PubSub | None = None\n        self._publish_script = self._redis.register_script(_PUBSUB_PUBLISH_SCRIPT)\n        self._has_subscribed = _LazyEvent()\n\n    @property\n    def _pub_sub(self) -> PubSub:\n        if self.__pub_sub is None:\n            self.__pub_sub = self._redis.pubsub()\n        return self.__pub_sub\n\n    async def on_startup(self) -> None:\n        # this method should not do anything in this case\n        pass\n\n    async def on_shutdown(self) -> None:\n        await self._pub_sub.reset()\n\n    async def subscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Subscribe to ``channels``, and enable publishing to them\"\"\"\n        await self._pub_sub.subscribe(*channels)\n        self._has_subscribed.set()\n\n    async def unsubscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Stop listening for events on ``channels``\"\"\"\n        await self._pub_sub.unsubscribe(*channels)\n        # if we have no active subscriptions, or only subscriptions which are pending\n        # to be unsubscribed we consider the backend to be unsubscribed from all\n        # channels, so we reset the event\n        if not self._pub_sub.channels.keys() - self._pub_sub.pending_unsubscribe_channels:\n            self._has_subscribed.clear()\n\n    async def publish(self, data: bytes, channels: Iterable[str]) -> None:\n        \"\"\"Publish ``data`` to ``channels``\n\n        .. note::\n            This operation is performed atomically, using a lua script\n        \"\"\"\n        await self._publish_script(keys=list(set(channels)), args=[data])\n\n    async def stream_events(self) -> AsyncGenerator[tuple[str, Any], None]:\n        \"\"\"Return a generator, iterating over events of subscribed channels as they become available.\n\n        If no channels have been subscribed to yet via :meth:`subscribe`, sleep for ``stream_sleep_no_subscriptions``\n        milliseconds.\n        \"\"\"\n\n        while True:\n            await self._has_subscribed.wait()\n            message = await self._pub_sub.get_message(\n                ignore_subscribe_messages=True, timeout=self._stream_sleep_no_subscriptions\n            )\n            if message is None:\n                continue\n\n            channel: str = message[\"channel\"].decode()\n            data: bytes = message[\"data\"]\n            # redis handles the unsubscibes with a queue; Unsubscribing doesn't mean the\n            # unsubscribe will happen immediately after requesting it, so we could\n            # receive a message on a channel that, from a client's perspective, it's not\n            # subscribed to anymore\n            if channel.encode() in self._pub_sub.channels.keys() - self._pub_sub.pending_unsubscribe_channels:\n                yield channel, data\n\n    async def get_history(self, channel: str, limit: int | None = None) -> list[bytes]:\n        \"\"\"Not implemented\"\"\"\n        raise NotImplementedError()\n\n\nclass RedisChannelsStreamBackend(RedisChannelsBackend):\n    def __init__(\n        self,\n        history: int,\n        *,\n        redis: Redis,\n        stream_sleep_no_subscriptions: int = 1,\n        cap_streams_approximate: bool = True,\n        stream_ttl: int | timedelta = timedelta(seconds=60),\n        key_prefix: str = \"LITESTAR_CHANNELS\",\n    ) -> None:\n        \"\"\"Redis channels backend, `streams <https://redis.io/docs/data-types/streams/>`_.\n\n        Args:\n            history: Amount of messages to keep. This will set a ``MAXLEN`` to the streams\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for streams\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n            cap_streams_approximate: Set the streams ``MAXLEN`` using the ``~`` approximation\n                operator for improved performance\n            stream_ttl: TTL of a stream in milliseconds or as a timedelta. A streams TTL will be set on each publishing\n                operation using ``PEXPIRE``\n        \"\"\"\n        super().__init__(\n            redis=redis, stream_sleep_no_subscriptions=stream_sleep_no_subscriptions, key_prefix=key_prefix\n        )\n\n        self._history_limit = history\n        self._subscribed_channels: set[str] = set()\n        self._cap_streams_approximate = cap_streams_approximate\n        self._stream_ttl = stream_ttl if isinstance(stream_ttl, int) else int(stream_ttl.total_seconds() * 1000)\n        self._flush_all_streams_script = self._redis.register_script(_FLUSHALL_STREAMS_SCRIPT)\n        self._publish_script = self._redis.register_script(_XADD_EXPIRE_SCRIPT)\n        self._has_subscribed_channels = _LazyEvent()\n\n    async def on_startup(self) -> None:\n        \"\"\"Called on application startup\"\"\"\n\n    async def on_shutdown(self) -> None:\n        \"\"\"Called on application shutdown\"\"\"\n\n    async def subscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Subscribe to ``channels``\"\"\"\n        self._subscribed_channels.update(channels)\n        self._has_subscribed_channels.set()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def _get_subscribed_channels(self) -> set[str]:\n        \"\"\"Get subscribed channels. If no channels are currently subscribed, wait\"\"\"\n        await self._has_subscribed_channels.wait()\n        return self._subscribed_channels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def flush_all(self) -> int:\n        \"\"\"Delete all stream keys with the ``key_prefix``.\n\n        .. important::\n            This method is incompatible with redis clusters\n        \"\"\"\n        deleted_streams = await self._flush_all_streams_script(keys=[], args=[f\"{self._key_prefix}*\"])\n        return cast(\"int\", deleted_streams)\n\n```\n# Function Signature: async def publish(self, data: bytes, channels: Iterable[str]) -> None:\n# Natural Language Annotations: \"\"\"Publish ``data`` to ``channels``.\n\n        .. note::\n            This operation is performed atomically, using a Lua script\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def publish(self, data: bytes, channels: Iter", "  ```\nasync def publish(self, data: bytes, channels: Iter", "  ```\nasync def publish(self, data: bytes, channels: Iter"]}, "litestar-litestar/channels/backends/redis.py-stream_events": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/channels/backends/redis.py:\n```\nfrom __future__ import annotations\n\nimport asyncio\nimport sys\n\nif sys.version_info < (3, 9):\n    import importlib_resources  # pyright: ignore\nelse:\n    import importlib.resources as importlib_resources\nfrom abc import ABC\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, Iterable, cast\n\nfrom litestar.channels.backends.base import ChannelsBackend\n\nif TYPE_CHECKING:\n    from redis.asyncio import Redis\n    from redis.asyncio.client import PubSub\n\n_resource_path = importlib_resources.files(\"litestar.channels.backends\")\n_PUBSUB_PUBLISH_SCRIPT = (_resource_path / \"_redis_pubsub_publish.lua\").read_text()\n_FLUSHALL_STREAMS_SCRIPT = (_resource_path / \"_redis_flushall_streams.lua\").read_text()\n_XADD_EXPIRE_SCRIPT = (_resource_path / \"_redis_xadd_expire.lua\").read_text()\n\n\nclass _LazyEvent:\n    \"\"\"A lazy proxy to asyncio.Event that only creates the event once it's accessed.\n\n    It ensures that the Event is created within a running event loop. If it's not, there can be an issue where a future\n    within the event itself is attached to a different loop.\n\n    This happens in our tests and could also happen when a user creates an instance of the backend outside an event loop\n    in their application.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.__event: asyncio.Event | None = None\n\n    @property\n    def _event(self) -> asyncio.Event:\n        if self.__event is None:\n            self.__event = asyncio.Event()\n        return self.__event\n\n    def set(self) -> None:\n        self._event.set()\n\n    def clear(self) -> None:\n        self._event.clear()\n\n    async def wait(self) -> None:\n        await self._event.wait()\n\n\nclass RedisChannelsBackend(ChannelsBackend, ABC):\n    def __init__(self, *, redis: Redis, key_prefix: str, stream_sleep_no_subscriptions: int) -> None:\n        \"\"\"Base redis channels backend.\n\n        Args:\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for storing data in redis\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n        \"\"\"\n        self._redis = redis\n        self._key_prefix = key_prefix\n        self._stream_sleep_no_subscriptions = stream_sleep_no_subscriptions\n\n    def _make_key(self, channel: str) -> str:\n        return f\"{self._key_prefix}_{channel.upper()}\"\n\n\nclass RedisChannelsPubSubBackend(RedisChannelsBackend):\n    def __init__(\n        self, *, redis: Redis, stream_sleep_no_subscriptions: int = 1, key_prefix: str = \"LITESTAR_CHANNELS\"\n    ) -> None:\n        \"\"\"Redis channels backend, `Pub/Sub <https://redis.io/docs/manual/pubsub/>`_.\n\n        This backend provides low overhead and resource usage but no support for history.\n\n        Args:\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for storing data in redis\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n        \"\"\"\n        super().__init__(\n            redis=redis, stream_sleep_no_subscriptions=stream_sleep_no_subscriptions, key_prefix=key_prefix\n        )\n        self.__pub_sub: PubSub | None = None\n        self._publish_script = self._redis.register_script(_PUBSUB_PUBLISH_SCRIPT)\n        self._has_subscribed = _LazyEvent()\n\n    @property\n    def _pub_sub(self) -> PubSub:\n        if self.__pub_sub is None:\n            self.__pub_sub = self._redis.pubsub()\n        return self.__pub_sub\n\n    async def on_startup(self) -> None:\n        # this method should not do anything in this case\n        pass\n\n    async def on_shutdown(self) -> None:\n        await self._pub_sub.reset()\n\n    async def subscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Subscribe to ``channels``, and enable publishing to them\"\"\"\n        await self._pub_sub.subscribe(*channels)\n        self._has_subscribed.set()\n\n    async def unsubscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Stop listening for events on ``channels``\"\"\"\n        await self._pub_sub.unsubscribe(*channels)\n        # if we have no active subscriptions, or only subscriptions which are pending\n        # to be unsubscribed we consider the backend to be unsubscribed from all\n        # channels, so we reset the event\n        if not self._pub_sub.channels.keys() - self._pub_sub.pending_unsubscribe_channels:\n            self._has_subscribed.clear()\n\n    async def publish(self, data: bytes, channels: Iterable[str]) -> None:\n        \"\"\"Publish ``data`` to ``channels``\n\n        .. note::\n            This operation is performed atomically, using a lua script\n        \"\"\"\n        await self._publish_script(keys=list(set(channels)), args=[data])\n\n    async def stream_events(self) -> AsyncGenerator[tuple[str, Any], None]:\n        \"\"\"Return a generator, iterating over events of subscribed channels as they become available.\n\n        If no channels have been subscribed to yet via :meth:`subscribe`, sleep for ``stream_sleep_no_subscriptions``\n        milliseconds.\n        \"\"\"\n\n        while True:\n            await self._has_subscribed.wait()\n            message = await self._pub_sub.get_message(\n                ignore_subscribe_messages=True, timeout=self._stream_sleep_no_subscriptions\n            )\n            if message is None:\n                continue\n\n            channel: str = message[\"channel\"].decode()\n            data: bytes = message[\"data\"]\n            # redis handles the unsubscibes with a queue; Unsubscribing doesn't mean the\n            # unsubscribe will happen immediately after requesting it, so we could\n            # receive a message on a channel that, from a client's perspective, it's not\n            # subscribed to anymore\n            if channel.encode() in self._pub_sub.channels.keys() - self._pub_sub.pending_unsubscribe_channels:\n                yield channel, data\n\n    async def get_history(self, channel: str, limit: int | None = None) -> list[bytes]:\n        \"\"\"Not implemented\"\"\"\n        raise NotImplementedError()\n\n\nclass RedisChannelsStreamBackend(RedisChannelsBackend):\n    def __init__(\n        self,\n        history: int,\n        *,\n        redis: Redis,\n        stream_sleep_no_subscriptions: int = 1,\n        cap_streams_approximate: bool = True,\n        stream_ttl: int | timedelta = timedelta(seconds=60),\n        key_prefix: str = \"LITESTAR_CHANNELS\",\n    ) -> None:\n        \"\"\"Redis channels backend, `streams <https://redis.io/docs/data-types/streams/>`_.\n\n        Args:\n            history: Amount of messages to keep. This will set a ``MAXLEN`` to the streams\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for streams\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n            cap_streams_approximate: Set the streams ``MAXLEN`` using the ``~`` approximation\n                operator for improved performance\n            stream_ttl: TTL of a stream in milliseconds or as a timedelta. A streams TTL will be set on each publishing\n                operation using ``PEXPIRE``\n        \"\"\"\n        super().__init__(\n            redis=redis, stream_sleep_no_subscriptions=stream_sleep_no_subscriptions, key_prefix=key_prefix\n        )\n\n        self._history_limit = history\n        self._subscribed_channels: set[str] = set()\n        self._cap_streams_approximate = cap_streams_approximate\n        self._stream_ttl = stream_ttl if isinstance(stream_ttl, int) else int(stream_ttl.total_seconds() * 1000)\n        self._flush_all_streams_script = self._redis.register_script(_FLUSHALL_STREAMS_SCRIPT)\n        self._publish_script = self._redis.register_script(_XADD_EXPIRE_SCRIPT)\n        self._has_subscribed_channels = _LazyEvent()\n\n    async def on_startup(self) -> None:\n        \"\"\"Called on application startup\"\"\"\n\n    async def on_shutdown(self) -> None:\n        \"\"\"Called on application shutdown\"\"\"\n\n    async def subscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Subscribe to ``channels``\"\"\"\n        self._subscribed_channels.update(channels)\n        self._has_subscribed_channels.set()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def _get_subscribed_channels(self) -> set[str]:\n        \"\"\"Get subscribed channels. If no channels are currently subscribed, wait\"\"\"\n        await self._has_subscribed_channels.wait()\n        return self._subscribed_channels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def flush_all(self) -> int:\n        \"\"\"Delete all stream keys with the ``key_prefix``.\n\n        .. important::\n            This method is incompatible with redis clusters\n        \"\"\"\n        deleted_streams = await self._flush_all_streams_script(keys=[], args=[f\"{self._key_prefix}*\"])\n        return cast(\"int\", deleted_streams)\n\n```\n# Function Signature: async def stream_events(self) -> AsyncGenerator[tuple[str, Any], None]:\n# Natural Language Annotations: \"\"\"Return a generator, iterating over events of subscribed channels as they become available.\n\n        If no channels have been subscribed to yet via :meth:`subscribe`, sleep for ``stream_sleep_no_subscriptions``\n        milliseconds.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nclass RedisChannelsStreamBackend(RedisChannels", "  ```\nasync def stream_events(self) -> AsyncGenerator[tuple", "  ```\nasync def stream_events(self) -> AsyncGenerator[tuple"]}, "litestar-litestar/channels/backends/redis.py-get_history": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/channels/backends/redis.py:\n```\nfrom __future__ import annotations\n\nimport asyncio\nimport sys\n\nif sys.version_info < (3, 9):\n    import importlib_resources  # pyright: ignore\nelse:\n    import importlib.resources as importlib_resources\nfrom abc import ABC\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, Iterable, cast\n\nfrom litestar.channels.backends.base import ChannelsBackend\n\nif TYPE_CHECKING:\n    from redis.asyncio import Redis\n    from redis.asyncio.client import PubSub\n\n_resource_path = importlib_resources.files(\"litestar.channels.backends\")\n_PUBSUB_PUBLISH_SCRIPT = (_resource_path / \"_redis_pubsub_publish.lua\").read_text()\n_FLUSHALL_STREAMS_SCRIPT = (_resource_path / \"_redis_flushall_streams.lua\").read_text()\n_XADD_EXPIRE_SCRIPT = (_resource_path / \"_redis_xadd_expire.lua\").read_text()\n\n\nclass _LazyEvent:\n    \"\"\"A lazy proxy to asyncio.Event that only creates the event once it's accessed.\n\n    It ensures that the Event is created within a running event loop. If it's not, there can be an issue where a future\n    within the event itself is attached to a different loop.\n\n    This happens in our tests and could also happen when a user creates an instance of the backend outside an event loop\n    in their application.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.__event: asyncio.Event | None = None\n\n    @property\n    def _event(self) -> asyncio.Event:\n        if self.__event is None:\n            self.__event = asyncio.Event()\n        return self.__event\n\n    def set(self) -> None:\n        self._event.set()\n\n    def clear(self) -> None:\n        self._event.clear()\n\n    async def wait(self) -> None:\n        await self._event.wait()\n\n\nclass RedisChannelsBackend(ChannelsBackend, ABC):\n    def __init__(self, *, redis: Redis, key_prefix: str, stream_sleep_no_subscriptions: int) -> None:\n        \"\"\"Base redis channels backend.\n\n        Args:\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for storing data in redis\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n        \"\"\"\n        self._redis = redis\n        self._key_prefix = key_prefix\n        self._stream_sleep_no_subscriptions = stream_sleep_no_subscriptions\n\n    def _make_key(self, channel: str) -> str:\n        return f\"{self._key_prefix}_{channel.upper()}\"\n\n\nclass RedisChannelsPubSubBackend(RedisChannelsBackend):\n    def __init__(\n        self, *, redis: Redis, stream_sleep_no_subscriptions: int = 1, key_prefix: str = \"LITESTAR_CHANNELS\"\n    ) -> None:\n        \"\"\"Redis channels backend, `Pub/Sub <https://redis.io/docs/manual/pubsub/>`_.\n\n        This backend provides low overhead and resource usage but no support for history.\n\n        Args:\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for storing data in redis\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n        \"\"\"\n        super().__init__(\n            redis=redis, stream_sleep_no_subscriptions=stream_sleep_no_subscriptions, key_prefix=key_prefix\n        )\n        self.__pub_sub: PubSub | None = None\n        self._publish_script = self._redis.register_script(_PUBSUB_PUBLISH_SCRIPT)\n        self._has_subscribed = _LazyEvent()\n\n    @property\n    def _pub_sub(self) -> PubSub:\n        if self.__pub_sub is None:\n            self.__pub_sub = self._redis.pubsub()\n        return self.__pub_sub\n\n    async def on_startup(self) -> None:\n        # this method should not do anything in this case\n        pass\n\n    async def on_shutdown(self) -> None:\n        await self._pub_sub.reset()\n\n    async def subscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Subscribe to ``channels``, and enable publishing to them\"\"\"\n        await self._pub_sub.subscribe(*channels)\n        self._has_subscribed.set()\n\n    async def unsubscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Stop listening for events on ``channels``\"\"\"\n        await self._pub_sub.unsubscribe(*channels)\n        # if we have no active subscriptions, or only subscriptions which are pending\n        # to be unsubscribed we consider the backend to be unsubscribed from all\n        # channels, so we reset the event\n        if not self._pub_sub.channels.keys() - self._pub_sub.pending_unsubscribe_channels:\n            self._has_subscribed.clear()\n\n    async def publish(self, data: bytes, channels: Iterable[str]) -> None:\n        \"\"\"Publish ``data`` to ``channels``\n\n        .. note::\n            This operation is performed atomically, using a lua script\n        \"\"\"\n        await self._publish_script(keys=list(set(channels)), args=[data])\n\n    async def stream_events(self) -> AsyncGenerator[tuple[str, Any], None]:\n        \"\"\"Return a generator, iterating over events of subscribed channels as they become available.\n\n        If no channels have been subscribed to yet via :meth:`subscribe`, sleep for ``stream_sleep_no_subscriptions``\n        milliseconds.\n        \"\"\"\n\n        while True:\n            await self._has_subscribed.wait()\n            message = await self._pub_sub.get_message(\n                ignore_subscribe_messages=True, timeout=self._stream_sleep_no_subscriptions\n            )\n            if message is None:\n                continue\n\n            channel: str = message[\"channel\"].decode()\n            data: bytes = message[\"data\"]\n            # redis handles the unsubscibes with a queue; Unsubscribing doesn't mean the\n            # unsubscribe will happen immediately after requesting it, so we could\n            # receive a message on a channel that, from a client's perspective, it's not\n            # subscribed to anymore\n            if channel.encode() in self._pub_sub.channels.keys() - self._pub_sub.pending_unsubscribe_channels:\n                yield channel, data\n\n    async def get_history(self, channel: str, limit: int | None = None) -> list[bytes]:\n        \"\"\"Not implemented\"\"\"\n        raise NotImplementedError()\n\n\nclass RedisChannelsStreamBackend(RedisChannelsBackend):\n    def __init__(\n        self,\n        history: int,\n        *,\n        redis: Redis,\n        stream_sleep_no_subscriptions: int = 1,\n        cap_streams_approximate: bool = True,\n        stream_ttl: int | timedelta = timedelta(seconds=60),\n        key_prefix: str = \"LITESTAR_CHANNELS\",\n    ) -> None:\n        \"\"\"Redis channels backend, `streams <https://redis.io/docs/data-types/streams/>`_.\n\n        Args:\n            history: Amount of messages to keep. This will set a ``MAXLEN`` to the streams\n            redis: A :class:`redis.asyncio.Redis` instance\n            key_prefix: Key prefix to use for streams\n            stream_sleep_no_subscriptions: Amount of time in milliseconds to pause the\n                :meth:`stream_events` generator, should no subscribers exist\n            cap_streams_approximate: Set the streams ``MAXLEN`` using the ``~`` approximation\n                operator for improved performance\n            stream_ttl: TTL of a stream in milliseconds or as a timedelta. A streams TTL will be set on each publishing\n                operation using ``PEXPIRE``\n        \"\"\"\n        super().__init__(\n            redis=redis, stream_sleep_no_subscriptions=stream_sleep_no_subscriptions, key_prefix=key_prefix\n        )\n\n        self._history_limit = history\n        self._subscribed_channels: set[str] = set()\n        self._cap_streams_approximate = cap_streams_approximate\n        self._stream_ttl = stream_ttl if isinstance(stream_ttl, int) else int(stream_ttl.total_seconds() * 1000)\n        self._flush_all_streams_script = self._redis.register_script(_FLUSHALL_STREAMS_SCRIPT)\n        self._publish_script = self._redis.register_script(_XADD_EXPIRE_SCRIPT)\n        self._has_subscribed_channels = _LazyEvent()\n\n    async def on_startup(self) -> None:\n        \"\"\"Called on application startup\"\"\"\n\n    async def on_shutdown(self) -> None:\n        \"\"\"Called on application shutdown\"\"\"\n\n    async def subscribe(self, channels: Iterable[str]) -> None:\n        \"\"\"Subscribe to ``channels``\"\"\"\n        self._subscribed_channels.update(channels)\n        self._has_subscribed_channels.set()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def _get_subscribed_channels(self) -> set[str]:\n        \"\"\"Get subscribed channels. If no channels are currently subscribed, wait\"\"\"\n        await self._has_subscribed_channels.wait()\n        return self._subscribed_channels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def flush_all(self) -> int:\n        \"\"\"Delete all stream keys with the ``key_prefix``.\n\n        .. important::\n            This method is incompatible with redis clusters\n        \"\"\"\n        deleted_streams = await self._flush_all_streams_script(keys=[], args=[f\"{self._key_prefix}*\"])\n        return cast(\"int\", deleted_streams)\n\n```\n# Function Signature: async def get_history(self, channel: str, limit: int | None = None) -> list[bytes]:\n# Natural Language Annotations: \"\"\"Return the history of ``channels``, returning at most ``limit`` messages\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def get_history(self, channel: str, limit", "  ```\nasync def get_history(self, channel: str, limit", "  ```\nasync def get_history(self, channel: str, limit"]}, "litestar-litestar/dto/_backend.py-create_transfer_model_type": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/dto/_backend.py:\n```\n\"\"\"DTO backends do the heavy lifting of decoding and validating raw bytes into domain models, and\nback again, to bytes.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import replace\nfrom typing import (\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    ClassVar,\n    Collection,\n    Final,\n    Mapping,\n    Protocol,\n    Union,\n    cast,\n)\n\nimport msgspec\nfrom msgspec import UNSET, Struct, UnsetType, convert, defstruct, field\nfrom typing_extensions import Annotated\n\nfrom litestar.dto._types import (\n    CollectionType,\n    CompositeType,\n    MappingType,\n    NestedFieldInfo,\n    SimpleType,\n    TransferDTOFieldDefinition,\n    TransferType,\n    TupleType,\n    UnionType,\n)\nfrom litestar.dto.data_structures import DTOData, DTOFieldDefinition\nfrom litestar.dto.field import Mark\nfrom litestar.enums import RequestEncodingType\nfrom litestar.params import KwargDefinition\nfrom litestar.serialization import decode_json, decode_msgpack\nfrom litestar.types import Empty\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils import unique_name_for_scope\n\nif TYPE_CHECKING:\n    from litestar.connection import ASGIConnection\n    from litestar.dto import AbstractDTO, RenameStrategy\n    from litestar.types.serialization import LitestarEncodableType\n\n__all__ = (\"DTOBackend\",)\n\n\nclass CompositeTypeHandler(Protocol):\n    def __call__(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType: ...\n\n\nclass DTOBackend:\n    __slots__ = (\n        \"annotation\",\n        \"dto_data_type\",\n        \"dto_factory\",\n        \"field_definition\",\n        \"handler_id\",\n        \"is_data_field\",\n        \"model_type\",\n        \"parsed_field_definitions\",\n        \"reverse_name_map\",\n        \"transfer_model_type\",\n        \"wrapper_attribute_name\",\n    )\n\n    _seen_model_names: ClassVar[set[str]] = set()\n\n    def __init__(\n        self,\n        dto_factory: type[AbstractDTO],\n        field_definition: FieldDefinition,\n        handler_id: str,\n        is_data_field: bool,\n        model_type: type[Any],\n        wrapper_attribute_name: str | None,\n    ) -> None:\n        \"\"\"Create dto backend instance.\n\n        Args:\n            dto_factory: The DTO factory class calling this backend.\n            field_definition: Parsed type.\n            handler_id: The name of the handler that this backend is for.\n            is_data_field: Whether the field is a subclass of DTOData.\n            model_type: Model type.\n            wrapper_attribute_name: If the data that DTO should operate upon is wrapped in a generic datastructure, this is the name of the attribute that the data is stored in.\n        \"\"\"\n        self.dto_factory: Final[type[AbstractDTO]] = dto_factory\n        self.field_definition: Final[FieldDefinition] = field_definition\n        self.is_data_field: Final[bool] = is_data_field\n        self.handler_id: Final[str] = handler_id\n        self.model_type: Final[type[Any]] = model_type\n        self.wrapper_attribute_name: Final[str | None] = wrapper_attribute_name\n\n        self.parsed_field_definitions = self.parse_model(\n            model_type=model_type,\n            exclude=self.dto_factory.config.exclude,\n            include=self.dto_factory.config.include,\n            rename_fields=self.dto_factory.config.rename_fields,\n        )\n        self.transfer_model_type = self.create_transfer_model_type(\n            model_name=model_type.__name__, field_definitions=self.parsed_field_definitions\n        )\n        self.dto_data_type: type[DTOData] | None = None\n\n        if field_definition.is_subclass_of(DTOData):\n            self.dto_data_type = field_definition.annotation\n            field_definition = self.field_definition.inner_types[0]\n\n        self.annotation = build_annotation_for_backend(model_type, field_definition, self.transfer_model_type)\n\n    def parse_model(\n        self,\n        model_type: Any,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        nested_depth: int = 0,\n    ) -> tuple[TransferDTOFieldDefinition, ...]:\n        \"\"\"Reduce :attr:`model_type` to a tuple :class:`TransferDTOFieldDefinition` instances.\n\n        Returns:\n        Fields for data transfer.\n        \"\"\"\n        defined_fields = []\n        generic_field_definitons = list(FieldDefinition.from_annotation(model_type).generic_types or ())\n        for field_definition in self.dto_factory.generate_field_definitions(model_type):\n            if field_definition.is_type_var:\n                base_arg_field = generic_field_definitons.pop()\n                field_definition = replace(\n                    field_definition, annotation=base_arg_field.annotation, raw=base_arg_field.raw\n                )\n\n            if _should_mark_private(field_definition, self.dto_factory.config.underscore_fields_private):\n                field_definition.dto_field.mark = Mark.PRIVATE\n\n            try:\n                transfer_type = self._create_transfer_type(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    rename_fields=rename_fields,\n                    field_name=field_definition.name,\n                    unique_name=field_definition.model_name,\n                    nested_depth=nested_depth,\n                )\n            except RecursionError:\n                continue\n\n            transfer_field_definition = TransferDTOFieldDefinition.from_dto_field_definition(\n                field_definition=field_definition,\n                serialization_name=rename_fields.get(field_definition.name),\n                transfer_type=transfer_type,\n                is_partial=self.dto_factory.config.partial,\n                is_excluded=_should_exclude_field(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    is_data_field=self.is_data_field,\n                ),\n            )\n            defined_fields.append(transfer_field_definition)\n        return tuple(defined_fields)\n\n    def _create_transfer_model_name(self, model_name: str) -> str:\n        long_name_prefix = self.handler_id.split(\"::\")[0]\n        short_name_prefix = _camelize(long_name_prefix.split(\".\")[-1], True)\n\n        name_suffix = \"RequestBody\" if self.is_data_field else \"ResponseBody\"\n\n        if (short_name := f\"{short_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = short_name\n        elif (long_name := f\"{long_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = long_name\n        else:\n            name = unique_name_for_scope(long_name, self._seen_model_names)\n\n        self._seen_model_names.add(name)\n\n        return name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def parse_raw(self, raw: bytes, asgi_connection: ASGIConnection) -> Struct | Collection[Struct]:\n        \"\"\"Parse raw bytes into transfer model type.\n\n        Args:\n            raw: bytes\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The raw bytes parsed into transfer model type.\n        \"\"\"\n        request_encoding = RequestEncodingType.JSON\n\n        if (content_type := getattr(asgi_connection, \"content_type\", None)) and (media_type := content_type[0]):\n            request_encoding = media_type\n\n        type_decoders = asgi_connection.route_handler.resolve_type_decoders()\n\n        if request_encoding == RequestEncodingType.MESSAGEPACK:\n            result = decode_msgpack(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n        else:\n            result = decode_json(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n\n        return cast(\"Struct | Collection[Struct]\", result)\n\n    def parse_builtins(self, builtins: Any, asgi_connection: ASGIConnection) -> Any:\n        \"\"\"Parse builtin types into transfer model type.\n\n        Args:\n            builtins: Builtin type.\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The builtin type parsed into transfer model type.\n        \"\"\"\n        return convert(\n            obj=builtins,\n            type=self.annotation,\n            dec_hook=asgi_connection.route_handler.default_deserializer,\n            strict=False,\n            str_keys=True,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def transfer_data_from_builtins(self, builtins: Any) -> Any:\n        \"\"\"Populate model instance from builtin types.\n\n        Args:\n            builtins: Builtin type.\n\n        Returns:\n            Instance or collection of ``model_type`` instances.\n        \"\"\"\n        return _transfer_data(\n            destination_type=self.model_type,\n            source_data=builtins,\n            field_definitions=self.parsed_field_definitions,\n            field_definition=self.field_definition,\n            is_data_field=self.is_data_field,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_handler_for_field_definition(self, field_definition: FieldDefinition) -> CompositeTypeHandler | None:\n        if field_definition.is_union:\n            return self._create_union_type\n\n        if field_definition.is_tuple:\n            if len(field_definition.inner_types) == 2 and field_definition.inner_types[1].annotation is Ellipsis:\n                return self._create_collection_type\n            return self._create_tuple_type\n\n        if field_definition.is_mapping:\n            return self._create_mapping_type\n\n        if field_definition.is_non_string_collection:\n            return self._create_collection_type\n        return None\n\n    def _create_transfer_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        field_name: str,\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType | SimpleType:\n        exclude = _filter_nested_field(exclude, field_name)\n        include = _filter_nested_field(include, field_name)\n        rename_fields = _filter_nested_field_mapping(rename_fields, field_name)\n\n        if composite_type_handler := self._get_handler_for_field_definition(field_definition):\n            return composite_type_handler(\n                field_definition=field_definition,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                unique_name=unique_name,\n                nested_depth=nested_depth,\n            )\n\n        transfer_model: NestedFieldInfo | None = None\n\n        if self.dto_factory.detect_nested_field(field_definition):\n            if nested_depth == self.dto_factory.config.max_nested_depth:\n                raise RecursionError\n\n            unique_name = f\"{unique_name}{field_definition.raw.__name__}\"\n\n            nested_field_definitions = self.parse_model(\n                model_type=field_definition.annotation,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                nested_depth=nested_depth + 1,\n            )\n\n            transfer_model = NestedFieldInfo(\n                model=self.create_transfer_model_type(unique_name, nested_field_definitions),\n                field_definitions=nested_field_definitions,\n            )\n\n        return SimpleType(field_definition, nested_field_info=transfer_model)\n\n    def _create_collection_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CollectionType:\n        inner_types = field_definition.inner_types\n        inner_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return CollectionType(\n            field_definition=field_definition, inner_type=inner_type, has_nested=inner_type.has_nested\n        )\n\n    def _create_mapping_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> MappingType:\n        inner_types = field_definition.inner_types\n        key_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        value_type = self._create_transfer_type(\n            field_definition=inner_types[1] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"1\",\n            unique_name=f\"{unique_name}_1\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return MappingType(\n            field_definition=field_definition,\n            key_type=key_type,\n            value_type=value_type,\n            has_nested=key_type.has_nested or value_type.has_nested,\n        )\n\n    def _create_tuple_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> TupleType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return TupleType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n    def _create_union_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> UnionType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return UnionType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n\ndef _camelize(value: str, capitalize_first_letter: bool) -> str:\n    return \"\".join(\n        word if index == 0 and not capitalize_first_letter else word.capitalize()\n        for index, word in enumerate(value.split(\"_\"))\n    )\n\n\ndef _filter_nested_field(field_name_set: AbstractSet[str], field_name: str) -> AbstractSet[str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {split[1] for s in field_name_set if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1}\n\n\ndef _filter_nested_field_mapping(field_name_mapping: Mapping[str, str], field_name: str) -> dict[str, str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {\n        split[1]: v\n        for s, v in field_name_mapping.items()\n        if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1\n    }\n\n\ndef _transfer_data(\n    destination_type: type[Any],\n    source_data: Any | Collection[Any],\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    field_definition: FieldDefinition,\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance or iterable of instances of ``destination_type``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_data: data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        field_definition: the parsed type that represents the handler annotation for which the DTO is being applied.\n        is_data_field: whether the DTO is being applied to a ``data`` field.\n\n    Returns:\n        Data parsed into ``destination_type``.\n    \"\"\"\n    if field_definition.is_non_string_collection:\n        if not field_definition.is_mapping:\n            return field_definition.instantiable_origin(\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=item,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[0],\n                    is_data_field=is_data_field,\n                )\n                for item in source_data\n            )\n        return field_definition.instantiable_origin(\n            (\n                key,\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=value,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[1],\n                    is_data_field=is_data_field,\n                ),\n            )\n            for key, value in source_data.items()  # type: ignore[union-attr]\n        )\n\n    return _transfer_instance_data(\n        destination_type=destination_type,\n        source_instance=source_data,\n        field_definitions=field_definitions,\n        is_data_field=is_data_field,\n    )\n\n\ndef _transfer_instance_data(\n    destination_type: type[Any],\n    source_instance: Any,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance of ``destination_type`` with data from ``source_instance``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_instance: primitive data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        is_data_field: whether the given field is a 'data' kwarg field.\n\n    Returns:\n        Data parsed into ``model_type``.\n    \"\"\"\n    unstructured_data = {}\n\n    for field_definition in field_definitions:\n        if not is_data_field:\n            if field_definition.is_excluded:\n                continue\n        elif not (\n            field_definition.name in source_instance\n            if isinstance(source_instance, Mapping)\n            else hasattr(source_instance, field_definition.name)\n        ):\n            continue\n\n        transfer_type = field_definition.transfer_type\n        source_value = (\n            source_instance[field_definition.name]\n            if isinstance(source_instance, Mapping)\n            else getattr(source_instance, field_definition.name)\n        )\n\n        if field_definition.is_partial and is_data_field and source_value is UNSET:\n            continue\n\n        unstructured_data[field_definition.name] = _transfer_type_data(\n            source_value=source_value,\n            transfer_type=transfer_type,\n            nested_as_dict=destination_type is dict,\n            is_data_field=is_data_field,\n        )\n\n    return destination_type(**unstructured_data)\n\n\ndef _transfer_type_data(\n    source_value: Any,\n    transfer_type: TransferType,\n    nested_as_dict: bool,\n    is_data_field: bool,\n) -> Any:\n    if isinstance(transfer_type, SimpleType) and transfer_type.nested_field_info:\n        if nested_as_dict:\n            destination_type: Any = dict\n        elif is_data_field:\n            destination_type = transfer_type.field_definition.annotation\n        else:\n            destination_type = transfer_type.nested_field_info.model\n\n        return _transfer_instance_data(\n            destination_type=destination_type,\n            source_instance=source_value,\n            field_definitions=transfer_type.nested_field_info.field_definitions,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, UnionType) and transfer_type.has_nested:\n        return _transfer_nested_union_type_data(\n            transfer_type=transfer_type,\n            source_value=source_value,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, CollectionType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                _transfer_type_data(\n                    source_value=item,\n                    transfer_type=transfer_type.inner_type,\n                    nested_as_dict=False,\n                    is_data_field=is_data_field,\n                )\n                for item in source_value\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    if isinstance(transfer_type, MappingType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                (\n                    key,\n                    _transfer_type_data(\n                        source_value=value,\n                        transfer_type=transfer_type.value_type,\n                        nested_as_dict=False,\n                        is_data_field=is_data_field,\n                    ),\n                )\n                for key, value in source_value.items()\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    return source_value\n\n\ndef _transfer_nested_union_type_data(\n    transfer_type: UnionType,\n    source_value: Any,\n    is_data_field: bool,\n) -> Any:\n    for inner_type in transfer_type.inner_types:\n        if isinstance(inner_type, CompositeType):\n            raise RuntimeError(\"Composite inner types not (yet) supported for nested unions.\")\n\n        if inner_type.nested_field_info and isinstance(\n            source_value,\n            inner_type.nested_field_info.model if is_data_field else inner_type.field_definition.annotation,\n        ):\n            return _transfer_instance_data(\n                destination_type=inner_type.field_definition.annotation\n                if is_data_field\n                else inner_type.nested_field_info.model,\n                source_instance=source_value,\n                field_definitions=inner_type.nested_field_info.field_definitions,\n                is_data_field=is_data_field,\n            )\n    return source_value\n\n\ndef _create_msgspec_field(field_definition: TransferDTOFieldDefinition) -> Any:\n    kwargs: dict[str, Any] = {}\n    if field_definition.is_partial:\n        kwargs[\"default\"] = UNSET\n\n    elif field_definition.default is not Empty:\n        kwargs[\"default\"] = field_definition.default\n\n    elif field_definition.default_factory is not None:\n        kwargs[\"default_factory\"] = field_definition.default_factory\n\n    if field_definition.serialization_name is not None:\n        kwargs[\"name\"] = field_definition.serialization_name\n\n    return field(**kwargs)\n\n\ndef _create_struct_field_meta_for_field_definition(field_definition: TransferDTOFieldDefinition) -> msgspec.Meta | None:\n    if (kwarg_definition := field_definition.kwarg_definition) is None or not isinstance(\n        kwarg_definition, KwargDefinition\n    ):\n        return None\n\n    return msgspec.Meta(\n        description=kwarg_definition.description,\n        examples=[e.value for e in kwarg_definition.examples or []],\n        ge=kwarg_definition.ge,\n        gt=kwarg_definition.gt,\n        le=kwarg_definition.le,\n        lt=kwarg_definition.lt,\n        max_length=kwarg_definition.max_length if not field_definition.is_partial else None,\n        min_length=kwarg_definition.min_length if not field_definition.is_partial else None,\n        multiple_of=kwarg_definition.multiple_of,\n        pattern=kwarg_definition.pattern,\n        title=kwarg_definition.title,\n    )\n\n\ndef _create_struct_for_field_definitions(\n    model_name: str,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    rename_strategy: RenameStrategy | dict[str, str] | None,\n) -> type[Struct]:\n    struct_fields: list[tuple[str, type] | tuple[str, type, type]] = []\n\n    for field_definition in field_definitions:\n        if field_definition.is_excluded:\n            continue\n\n        field_type = _create_transfer_model_type_annotation(field_definition.transfer_type)\n        if field_definition.is_partial:\n            field_type = Union[field_type, UnsetType]\n\n        if (field_meta := _create_struct_field_meta_for_field_definition(field_definition)) is not None:\n            field_type = Annotated[field_type, field_meta]\n\n        struct_fields.append(\n            (\n                field_definition.name,\n                field_type,\n                _create_msgspec_field(field_definition),\n            )\n        )\n    return defstruct(model_name, struct_fields, frozen=True, kw_only=True, rename=rename_strategy)\n\n\ndef build_annotation_for_backend(\n    model_type: type[Any], field_definition: FieldDefinition, transfer_model: type[Struct]\n) -> Any:\n    \"\"\"A helper to re-build a generic outer type with new inner type.\n\n    Args:\n        model_type: The original model type.\n        field_definition: The parsed type that represents the handler annotation for which the DTO is being applied.\n        transfer_model: The transfer model generated to represent the model type.\n\n    Returns:\n\n```\n# Function Signature: def create_transfer_model_type(\n# Natural Language Annotations: \"\"\"Create a model for data transfer.\n\n        Args:\n            model_name: name for the type that should be unique across all transfer types.\n            field_definitions: field definitions for the container type.\n\n        Returns:\n            A ``BackendT`` class.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  baz\nWhen we reach a stopping point, we can step back and think", "  [CK]\ndef create_transfer_model_type(model", "  ```\ndef create_transfer_model_type(model_name"]}, "litestar-litestar/dto/_backend.py-populate_data_from_builtins": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/dto/_backend.py:\n```\n\"\"\"DTO backends do the heavy lifting of decoding and validating raw bytes into domain models, and\nback again, to bytes.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import replace\nfrom typing import (\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    ClassVar,\n    Collection,\n    Final,\n    Mapping,\n    Protocol,\n    Union,\n    cast,\n)\n\nimport msgspec\nfrom msgspec import UNSET, Struct, UnsetType, convert, defstruct, field\nfrom typing_extensions import Annotated\n\nfrom litestar.dto._types import (\n    CollectionType,\n    CompositeType,\n    MappingType,\n    NestedFieldInfo,\n    SimpleType,\n    TransferDTOFieldDefinition,\n    TransferType,\n    TupleType,\n    UnionType,\n)\nfrom litestar.dto.data_structures import DTOData, DTOFieldDefinition\nfrom litestar.dto.field import Mark\nfrom litestar.enums import RequestEncodingType\nfrom litestar.params import KwargDefinition\nfrom litestar.serialization import decode_json, decode_msgpack\nfrom litestar.types import Empty\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils import unique_name_for_scope\n\nif TYPE_CHECKING:\n    from litestar.connection import ASGIConnection\n    from litestar.dto import AbstractDTO, RenameStrategy\n    from litestar.types.serialization import LitestarEncodableType\n\n__all__ = (\"DTOBackend\",)\n\n\nclass CompositeTypeHandler(Protocol):\n    def __call__(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType: ...\n\n\nclass DTOBackend:\n    __slots__ = (\n        \"annotation\",\n        \"dto_data_type\",\n        \"dto_factory\",\n        \"field_definition\",\n        \"handler_id\",\n        \"is_data_field\",\n        \"model_type\",\n        \"parsed_field_definitions\",\n        \"reverse_name_map\",\n        \"transfer_model_type\",\n        \"wrapper_attribute_name\",\n    )\n\n    _seen_model_names: ClassVar[set[str]] = set()\n\n    def __init__(\n        self,\n        dto_factory: type[AbstractDTO],\n        field_definition: FieldDefinition,\n        handler_id: str,\n        is_data_field: bool,\n        model_type: type[Any],\n        wrapper_attribute_name: str | None,\n    ) -> None:\n        \"\"\"Create dto backend instance.\n\n        Args:\n            dto_factory: The DTO factory class calling this backend.\n            field_definition: Parsed type.\n            handler_id: The name of the handler that this backend is for.\n            is_data_field: Whether the field is a subclass of DTOData.\n            model_type: Model type.\n            wrapper_attribute_name: If the data that DTO should operate upon is wrapped in a generic datastructure, this is the name of the attribute that the data is stored in.\n        \"\"\"\n        self.dto_factory: Final[type[AbstractDTO]] = dto_factory\n        self.field_definition: Final[FieldDefinition] = field_definition\n        self.is_data_field: Final[bool] = is_data_field\n        self.handler_id: Final[str] = handler_id\n        self.model_type: Final[type[Any]] = model_type\n        self.wrapper_attribute_name: Final[str | None] = wrapper_attribute_name\n\n        self.parsed_field_definitions = self.parse_model(\n            model_type=model_type,\n            exclude=self.dto_factory.config.exclude,\n            include=self.dto_factory.config.include,\n            rename_fields=self.dto_factory.config.rename_fields,\n        )\n        self.transfer_model_type = self.create_transfer_model_type(\n            model_name=model_type.__name__, field_definitions=self.parsed_field_definitions\n        )\n        self.dto_data_type: type[DTOData] | None = None\n\n        if field_definition.is_subclass_of(DTOData):\n            self.dto_data_type = field_definition.annotation\n            field_definition = self.field_definition.inner_types[0]\n\n        self.annotation = build_annotation_for_backend(model_type, field_definition, self.transfer_model_type)\n\n    def parse_model(\n        self,\n        model_type: Any,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        nested_depth: int = 0,\n    ) -> tuple[TransferDTOFieldDefinition, ...]:\n        \"\"\"Reduce :attr:`model_type` to a tuple :class:`TransferDTOFieldDefinition` instances.\n\n        Returns:\n        Fields for data transfer.\n        \"\"\"\n        defined_fields = []\n        generic_field_definitons = list(FieldDefinition.from_annotation(model_type).generic_types or ())\n        for field_definition in self.dto_factory.generate_field_definitions(model_type):\n            if field_definition.is_type_var:\n                base_arg_field = generic_field_definitons.pop()\n                field_definition = replace(\n                    field_definition, annotation=base_arg_field.annotation, raw=base_arg_field.raw\n                )\n\n            if _should_mark_private(field_definition, self.dto_factory.config.underscore_fields_private):\n                field_definition.dto_field.mark = Mark.PRIVATE\n\n            try:\n                transfer_type = self._create_transfer_type(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    rename_fields=rename_fields,\n                    field_name=field_definition.name,\n                    unique_name=field_definition.model_name,\n                    nested_depth=nested_depth,\n                )\n            except RecursionError:\n                continue\n\n            transfer_field_definition = TransferDTOFieldDefinition.from_dto_field_definition(\n                field_definition=field_definition,\n                serialization_name=rename_fields.get(field_definition.name),\n                transfer_type=transfer_type,\n                is_partial=self.dto_factory.config.partial,\n                is_excluded=_should_exclude_field(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    is_data_field=self.is_data_field,\n                ),\n            )\n            defined_fields.append(transfer_field_definition)\n        return tuple(defined_fields)\n\n    def _create_transfer_model_name(self, model_name: str) -> str:\n        long_name_prefix = self.handler_id.split(\"::\")[0]\n        short_name_prefix = _camelize(long_name_prefix.split(\".\")[-1], True)\n\n        name_suffix = \"RequestBody\" if self.is_data_field else \"ResponseBody\"\n\n        if (short_name := f\"{short_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = short_name\n        elif (long_name := f\"{long_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = long_name\n        else:\n            name = unique_name_for_scope(long_name, self._seen_model_names)\n\n        self._seen_model_names.add(name)\n\n        return name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def parse_raw(self, raw: bytes, asgi_connection: ASGIConnection) -> Struct | Collection[Struct]:\n        \"\"\"Parse raw bytes into transfer model type.\n\n        Args:\n            raw: bytes\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The raw bytes parsed into transfer model type.\n        \"\"\"\n        request_encoding = RequestEncodingType.JSON\n\n        if (content_type := getattr(asgi_connection, \"content_type\", None)) and (media_type := content_type[0]):\n            request_encoding = media_type\n\n        type_decoders = asgi_connection.route_handler.resolve_type_decoders()\n\n        if request_encoding == RequestEncodingType.MESSAGEPACK:\n            result = decode_msgpack(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n        else:\n            result = decode_json(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n\n        return cast(\"Struct | Collection[Struct]\", result)\n\n    def parse_builtins(self, builtins: Any, asgi_connection: ASGIConnection) -> Any:\n        \"\"\"Parse builtin types into transfer model type.\n\n        Args:\n            builtins: Builtin type.\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The builtin type parsed into transfer model type.\n        \"\"\"\n        return convert(\n            obj=builtins,\n            type=self.annotation,\n            dec_hook=asgi_connection.route_handler.default_deserializer,\n            strict=False,\n            str_keys=True,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def transfer_data_from_builtins(self, builtins: Any) -> Any:\n        \"\"\"Populate model instance from builtin types.\n\n        Args:\n            builtins: Builtin type.\n\n        Returns:\n            Instance or collection of ``model_type`` instances.\n        \"\"\"\n        return _transfer_data(\n            destination_type=self.model_type,\n            source_data=builtins,\n            field_definitions=self.parsed_field_definitions,\n            field_definition=self.field_definition,\n            is_data_field=self.is_data_field,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_handler_for_field_definition(self, field_definition: FieldDefinition) -> CompositeTypeHandler | None:\n        if field_definition.is_union:\n            return self._create_union_type\n\n        if field_definition.is_tuple:\n            if len(field_definition.inner_types) == 2 and field_definition.inner_types[1].annotation is Ellipsis:\n                return self._create_collection_type\n            return self._create_tuple_type\n\n        if field_definition.is_mapping:\n            return self._create_mapping_type\n\n        if field_definition.is_non_string_collection:\n            return self._create_collection_type\n        return None\n\n    def _create_transfer_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        field_name: str,\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType | SimpleType:\n        exclude = _filter_nested_field(exclude, field_name)\n        include = _filter_nested_field(include, field_name)\n        rename_fields = _filter_nested_field_mapping(rename_fields, field_name)\n\n        if composite_type_handler := self._get_handler_for_field_definition(field_definition):\n            return composite_type_handler(\n                field_definition=field_definition,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                unique_name=unique_name,\n                nested_depth=nested_depth,\n            )\n\n        transfer_model: NestedFieldInfo | None = None\n\n        if self.dto_factory.detect_nested_field(field_definition):\n            if nested_depth == self.dto_factory.config.max_nested_depth:\n                raise RecursionError\n\n            unique_name = f\"{unique_name}{field_definition.raw.__name__}\"\n\n            nested_field_definitions = self.parse_model(\n                model_type=field_definition.annotation,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                nested_depth=nested_depth + 1,\n            )\n\n            transfer_model = NestedFieldInfo(\n                model=self.create_transfer_model_type(unique_name, nested_field_definitions),\n                field_definitions=nested_field_definitions,\n            )\n\n        return SimpleType(field_definition, nested_field_info=transfer_model)\n\n    def _create_collection_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CollectionType:\n        inner_types = field_definition.inner_types\n        inner_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return CollectionType(\n            field_definition=field_definition, inner_type=inner_type, has_nested=inner_type.has_nested\n        )\n\n    def _create_mapping_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> MappingType:\n        inner_types = field_definition.inner_types\n        key_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        value_type = self._create_transfer_type(\n            field_definition=inner_types[1] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"1\",\n            unique_name=f\"{unique_name}_1\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return MappingType(\n            field_definition=field_definition,\n            key_type=key_type,\n            value_type=value_type,\n            has_nested=key_type.has_nested or value_type.has_nested,\n        )\n\n    def _create_tuple_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> TupleType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return TupleType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n    def _create_union_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> UnionType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return UnionType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n\ndef _camelize(value: str, capitalize_first_letter: bool) -> str:\n    return \"\".join(\n        word if index == 0 and not capitalize_first_letter else word.capitalize()\n        for index, word in enumerate(value.split(\"_\"))\n    )\n\n\ndef _filter_nested_field(field_name_set: AbstractSet[str], field_name: str) -> AbstractSet[str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {split[1] for s in field_name_set if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1}\n\n\ndef _filter_nested_field_mapping(field_name_mapping: Mapping[str, str], field_name: str) -> dict[str, str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {\n        split[1]: v\n        for s, v in field_name_mapping.items()\n        if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1\n    }\n\n\ndef _transfer_data(\n    destination_type: type[Any],\n    source_data: Any | Collection[Any],\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    field_definition: FieldDefinition,\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance or iterable of instances of ``destination_type``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_data: data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        field_definition: the parsed type that represents the handler annotation for which the DTO is being applied.\n        is_data_field: whether the DTO is being applied to a ``data`` field.\n\n    Returns:\n        Data parsed into ``destination_type``.\n    \"\"\"\n    if field_definition.is_non_string_collection:\n        if not field_definition.is_mapping:\n            return field_definition.instantiable_origin(\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=item,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[0],\n                    is_data_field=is_data_field,\n                )\n                for item in source_data\n            )\n        return field_definition.instantiable_origin(\n            (\n                key,\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=value,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[1],\n                    is_data_field=is_data_field,\n                ),\n            )\n            for key, value in source_data.items()  # type: ignore[union-attr]\n        )\n\n    return _transfer_instance_data(\n        destination_type=destination_type,\n        source_instance=source_data,\n        field_definitions=field_definitions,\n        is_data_field=is_data_field,\n    )\n\n\ndef _transfer_instance_data(\n    destination_type: type[Any],\n    source_instance: Any,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance of ``destination_type`` with data from ``source_instance``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_instance: primitive data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        is_data_field: whether the given field is a 'data' kwarg field.\n\n    Returns:\n        Data parsed into ``model_type``.\n    \"\"\"\n    unstructured_data = {}\n\n    for field_definition in field_definitions:\n        if not is_data_field:\n            if field_definition.is_excluded:\n                continue\n        elif not (\n            field_definition.name in source_instance\n            if isinstance(source_instance, Mapping)\n            else hasattr(source_instance, field_definition.name)\n        ):\n            continue\n\n        transfer_type = field_definition.transfer_type\n        source_value = (\n            source_instance[field_definition.name]\n            if isinstance(source_instance, Mapping)\n            else getattr(source_instance, field_definition.name)\n        )\n\n        if field_definition.is_partial and is_data_field and source_value is UNSET:\n            continue\n\n        unstructured_data[field_definition.name] = _transfer_type_data(\n            source_value=source_value,\n            transfer_type=transfer_type,\n            nested_as_dict=destination_type is dict,\n            is_data_field=is_data_field,\n        )\n\n    return destination_type(**unstructured_data)\n\n\ndef _transfer_type_data(\n    source_value: Any,\n    transfer_type: TransferType,\n    nested_as_dict: bool,\n    is_data_field: bool,\n) -> Any:\n    if isinstance(transfer_type, SimpleType) and transfer_type.nested_field_info:\n        if nested_as_dict:\n            destination_type: Any = dict\n        elif is_data_field:\n            destination_type = transfer_type.field_definition.annotation\n        else:\n            destination_type = transfer_type.nested_field_info.model\n\n        return _transfer_instance_data(\n            destination_type=destination_type,\n            source_instance=source_value,\n            field_definitions=transfer_type.nested_field_info.field_definitions,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, UnionType) and transfer_type.has_nested:\n        return _transfer_nested_union_type_data(\n            transfer_type=transfer_type,\n            source_value=source_value,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, CollectionType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                _transfer_type_data(\n                    source_value=item,\n                    transfer_type=transfer_type.inner_type,\n                    nested_as_dict=False,\n                    is_data_field=is_data_field,\n                )\n                for item in source_value\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    if isinstance(transfer_type, MappingType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                (\n                    key,\n                    _transfer_type_data(\n                        source_value=value,\n                        transfer_type=transfer_type.value_type,\n                        nested_as_dict=False,\n                        is_data_field=is_data_field,\n                    ),\n                )\n                for key, value in source_value.items()\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    return source_value\n\n\ndef _transfer_nested_union_type_data(\n    transfer_type: UnionType,\n    source_value: Any,\n    is_data_field: bool,\n) -> Any:\n    for inner_type in transfer_type.inner_types:\n        if isinstance(inner_type, CompositeType):\n            raise RuntimeError(\"Composite inner types not (yet) supported for nested unions.\")\n\n        if inner_type.nested_field_info and isinstance(\n            source_value,\n            inner_type.nested_field_info.model if is_data_field else inner_type.field_definition.annotation,\n        ):\n            return _transfer_instance_data(\n                destination_type=inner_type.field_definition.annotation\n                if is_data_field\n                else inner_type.nested_field_info.model,\n                source_instance=source_value,\n                field_definitions=inner_type.nested_field_info.field_definitions,\n                is_data_field=is_data_field,\n            )\n    return source_value\n\n\ndef _create_msgspec_field(field_definition: TransferDTOFieldDefinition) -> Any:\n    kwargs: dict[str, Any] = {}\n    if field_definition.is_partial:\n        kwargs[\"default\"] = UNSET\n\n    elif field_definition.default is not Empty:\n        kwargs[\"default\"] = field_definition.default\n\n    elif field_definition.default_factory is not None:\n        kwargs[\"default_factory\"] = field_definition.default_factory\n\n    if field_definition.serialization_name is not None:\n        kwargs[\"name\"] = field_definition.serialization_name\n\n    return field(**kwargs)\n\n\ndef _create_struct_field_meta_for_field_definition(field_definition: TransferDTOFieldDefinition) -> msgspec.Meta | None:\n    if (kwarg_definition := field_definition.kwarg_definition) is None or not isinstance(\n        kwarg_definition, KwargDefinition\n    ):\n        return None\n\n    return msgspec.Meta(\n        description=kwarg_definition.description,\n        examples=[e.value for e in kwarg_definition.examples or []],\n        ge=kwarg_definition.ge,\n        gt=kwarg_definition.gt,\n        le=kwarg_definition.le,\n        lt=kwarg_definition.lt,\n        max_length=kwarg_definition.max_length if not field_definition.is_partial else None,\n        min_length=kwarg_definition.min_length if not field_definition.is_partial else None,\n        multiple_of=kwarg_definition.multiple_of,\n        pattern=kwarg_definition.pattern,\n        title=kwarg_definition.title,\n    )\n\n\ndef _create_struct_for_field_definitions(\n    model_name: str,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    rename_strategy: RenameStrategy | dict[str, str] | None,\n) -> type[Struct]:\n    struct_fields: list[tuple[str, type] | tuple[str, type, type]] = []\n\n    for field_definition in field_definitions:\n        if field_definition.is_excluded:\n            continue\n\n        field_type = _create_transfer_model_type_annotation(field_definition.transfer_type)\n        if field_definition.is_partial:\n            field_type = Union[field_type, UnsetType]\n\n        if (field_meta := _create_struct_field_meta_for_field_definition(field_definition)) is not None:\n            field_type = Annotated[field_type, field_meta]\n\n        struct_fields.append(\n            (\n                field_definition.name,\n                field_type,\n                _create_msgspec_field(field_definition),\n            )\n        )\n    return defstruct(model_name, struct_fields, frozen=True, kw_only=True, rename=rename_strategy)\n\n\ndef build_annotation_for_backend(\n    model_type: type[Any], field_definition: FieldDefinition, transfer_model: type[Struct]\n) -> Any:\n    \"\"\"A helper to re-build a generic outer type with new inner type.\n\n    Args:\n        model_type: The original model type.\n        field_definition: The parsed type that represents the handler annotation for which the DTO is being applied.\n        transfer_model: The transfer model generated to represent the model type.\n\n    Returns:\n\n```\n# Function Signature: def populate_data_from_builtins(self, builtins: Any, asgi_connection: ASGIConnection) -> Any:\n# Natural Language Annotations: \"\"\"Populate model instance from builtin types.\n\n        Args:\n            builtins: Builtin type.\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            Instance or collection of ``model_type`` instances.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  [PYTHON]\n        result = self.transfer_data", "  [PYTHON]\ndef populate_data_from_builtins", "  [PYTHON]\ndef populate_data_from_builtins"]}, "litestar-litestar/dto/_backend.py-populate_data_from_raw": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/dto/_backend.py:\n```\n\"\"\"DTO backends do the heavy lifting of decoding and validating raw bytes into domain models, and\nback again, to bytes.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import replace\nfrom typing import (\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    ClassVar,\n    Collection,\n    Final,\n    Mapping,\n    Protocol,\n    Union,\n    cast,\n)\n\nimport msgspec\nfrom msgspec import UNSET, Struct, UnsetType, convert, defstruct, field\nfrom typing_extensions import Annotated\n\nfrom litestar.dto._types import (\n    CollectionType,\n    CompositeType,\n    MappingType,\n    NestedFieldInfo,\n    SimpleType,\n    TransferDTOFieldDefinition,\n    TransferType,\n    TupleType,\n    UnionType,\n)\nfrom litestar.dto.data_structures import DTOData, DTOFieldDefinition\nfrom litestar.dto.field import Mark\nfrom litestar.enums import RequestEncodingType\nfrom litestar.params import KwargDefinition\nfrom litestar.serialization import decode_json, decode_msgpack\nfrom litestar.types import Empty\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils import unique_name_for_scope\n\nif TYPE_CHECKING:\n    from litestar.connection import ASGIConnection\n    from litestar.dto import AbstractDTO, RenameStrategy\n    from litestar.types.serialization import LitestarEncodableType\n\n__all__ = (\"DTOBackend\",)\n\n\nclass CompositeTypeHandler(Protocol):\n    def __call__(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType: ...\n\n\nclass DTOBackend:\n    __slots__ = (\n        \"annotation\",\n        \"dto_data_type\",\n        \"dto_factory\",\n        \"field_definition\",\n        \"handler_id\",\n        \"is_data_field\",\n        \"model_type\",\n        \"parsed_field_definitions\",\n        \"reverse_name_map\",\n        \"transfer_model_type\",\n        \"wrapper_attribute_name\",\n    )\n\n    _seen_model_names: ClassVar[set[str]] = set()\n\n    def __init__(\n        self,\n        dto_factory: type[AbstractDTO],\n        field_definition: FieldDefinition,\n        handler_id: str,\n        is_data_field: bool,\n        model_type: type[Any],\n        wrapper_attribute_name: str | None,\n    ) -> None:\n        \"\"\"Create dto backend instance.\n\n        Args:\n            dto_factory: The DTO factory class calling this backend.\n            field_definition: Parsed type.\n            handler_id: The name of the handler that this backend is for.\n            is_data_field: Whether the field is a subclass of DTOData.\n            model_type: Model type.\n            wrapper_attribute_name: If the data that DTO should operate upon is wrapped in a generic datastructure, this is the name of the attribute that the data is stored in.\n        \"\"\"\n        self.dto_factory: Final[type[AbstractDTO]] = dto_factory\n        self.field_definition: Final[FieldDefinition] = field_definition\n        self.is_data_field: Final[bool] = is_data_field\n        self.handler_id: Final[str] = handler_id\n        self.model_type: Final[type[Any]] = model_type\n        self.wrapper_attribute_name: Final[str | None] = wrapper_attribute_name\n\n        self.parsed_field_definitions = self.parse_model(\n            model_type=model_type,\n            exclude=self.dto_factory.config.exclude,\n            include=self.dto_factory.config.include,\n            rename_fields=self.dto_factory.config.rename_fields,\n        )\n        self.transfer_model_type = self.create_transfer_model_type(\n            model_name=model_type.__name__, field_definitions=self.parsed_field_definitions\n        )\n        self.dto_data_type: type[DTOData] | None = None\n\n        if field_definition.is_subclass_of(DTOData):\n            self.dto_data_type = field_definition.annotation\n            field_definition = self.field_definition.inner_types[0]\n\n        self.annotation = build_annotation_for_backend(model_type, field_definition, self.transfer_model_type)\n\n    def parse_model(\n        self,\n        model_type: Any,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        nested_depth: int = 0,\n    ) -> tuple[TransferDTOFieldDefinition, ...]:\n        \"\"\"Reduce :attr:`model_type` to a tuple :class:`TransferDTOFieldDefinition` instances.\n\n        Returns:\n        Fields for data transfer.\n        \"\"\"\n        defined_fields = []\n        generic_field_definitons = list(FieldDefinition.from_annotation(model_type).generic_types or ())\n        for field_definition in self.dto_factory.generate_field_definitions(model_type):\n            if field_definition.is_type_var:\n                base_arg_field = generic_field_definitons.pop()\n                field_definition = replace(\n                    field_definition, annotation=base_arg_field.annotation, raw=base_arg_field.raw\n                )\n\n            if _should_mark_private(field_definition, self.dto_factory.config.underscore_fields_private):\n                field_definition.dto_field.mark = Mark.PRIVATE\n\n            try:\n                transfer_type = self._create_transfer_type(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    rename_fields=rename_fields,\n                    field_name=field_definition.name,\n                    unique_name=field_definition.model_name,\n                    nested_depth=nested_depth,\n                )\n            except RecursionError:\n                continue\n\n            transfer_field_definition = TransferDTOFieldDefinition.from_dto_field_definition(\n                field_definition=field_definition,\n                serialization_name=rename_fields.get(field_definition.name),\n                transfer_type=transfer_type,\n                is_partial=self.dto_factory.config.partial,\n                is_excluded=_should_exclude_field(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    is_data_field=self.is_data_field,\n                ),\n            )\n            defined_fields.append(transfer_field_definition)\n        return tuple(defined_fields)\n\n    def _create_transfer_model_name(self, model_name: str) -> str:\n        long_name_prefix = self.handler_id.split(\"::\")[0]\n        short_name_prefix = _camelize(long_name_prefix.split(\".\")[-1], True)\n\n        name_suffix = \"RequestBody\" if self.is_data_field else \"ResponseBody\"\n\n        if (short_name := f\"{short_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = short_name\n        elif (long_name := f\"{long_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = long_name\n        else:\n            name = unique_name_for_scope(long_name, self._seen_model_names)\n\n        self._seen_model_names.add(name)\n\n        return name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def parse_raw(self, raw: bytes, asgi_connection: ASGIConnection) -> Struct | Collection[Struct]:\n        \"\"\"Parse raw bytes into transfer model type.\n\n        Args:\n            raw: bytes\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The raw bytes parsed into transfer model type.\n        \"\"\"\n        request_encoding = RequestEncodingType.JSON\n\n        if (content_type := getattr(asgi_connection, \"content_type\", None)) and (media_type := content_type[0]):\n            request_encoding = media_type\n\n        type_decoders = asgi_connection.route_handler.resolve_type_decoders()\n\n        if request_encoding == RequestEncodingType.MESSAGEPACK:\n            result = decode_msgpack(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n        else:\n            result = decode_json(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n\n        return cast(\"Struct | Collection[Struct]\", result)\n\n    def parse_builtins(self, builtins: Any, asgi_connection: ASGIConnection) -> Any:\n        \"\"\"Parse builtin types into transfer model type.\n\n        Args:\n            builtins: Builtin type.\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The builtin type parsed into transfer model type.\n        \"\"\"\n        return convert(\n            obj=builtins,\n            type=self.annotation,\n            dec_hook=asgi_connection.route_handler.default_deserializer,\n            strict=False,\n            str_keys=True,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def transfer_data_from_builtins(self, builtins: Any) -> Any:\n        \"\"\"Populate model instance from builtin types.\n\n        Args:\n            builtins: Builtin type.\n\n        Returns:\n            Instance or collection of ``model_type`` instances.\n        \"\"\"\n        return _transfer_data(\n            destination_type=self.model_type,\n            source_data=builtins,\n            field_definitions=self.parsed_field_definitions,\n            field_definition=self.field_definition,\n            is_data_field=self.is_data_field,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_handler_for_field_definition(self, field_definition: FieldDefinition) -> CompositeTypeHandler | None:\n        if field_definition.is_union:\n            return self._create_union_type\n\n        if field_definition.is_tuple:\n            if len(field_definition.inner_types) == 2 and field_definition.inner_types[1].annotation is Ellipsis:\n                return self._create_collection_type\n            return self._create_tuple_type\n\n        if field_definition.is_mapping:\n            return self._create_mapping_type\n\n        if field_definition.is_non_string_collection:\n            return self._create_collection_type\n        return None\n\n    def _create_transfer_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        field_name: str,\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType | SimpleType:\n        exclude = _filter_nested_field(exclude, field_name)\n        include = _filter_nested_field(include, field_name)\n        rename_fields = _filter_nested_field_mapping(rename_fields, field_name)\n\n        if composite_type_handler := self._get_handler_for_field_definition(field_definition):\n            return composite_type_handler(\n                field_definition=field_definition,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                unique_name=unique_name,\n                nested_depth=nested_depth,\n            )\n\n        transfer_model: NestedFieldInfo | None = None\n\n        if self.dto_factory.detect_nested_field(field_definition):\n            if nested_depth == self.dto_factory.config.max_nested_depth:\n                raise RecursionError\n\n            unique_name = f\"{unique_name}{field_definition.raw.__name__}\"\n\n            nested_field_definitions = self.parse_model(\n                model_type=field_definition.annotation,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                nested_depth=nested_depth + 1,\n            )\n\n            transfer_model = NestedFieldInfo(\n                model=self.create_transfer_model_type(unique_name, nested_field_definitions),\n                field_definitions=nested_field_definitions,\n            )\n\n        return SimpleType(field_definition, nested_field_info=transfer_model)\n\n    def _create_collection_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CollectionType:\n        inner_types = field_definition.inner_types\n        inner_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return CollectionType(\n            field_definition=field_definition, inner_type=inner_type, has_nested=inner_type.has_nested\n        )\n\n    def _create_mapping_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> MappingType:\n        inner_types = field_definition.inner_types\n        key_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        value_type = self._create_transfer_type(\n            field_definition=inner_types[1] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"1\",\n            unique_name=f\"{unique_name}_1\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return MappingType(\n            field_definition=field_definition,\n            key_type=key_type,\n            value_type=value_type,\n            has_nested=key_type.has_nested or value_type.has_nested,\n        )\n\n    def _create_tuple_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> TupleType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return TupleType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n    def _create_union_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> UnionType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return UnionType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n\ndef _camelize(value: str, capitalize_first_letter: bool) -> str:\n    return \"\".join(\n        word if index == 0 and not capitalize_first_letter else word.capitalize()\n        for index, word in enumerate(value.split(\"_\"))\n    )\n\n\ndef _filter_nested_field(field_name_set: AbstractSet[str], field_name: str) -> AbstractSet[str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {split[1] for s in field_name_set if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1}\n\n\ndef _filter_nested_field_mapping(field_name_mapping: Mapping[str, str], field_name: str) -> dict[str, str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {\n        split[1]: v\n        for s, v in field_name_mapping.items()\n        if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1\n    }\n\n\ndef _transfer_data(\n    destination_type: type[Any],\n    source_data: Any | Collection[Any],\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    field_definition: FieldDefinition,\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance or iterable of instances of ``destination_type``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_data: data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        field_definition: the parsed type that represents the handler annotation for which the DTO is being applied.\n        is_data_field: whether the DTO is being applied to a ``data`` field.\n\n    Returns:\n        Data parsed into ``destination_type``.\n    \"\"\"\n    if field_definition.is_non_string_collection:\n        if not field_definition.is_mapping:\n            return field_definition.instantiable_origin(\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=item,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[0],\n                    is_data_field=is_data_field,\n                )\n                for item in source_data\n            )\n        return field_definition.instantiable_origin(\n            (\n                key,\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=value,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[1],\n                    is_data_field=is_data_field,\n                ),\n            )\n            for key, value in source_data.items()  # type: ignore[union-attr]\n        )\n\n    return _transfer_instance_data(\n        destination_type=destination_type,\n        source_instance=source_data,\n        field_definitions=field_definitions,\n        is_data_field=is_data_field,\n    )\n\n\ndef _transfer_instance_data(\n    destination_type: type[Any],\n    source_instance: Any,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance of ``destination_type`` with data from ``source_instance``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_instance: primitive data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        is_data_field: whether the given field is a 'data' kwarg field.\n\n    Returns:\n        Data parsed into ``model_type``.\n    \"\"\"\n    unstructured_data = {}\n\n    for field_definition in field_definitions:\n        if not is_data_field:\n            if field_definition.is_excluded:\n                continue\n        elif not (\n            field_definition.name in source_instance\n            if isinstance(source_instance, Mapping)\n            else hasattr(source_instance, field_definition.name)\n        ):\n            continue\n\n        transfer_type = field_definition.transfer_type\n        source_value = (\n            source_instance[field_definition.name]\n            if isinstance(source_instance, Mapping)\n            else getattr(source_instance, field_definition.name)\n        )\n\n        if field_definition.is_partial and is_data_field and source_value is UNSET:\n            continue\n\n        unstructured_data[field_definition.name] = _transfer_type_data(\n            source_value=source_value,\n            transfer_type=transfer_type,\n            nested_as_dict=destination_type is dict,\n            is_data_field=is_data_field,\n        )\n\n    return destination_type(**unstructured_data)\n\n\ndef _transfer_type_data(\n    source_value: Any,\n    transfer_type: TransferType,\n    nested_as_dict: bool,\n    is_data_field: bool,\n) -> Any:\n    if isinstance(transfer_type, SimpleType) and transfer_type.nested_field_info:\n        if nested_as_dict:\n            destination_type: Any = dict\n        elif is_data_field:\n            destination_type = transfer_type.field_definition.annotation\n        else:\n            destination_type = transfer_type.nested_field_info.model\n\n        return _transfer_instance_data(\n            destination_type=destination_type,\n            source_instance=source_value,\n            field_definitions=transfer_type.nested_field_info.field_definitions,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, UnionType) and transfer_type.has_nested:\n        return _transfer_nested_union_type_data(\n            transfer_type=transfer_type,\n            source_value=source_value,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, CollectionType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                _transfer_type_data(\n                    source_value=item,\n                    transfer_type=transfer_type.inner_type,\n                    nested_as_dict=False,\n                    is_data_field=is_data_field,\n                )\n                for item in source_value\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    if isinstance(transfer_type, MappingType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                (\n                    key,\n                    _transfer_type_data(\n                        source_value=value,\n                        transfer_type=transfer_type.value_type,\n                        nested_as_dict=False,\n                        is_data_field=is_data_field,\n                    ),\n                )\n                for key, value in source_value.items()\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    return source_value\n\n\ndef _transfer_nested_union_type_data(\n    transfer_type: UnionType,\n    source_value: Any,\n    is_data_field: bool,\n) -> Any:\n    for inner_type in transfer_type.inner_types:\n        if isinstance(inner_type, CompositeType):\n            raise RuntimeError(\"Composite inner types not (yet) supported for nested unions.\")\n\n        if inner_type.nested_field_info and isinstance(\n            source_value,\n            inner_type.nested_field_info.model if is_data_field else inner_type.field_definition.annotation,\n        ):\n            return _transfer_instance_data(\n                destination_type=inner_type.field_definition.annotation\n                if is_data_field\n                else inner_type.nested_field_info.model,\n                source_instance=source_value,\n                field_definitions=inner_type.nested_field_info.field_definitions,\n                is_data_field=is_data_field,\n            )\n    return source_value\n\n\ndef _create_msgspec_field(field_definition: TransferDTOFieldDefinition) -> Any:\n    kwargs: dict[str, Any] = {}\n    if field_definition.is_partial:\n        kwargs[\"default\"] = UNSET\n\n    elif field_definition.default is not Empty:\n        kwargs[\"default\"] = field_definition.default\n\n    elif field_definition.default_factory is not None:\n        kwargs[\"default_factory\"] = field_definition.default_factory\n\n    if field_definition.serialization_name is not None:\n        kwargs[\"name\"] = field_definition.serialization_name\n\n    return field(**kwargs)\n\n\ndef _create_struct_field_meta_for_field_definition(field_definition: TransferDTOFieldDefinition) -> msgspec.Meta | None:\n    if (kwarg_definition := field_definition.kwarg_definition) is None or not isinstance(\n        kwarg_definition, KwargDefinition\n    ):\n        return None\n\n    return msgspec.Meta(\n        description=kwarg_definition.description,\n        examples=[e.value for e in kwarg_definition.examples or []],\n        ge=kwarg_definition.ge,\n        gt=kwarg_definition.gt,\n        le=kwarg_definition.le,\n        lt=kwarg_definition.lt,\n        max_length=kwarg_definition.max_length if not field_definition.is_partial else None,\n        min_length=kwarg_definition.min_length if not field_definition.is_partial else None,\n        multiple_of=kwarg_definition.multiple_of,\n        pattern=kwarg_definition.pattern,\n        title=kwarg_definition.title,\n    )\n\n\ndef _create_struct_for_field_definitions(\n    model_name: str,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    rename_strategy: RenameStrategy | dict[str, str] | None,\n) -> type[Struct]:\n    struct_fields: list[tuple[str, type] | tuple[str, type, type]] = []\n\n    for field_definition in field_definitions:\n        if field_definition.is_excluded:\n            continue\n\n        field_type = _create_transfer_model_type_annotation(field_definition.transfer_type)\n        if field_definition.is_partial:\n            field_type = Union[field_type, UnsetType]\n\n        if (field_meta := _create_struct_field_meta_for_field_definition(field_definition)) is not None:\n            field_type = Annotated[field_type, field_meta]\n\n        struct_fields.append(\n            (\n                field_definition.name,\n                field_type,\n                _create_msgspec_field(field_definition),\n            )\n        )\n    return defstruct(model_name, struct_fields, frozen=True, kw_only=True, rename=rename_strategy)\n\n\ndef build_annotation_for_backend(\n    model_type: type[Any], field_definition: FieldDefinition, transfer_model: type[Struct]\n) -> Any:\n    \"\"\"A helper to re-build a generic outer type with new inner type.\n\n    Args:\n        model_type: The original model type.\n        field_definition: The parsed type that represents the handler annotation for which the DTO is being applied.\n        transfer_model: The transfer model generated to represent the model type.\n\n    Returns:\n\n```\n# Function Signature: def populate_data_from_raw(self, raw: bytes, asgi_connection: ASGIConnection) -> Any:\n# Natural Language Annotations: \"\"\"Parse raw bytes into instance of `model_type`.\n\n        Args:\n            raw: bytes\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            Instance or collection of ``model_type`` instances.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef populate_data_from_raw(self, raw:", "  [COLOR=\"Red\"][I]ERROR[/I][/", "  ```\ndef populate_data_from_raw(self, raw:"]}, "litestar-litestar/dto/_backend.py-encode_data": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/dto/_backend.py:\n```\n\"\"\"DTO backends do the heavy lifting of decoding and validating raw bytes into domain models, and\nback again, to bytes.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import replace\nfrom typing import (\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    ClassVar,\n    Collection,\n    Final,\n    Mapping,\n    Protocol,\n    Union,\n    cast,\n)\n\nimport msgspec\nfrom msgspec import UNSET, Struct, UnsetType, convert, defstruct, field\nfrom typing_extensions import Annotated\n\nfrom litestar.dto._types import (\n    CollectionType,\n    CompositeType,\n    MappingType,\n    NestedFieldInfo,\n    SimpleType,\n    TransferDTOFieldDefinition,\n    TransferType,\n    TupleType,\n    UnionType,\n)\nfrom litestar.dto.data_structures import DTOData, DTOFieldDefinition\nfrom litestar.dto.field import Mark\nfrom litestar.enums import RequestEncodingType\nfrom litestar.params import KwargDefinition\nfrom litestar.serialization import decode_json, decode_msgpack\nfrom litestar.types import Empty\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils import unique_name_for_scope\n\nif TYPE_CHECKING:\n    from litestar.connection import ASGIConnection\n    from litestar.dto import AbstractDTO, RenameStrategy\n    from litestar.types.serialization import LitestarEncodableType\n\n__all__ = (\"DTOBackend\",)\n\n\nclass CompositeTypeHandler(Protocol):\n    def __call__(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType: ...\n\n\nclass DTOBackend:\n    __slots__ = (\n        \"annotation\",\n        \"dto_data_type\",\n        \"dto_factory\",\n        \"field_definition\",\n        \"handler_id\",\n        \"is_data_field\",\n        \"model_type\",\n        \"parsed_field_definitions\",\n        \"reverse_name_map\",\n        \"transfer_model_type\",\n        \"wrapper_attribute_name\",\n    )\n\n    _seen_model_names: ClassVar[set[str]] = set()\n\n    def __init__(\n        self,\n        dto_factory: type[AbstractDTO],\n        field_definition: FieldDefinition,\n        handler_id: str,\n        is_data_field: bool,\n        model_type: type[Any],\n        wrapper_attribute_name: str | None,\n    ) -> None:\n        \"\"\"Create dto backend instance.\n\n        Args:\n            dto_factory: The DTO factory class calling this backend.\n            field_definition: Parsed type.\n            handler_id: The name of the handler that this backend is for.\n            is_data_field: Whether the field is a subclass of DTOData.\n            model_type: Model type.\n            wrapper_attribute_name: If the data that DTO should operate upon is wrapped in a generic datastructure, this is the name of the attribute that the data is stored in.\n        \"\"\"\n        self.dto_factory: Final[type[AbstractDTO]] = dto_factory\n        self.field_definition: Final[FieldDefinition] = field_definition\n        self.is_data_field: Final[bool] = is_data_field\n        self.handler_id: Final[str] = handler_id\n        self.model_type: Final[type[Any]] = model_type\n        self.wrapper_attribute_name: Final[str | None] = wrapper_attribute_name\n\n        self.parsed_field_definitions = self.parse_model(\n            model_type=model_type,\n            exclude=self.dto_factory.config.exclude,\n            include=self.dto_factory.config.include,\n            rename_fields=self.dto_factory.config.rename_fields,\n        )\n        self.transfer_model_type = self.create_transfer_model_type(\n            model_name=model_type.__name__, field_definitions=self.parsed_field_definitions\n        )\n        self.dto_data_type: type[DTOData] | None = None\n\n        if field_definition.is_subclass_of(DTOData):\n            self.dto_data_type = field_definition.annotation\n            field_definition = self.field_definition.inner_types[0]\n\n        self.annotation = build_annotation_for_backend(model_type, field_definition, self.transfer_model_type)\n\n    def parse_model(\n        self,\n        model_type: Any,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        nested_depth: int = 0,\n    ) -> tuple[TransferDTOFieldDefinition, ...]:\n        \"\"\"Reduce :attr:`model_type` to a tuple :class:`TransferDTOFieldDefinition` instances.\n\n        Returns:\n        Fields for data transfer.\n        \"\"\"\n        defined_fields = []\n        generic_field_definitons = list(FieldDefinition.from_annotation(model_type).generic_types or ())\n        for field_definition in self.dto_factory.generate_field_definitions(model_type):\n            if field_definition.is_type_var:\n                base_arg_field = generic_field_definitons.pop()\n                field_definition = replace(\n                    field_definition, annotation=base_arg_field.annotation, raw=base_arg_field.raw\n                )\n\n            if _should_mark_private(field_definition, self.dto_factory.config.underscore_fields_private):\n                field_definition.dto_field.mark = Mark.PRIVATE\n\n            try:\n                transfer_type = self._create_transfer_type(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    rename_fields=rename_fields,\n                    field_name=field_definition.name,\n                    unique_name=field_definition.model_name,\n                    nested_depth=nested_depth,\n                )\n            except RecursionError:\n                continue\n\n            transfer_field_definition = TransferDTOFieldDefinition.from_dto_field_definition(\n                field_definition=field_definition,\n                serialization_name=rename_fields.get(field_definition.name),\n                transfer_type=transfer_type,\n                is_partial=self.dto_factory.config.partial,\n                is_excluded=_should_exclude_field(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    is_data_field=self.is_data_field,\n                ),\n            )\n            defined_fields.append(transfer_field_definition)\n        return tuple(defined_fields)\n\n    def _create_transfer_model_name(self, model_name: str) -> str:\n        long_name_prefix = self.handler_id.split(\"::\")[0]\n        short_name_prefix = _camelize(long_name_prefix.split(\".\")[-1], True)\n\n        name_suffix = \"RequestBody\" if self.is_data_field else \"ResponseBody\"\n\n        if (short_name := f\"{short_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = short_name\n        elif (long_name := f\"{long_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = long_name\n        else:\n            name = unique_name_for_scope(long_name, self._seen_model_names)\n\n        self._seen_model_names.add(name)\n\n        return name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def parse_raw(self, raw: bytes, asgi_connection: ASGIConnection) -> Struct | Collection[Struct]:\n        \"\"\"Parse raw bytes into transfer model type.\n\n        Args:\n            raw: bytes\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The raw bytes parsed into transfer model type.\n        \"\"\"\n        request_encoding = RequestEncodingType.JSON\n\n        if (content_type := getattr(asgi_connection, \"content_type\", None)) and (media_type := content_type[0]):\n            request_encoding = media_type\n\n        type_decoders = asgi_connection.route_handler.resolve_type_decoders()\n\n        if request_encoding == RequestEncodingType.MESSAGEPACK:\n            result = decode_msgpack(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n        else:\n            result = decode_json(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n\n        return cast(\"Struct | Collection[Struct]\", result)\n\n    def parse_builtins(self, builtins: Any, asgi_connection: ASGIConnection) -> Any:\n        \"\"\"Parse builtin types into transfer model type.\n\n        Args:\n            builtins: Builtin type.\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The builtin type parsed into transfer model type.\n        \"\"\"\n        return convert(\n            obj=builtins,\n            type=self.annotation,\n            dec_hook=asgi_connection.route_handler.default_deserializer,\n            strict=False,\n            str_keys=True,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def transfer_data_from_builtins(self, builtins: Any) -> Any:\n        \"\"\"Populate model instance from builtin types.\n\n        Args:\n            builtins: Builtin type.\n\n        Returns:\n            Instance or collection of ``model_type`` instances.\n        \"\"\"\n        return _transfer_data(\n            destination_type=self.model_type,\n            source_data=builtins,\n            field_definitions=self.parsed_field_definitions,\n            field_definition=self.field_definition,\n            is_data_field=self.is_data_field,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_handler_for_field_definition(self, field_definition: FieldDefinition) -> CompositeTypeHandler | None:\n        if field_definition.is_union:\n            return self._create_union_type\n\n        if field_definition.is_tuple:\n            if len(field_definition.inner_types) == 2 and field_definition.inner_types[1].annotation is Ellipsis:\n                return self._create_collection_type\n            return self._create_tuple_type\n\n        if field_definition.is_mapping:\n            return self._create_mapping_type\n\n        if field_definition.is_non_string_collection:\n            return self._create_collection_type\n        return None\n\n    def _create_transfer_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        field_name: str,\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType | SimpleType:\n        exclude = _filter_nested_field(exclude, field_name)\n        include = _filter_nested_field(include, field_name)\n        rename_fields = _filter_nested_field_mapping(rename_fields, field_name)\n\n        if composite_type_handler := self._get_handler_for_field_definition(field_definition):\n            return composite_type_handler(\n                field_definition=field_definition,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                unique_name=unique_name,\n                nested_depth=nested_depth,\n            )\n\n        transfer_model: NestedFieldInfo | None = None\n\n        if self.dto_factory.detect_nested_field(field_definition):\n            if nested_depth == self.dto_factory.config.max_nested_depth:\n                raise RecursionError\n\n            unique_name = f\"{unique_name}{field_definition.raw.__name__}\"\n\n            nested_field_definitions = self.parse_model(\n                model_type=field_definition.annotation,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                nested_depth=nested_depth + 1,\n            )\n\n            transfer_model = NestedFieldInfo(\n                model=self.create_transfer_model_type(unique_name, nested_field_definitions),\n                field_definitions=nested_field_definitions,\n            )\n\n        return SimpleType(field_definition, nested_field_info=transfer_model)\n\n    def _create_collection_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CollectionType:\n        inner_types = field_definition.inner_types\n        inner_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return CollectionType(\n            field_definition=field_definition, inner_type=inner_type, has_nested=inner_type.has_nested\n        )\n\n    def _create_mapping_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> MappingType:\n        inner_types = field_definition.inner_types\n        key_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        value_type = self._create_transfer_type(\n            field_definition=inner_types[1] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"1\",\n            unique_name=f\"{unique_name}_1\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return MappingType(\n            field_definition=field_definition,\n            key_type=key_type,\n            value_type=value_type,\n            has_nested=key_type.has_nested or value_type.has_nested,\n        )\n\n    def _create_tuple_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> TupleType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return TupleType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n    def _create_union_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> UnionType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return UnionType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n\ndef _camelize(value: str, capitalize_first_letter: bool) -> str:\n    return \"\".join(\n        word if index == 0 and not capitalize_first_letter else word.capitalize()\n        for index, word in enumerate(value.split(\"_\"))\n    )\n\n\ndef _filter_nested_field(field_name_set: AbstractSet[str], field_name: str) -> AbstractSet[str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {split[1] for s in field_name_set if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1}\n\n\ndef _filter_nested_field_mapping(field_name_mapping: Mapping[str, str], field_name: str) -> dict[str, str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {\n        split[1]: v\n        for s, v in field_name_mapping.items()\n        if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1\n    }\n\n\ndef _transfer_data(\n    destination_type: type[Any],\n    source_data: Any | Collection[Any],\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    field_definition: FieldDefinition,\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance or iterable of instances of ``destination_type``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_data: data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        field_definition: the parsed type that represents the handler annotation for which the DTO is being applied.\n        is_data_field: whether the DTO is being applied to a ``data`` field.\n\n    Returns:\n        Data parsed into ``destination_type``.\n    \"\"\"\n    if field_definition.is_non_string_collection:\n        if not field_definition.is_mapping:\n            return field_definition.instantiable_origin(\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=item,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[0],\n                    is_data_field=is_data_field,\n                )\n                for item in source_data\n            )\n        return field_definition.instantiable_origin(\n            (\n                key,\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=value,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[1],\n                    is_data_field=is_data_field,\n                ),\n            )\n            for key, value in source_data.items()  # type: ignore[union-attr]\n        )\n\n    return _transfer_instance_data(\n        destination_type=destination_type,\n        source_instance=source_data,\n        field_definitions=field_definitions,\n        is_data_field=is_data_field,\n    )\n\n\ndef _transfer_instance_data(\n    destination_type: type[Any],\n    source_instance: Any,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance of ``destination_type`` with data from ``source_instance``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_instance: primitive data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        is_data_field: whether the given field is a 'data' kwarg field.\n\n    Returns:\n        Data parsed into ``model_type``.\n    \"\"\"\n    unstructured_data = {}\n\n    for field_definition in field_definitions:\n        if not is_data_field:\n            if field_definition.is_excluded:\n                continue\n        elif not (\n            field_definition.name in source_instance\n            if isinstance(source_instance, Mapping)\n            else hasattr(source_instance, field_definition.name)\n        ):\n            continue\n\n        transfer_type = field_definition.transfer_type\n        source_value = (\n            source_instance[field_definition.name]\n            if isinstance(source_instance, Mapping)\n            else getattr(source_instance, field_definition.name)\n        )\n\n        if field_definition.is_partial and is_data_field and source_value is UNSET:\n            continue\n\n        unstructured_data[field_definition.name] = _transfer_type_data(\n            source_value=source_value,\n            transfer_type=transfer_type,\n            nested_as_dict=destination_type is dict,\n            is_data_field=is_data_field,\n        )\n\n    return destination_type(**unstructured_data)\n\n\ndef _transfer_type_data(\n    source_value: Any,\n    transfer_type: TransferType,\n    nested_as_dict: bool,\n    is_data_field: bool,\n) -> Any:\n    if isinstance(transfer_type, SimpleType) and transfer_type.nested_field_info:\n        if nested_as_dict:\n            destination_type: Any = dict\n        elif is_data_field:\n            destination_type = transfer_type.field_definition.annotation\n        else:\n            destination_type = transfer_type.nested_field_info.model\n\n        return _transfer_instance_data(\n            destination_type=destination_type,\n            source_instance=source_value,\n            field_definitions=transfer_type.nested_field_info.field_definitions,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, UnionType) and transfer_type.has_nested:\n        return _transfer_nested_union_type_data(\n            transfer_type=transfer_type,\n            source_value=source_value,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, CollectionType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                _transfer_type_data(\n                    source_value=item,\n                    transfer_type=transfer_type.inner_type,\n                    nested_as_dict=False,\n                    is_data_field=is_data_field,\n                )\n                for item in source_value\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    if isinstance(transfer_type, MappingType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                (\n                    key,\n                    _transfer_type_data(\n                        source_value=value,\n                        transfer_type=transfer_type.value_type,\n                        nested_as_dict=False,\n                        is_data_field=is_data_field,\n                    ),\n                )\n                for key, value in source_value.items()\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    return source_value\n\n\ndef _transfer_nested_union_type_data(\n    transfer_type: UnionType,\n    source_value: Any,\n    is_data_field: bool,\n) -> Any:\n    for inner_type in transfer_type.inner_types:\n        if isinstance(inner_type, CompositeType):\n            raise RuntimeError(\"Composite inner types not (yet) supported for nested unions.\")\n\n        if inner_type.nested_field_info and isinstance(\n            source_value,\n            inner_type.nested_field_info.model if is_data_field else inner_type.field_definition.annotation,\n        ):\n            return _transfer_instance_data(\n                destination_type=inner_type.field_definition.annotation\n                if is_data_field\n                else inner_type.nested_field_info.model,\n                source_instance=source_value,\n                field_definitions=inner_type.nested_field_info.field_definitions,\n                is_data_field=is_data_field,\n            )\n    return source_value\n\n\ndef _create_msgspec_field(field_definition: TransferDTOFieldDefinition) -> Any:\n    kwargs: dict[str, Any] = {}\n    if field_definition.is_partial:\n        kwargs[\"default\"] = UNSET\n\n    elif field_definition.default is not Empty:\n        kwargs[\"default\"] = field_definition.default\n\n    elif field_definition.default_factory is not None:\n        kwargs[\"default_factory\"] = field_definition.default_factory\n\n    if field_definition.serialization_name is not None:\n        kwargs[\"name\"] = field_definition.serialization_name\n\n    return field(**kwargs)\n\n\ndef _create_struct_field_meta_for_field_definition(field_definition: TransferDTOFieldDefinition) -> msgspec.Meta | None:\n    if (kwarg_definition := field_definition.kwarg_definition) is None or not isinstance(\n        kwarg_definition, KwargDefinition\n    ):\n        return None\n\n    return msgspec.Meta(\n        description=kwarg_definition.description,\n        examples=[e.value for e in kwarg_definition.examples or []],\n        ge=kwarg_definition.ge,\n        gt=kwarg_definition.gt,\n        le=kwarg_definition.le,\n        lt=kwarg_definition.lt,\n        max_length=kwarg_definition.max_length if not field_definition.is_partial else None,\n        min_length=kwarg_definition.min_length if not field_definition.is_partial else None,\n        multiple_of=kwarg_definition.multiple_of,\n        pattern=kwarg_definition.pattern,\n        title=kwarg_definition.title,\n    )\n\n\ndef _create_struct_for_field_definitions(\n    model_name: str,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    rename_strategy: RenameStrategy | dict[str, str] | None,\n) -> type[Struct]:\n    struct_fields: list[tuple[str, type] | tuple[str, type, type]] = []\n\n    for field_definition in field_definitions:\n        if field_definition.is_excluded:\n            continue\n\n        field_type = _create_transfer_model_type_annotation(field_definition.transfer_type)\n        if field_definition.is_partial:\n            field_type = Union[field_type, UnsetType]\n\n        if (field_meta := _create_struct_field_meta_for_field_definition(field_definition)) is not None:\n            field_type = Annotated[field_type, field_meta]\n\n        struct_fields.append(\n            (\n                field_definition.name,\n                field_type,\n                _create_msgspec_field(field_definition),\n            )\n        )\n    return defstruct(model_name, struct_fields, frozen=True, kw_only=True, rename=rename_strategy)\n\n\ndef build_annotation_for_backend(\n    model_type: type[Any], field_definition: FieldDefinition, transfer_model: type[Struct]\n) -> Any:\n    \"\"\"A helper to re-build a generic outer type with new inner type.\n\n    Args:\n        model_type: The original model type.\n        field_definition: The parsed type that represents the handler annotation for which the DTO is being applied.\n        transfer_model: The transfer model generated to represent the model type.\n\n    Returns:\n\n```\n# Function Signature: def encode_data(self, data: Any) -> LitestarEncodableType:\n# Natural Language Annotations: \"\"\"Encode data into a ``LitestarEncodableType``.\n\n        Args:\n            data: Data to encode.\n\n        Returns:\n            Encoded data.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  def encode_data(self, data: Any) -> Litestar", "  ```\ndef encode_data(self, data: Any) -> Lit", "  ```\ndef encode_data(self, data: Any) -> Lit"]}, "litestar-litestar/dto/_backend.py-_create_transfer_model_type_annotation": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/dto/_backend.py:\n```\n\"\"\"DTO backends do the heavy lifting of decoding and validating raw bytes into domain models, and\nback again, to bytes.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import replace\nfrom typing import (\n    TYPE_CHECKING,\n    AbstractSet,\n    Any,\n    ClassVar,\n    Collection,\n    Final,\n    Mapping,\n    Protocol,\n    Union,\n    cast,\n)\n\nimport msgspec\nfrom msgspec import UNSET, Struct, UnsetType, convert, defstruct, field\nfrom typing_extensions import Annotated\n\nfrom litestar.dto._types import (\n    CollectionType,\n    CompositeType,\n    MappingType,\n    NestedFieldInfo,\n    SimpleType,\n    TransferDTOFieldDefinition,\n    TransferType,\n    TupleType,\n    UnionType,\n)\nfrom litestar.dto.data_structures import DTOData, DTOFieldDefinition\nfrom litestar.dto.field import Mark\nfrom litestar.enums import RequestEncodingType\nfrom litestar.params import KwargDefinition\nfrom litestar.serialization import decode_json, decode_msgpack\nfrom litestar.types import Empty\nfrom litestar.typing import FieldDefinition\nfrom litestar.utils import unique_name_for_scope\n\nif TYPE_CHECKING:\n    from litestar.connection import ASGIConnection\n    from litestar.dto import AbstractDTO, RenameStrategy\n    from litestar.types.serialization import LitestarEncodableType\n\n__all__ = (\"DTOBackend\",)\n\n\nclass CompositeTypeHandler(Protocol):\n    def __call__(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType: ...\n\n\nclass DTOBackend:\n    __slots__ = (\n        \"annotation\",\n        \"dto_data_type\",\n        \"dto_factory\",\n        \"field_definition\",\n        \"handler_id\",\n        \"is_data_field\",\n        \"model_type\",\n        \"parsed_field_definitions\",\n        \"reverse_name_map\",\n        \"transfer_model_type\",\n        \"wrapper_attribute_name\",\n    )\n\n    _seen_model_names: ClassVar[set[str]] = set()\n\n    def __init__(\n        self,\n        dto_factory: type[AbstractDTO],\n        field_definition: FieldDefinition,\n        handler_id: str,\n        is_data_field: bool,\n        model_type: type[Any],\n        wrapper_attribute_name: str | None,\n    ) -> None:\n        \"\"\"Create dto backend instance.\n\n        Args:\n            dto_factory: The DTO factory class calling this backend.\n            field_definition: Parsed type.\n            handler_id: The name of the handler that this backend is for.\n            is_data_field: Whether the field is a subclass of DTOData.\n            model_type: Model type.\n            wrapper_attribute_name: If the data that DTO should operate upon is wrapped in a generic datastructure, this is the name of the attribute that the data is stored in.\n        \"\"\"\n        self.dto_factory: Final[type[AbstractDTO]] = dto_factory\n        self.field_definition: Final[FieldDefinition] = field_definition\n        self.is_data_field: Final[bool] = is_data_field\n        self.handler_id: Final[str] = handler_id\n        self.model_type: Final[type[Any]] = model_type\n        self.wrapper_attribute_name: Final[str | None] = wrapper_attribute_name\n\n        self.parsed_field_definitions = self.parse_model(\n            model_type=model_type,\n            exclude=self.dto_factory.config.exclude,\n            include=self.dto_factory.config.include,\n            rename_fields=self.dto_factory.config.rename_fields,\n        )\n        self.transfer_model_type = self.create_transfer_model_type(\n            model_name=model_type.__name__, field_definitions=self.parsed_field_definitions\n        )\n        self.dto_data_type: type[DTOData] | None = None\n\n        if field_definition.is_subclass_of(DTOData):\n            self.dto_data_type = field_definition.annotation\n            field_definition = self.field_definition.inner_types[0]\n\n        self.annotation = build_annotation_for_backend(model_type, field_definition, self.transfer_model_type)\n\n    def parse_model(\n        self,\n        model_type: Any,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        nested_depth: int = 0,\n    ) -> tuple[TransferDTOFieldDefinition, ...]:\n        \"\"\"Reduce :attr:`model_type` to a tuple :class:`TransferDTOFieldDefinition` instances.\n\n        Returns:\n        Fields for data transfer.\n        \"\"\"\n        defined_fields = []\n        generic_field_definitons = list(FieldDefinition.from_annotation(model_type).generic_types or ())\n        for field_definition in self.dto_factory.generate_field_definitions(model_type):\n            if field_definition.is_type_var:\n                base_arg_field = generic_field_definitons.pop()\n                field_definition = replace(\n                    field_definition, annotation=base_arg_field.annotation, raw=base_arg_field.raw\n                )\n\n            if _should_mark_private(field_definition, self.dto_factory.config.underscore_fields_private):\n                field_definition.dto_field.mark = Mark.PRIVATE\n\n            try:\n                transfer_type = self._create_transfer_type(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    rename_fields=rename_fields,\n                    field_name=field_definition.name,\n                    unique_name=field_definition.model_name,\n                    nested_depth=nested_depth,\n                )\n            except RecursionError:\n                continue\n\n            transfer_field_definition = TransferDTOFieldDefinition.from_dto_field_definition(\n                field_definition=field_definition,\n                serialization_name=rename_fields.get(field_definition.name),\n                transfer_type=transfer_type,\n                is_partial=self.dto_factory.config.partial,\n                is_excluded=_should_exclude_field(\n                    field_definition=field_definition,\n                    exclude=exclude,\n                    include=include,\n                    is_data_field=self.is_data_field,\n                ),\n            )\n            defined_fields.append(transfer_field_definition)\n        return tuple(defined_fields)\n\n    def _create_transfer_model_name(self, model_name: str) -> str:\n        long_name_prefix = self.handler_id.split(\"::\")[0]\n        short_name_prefix = _camelize(long_name_prefix.split(\".\")[-1], True)\n\n        name_suffix = \"RequestBody\" if self.is_data_field else \"ResponseBody\"\n\n        if (short_name := f\"{short_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = short_name\n        elif (long_name := f\"{long_name_prefix}{model_name}{name_suffix}\") not in self._seen_model_names:\n            name = long_name\n        else:\n            name = unique_name_for_scope(long_name, self._seen_model_names)\n\n        self._seen_model_names.add(name)\n\n        return name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def parse_raw(self, raw: bytes, asgi_connection: ASGIConnection) -> Struct | Collection[Struct]:\n        \"\"\"Parse raw bytes into transfer model type.\n\n        Args:\n            raw: bytes\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The raw bytes parsed into transfer model type.\n        \"\"\"\n        request_encoding = RequestEncodingType.JSON\n\n        if (content_type := getattr(asgi_connection, \"content_type\", None)) and (media_type := content_type[0]):\n            request_encoding = media_type\n\n        type_decoders = asgi_connection.route_handler.resolve_type_decoders()\n\n        if request_encoding == RequestEncodingType.MESSAGEPACK:\n            result = decode_msgpack(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n        else:\n            result = decode_json(value=raw, target_type=self.annotation, type_decoders=type_decoders)\n\n        return cast(\"Struct | Collection[Struct]\", result)\n\n    def parse_builtins(self, builtins: Any, asgi_connection: ASGIConnection) -> Any:\n        \"\"\"Parse builtin types into transfer model type.\n\n        Args:\n            builtins: Builtin type.\n            asgi_connection: The current ASGI Connection\n\n        Returns:\n            The builtin type parsed into transfer model type.\n        \"\"\"\n        return convert(\n            obj=builtins,\n            type=self.annotation,\n            dec_hook=asgi_connection.route_handler.default_deserializer,\n            strict=False,\n            str_keys=True,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def transfer_data_from_builtins(self, builtins: Any) -> Any:\n        \"\"\"Populate model instance from builtin types.\n\n        Args:\n            builtins: Builtin type.\n\n        Returns:\n            Instance or collection of ``model_type`` instances.\n        \"\"\"\n        return _transfer_data(\n            destination_type=self.model_type,\n            source_data=builtins,\n            field_definitions=self.parsed_field_definitions,\n            field_definition=self.field_definition,\n            is_data_field=self.is_data_field,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_handler_for_field_definition(self, field_definition: FieldDefinition) -> CompositeTypeHandler | None:\n        if field_definition.is_union:\n            return self._create_union_type\n\n        if field_definition.is_tuple:\n            if len(field_definition.inner_types) == 2 and field_definition.inner_types[1].annotation is Ellipsis:\n                return self._create_collection_type\n            return self._create_tuple_type\n\n        if field_definition.is_mapping:\n            return self._create_mapping_type\n\n        if field_definition.is_non_string_collection:\n            return self._create_collection_type\n        return None\n\n    def _create_transfer_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        field_name: str,\n        unique_name: str,\n        nested_depth: int,\n    ) -> CompositeType | SimpleType:\n        exclude = _filter_nested_field(exclude, field_name)\n        include = _filter_nested_field(include, field_name)\n        rename_fields = _filter_nested_field_mapping(rename_fields, field_name)\n\n        if composite_type_handler := self._get_handler_for_field_definition(field_definition):\n            return composite_type_handler(\n                field_definition=field_definition,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                unique_name=unique_name,\n                nested_depth=nested_depth,\n            )\n\n        transfer_model: NestedFieldInfo | None = None\n\n        if self.dto_factory.detect_nested_field(field_definition):\n            if nested_depth == self.dto_factory.config.max_nested_depth:\n                raise RecursionError\n\n            unique_name = f\"{unique_name}{field_definition.raw.__name__}\"\n\n            nested_field_definitions = self.parse_model(\n                model_type=field_definition.annotation,\n                exclude=exclude,\n                include=include,\n                rename_fields=rename_fields,\n                nested_depth=nested_depth + 1,\n            )\n\n            transfer_model = NestedFieldInfo(\n                model=self.create_transfer_model_type(unique_name, nested_field_definitions),\n                field_definitions=nested_field_definitions,\n            )\n\n        return SimpleType(field_definition, nested_field_info=transfer_model)\n\n    def _create_collection_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> CollectionType:\n        inner_types = field_definition.inner_types\n        inner_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return CollectionType(\n            field_definition=field_definition, inner_type=inner_type, has_nested=inner_type.has_nested\n        )\n\n    def _create_mapping_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> MappingType:\n        inner_types = field_definition.inner_types\n        key_type = self._create_transfer_type(\n            field_definition=inner_types[0] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"0\",\n            unique_name=f\"{unique_name}_0\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        value_type = self._create_transfer_type(\n            field_definition=inner_types[1] if inner_types else FieldDefinition.from_annotation(Any),\n            exclude=exclude,\n            include=include,\n            field_name=\"1\",\n            unique_name=f\"{unique_name}_1\",\n            nested_depth=nested_depth,\n            rename_fields=rename_fields,\n        )\n        return MappingType(\n            field_definition=field_definition,\n            key_type=key_type,\n            value_type=value_type,\n            has_nested=key_type.has_nested or value_type.has_nested,\n        )\n\n    def _create_tuple_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> TupleType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return TupleType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n    def _create_union_type(\n        self,\n        field_definition: FieldDefinition,\n        exclude: AbstractSet[str],\n        include: AbstractSet[str],\n        rename_fields: dict[str, str],\n        unique_name: str,\n        nested_depth: int,\n    ) -> UnionType:\n        inner_types = tuple(\n            self._create_transfer_type(\n                field_definition=inner_type,\n                exclude=exclude,\n                include=include,\n                field_name=str(i),\n                unique_name=f\"{unique_name}_{i}\",\n                nested_depth=nested_depth,\n                rename_fields=rename_fields,\n            )\n            for i, inner_type in enumerate(field_definition.inner_types)\n        )\n        return UnionType(\n            field_definition=field_definition,\n            inner_types=inner_types,\n            has_nested=any(t.has_nested for t in inner_types),\n        )\n\n\ndef _camelize(value: str, capitalize_first_letter: bool) -> str:\n    return \"\".join(\n        word if index == 0 and not capitalize_first_letter else word.capitalize()\n        for index, word in enumerate(value.split(\"_\"))\n    )\n\n\ndef _filter_nested_field(field_name_set: AbstractSet[str], field_name: str) -> AbstractSet[str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {split[1] for s in field_name_set if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1}\n\n\ndef _filter_nested_field_mapping(field_name_mapping: Mapping[str, str], field_name: str) -> dict[str, str]:\n    \"\"\"Filter a nested field name.\"\"\"\n    return {\n        split[1]: v\n        for s, v in field_name_mapping.items()\n        if (split := s.split(\".\", 1))[0] == field_name and len(split) > 1\n    }\n\n\ndef _transfer_data(\n    destination_type: type[Any],\n    source_data: Any | Collection[Any],\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    field_definition: FieldDefinition,\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance or iterable of instances of ``destination_type``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_data: data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        field_definition: the parsed type that represents the handler annotation for which the DTO is being applied.\n        is_data_field: whether the DTO is being applied to a ``data`` field.\n\n    Returns:\n        Data parsed into ``destination_type``.\n    \"\"\"\n    if field_definition.is_non_string_collection:\n        if not field_definition.is_mapping:\n            return field_definition.instantiable_origin(\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=item,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[0],\n                    is_data_field=is_data_field,\n                )\n                for item in source_data\n            )\n        return field_definition.instantiable_origin(\n            (\n                key,\n                _transfer_data(\n                    destination_type=destination_type,\n                    source_data=value,\n                    field_definitions=field_definitions,\n                    field_definition=field_definition.inner_types[1],\n                    is_data_field=is_data_field,\n                ),\n            )\n            for key, value in source_data.items()  # type: ignore[union-attr]\n        )\n\n    return _transfer_instance_data(\n        destination_type=destination_type,\n        source_instance=source_data,\n        field_definitions=field_definitions,\n        is_data_field=is_data_field,\n    )\n\n\ndef _transfer_instance_data(\n    destination_type: type[Any],\n    source_instance: Any,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    is_data_field: bool,\n) -> Any:\n    \"\"\"Create instance of ``destination_type`` with data from ``source_instance``.\n\n    Args:\n        destination_type: the model type received by the DTO on type narrowing.\n        source_instance: primitive data that has been parsed and validated via the backend.\n        field_definitions: model field definitions.\n        is_data_field: whether the given field is a 'data' kwarg field.\n\n    Returns:\n        Data parsed into ``model_type``.\n    \"\"\"\n    unstructured_data = {}\n\n    for field_definition in field_definitions:\n        if not is_data_field:\n            if field_definition.is_excluded:\n                continue\n        elif not (\n            field_definition.name in source_instance\n            if isinstance(source_instance, Mapping)\n            else hasattr(source_instance, field_definition.name)\n        ):\n            continue\n\n        transfer_type = field_definition.transfer_type\n        source_value = (\n            source_instance[field_definition.name]\n            if isinstance(source_instance, Mapping)\n            else getattr(source_instance, field_definition.name)\n        )\n\n        if field_definition.is_partial and is_data_field and source_value is UNSET:\n            continue\n\n        unstructured_data[field_definition.name] = _transfer_type_data(\n            source_value=source_value,\n            transfer_type=transfer_type,\n            nested_as_dict=destination_type is dict,\n            is_data_field=is_data_field,\n        )\n\n    return destination_type(**unstructured_data)\n\n\ndef _transfer_type_data(\n    source_value: Any,\n    transfer_type: TransferType,\n    nested_as_dict: bool,\n    is_data_field: bool,\n) -> Any:\n    if isinstance(transfer_type, SimpleType) and transfer_type.nested_field_info:\n        if nested_as_dict:\n            destination_type: Any = dict\n        elif is_data_field:\n            destination_type = transfer_type.field_definition.annotation\n        else:\n            destination_type = transfer_type.nested_field_info.model\n\n        return _transfer_instance_data(\n            destination_type=destination_type,\n            source_instance=source_value,\n            field_definitions=transfer_type.nested_field_info.field_definitions,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, UnionType) and transfer_type.has_nested:\n        return _transfer_nested_union_type_data(\n            transfer_type=transfer_type,\n            source_value=source_value,\n            is_data_field=is_data_field,\n        )\n\n    if isinstance(transfer_type, CollectionType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                _transfer_type_data(\n                    source_value=item,\n                    transfer_type=transfer_type.inner_type,\n                    nested_as_dict=False,\n                    is_data_field=is_data_field,\n                )\n                for item in source_value\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    if isinstance(transfer_type, MappingType):\n        if transfer_type.has_nested:\n            return transfer_type.field_definition.instantiable_origin(\n                (\n                    key,\n                    _transfer_type_data(\n                        source_value=value,\n                        transfer_type=transfer_type.value_type,\n                        nested_as_dict=False,\n                        is_data_field=is_data_field,\n                    ),\n                )\n                for key, value in source_value.items()\n            )\n\n        return transfer_type.field_definition.instantiable_origin(source_value)\n\n    return source_value\n\n\ndef _transfer_nested_union_type_data(\n    transfer_type: UnionType,\n    source_value: Any,\n    is_data_field: bool,\n) -> Any:\n    for inner_type in transfer_type.inner_types:\n        if isinstance(inner_type, CompositeType):\n            raise RuntimeError(\"Composite inner types not (yet) supported for nested unions.\")\n\n        if inner_type.nested_field_info and isinstance(\n            source_value,\n            inner_type.nested_field_info.model if is_data_field else inner_type.field_definition.annotation,\n        ):\n            return _transfer_instance_data(\n                destination_type=inner_type.field_definition.annotation\n                if is_data_field\n                else inner_type.nested_field_info.model,\n                source_instance=source_value,\n                field_definitions=inner_type.nested_field_info.field_definitions,\n                is_data_field=is_data_field,\n            )\n    return source_value\n\n\ndef _create_msgspec_field(field_definition: TransferDTOFieldDefinition) -> Any:\n    kwargs: dict[str, Any] = {}\n    if field_definition.is_partial:\n        kwargs[\"default\"] = UNSET\n\n    elif field_definition.default is not Empty:\n        kwargs[\"default\"] = field_definition.default\n\n    elif field_definition.default_factory is not None:\n        kwargs[\"default_factory\"] = field_definition.default_factory\n\n    if field_definition.serialization_name is not None:\n        kwargs[\"name\"] = field_definition.serialization_name\n\n    return field(**kwargs)\n\n\ndef _create_struct_field_meta_for_field_definition(field_definition: TransferDTOFieldDefinition) -> msgspec.Meta | None:\n    if (kwarg_definition := field_definition.kwarg_definition) is None or not isinstance(\n        kwarg_definition, KwargDefinition\n    ):\n        return None\n\n    return msgspec.Meta(\n        description=kwarg_definition.description,\n        examples=[e.value for e in kwarg_definition.examples or []],\n        ge=kwarg_definition.ge,\n        gt=kwarg_definition.gt,\n        le=kwarg_definition.le,\n        lt=kwarg_definition.lt,\n        max_length=kwarg_definition.max_length if not field_definition.is_partial else None,\n        min_length=kwarg_definition.min_length if not field_definition.is_partial else None,\n        multiple_of=kwarg_definition.multiple_of,\n        pattern=kwarg_definition.pattern,\n        title=kwarg_definition.title,\n    )\n\n\ndef _create_struct_for_field_definitions(\n    model_name: str,\n    field_definitions: tuple[TransferDTOFieldDefinition, ...],\n    rename_strategy: RenameStrategy | dict[str, str] | None,\n) -> type[Struct]:\n    struct_fields: list[tuple[str, type] | tuple[str, type, type]] = []\n\n    for field_definition in field_definitions:\n        if field_definition.is_excluded:\n            continue\n\n        field_type = _create_transfer_model_type_annotation(field_definition.transfer_type)\n        if field_definition.is_partial:\n            field_type = Union[field_type, UnsetType]\n\n        if (field_meta := _create_struct_field_meta_for_field_definition(field_definition)) is not None:\n            field_type = Annotated[field_type, field_meta]\n\n        struct_fields.append(\n            (\n                field_definition.name,\n                field_type,\n                _create_msgspec_field(field_definition),\n            )\n        )\n    return defstruct(model_name, struct_fields, frozen=True, kw_only=True, rename=rename_strategy)\n\n\ndef build_annotation_for_backend(\n    model_type: type[Any], field_definition: FieldDefinition, transfer_model: type[Struct]\n) -> Any:\n    \"\"\"A helper to re-build a generic outer type with new inner type.\n\n    Args:\n        model_type: The original model type.\n        field_definition: The parsed type that represents the handler annotation for which the DTO is being applied.\n        transfer_model: The transfer model generated to represent the model type.\n\n    Returns:\n\n```\n# Function Signature: def _create_transfer_model_type_annotation(transfer_type: TransferType) -> Any:\n# Natural Language Annotations: \"\"\"Create a type annotation for a transfer model.\n\n    Uses the parsed type that originates from the data model and the transfer model generated to represent a nested\n    type to reconstruct the type annotation for the transfer model.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```python\ndef _create_transfer_model_type_annotation", "  return transfer_model\n", "  pass"]}, "litestar-litestar/response/file.py-start_response": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/response/file.py:\n```\nfrom __future__ import annotations\n\nimport itertools\nfrom email.utils import formatdate\nfrom inspect import iscoroutine\nfrom mimetypes import encodings_map, guess_type\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, Coroutine, Iterable, Literal, cast\nfrom urllib.parse import quote\nfrom zlib import adler32\n\nfrom litestar.constants import ONE_MEGABYTE\nfrom litestar.exceptions import ImproperlyConfiguredException\nfrom litestar.file_system import BaseLocalFileSystem, FileSystemAdapter\nfrom litestar.response.base import Response\nfrom litestar.response.streaming import ASGIStreamingResponse\nfrom litestar.utils.deprecation import warn_deprecation\nfrom litestar.utils.helpers import get_enum_string_value\n\nif TYPE_CHECKING:\n    from os import PathLike\n    from os import stat_result as stat_result_type\n\n    from anyio import Path\n\n    from litestar.app import Litestar\n    from litestar.background_tasks import BackgroundTask, BackgroundTasks\n    from litestar.connection import Request\n    from litestar.datastructures.cookie import Cookie\n    from litestar.datastructures.headers import ETag\n    from litestar.enums import MediaType\n    from litestar.types import (\n        HTTPResponseBodyEvent,\n        PathType,\n        Receive,\n        ResponseCookies,\n        ResponseHeaders,\n        Send,\n        TypeEncodersMap,\n    )\n    from litestar.types.file_types import FileInfo, FileSystemProtocol\n\n__all__ = (\n    \"ASGIFileResponse\",\n    \"File\",\n    \"async_file_iterator\",\n    \"create_etag_for_file\",\n)\n\n# brotli not supported in 'mimetypes.encodings_map' until py 3.9.\nencodings_map[\".br\"] = \"br\"\n\n\nasync def async_file_iterator(\n    file_path: PathType, chunk_size: int, adapter: FileSystemAdapter\n) -> AsyncGenerator[bytes, None]:\n    \"\"\"Return an async that asynchronously reads a file and yields its chunks.\n\n    Args:\n        file_path: A path to a file.\n        chunk_size: The chunk file to use.\n        adapter: File system adapter class.\n        adapter: File system adapter class.\n\n    Returns:\n        An async generator.\n    \"\"\"\n    async with await adapter.open(file_path) as file:\n        while chunk := await file.read(chunk_size):\n            yield chunk\n\n\ndef create_etag_for_file(path: PathType, modified_time: float, file_size: int) -> str:\n    \"\"\"Create an etag.\n\n    Notes:\n        - Function is derived from flask.\n\n    Returns:\n        An etag.\n    \"\"\"\n    check = adler32(str(path).encode(\"utf-8\")) & 0xFFFFFFFF\n    return f'\"{modified_time}-{file_size}-{check}\"'\n\n\nclass ASGIFileResponse(ASGIStreamingResponse):\n    \"\"\"A low-level ASGI response, streaming a file as response body.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        background: BackgroundTask | BackgroundTasks | None = None,\n        body: bytes | str = b\"\",\n        chunk_size: int = ONE_MEGABYTE,\n        content_disposition_type: Literal[\"attachment\", \"inline\"] = \"attachment\",\n        content_length: int | None = None,\n        cookies: Iterable[Cookie] | None = None,\n        encoded_headers: Iterable[tuple[bytes, bytes]] | None = None,\n        encoding: str = \"utf-8\",\n        etag: ETag | None = None,\n        file_info: FileInfo | Coroutine[None, None, FileInfo] | None = None,\n        file_path: str | PathLike | Path,\n        file_system: FileSystemProtocol | None = None,\n        filename: str = \"\",\n        headers: dict[str, str] | None = None,\n        is_head_response: bool = False,\n        media_type: MediaType | str | None = None,\n        stat_result: stat_result_type | None = None,\n        status_code: int | None = None,\n    ) -> None:\n        \"\"\"A low-level ASGI response, streaming a file as response body.\n\n        Args:\n            background: A background task or a list of background tasks to be executed after the response is sent.\n            body: encoded content to send in the response body.\n            chunk_size: The chunk size to use.\n            content_disposition_type: The type of the ``Content-Disposition``. Either ``inline`` or ``attachment``.\n            content_length: The response content length.\n            cookies: The response cookies.\n            encoded_headers: A list of encoded headers.\n            encoding: The response encoding.\n            etag: An etag.\n            file_info: A file info.\n            file_path: A path to a file.\n            file_system: A file system adapter.\n            filename: The name of the file.\n            headers: A dictionary of headers.\n            headers: The response headers.\n            is_head_response: A boolean indicating if the response is a HEAD response.\n            media_type: The media type of the file.\n            stat_result: A stat result.\n            status_code: The response status code.\n        \"\"\"\n        headers = headers or {}\n        if not media_type:\n            mimetype, content_encoding = guess_type(filename) if filename else (None, None)\n            media_type = mimetype or \"application/octet-stream\"\n            if content_encoding is not None:\n                headers.update({\"content-encoding\": content_encoding})\n\n        self.adapter = FileSystemAdapter(file_system or BaseLocalFileSystem())\n\n        super().__init__(\n            iterator=async_file_iterator(file_path=file_path, chunk_size=chunk_size, adapter=self.adapter),\n            headers=headers,\n            media_type=media_type,\n            cookies=cookies,\n            background=background,\n            status_code=status_code,\n            body=body,\n            content_length=content_length,\n            encoding=encoding,\n            is_head_response=is_head_response,\n            encoded_headers=encoded_headers,\n        )\n\n        quoted_filename = quote(filename)\n        is_utf8 = quoted_filename == filename\n        if is_utf8:\n            content_disposition = f'{content_disposition_type}; filename=\"{filename}\"'\n        else:\n            content_disposition = f\"{content_disposition_type}; filename*=utf-8''{quoted_filename}\"\n\n        self.headers.setdefault(\"content-disposition\", content_disposition)\n\n        self.chunk_size = chunk_size\n        self.etag = etag\n        self.file_path = file_path\n\n        if file_info:\n            self.file_info: FileInfo | Coroutine[Any, Any, FileInfo] = file_info\n        elif stat_result:\n            self.file_info = self.adapter.parse_stat_result(result=stat_result, path=file_path)\n        else:\n            self.file_info = self.adapter.info(self.file_path)\n\n    async def send_body(self, send: Send, receive: Receive) -> None:\n        \"\"\"Emit a stream of events correlating with the response body.\n\n        Args:\n            send: The ASGI send function.\n            receive: The ASGI receive function.\n\n        Returns:\n            None\n        \"\"\"\n        if self.chunk_size < self.content_length:\n            await super().send_body(send=send, receive=receive)\n            return\n\n        async with await self.adapter.open(self.file_path) as file:\n            body_event: HTTPResponseBodyEvent = {\n                \"type\": \"http.response.body\",\n                \"body\": await file.read(),\n                \"more_body\": False,\n            }\n            await send(body_event)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass File(Response):\n    \"\"\"A response, streaming a file as response body.\"\"\"\n\n    __slots__ = (\n        \"chunk_size\",\n        \"content_disposition_type\",\n        \"etag\",\n        \"file_path\",\n        \"file_system\",\n        \"filename\",\n        \"file_info\",\n        \"stat_result\",\n    )\n\n    def __init__(\n        self,\n        path: str | PathLike | Path,\n        *,\n        background: BackgroundTask | BackgroundTasks | None = None,\n        chunk_size: int = ONE_MEGABYTE,\n        content_disposition_type: Literal[\"attachment\", \"inline\"] = \"attachment\",\n        cookies: ResponseCookies | None = None,\n        encoding: str = \"utf-8\",\n        etag: ETag | None = None,\n        file_info: FileInfo | Coroutine[Any, Any, FileInfo] | None = None,\n        file_system: FileSystemProtocol | None = None,\n        filename: str | None = None,\n        headers: ResponseHeaders | None = None,\n        media_type: Literal[MediaType.TEXT] | str | None = None,\n        stat_result: stat_result_type | None = None,\n        status_code: int | None = None,\n    ) -> None:\n        \"\"\"Initialize ``File``\n\n        Notes:\n            - This class extends the :class:`Stream <.response.Stream>` class.\n\n        Args:\n            path: A file path in one of the supported formats.\n            background: A :class:`BackgroundTask <.background_tasks.BackgroundTask>` instance or\n                :class:`BackgroundTasks <.background_tasks.BackgroundTasks>` to execute after the response is finished.\n                Defaults to None.\n            chunk_size: The chunk sizes to use when streaming the file. Defaults to 1MB.\n            content_disposition_type: The type of the ``Content-Disposition``. Either ``inline`` or ``attachment``.\n            cookies: A list of :class:`Cookie <.datastructures.Cookie>` instances to be set under the response\n                ``Set-Cookie`` header.\n            encoding: The encoding to be used for the response headers.\n            etag: An optional :class:`ETag <.datastructures.ETag>` instance. If not provided, an etag will be\n                generated.\n            file_info: The output of calling :meth:`file_system.info <types.FileSystemProtocol.info>`, equivalent to\n                providing an :class:`os.stat_result`.\n            file_system: An implementation of the :class:`FileSystemProtocol <.types.FileSystemProtocol>`. If provided\n                it will be used to load the file.\n            filename: An optional filename to set in the header.\n            headers: A string keyed dictionary of response headers. Header keys are insensitive.\n            media_type: A value for the response ``Content-Type`` header. If not provided, the value will be either\n                derived from the filename if provided and supported by the stdlib, or will default to\n                ``application/octet-stream``.\n            stat_result: An optional result of calling :func:os.stat:. If not provided, this will be done by the\n                response constructor.\n            status_code: An HTTP status code.\n        \"\"\"\n\n        if file_system is not None and not (\n            callable(getattr(file_system, \"info\", None)) and callable(getattr(file_system, \"open\", None))\n        ):\n            raise ImproperlyConfiguredException(\"file_system must adhere to the FileSystemProtocol type\")\n\n        self.chunk_size = chunk_size\n        self.content_disposition_type = content_disposition_type\n        self.etag = etag\n        self.file_info = file_info\n        self.file_path = path\n        self.file_system = file_system\n        self.filename = filename or \"\"\n        self.stat_result = stat_result\n\n        super().__init__(\n            content=None,\n            status_code=status_code,\n            media_type=media_type,\n            background=background,\n            headers=headers,\n            cookies=cookies,\n            encoding=encoding,\n        )\n\n    def to_asgi_response(\n        self,\n        app: Litestar | None,\n        request: Request,\n        *,\n        background: BackgroundTask | BackgroundTasks | None = None,\n        encoded_headers: Iterable[tuple[bytes, bytes]] | None = None,\n        cookies: Iterable[Cookie] | None = None,\n        headers: dict[str, str] | None = None,\n        is_head_response: bool = False,\n        media_type: MediaType | str | None = None,\n        status_code: int | None = None,\n        type_encoders: TypeEncodersMap | None = None,\n    ) -> ASGIFileResponse:\n        \"\"\"Create an :class:`ASGIFileResponse <litestar.response.file.ASGIFileResponse>` instance.\n\n        Args:\n            app: The :class:`Litestar <.app.Litestar>` application instance.\n            background: Background task(s) to be executed after the response is sent.\n            cookies: A list of cookies to be set on the response.\n            encoded_headers: A list of already encoded headers.\n            headers: Additional headers to be merged with the response headers. Response headers take precedence.\n            is_head_response: Whether the response is a HEAD response.\n            media_type: Media type for the response. If ``media_type`` is already set on the response, this is ignored.\n            request: The :class:`Request <.connection.Request>` instance.\n            status_code: Status code for the response. If ``status_code`` is already set on the response, this is\n            type_encoders: A dictionary of type encoders to use for encoding the response content.\n\n        Returns:\n            A low-level ASGI file response.\n        \"\"\"\n        if app is not None:\n            warn_deprecation(\n                version=\"2.1\",\n                deprecated_name=\"app\",\n                kind=\"parameter\",\n                removal_in=\"3.0.0\",\n                alternative=\"request.app\",\n            )\n\n        headers = {**headers, **self.headers} if headers is not None else self.headers\n        cookies = self.cookies if cookies is None else itertools.chain(self.cookies, cookies)\n\n        media_type = self.media_type or media_type\n        if media_type is not None:\n            media_type = get_enum_string_value(media_type)\n\n        return ASGIFileResponse(\n            background=self.background or background,\n            body=b\"\",\n            chunk_size=self.chunk_size,\n            content_disposition_type=self.content_disposition_type,  # pyright: ignore\n            content_length=0,\n            cookies=cookies,\n            encoded_headers=encoded_headers,\n            encoding=self.encoding,\n            etag=self.etag,\n            file_info=self.file_info,\n            file_path=self.file_path,\n            file_system=self.file_system,\n            filename=self.filename,\n            headers=headers,\n            is_head_response=is_head_response,\n            media_type=media_type,\n            stat_result=self.stat_result,\n            status_code=self.status_code or status_code,\n        )\n\n```\n# Function Signature: async def start_response(self, send: Send) -> None:\n# Natural Language Annotations: \"\"\"Emit the start event of the response. This event includes the headers and status codes.\n\n        Args:\n            send: The ASGI send function.\n\n        Returns:\n            None\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def start_response(self, send: Send) ->", "  ```\nasync def start_response(self, send: Send) ->", "  ```\nasync def start_response(self, send: Send) ->"]}, "litestar-litestar/connection/request.py-json": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/connection/request.py:\n```\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, Generic\n\nfrom litestar._multipart import parse_content_header, parse_multipart_form\nfrom litestar._parsers import parse_url_encoded_form_data\nfrom litestar.connection.base import (\n    ASGIConnection,\n    AuthT,\n    StateT,\n    UserT,\n    empty_receive,\n    empty_send,\n)\nfrom litestar.datastructures.headers import Accept\nfrom litestar.datastructures.multi_dicts import FormMultiDict\nfrom litestar.enums import ASGIExtension, RequestEncodingType\nfrom litestar.exceptions import (\n    InternalServerException,\n    LitestarException,\n    LitestarWarning,\n)\nfrom litestar.serialization import decode_json, decode_msgpack\nfrom litestar.types import Empty\n\n__all__ = (\"Request\",)\n\n\nif TYPE_CHECKING:\n    from litestar.handlers.http_handlers import HTTPRouteHandler  # noqa: F401\n    from litestar.types.asgi_types import HTTPScope, Method, Receive, Scope, Send\n    from litestar.types.empty import EmptyType\n\n\nSERVER_PUSH_HEADERS = {\n    \"accept\",\n    \"accept-encoding\",\n    \"accept-language\",\n    \"cache-control\",\n    \"user-agent\",\n}\n\n\nclass Request(Generic[UserT, AuthT, StateT], ASGIConnection[\"HTTPRouteHandler\", UserT, AuthT, StateT]):\n    \"\"\"The Litestar Request class.\"\"\"\n\n    __slots__ = (\n        \"_json\",\n        \"_form\",\n        \"_body\",\n        \"_msgpack\",\n        \"_content_type\",\n        \"_accept\",\n        \"is_connected\",\n        \"supports_push_promise\",\n    )\n\n    scope: HTTPScope  # pyright: ignore\n    \"\"\"The ASGI scope attached to the connection.\"\"\"\n    receive: Receive\n    \"\"\"The ASGI receive function.\"\"\"\n    send: Send\n    \"\"\"The ASGI send function.\"\"\"\n\n    def __init__(self, scope: Scope, receive: Receive = empty_receive, send: Send = empty_send) -> None:\n        \"\"\"Initialize ``Request``.\n\n        Args:\n            scope: The ASGI connection scope.\n            receive: The ASGI receive function.\n            send: The ASGI send function.\n        \"\"\"\n        super().__init__(scope, receive, send)\n        self.is_connected: bool = True\n        self._body: bytes | EmptyType = Empty\n        self._form: dict[str, str | list[str]] | EmptyType = Empty\n        self._json: Any = Empty\n        self._msgpack: Any = Empty\n        self._content_type: tuple[str, dict[str, str]] | EmptyType = Empty\n        self._accept: Accept | EmptyType = Empty\n        self.supports_push_promise = ASGIExtension.SERVER_PUSH in self._server_extensions\n\n    @property\n    def method(self) -> Method:\n        \"\"\"Return the request method.\n\n        Returns:\n            The request :class:`Method <litestar.types.Method>`\n        \"\"\"\n        return self.scope[\"method\"]\n\n    @property\n    def content_type(self) -> tuple[str, dict[str, str]]:\n        \"\"\"Parse the request's 'Content-Type' header, returning the header value and any options as a dictionary.\n\n        Returns:\n            A tuple with the parsed value and a dictionary containing any options send in it.\n        \"\"\"\n        if self._content_type is Empty:\n            if (content_type := self._connection_state.content_type) is not Empty:\n                self._content_type = content_type\n            else:\n                self._content_type = self._connection_state.content_type = parse_content_header(\n                    self.headers.get(\"Content-Type\", \"\")\n                )\n        return self._content_type\n\n    @property\n    def accept(self) -> Accept:\n        \"\"\"Parse the request's 'Accept' header, returning an :class:`Accept <litestar.datastructures.headers.Accept>` instance.\n\n        Returns:\n            An :class:`Accept <litestar.datastructures.headers.Accept>` instance, representing the list of acceptable media types.\n        \"\"\"\n        if self._accept is Empty:\n            if (accept := self._connection_state.accept) is not Empty:\n                self._accept = accept\n            else:\n                self._accept = self._connection_state.accept = Accept(self.headers.get(\"Accept\", \"*/*\"))\n        return self._accept\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def msgpack(self) -> Any:\n        \"\"\"Retrieve the MessagePack request body from the request.\n\n        Returns:\n            An arbitrary value\n        \"\"\"\n        if self._msgpack is Empty:\n            if (msgpack := self._connection_state.msgpack) is not Empty:\n                self._msgpack = msgpack\n            else:\n                body = await self.body()\n                self._msgpack = self._connection_state.msgpack = decode_msgpack(\n                    body or b\"\\xc0\", type_decoders=self.route_handler.resolve_type_decoders()\n                )\n        return self._msgpack\n\n    async def stream(self) -> AsyncGenerator[bytes, None]:\n        \"\"\"Return an async generator that streams chunks of bytes.\n\n        Returns:\n            An async generator.\n\n        Raises:\n            RuntimeError: if the stream is already consumed\n        \"\"\"\n        if self._body is Empty:\n            if not self.is_connected:\n                raise InternalServerException(\"stream consumed\")\n            while event := await self.receive():\n                if event[\"type\"] == \"http.request\":\n                    if event[\"body\"]:\n                        yield event[\"body\"]\n\n                    if not event.get(\"more_body\", False):\n                        break\n\n                if event[\"type\"] == \"http.disconnect\":\n                    raise InternalServerException(\"client disconnected prematurely\")\n\n            self.is_connected = False\n            yield b\"\"\n\n        else:\n            yield self._body\n            yield b\"\"\n            return\n\n    async def body(self) -> bytes:\n        \"\"\"Return the body of the request.\n\n        Returns:\n            A byte-string representing the body of the request.\n        \"\"\"\n        if self._body is Empty:\n            if (body := self._connection_state.body) is not Empty:\n                self._body = body\n            else:\n                self._body = self._connection_state.body = b\"\".join([c async for c in self.stream()])\n        return self._body\n\n    async def form(self) -> FormMultiDict:\n        \"\"\"Retrieve form data from the request. If the request is either a 'multipart/form-data' or an\n        'application/x-www-form- urlencoded', return a FormMultiDict instance populated with the values sent in the\n        request, otherwise, an empty instance.\n\n        Returns:\n            A FormMultiDict instance\n        \"\"\"\n        if self._form is Empty:\n            if (form := self._connection_state.form) is not Empty:\n                self._form = form\n            else:\n                content_type, options = self.content_type\n                if content_type == RequestEncodingType.MULTI_PART:\n                    self._form = parse_multipart_form(\n                        body=await self.body(),\n                        boundary=options.get(\"boundary\", \"\").encode(),\n                        multipart_form_part_limit=self.app.multipart_form_part_limit,\n                    )\n                elif content_type == RequestEncodingType.URL_ENCODED:\n                    self._form = parse_url_encoded_form_data(\n                        await self.body(),\n                    )\n                else:\n                    self._form = {}\n\n                self._connection_state.form = self._form\n\n        return FormMultiDict(self._form)\n\n    async def send_push_promise(self, path: str, raise_if_unavailable: bool = False) -> None:\n        \"\"\"Send a push promise.\n\n        This method requires the `http.response.push` extension to be sent from the ASGI server.\n\n        Args:\n            path: Path to send the promise to.\n            raise_if_unavailable: Raise an exception if server push is not supported by\n                the server\n\n        Returns:\n            None\n        \"\"\"\n        if not self.supports_push_promise:\n            if raise_if_unavailable:\n                raise LitestarException(\"Attempted to send a push promise but the server does not support it\")\n\n            warnings.warn(\n                \"Attempted to send a push promise but the server does not support it. In a future version, this will \"\n                \"raise an exception. To enable this behaviour in the current version, set raise_if_unavailable=True. \"\n                \"To prevent this behaviour, make sure that the server you are using supports the 'http.response.push' \"\n                \"ASGI extension, or check this dynamically via \"\n                \":attr:`~litestar.connection.Request.supports_push_promise`\",\n                stacklevel=2,\n                category=LitestarWarning,\n            )\n\n            return\n\n        raw_headers = [\n            (header_name.encode(\"latin-1\"), value.encode(\"latin-1\"))\n            for header_name in (self.headers.keys() & SERVER_PUSH_HEADERS)\n            for value in self.headers.getall(header_name, [])\n        ]\n        await self.send({\"type\": \"http.response.push\", \"path\": path, \"headers\": raw_headers})\n\n```\n# Function Signature: async def json(self) -> Any:\n# Natural Language Annotations: \"\"\"Retrieve the json request body from the request.\n\n        Returns:\n            An arbitrary value\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def json(self) -> Any:\n    \"\"\"Ret", "  ```\nasync def json(self) -> Any:\n    \"\"\"Ret", "  ```\nasync def json(self: \"Request[UserT, Auth"]}, "litestar-litestar/datastructures/multi_dicts.py-close": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/datastructures/multi_dicts.py:\n```\nfrom __future__ import annotations\n\nfrom abc import ABC\nfrom typing import TYPE_CHECKING, Any, Generator, Generic, Iterable, Mapping, TypeVar\n\nfrom multidict import MultiDict as BaseMultiDict\nfrom multidict import MultiDictProxy, MultiMapping\n\nfrom litestar.datastructures.upload_file import UploadFile\n\nif TYPE_CHECKING:\n    from typing_extensions import Self\n\n\n__all__ = (\"FormMultiDict\", \"ImmutableMultiDict\", \"MultiDict\", \"MultiMixin\")\n\n\nT = TypeVar(\"T\")\n\n\nclass MultiMixin(Generic[T], MultiMapping[T], ABC):\n    \"\"\"Mixin providing common methods for multi dicts, used by :class:`ImmutableMultiDict` and :class:`MultiDict`\"\"\"\n\n    def dict(self) -> dict[str, list[Any]]:\n        \"\"\"Return the multi-dict as a dict of lists.\n\n        Returns:\n            A dict of lists\n        \"\"\"\n        return {k: self.getall(k) for k in set(self.keys())}\n\n    def multi_items(self) -> Generator[tuple[str, T], None, None]:\n        \"\"\"Get all keys and values, including duplicates.\n\n        Returns:\n            A list of tuples containing key-value pairs\n        \"\"\"\n        for key in set(self):\n            for value in self.getall(key):\n                yield key, value\n\n\nclass MultiDict(BaseMultiDict[T], MultiMixin[T], Generic[T]):\n    \"\"\"MultiDict, using :class:`MultiDict <multidict.MultiDictProxy>`.\"\"\"\n\n    def __init__(self, args: MultiMapping | Mapping[str, T] | Iterable[tuple[str, T]] | None = None) -> None:\n        \"\"\"Initialize ``MultiDict`` from a`MultiMapping``,\n        :class:`Mapping <typing.Mapping>` or an iterable of tuples.\n\n        Args:\n            args: Mapping-like structure to create the ``MultiDict`` from\n        \"\"\"\n        super().__init__(args or {})\n\n    def immutable(self) -> ImmutableMultiDict[T]:\n        \"\"\"Create an.\n\n        :class:`ImmutableMultiDict` view.\n\n        Returns:\n            An immutable multi dict\n        \"\"\"\n        return ImmutableMultiDict[T](self)  # pyright: ignore\n\n    def copy(self) -> Self:\n        \"\"\"Return a shallow copy\"\"\"\n        return type(self)(list(self.multi_items()))\n\n\nclass ImmutableMultiDict(MultiDictProxy[T], MultiMixin[T], Generic[T]):\n    \"\"\"Immutable MultiDict, using class:`MultiDictProxy <multidict.MultiDictProxy>`.\"\"\"\n\n    def __init__(self, args: MultiMapping | Mapping[str, Any] | Iterable[tuple[str, Any]] | None = None) -> None:\n        \"\"\"Initialize ``ImmutableMultiDict`` from a `MultiMapping``,\n        :class:`Mapping <typing.Mapping>` or an iterable of tuples.\n\n        Args:\n            args: Mapping-like structure to create the ``ImmutableMultiDict`` from\n        \"\"\"\n        super().__init__(BaseMultiDict(args or {}))\n\n    def mutable_copy(self) -> MultiDict[T]:\n        \"\"\"Create a mutable copy as a :class:`MultiDict`\n\n        Returns:\n            A mutable multi dict\n        \"\"\"\n        return MultiDict(list(self.multi_items()))\n\n    def copy(self) -> Self:  # type: ignore[override]\n        \"\"\"Return a shallow copy\"\"\"\n        return type(self)(self.items())\n\n\nclass FormMultiDict(ImmutableMultiDict[Any]):\n    \"\"\"MultiDict for form data.\"\"\"\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: async def close(self) -> None:\n# Natural Language Annotations: \"\"\"Close all files in the multi-dict.\n\n        Returns:\n            None\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def close(self) -> None:\n    \"\"\"Close", "  ```py\nasync def close(self) -> None:\n    \"\"\"", "  ```\nasync def close(self) -> None:\n    \"\"\"Close"]}, "litestar-litestar/datastructures/url.py-from_scope": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/datastructures/url.py:\n```\nfrom __future__ import annotations\n\nfrom functools import lru_cache\nfrom typing import TYPE_CHECKING, Any, NamedTuple\nfrom urllib.parse import SplitResult, urlencode, urlsplit, urlunsplit\n\nfrom litestar._parsers import parse_query_string\nfrom litestar.datastructures import MultiDict\nfrom litestar.types import Empty\n\nif TYPE_CHECKING:\n    from typing_extensions import Self\n\n    from litestar.types import EmptyType, Scope\n\n__all__ = (\"Address\", \"URL\")\n\n_DEFAULT_SCHEME_PORTS = {\"http\": 80, \"https\": 443, \"ftp\": 21, \"ws\": 80, \"wss\": 443}\n\n\nclass Address(NamedTuple):\n    \"\"\"Just a network address.\"\"\"\n\n    host: str\n    \"\"\"Address host.\"\"\"\n    port: int\n    \"\"\"Address port.\"\"\"\n\n\ndef make_absolute_url(path: str | URL, base: str | URL) -> str:\n    \"\"\"Create an absolute URL.\n\n    Args:\n        path: URL path to make absolute\n        base: URL to use as a base\n\n    Returns:\n        A string representing the new, absolute URL\n    \"\"\"\n    url = base if isinstance(base, URL) else URL(base)\n    netloc = url.netloc\n    path = url.path.rstrip(\"/\") + str(path)\n    return str(URL.from_components(scheme=url.scheme, netloc=netloc, path=path))\n\n\nclass URL:\n    \"\"\"Representation and modification utilities of a URL.\"\"\"\n\n    __slots__ = (\n        \"_query_params\",\n        \"_parsed_url\",\n        \"fragment\",\n        \"hostname\",\n        \"netloc\",\n        \"password\",\n        \"path\",\n        \"port\",\n        \"query\",\n        \"scheme\",\n        \"username\",\n    )\n\n    _query_params: EmptyType | MultiDict\n    _parsed_url: str | None\n\n    scheme: str\n    \"\"\"URL scheme.\"\"\"\n    netloc: str\n    \"\"\"Network location.\"\"\"\n    path: str\n    \"\"\"Hierarchical path.\"\"\"\n    fragment: str\n    \"\"\"Fragment component.\"\"\"\n    query: str\n    \"\"\"Query string.\"\"\"\n    username: str | None\n    \"\"\"Username if specified.\"\"\"\n    password: str | None\n    \"\"\"Password if specified.\"\"\"\n    port: int | None\n    \"\"\"Port if specified.\"\"\"\n    hostname: str | None\n    \"\"\"Hostname if specified.\"\"\"\n\n    def __new__(cls, url: str | SplitResult) -> URL:\n        \"\"\"Create a new instance.\n\n        Args:\n            url: url string or split result to represent.\n        \"\"\"\n        return cls._new(url=url)\n\n    @classmethod\n    @lru_cache\n    def _new(cls, url: str | SplitResult) -> URL:\n        instance = super().__new__(cls)\n        instance._parsed_url = None\n\n        if isinstance(url, str):\n            result = urlsplit(url)\n            instance._parsed_url = url\n        else:\n            result = url\n\n        instance.scheme = result.scheme\n        instance.netloc = result.netloc\n        instance.path = result.path\n        instance.fragment = result.fragment\n        instance.query = result.query\n        instance.username = result.username\n        instance.password = result.password\n        instance.port = result.port\n        instance.hostname = result.hostname\n        instance._query_params = Empty\n\n        return instance\n\n    @property\n    def _url(self) -> str:\n        if not self._parsed_url:\n            self._parsed_url = str(\n                urlunsplit(\n                    SplitResult(\n                        scheme=self.scheme,\n                        netloc=self.netloc,\n                        path=self.path,\n                        fragment=self.fragment,\n                        query=self.query,\n                    )\n                )\n            )\n        return self._parsed_url\n\n    @classmethod\n    @lru_cache\n    def from_components(\n        cls,\n        scheme: str = \"\",\n        netloc: str = \"\",\n        path: str = \"\",\n        fragment: str = \"\",\n        query: str = \"\",\n    ) -> Self:\n        \"\"\"Create a new URL from components.\n\n        Args:\n            scheme: URL scheme\n            netloc: Network location\n            path: Hierarchical path\n            query: Query component\n            fragment: Fragment identifier\n\n        Returns:\n            A new URL with the given components\n        \"\"\"\n        return cls(\n            SplitResult(\n                scheme=scheme,\n                netloc=netloc,\n                path=path,\n                fragment=fragment,\n                query=query,\n            )\n        )\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def with_replacements(\n        self,\n        scheme: str = \"\",\n        netloc: str = \"\",\n        path: str = \"\",\n        query: str | MultiDict | None | EmptyType = Empty,\n        fragment: str = \"\",\n    ) -> Self:\n        \"\"\"Create a new URL, replacing the given components.\n\n        Args:\n            scheme: URL scheme\n            netloc: Network location\n            path: Hierarchical path\n            query: Raw query string\n            fragment: Fragment identifier\n\n        Returns:\n            A new URL with the given components replaced\n        \"\"\"\n        if isinstance(query, MultiDict):\n            query = urlencode(query=query)\n\n        query = (query if query is not Empty else self.query) or \"\"\n\n        return type(self).from_components(\n            scheme=scheme or self.scheme,\n            netloc=netloc or self.netloc,\n            path=path or self.path,\n            query=query,\n            fragment=fragment or self.fragment,\n        )\n\n    @property\n    def query_params(self) -> MultiDict:\n        \"\"\"Query parameters of a URL as a :class:`MultiDict <.datastructures.multi_dicts.MultiDict>`\n\n        Returns:\n            A :class:`MultiDict <.datastructures.multi_dicts.MultiDict>` with query parameters\n\n        Notes:\n            - The returned ``MultiDict`` is mutable, :class:`URL` itself is *immutable*,\n                therefore mutating the query parameters will not directly mutate the ``URL``.\n                If you want to modify query parameters, make  modifications in the\n                multidict and pass them back to :meth:`with_replacements`\n        \"\"\"\n        if self._query_params is Empty:\n            self._query_params = MultiDict(parse_query_string(query_string=self.query.encode()))\n        return self._query_params\n\n    def __str__(self) -> str:\n        return self._url\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, (str, URL)):\n            return str(self) == str(other)\n        return NotImplemented  # pragma: no cover\n\n    def __repr__(self) -> str:\n        return f\"{type(self).__name__}({self._url!r})\"\n\n```\n# Function Signature: def from_scope(cls, scope: Scope) -> Self:\n# Natural Language Annotations: \"\"\"Construct a URL from a :class:`Scope <.types.Scope>`\n\n        Args:\n            scope: A scope\n\n        Returns:\n            A URL\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef from_scope(cls, scope: Scope) ->", "  ```\ndef from_scope(cls, scope: Scope) ->", "  ```\nfrom litestar.datastructures.url import URL"]}, "litestar-litestar/datastructures/headers.py-to_header": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/datastructures/headers.py:\n```\nimport re\nfrom abc import ABC, abstractmethod\nfrom contextlib import suppress\nfrom copy import copy\nfrom dataclasses import dataclass, fields\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    ClassVar,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Pattern,\n    Tuple,\n    Union,\n    cast,\n)\n\nimport msgspec\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiMapping\n\nfrom litestar._multipart import parse_content_header\nfrom litestar.datastructures.multi_dicts import MultiMixin\nfrom litestar.exceptions import ImproperlyConfiguredException, ValidationException\nfrom litestar.types.empty import Empty\nfrom litestar.utils.dataclass import simple_asdict\nfrom litestar.utils.scope.state import ScopeState\n\nif TYPE_CHECKING:\n    from litestar.types.asgi_types import (\n        HeaderScope,\n        Message,\n        RawHeaders,\n        RawHeadersList,\n        Scope,\n    )\n\n__all__ = (\"Accept\", \"CacheControlHeader\", \"ETag\", \"Header\", \"Headers\", \"MutableScopeHeaders\")\n\nETAG_RE = re.compile(r'([Ww]/)?\"(.+)\"')\nPRINTABLE_ASCII_RE: Pattern[str] = re.compile(r\"^[ -~]+$\")\n\n\ndef _encode_headers(headers: Iterable[Tuple[str, str]]) -> \"RawHeadersList\":\n    return [(key.lower().encode(\"latin-1\"), value.encode(\"latin-1\")) for key, value in headers]\n\n\nclass Headers(CIMultiDictProxy[str], MultiMixin[str]):\n    \"\"\"An immutable, case-insensitive multi dict for HTTP headers.\"\"\"\n\n    def __init__(self, headers: Optional[Union[Mapping[str, str], \"RawHeaders\", MultiMapping]] = None) -> None:\n        \"\"\"Initialize ``Headers``.\n\n        Args:\n            headers: Initial value.\n        \"\"\"\n        if not isinstance(headers, MultiMapping):\n            headers_: Union[Mapping[str, str], List[Tuple[str, str]]] = {}\n            if headers:\n                if isinstance(headers, Mapping):\n                    headers_ = headers  # pyright: ignore\n                else:\n                    headers_ = [(key.decode(\"latin-1\"), value.decode(\"latin-1\")) for key, value in headers]\n\n            super().__init__(CIMultiDict(headers_))\n        else:\n            super().__init__(headers)\n        self._header_list: Optional[RawHeadersList] = None\n\n    @classmethod\n    def from_scope(cls, scope: \"Scope\") -> \"Headers\":\n        \"\"\"Create headers from a send-message.\n\n        Args:\n            scope: The ASGI connection scope.\n\n        Returns:\n            Headers\n\n        Raises:\n            ValueError: If the message does not have a ``headers`` key\n        \"\"\"\n        connection_state = ScopeState.from_scope(scope)\n        if (headers := connection_state.headers) is Empty:\n            headers = connection_state.headers = cls(scope[\"headers\"])\n        return headers\n\n    def to_header_list(self) -> \"RawHeadersList\":\n        \"\"\"Raw header value.\n\n        Returns:\n            A list of tuples contain the header and header-value as bytes\n        \"\"\"\n        # Since ``Headers`` are immutable, this can be cached\n        if not self._header_list:\n            self._header_list = _encode_headers((key, value) for key in set(self) for value in self.getall(key))\n        return self._header_list\n\n\nclass MutableScopeHeaders(MutableMapping):\n    \"\"\"A case-insensitive, multidict-like structure that can be used to mutate headers within a\n    :class:`Scope <.types.Scope>`\n    \"\"\"\n\n    def __init__(self, scope: Optional[\"HeaderScope\"] = None) -> None:\n        \"\"\"Initialize ``MutableScopeHeaders`` from a ``HeaderScope``.\n\n        Args:\n            scope: The ASGI connection scope.\n        \"\"\"\n        self.headers: RawHeadersList\n        if scope is not None:\n            if not isinstance(scope[\"headers\"], list):\n                scope[\"headers\"] = list(scope[\"headers\"])\n\n            self.headers = cast(\"RawHeadersList\", scope[\"headers\"])\n        else:\n            self.headers = []\n\n    @classmethod\n    def from_message(cls, message: \"Message\") -> \"MutableScopeHeaders\":\n        \"\"\"Construct a header from a message object.\n\n        Args:\n            message: :class:`Message <.types.Message>`.\n\n        Returns:\n            MutableScopeHeaders.\n\n        Raises:\n            ValueError: If the message does not have a ``headers`` key.\n        \"\"\"\n        if \"headers\" not in message:\n            raise ValueError(f\"Invalid message type: {message['type']!r}\")\n\n        return cls(cast(\"HeaderScope\", message))\n\n    def add(self, key: str, value: str) -> None:\n        \"\"\"Add a header to the scope.\n\n        Notes:\n             - This method keeps duplicates.\n\n        Args:\n            key: Header key.\n            value: Header value.\n\n        Returns:\n            None.\n        \"\"\"\n        self.headers.append((key.lower().encode(\"latin-1\"), value.encode(\"latin-1\")))\n\n    def getall(self, key: str, default: Optional[List[str]] = None) -> List[str]:\n        \"\"\"Get all values of a header.\n\n        Args:\n            key: Header key.\n            default: Default value to return if ``name`` is not found.\n\n        Returns:\n            A list of strings.\n\n        Raises:\n            KeyError: if no header for ``name`` was found and ``default`` is not given.\n        \"\"\"\n        name = key.lower()\n        values = [\n            header_value.decode(\"latin-1\")\n            for header_name, header_value in self.headers\n            if header_name.decode(\"latin-1\").lower() == name\n        ]\n        if not values:\n            if default:\n                return default\n            raise KeyError\n        return values\n\n    def extend_header_value(self, key: str, value: str) -> None:\n        \"\"\"Extend a multivalued header.\n\n        Notes:\n            - A multivalues header is a header that can take a comma separated list.\n            - If the header previously did not exist, it will be added.\n\n        Args:\n            key: Header key.\n            value: Header value to add,\n\n        Returns:\n            None\n        \"\"\"\n        existing = self.get(key)\n        if existing is not None:\n            value = \",\".join([*existing.split(\",\"), value])\n        self[key] = value\n\n    def __getitem__(self, key: str) -> str:\n        \"\"\"Get the first header matching ``name``\"\"\"\n        name = key.lower()\n        for header in self.headers:\n            if header[0].decode(\"latin-1\").lower() == name:\n                return header[1].decode(\"latin-1\")\n        raise KeyError\n\n    def _find_indices(self, key: str) -> List[int]:\n        name = key.lower()\n        return [i for i, (name_, _) in enumerate(self.headers) if name_.decode(\"latin-1\").lower() == name]\n\n    def __setitem__(self, key: str, value: str) -> None:\n        \"\"\"Set a header in the scope, overwriting duplicates.\"\"\"\n        name_encoded = key.lower().encode(\"latin-1\")\n        value_encoded = value.encode(\"latin-1\")\n        if indices := self._find_indices(key):\n            for i in indices[1:]:\n                del self.headers[i]\n            self.headers[indices[0]] = (name_encoded, value_encoded)\n        else:\n            self.headers.append((name_encoded, value_encoded))\n\n    def __delitem__(self, key: str) -> None:\n        \"\"\"Delete all headers matching ``name``\"\"\"\n        indices = self._find_indices(key)\n        for i in indices[::-1]:\n            del self.headers[i]\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the internally stored headers, including duplicates.\"\"\"\n        return len(self.headers)\n\n    def __iter__(self) -> Iterator[str]:\n        \"\"\"Create an iterator of header names including duplicates.\"\"\"\n        return iter(h[0].decode(\"latin-1\") for h in self.headers)\n\n\n@dataclass\nclass Header(ABC):\n    \"\"\"An abstract type for HTTP headers.\"\"\"\n\n    HEADER_NAME: ClassVar[str] = \"\"\n\n    documentation_only: bool = False\n    \"\"\"Defines the header instance as for OpenAPI documentation purpose only.\"\"\"\n\n    @abstractmethod\n    def _get_header_value(self) -> str:\n        \"\"\"Get the header value as string.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def from_header(cls, header_value: str) -> \"Header\":\n        \"\"\"Construct a header from its string representation.\"\"\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@dataclass\nclass CacheControlHeader(Header):\n    \"\"\"A ``cache-control`` header.\"\"\"\n\n    HEADER_NAME: ClassVar[str] = \"cache-control\"\n\n    max_age: Optional[int] = None\n    \"\"\"Accessor for the ``max-age`` directive.\"\"\"\n    s_maxage: Optional[int] = None\n    \"\"\"Accessor for the ``s-maxage`` directive.\"\"\"\n    no_cache: Optional[bool] = None\n    \"\"\"Accessor for the ``no-cache`` directive.\"\"\"\n    no_store: Optional[bool] = None\n    \"\"\"Accessor for the ``no-store`` directive.\"\"\"\n    private: Optional[bool] = None\n    \"\"\"Accessor for the ``private`` directive.\"\"\"\n    public: Optional[bool] = None\n    \"\"\"Accessor for the ``public`` directive.\"\"\"\n    no_transform: Optional[bool] = None\n    \"\"\"Accessor for the ``no-transform`` directive.\"\"\"\n    must_revalidate: Optional[bool] = None\n    \"\"\"Accessor for the ``must-revalidate`` directive.\"\"\"\n    proxy_revalidate: Optional[bool] = None\n    \"\"\"Accessor for the ``proxy-revalidate`` directive.\"\"\"\n    must_understand: Optional[bool] = None\n    \"\"\"Accessor for the ``must-understand`` directive.\"\"\"\n    immutable: Optional[bool] = None\n    \"\"\"Accessor for the ``immutable`` directive.\"\"\"\n    stale_while_revalidate: Optional[int] = None\n    \"\"\"Accessor for the ``stale-while-revalidate`` directive.\"\"\"\n\n    def _get_header_value(self) -> str:\n        \"\"\"Get the header value as string.\"\"\"\n\n        cc_items = [\n            key.replace(\"_\", \"-\") if isinstance(value, bool) else f\"{key.replace('_', '-')}={value}\"\n            for key, value in simple_asdict(self, exclude_none=True, exclude={\"documentation_only\"}).items()\n        ]\n        return \", \".join(cc_items)\n\n    @classmethod\n    def from_header(cls, header_value: str) -> \"CacheControlHeader\":\n        \"\"\"Create a ``CacheControlHeader`` instance from the header value.\n\n        Args:\n            header_value: the header value as string\n\n        Returns:\n            An instance of ``CacheControlHeader``\n        \"\"\"\n\n        kwargs: Dict[str, Any] = {}\n        field_names = {f.name for f in fields(cls)}\n        for cc_item in (stripped for v in header_value.split(\",\") if (stripped := v.strip())):\n            key, *value = cc_item.split(\"=\", maxsplit=1)\n            key = key.replace(\"-\", \"_\")\n            if key not in field_names:\n                raise ImproperlyConfiguredException(\"Invalid cache-control header\")\n            if not value:\n                kwargs[key] = True\n            else:\n                (kwargs[key],) = value\n\n        try:\n            return msgspec.convert(kwargs, CacheControlHeader, strict=False)\n        except msgspec.ValidationError as exc:\n            raise ImproperlyConfiguredException from exc\n\n    @classmethod\n    def prevent_storing(cls) -> \"CacheControlHeader\":\n        \"\"\"Create a ``cache-control`` header with the ``no-store`` directive which indicates that any caches of any kind\n        (private or shared) should not store this response.\n        \"\"\"\n\n        return cls(no_store=True)\n\n\n@dataclass\nclass ETag(Header):\n    \"\"\"An ``etag`` header.\"\"\"\n\n    HEADER_NAME: ClassVar[str] = \"etag\"\n\n    weak: bool = False\n    value: Optional[str] = None  # only ASCII characters\n\n    def _get_header_value(self) -> str:\n        value = f'\"{self.value}\"'\n        return f\"W/{value}\" if self.weak else value\n\n    @classmethod\n    def from_header(cls, header_value: str) -> \"ETag\":\n        \"\"\"Construct an ``etag`` header from its string representation.\n\n        Note that this will unquote etag-values\n        \"\"\"\n        match = ETAG_RE.match(header_value)\n        if not match:\n            raise ImproperlyConfiguredException\n        weak, value = match.group(1, 2)\n        try:\n            return cls(weak=bool(weak), value=value)\n        except ValueError as exc:\n            raise ImproperlyConfiguredException from exc\n\n    def __post_init__(self) -> None:\n        if self.documentation_only is False and self.value is None:\n            raise ValidationException(\"value must be set if documentation_only is false\")\n        if self.value and not PRINTABLE_ASCII_RE.fullmatch(self.value):\n            raise ValidationException(\"value must only contain ASCII printable characters\")\n\n\nclass MediaTypeHeader:\n    \"\"\"A helper class for ``Accept`` header parsing.\"\"\"\n\n    __slots__ = (\"maintype\", \"subtype\", \"params\", \"_params_str\")\n\n    def __init__(self, type_str: str) -> None:\n        # preserve the original parameters, because the order might be\n        # changed in the dict\n        self._params_str = \"\".join(type_str.partition(\";\")[1:])\n\n        full_type, self.params = parse_content_header(type_str)\n        self.maintype, _, self.subtype = full_type.partition(\"/\")\n\n    def __str__(self) -> str:\n        return f\"{self.maintype}/{self.subtype}{self._params_str}\"\n\n    @property\n    def priority(self) -> Tuple[int, int]:\n        # Use fixed point values with two decimals to avoid problems\n        # when comparing float values\n        quality = 100\n        if \"q\" in self.params:\n            with suppress(ValueError):\n                quality = int(100 * float(self.params[\"q\"]))\n\n        if self.maintype == \"*\":\n            specificity = 0\n        elif self.subtype == \"*\":\n            specificity = 1\n        elif not self.params or (\"q\" in self.params and len(self.params) == 1):\n            # no params or 'q' is the only one which we ignore\n            specificity = 2\n        else:\n            specificity = 3\n\n        return quality, specificity\n\n    def match(self, other: \"MediaTypeHeader\") -> bool:\n        return next(\n            (False for key, value in self.params.items() if key != \"q\" and value != other.params.get(key)),\n            False\n            if self.subtype != \"*\" and other.subtype != \"*\" and self.subtype != other.subtype\n            else self.maintype == \"*\" or other.maintype == \"*\" or self.maintype == other.maintype,\n        )\n\n\nclass Accept:\n    \"\"\"An ``Accept`` header.\"\"\"\n\n    __slots__ = (\"_accepted_types\",)\n\n    def __init__(self, accept_value: str) -> None:\n        self._accepted_types = [MediaTypeHeader(t) for t in accept_value.split(\",\")]\n        self._accepted_types.sort(key=lambda t: t.priority, reverse=True)\n\n    def __len__(self) -> int:\n        return len(self._accepted_types)\n\n    def __getitem__(self, key: int) -> str:\n        return str(self._accepted_types[key])\n\n    def __iter__(self) -> Iterator[str]:\n        return map(str, self._accepted_types)\n\n    def best_match(self, provided_types: List[str], default: Optional[str] = None) -> Optional[str]:\n        \"\"\"Find the best matching media type for the request.\n\n        Args:\n            provided_types: A list of media types that can be provided as a response. These types\n                            can contain a wildcard ``*`` character in the main- or subtype part.\n            default: The media type that is returned if none of the provided types match.\n\n        Returns:\n            The best matching media type. If the matching provided type contains wildcard characters,\n            they are replaced with the corresponding part of the accepted type. Otherwise the\n            provided type is returned as-is.\n        \"\"\"\n        types = [MediaTypeHeader(t) for t in provided_types]\n\n        for accepted in self._accepted_types:\n            for provided in types:\n                if provided.match(accepted):\n                    # Return the accepted type with wildcards replaced\n                    # by concrete parts from the provided type\n                    result = copy(provided)\n                    if result.subtype == \"*\":\n                        result.subtype = accepted.subtype\n                    if result.maintype == \"*\":\n                        result.maintype = accepted.maintype\n                    return str(result)\n        return default\n\n    def accepts(self, media_type: str) -> bool:\n        \"\"\"Check if the request accepts the specified media type.\n\n        If multiple media types can be provided, it is better to use :func:`best_match`.\n\n        Args:\n            media_type: The media type to check for.\n\n        Returns:\n            True if the request accepts ``media_type``.\n        \"\"\"\n        return self.best_match([media_type]) == media_type\n\n```\n# Function Signature: def to_header(self, include_header_name: bool = False) -> str:\n# Natural Language Annotations: \"\"\"Get the header as string.\n\n        Args:\n            include_header_name: should include the header name in the return value. If set to false\n                the return value will only include the header value. if set to true the return value\n                will be: ``<header name>: <header value>``. Defaults to false.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n@dataclass\nclass Header:\n    \"\"\"An abstract type", "  ```\ndef to_header(self, include_header_name:", "  ```\ndef to_header(self, include_header_name:"]}, "litestar-litestar/middleware/session/server_side.py-delete": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/litestar/litestar/middleware/session/server_side.py:\n```\nfrom __future__ import annotations\n\nimport secrets\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any, Literal\n\nfrom litestar.datastructures import Cookie, MutableScopeHeaders\nfrom litestar.enums import ScopeType\nfrom litestar.exceptions import ImproperlyConfiguredException\nfrom litestar.middleware.session.base import ONE_DAY_IN_SECONDS, BaseBackendConfig, BaseSessionBackend\nfrom litestar.types import Empty, Message, Scopes, ScopeSession\nfrom litestar.utils.dataclass import extract_dataclass_items\n\n__all__ = (\"ServerSideSessionBackend\", \"ServerSideSessionConfig\")\n\n\nif TYPE_CHECKING:\n    from litestar import Litestar\n    from litestar.connection import ASGIConnection\n    from litestar.stores.base import Store\n\n\nclass ServerSideSessionBackend(BaseSessionBackend[\"ServerSideSessionConfig\"]):\n    \"\"\"Base class for server-side backends.\n\n    Implements :class:`BaseSessionBackend` and defines and interface which subclasses can\n    implement to facilitate the storage of session data.\n    \"\"\"\n\n    def __init__(self, config: ServerSideSessionConfig) -> None:\n        \"\"\"Initialize ``ServerSideSessionBackend``\n\n        Args:\n            config: A subclass of ``ServerSideSessionConfig``\n        \"\"\"\n        super().__init__(config=config)\n\n    async def get(self, session_id: str, store: Store) -> bytes | None:\n        \"\"\"Retrieve data associated with ``session_id``.\n\n        Args:\n            session_id: The session-ID\n            store: Store to retrieve the session data from\n\n        Returns:\n            The session data, if existing, otherwise ``None``.\n        \"\"\"\n        max_age = int(self.config.max_age) if self.config.max_age is not None else None\n        return await store.get(session_id, renew_for=max_age if self.config.renew_on_access else None)\n\n    async def set(self, session_id: str, data: bytes, store: Store) -> None:\n        \"\"\"Store ``data`` under the ``session_id`` for later retrieval.\n\n        If there is already data associated with ``session_id``, replace\n        it with ``data`` and reset its expiry time\n\n        Args:\n            session_id: The session-ID\n            data: Serialized session data\n            store: Store to save the session data in\n\n        Returns:\n            None\n        \"\"\"\n        expires_in = int(self.config.max_age) if self.config.max_age is not None else None\n        await store.set(session_id, data, expires_in=expires_in)\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_session_id(self, connection: ASGIConnection) -> str:\n        \"\"\"Try to fetch session id from the connection. If one does not exist, generate one.\n\n        If a session ID already exists in the cookies, it is returned.\n        If there is no ID in the cookies but one in the connection state, then the session exists but has not yet\n        been returned to the user.\n        Otherwise, a new session must be created.\n\n        Args:\n            connection: Originating ASGIConnection containing the scope\n        Returns:\n            Session id str or None if the concept of a session id does not apply.\n        \"\"\"\n        session_id = connection.cookies.get(self.config.key)\n        if not session_id or session_id == \"null\":\n            session_id = connection.get_session_id()\n            if not session_id:\n                session_id = self.generate_session_id()\n        return session_id\n\n    def generate_session_id(self) -> str:\n        \"\"\"Generate a new session-ID, with\n        n=:attr:`session_id_bytes <ServerSideSessionConfig.session_id_bytes>` random bytes.\n\n        Returns:\n            A session-ID\n        \"\"\"\n        return secrets.token_hex(self.config.session_id_bytes)\n\n    async def store_in_message(self, scope_session: ScopeSession, message: Message, connection: ASGIConnection) -> None:\n        \"\"\"Store the necessary information in the outgoing ``Message`` by setting a cookie containing the session-ID.\n\n        If the session is empty, a null-cookie will be set. Otherwise, the serialised\n        data will be stored using :meth:`set <ServerSideSessionBackend.set>`, under the current session-id. If no session-ID\n        exists, a new ID will be generated using :meth:`generate_session_id <ServerSideSessionBackend.generate_session_id>`.\n\n        Args:\n            scope_session: Current session to store\n            message: Outgoing send-message\n            connection: Originating ASGIConnection containing the scope\n\n        Returns:\n            None\n        \"\"\"\n        scope = connection.scope\n        store = self.config.get_store_from_app(scope[\"app\"])\n        headers = MutableScopeHeaders.from_message(message)\n        session_id = self.get_session_id(connection)\n\n        cookie_params = dict(extract_dataclass_items(self.config, exclude_none=True, include=Cookie.__dict__.keys()))\n\n        if scope_session is Empty:\n            await self.delete(session_id, store=store)\n            headers.add(\n                \"Set-Cookie\",\n                Cookie(value=\"null\", key=self.config.key, expires=0, **cookie_params).to_header(header=\"\"),\n            )\n        else:\n            serialised_data = self.serialize_data(scope_session, scope)\n            await self.set(session_id=session_id, data=serialised_data, store=store)\n            headers.add(\n                \"Set-Cookie\", Cookie(value=session_id, key=self.config.key, **cookie_params).to_header(header=\"\")\n            )\n\n    async def load_from_connection(self, connection: ASGIConnection) -> dict[str, Any]:\n        \"\"\"Load session data from a connection and return it as a dictionary to be used in the current application\n        scope.\n\n        The session-ID will be gathered from a cookie with the key set in\n        :attr:`BaseBackendConfig.key`. If a cookie is found, its value will be used as the session-ID and data associated\n        with this ID will be loaded using :meth:`get <ServerSideSessionBackend.get>`.\n        If no cookie was found or no data was loaded from the store, this will return an\n        empty dictionary.\n\n        Args:\n            connection: An ASGIConnection instance\n\n        Returns:\n            The current session data\n        \"\"\"\n        if session_id := connection.cookies.get(self.config.key):\n            store = self.config.get_store_from_app(connection.scope[\"app\"])\n            data = await self.get(session_id, store=store)\n            if data is not None:\n                return self.deserialize_data(data)\n        return {}\n\n\n@dataclass\nclass ServerSideSessionConfig(BaseBackendConfig[ServerSideSessionBackend]):  # pyright: ignore\n    \"\"\"Base configuration for server side backends.\"\"\"\n\n    _backend_class = ServerSideSessionBackend\n\n    session_id_bytes: int = field(default=32)\n    \"\"\"Number of bytes used to generate a random session-ID.\"\"\"\n    renew_on_access: bool = field(default=False)\n    \"\"\"Renew expiry times of sessions when they're being accessed\"\"\"\n    key: str = field(default=\"session\")\n    \"\"\"Key to use for the cookie inside the header, e.g. ``session=<data>`` where ``session`` is the cookie key and\n    ``<data>`` is the session data.\n\n    Notes:\n        - If a session cookie exceeds 4KB in size it is split. In this case the key will be of the format\n          ``session-{segment number}``.\n\n    \"\"\"\n    max_age: int = field(default=ONE_DAY_IN_SECONDS * 14)\n    \"\"\"Maximal age of the cookie before its invalidated.\"\"\"\n    scopes: Scopes = field(default_factory=lambda: {ScopeType.HTTP, ScopeType.WEBSOCKET})\n    \"\"\"Scopes for the middleware - options are ``http`` and ``websocket`` with the default being both\"\"\"\n    path: str = field(default=\"/\")\n    \"\"\"Path fragment that must exist in the request url for the cookie to be valid.\n\n    Defaults to ``'/'``.\n    \"\"\"\n    domain: str | None = field(default=None)\n    \"\"\"Domain for which the cookie is valid.\"\"\"\n    secure: bool = field(default=False)\n    \"\"\"Https is required for the cookie.\"\"\"\n    httponly: bool = field(default=True)\n    \"\"\"Forbids javascript to access the cookie via 'Document.cookie'.\"\"\"\n    samesite: Literal[\"lax\", \"strict\", \"none\"] = field(default=\"lax\")\n    \"\"\"Controls whether or not a cookie is sent with cross-site requests. Defaults to ``lax``.\"\"\"\n    exclude: str | list[str] | None = field(default=None)\n    \"\"\"A pattern or list of patterns to skip in the session middleware.\"\"\"\n    exclude_opt_key: str = field(default=\"skip_session\")\n    \"\"\"An identifier to use on routes to disable the session middleware for a particular route.\"\"\"\n    store: str = \"sessions\"\n    \"\"\"Name of the :class:`Store <.stores.base.Store>` to use\"\"\"\n\n    def __post_init__(self) -> None:\n        if len(self.key) < 1 or len(self.key) > 256:\n            raise ImproperlyConfiguredException(\"key must be a string with a length between 1-256\")\n        if self.max_age < 1:\n            raise ImproperlyConfiguredException(\"max_age must be greater than 0\")\n\n    def get_store_from_app(self, app: Litestar) -> Store:\n        \"\"\"Get the store defined in :attr:`store` from an :class:`Litestar <.app.Litestar>` instance\"\"\"\n        return app.stores.get(self.store)\n\n```\n# Function Signature: async def delete(self, session_id: str, store: Store) -> None:\n# Natural Language Annotations: \"\"\"Delete the data associated with ``session_id``. Fails silently if no such session-ID exists.\n\n        Args:\n            session_id: The session-ID\n            store: Store to delete the session data from\n\n        Returns:\n            None\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def delete(self, session_id: str, store", "  ```\nasync def delete(self, session_id: str, store", "  ```\nasync def delete(self, session_id: str, store"]}, "vllm-vllm/sequence.py-get_max_num_running_seqs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/sequence.py:\n```\n\"\"\"Sequence and its related classes.\"\"\"\nimport copy\nimport enum\nfrom abc import ABC, abstractmethod\nfrom array import array\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom typing import (TYPE_CHECKING, Any, Dict, List, Mapping, Optional, Set,\n                    Tuple, Union, cast)\n\nimport msgspec\nimport numpy\nimport torch\n\nfrom vllm.inputs.parse import is_valid_encoder_decoder_llm_inputs\nfrom vllm.lora.request import LoRARequest\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.spec_decode.metrics import SpecDecodeWorkerMetrics\n\nif TYPE_CHECKING:\n    from vllm.inputs import LLMInputs\n    from vllm.multimodal.base import MultiModalDataDict\n\nVLLM_TOKEN_ID_ARRAY_TYPE = \"l\"\n\n\n# We use dataclass for now because it is used for\n# openai server output, and msgspec is not serializable.\n# TODO(sang): Fix it.\n@dataclass\nclass Logprob:\n    \"\"\"Infos for supporting OpenAI compatible logprobs and token ranks.\n\n    Attributes:\n        logprob: The logprob of chosen token\n        rank: The vocab rank of chosen token (>=1)\n        decoded_token: The decoded chosen token index\n    \"\"\"\n    logprob: float\n    rank: Optional[int] = None\n    decoded_token: Optional[str] = None\n\n\n# {token_id -> logprob} per each sequence group. None if the corresponding\n# sequence group doesn't require prompt logprob.\nPromptLogprobs = List[Optional[Dict[int, Logprob]]]\n# {token_id -> logprob} for each sequence group.\nSampleLogprobs = List[Dict[int, Logprob]]\n\n\nclass SequenceStatus(enum.IntEnum):\n    \"\"\"Status of a sequence.\"\"\"\n    WAITING = 0\n    RUNNING = 1\n    SWAPPED = 2\n    # Note: anything after SWAPPED (2) will be considered\n    # as a finished status.\n    FINISHED_STOPPED = 3\n    FINISHED_LENGTH_CAPPED = 4\n    FINISHED_ABORTED = 5\n    FINISHED_IGNORED = 6\n\n    @staticmethod\n    def is_finished(status: \"SequenceStatus\") -> bool:\n        return status > SequenceStatus.SWAPPED\n\n    @staticmethod\n    def get_finished_reason(status: \"SequenceStatus\") -> Union[str, None]:\n        if status == SequenceStatus.FINISHED_STOPPED:\n            finish_reason = \"stop\"\n        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:\n            finish_reason = \"length\"\n        elif status == SequenceStatus.FINISHED_ABORTED:\n            finish_reason = \"abort\"\n        elif status == SequenceStatus.FINISHED_IGNORED:\n            # The ignored sequences are the sequences whose prompt lengths\n            # are longer than the model's length cap. Therefore, the stop\n            # reason should also be \"length\" as in OpenAI API.\n            finish_reason = \"length\"\n        else:\n            finish_reason = None\n        return finish_reason\n\n\nclass SequenceStage(enum.Enum):\n    PREFILL = enum.auto()\n    DECODE = enum.auto()\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Metrics associated with a request.\n\n    Attributes:\n        arrival_time: The time when the request arrived.\n        first_scheduled_time: The time when the request was first scheduled.\n        first_token_time: The time when the first token was generated.\n        time_in_queue: The time the request spent in the queue.\n        finished_time: The time when the request was finished.\n        scheduler_time: The time spent in the scheduler when this request was\n                        being considered by the scheduler.\n        model_forward_time: The time spent in the model forward pass when this\n                            request was in the batch.\n        model_execute_time: The time spent in the model execute function. This\n                            will include model forward, block/sync across\n                            workers, cpu-gpu sync time and sampling time.\n    \"\"\"\n    arrival_time: float\n    last_token_time: float\n    first_scheduled_time: Optional[float]\n    first_token_time: Optional[float]\n    time_in_queue: Optional[float]\n    finished_time: Optional[float] = None\n    scheduler_time: Optional[float] = None\n    model_forward_time: Optional[float] = None\n    model_execute_time: Optional[float] = None\n\n\nclass SequenceDataDelta(\n        msgspec.Struct,\n        array_like=True,  # type: ignore[call-arg]\n        omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"Delta SequenceData to send to workers per step.\"\"\"\n    # A new token to be appended to existing SequenceData.\n    new_output_token_ids: List[int]\n    # Overwriting existing `cumulative_logprob`\n    new_cumulative_logprob: float\n    # Overwriting existing `num_computed_tokens`.\n    new_num_computed_tokens: int\n    # Overwriting existing `stage`.\n    new_stage: SequenceStage\n\n\nclass SequenceData(msgspec.Struct,\n                   omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"Data associated with a sequence.\n\n    Args:\n        prompt_token_ids: The token IDs of the prompt.\n        output_token_ids: The token IDs of the output. Set to an empty list if\n            None.\n\n    Attributes:\n        prompt_token_ids: The token IDs of the prompt.\n        output_token_ids: The token IDs of the output.\n        cumulative_logprob: The cumulative log probability of the output.\n    \"\"\"\n    # NOTE: we cannot use Union[List, array] because msgspec cannot support\n    # union of 2 list types.\n    _prompt_token_ids: array\n    _output_token_ids: array = msgspec.field(\n        default_factory=lambda: array(VLLM_TOKEN_ID_ARRAY_TYPE, []))\n\n    ### The below fields should not be passed as an argument ###\n    _cumulative_logprob: float = 0.0\n    _prompt_token_ids_tuple: Tuple[int,\n                                   ...] = msgspec.field(default_factory=tuple)\n    # The number of tokens that are computed (that run against the model).\n    _num_computed_tokens: int = 0\n    _stage: SequenceStage = SequenceStage.PREFILL\n    _cached_all_token_ids: List[int] = msgspec.field(default_factory=list)\n\n    # It is used to get delta input. It is reset when `get_delta_and_reset`\n    # is called.\n    _new_appended_tokens: List[int] = msgspec.field(default_factory=list)\n\n    def __post_init__(self) -> None:\n        assert self._prompt_token_ids.typecode == \"l\"\n        assert self._output_token_ids.typecode == \"l\"\n        self._prompt_token_ids_tuple: Tuple[int, ...] = tuple(\n            self._prompt_token_ids)\n        self._update_cached_all_tokens()\n\n    def _update_cached_all_tokens(self):\n        assert isinstance(self._prompt_token_ids, array)\n        assert isinstance(self._output_token_ids, array)\n        self._cached_all_token_ids: List[int] = list(self._prompt_token_ids +\n                                                     self._output_token_ids)\n\n    @property\n    def cumulative_logprob(self) -> float:\n        return self._cumulative_logprob\n\n    @property\n    def prompt_token_ids(self) -> Tuple[int, ...]:\n        return self._prompt_token_ids_tuple\n\n    @prompt_token_ids.setter\n    def prompt_token_ids(self, new_prompt_token_ids) -> None:\n        raise NotImplementedError\n\n    @property\n    def prompt_token_ids_array(self) -> array:\n        \"\"\"Return the prompt token ids in array type.\n\n        Note that the array is in \"I\" type, and it is not compatible\n        with torch.long (2 bytes vs 4 bytes). So beware of the usage.\n        \"\"\"\n        return self._prompt_token_ids\n\n    @property\n    def output_token_ids(self) -> Tuple[int, ...]:\n        return tuple(self._output_token_ids)\n\n    @output_token_ids.setter\n    def output_token_ids(self, new_output_token_ids: List[int]) -> None:\n        self._output_token_ids = array(VLLM_TOKEN_ID_ARRAY_TYPE,\n                                       new_output_token_ids)\n        self._update_cached_all_tokens()\n\n    @property\n    def output_token_ids_array(self) -> array:\n        \"\"\"Return the prompt token ids in array type.\n\n        Note that the array is in \"I\" type, and it is not compatible\n        with torch.long (2 bytes vs 4 bytes). So beware of the usage.\n        \"\"\"\n        assert isinstance(self._output_token_ids, array)\n        return self._output_token_ids\n\n    def append_token_id(self, token_id: int, logprob: float) -> None:\n        self._output_token_ids.append(token_id)\n        self._new_appended_tokens.append(token_id)\n        self._cached_all_token_ids.append(token_id)\n        self._cumulative_logprob += logprob\n\n    def get_len(self) -> int:\n        return len(self._output_token_ids) + len(self._prompt_token_ids)\n\n    def get_prompt_len(self) -> int:\n        return len(self._prompt_token_ids)\n\n    def get_output_len(self) -> int:\n        return len(self._output_token_ids)\n\n    def get_token_ids(self) -> List[int]:\n        return self._cached_all_token_ids\n\n    def get_prefix_token_ids(\n            self, num_tokens: int\n    ) -> Tuple[Tuple[int, ...], Optional[Tuple[int, ...]]]:\n        \"\"\"Get prefix tokens, and make the return value hashable\"\"\"\n        prompt_length = self.get_prompt_len()\n        if num_tokens > prompt_length:\n            return (self._prompt_token_ids_tuple,\n                    tuple(self._output_token_ids[:num_tokens - prompt_length]))\n        else:\n            return (self._prompt_token_ids_tuple[:num_tokens], None)\n\n    def get_num_computed_tokens(self) -> int:\n        \"\"\"Return the number of prefill tokens that are already computed.\"\"\"\n        return self._num_computed_tokens\n\n    def update_num_computed_tokens(self, num_new_computed_tokens: int):\n        \"\"\"Update number of tokens computed so far.\"\"\"\n        self._num_computed_tokens += num_new_computed_tokens\n        assert self._num_computed_tokens <= self.get_len(), (\n            self._num_computed_tokens, self.get_len())\n        # If all tokens are computed, it means it is in decoding phase.\n        if self.get_num_uncomputed_tokens() == 0:\n            self._stage = SequenceStage.DECODE\n\n    def reset_state_for_recompute(self) -> None:\n        \"\"\"Reset the number of computed tokens from this sequence. It is\n        supposed to be called when a sequence needs to be started from\n        the beginning again (e.g., sequence is preempted).\n        \"\"\"\n        self._num_computed_tokens = 0\n        self._stage = SequenceStage.PREFILL\n        self._new_appended_tokens = []\n\n    def get_num_uncomputed_tokens(self) -> int:\n        \"\"\"Return the number of prefill tokens that are not computed.\"\"\"\n        # we use `get_len()` which includes prompt_len + output_len instead\n        # of prompt_len here. This is because during recompute we need to\n        # prefill for both prompt and output.\n        return self.get_len() - self.get_num_computed_tokens()\n\n    def get_last_token_id(self) -> int:\n        if not self._output_token_ids:\n            return self._prompt_token_ids[-1]\n        return self._output_token_ids[-1]\n\n    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n        return self.prompt_token_ids\n\n    def get_output_token_ids(self) -> Tuple[int, ...]:\n        return self.output_token_ids\n\n    def get_delta_and_reset(self) -> SequenceDataDelta:\n        delta = SequenceDataDelta(self._new_appended_tokens,\n                                  self._cumulative_logprob,\n                                  self.get_num_computed_tokens(), self.stage)\n        # Reset delta state.\n        self._new_appended_tokens = []\n        return delta\n\n    def apply_delta(self, delta: SequenceDataDelta):\n        self._num_computed_tokens = delta.new_num_computed_tokens\n        self._cumulative_logprob = delta.new_cumulative_logprob\n        self._stage = delta.new_stage\n        self._output_token_ids.extend(delta.new_output_token_ids)\n        self._cached_all_token_ids.extend(delta.new_output_token_ids)\n\n    @property\n    def stage(self) -> SequenceStage:\n        return self._stage\n\n    def __repr__(self) -> str:\n        return (f\"SequenceData(\"\n                f\"prompt_token_ids={self._prompt_token_ids}, \"\n                f\"output_token_ids={self.output_token_ids}, \"\n                f\"cumulative_logprob={self.cumulative_logprob}, \"\n                f\"get_num_computed_tokens={self.get_num_computed_tokens()}\")\n\n\nclass Sequence:\n    \"\"\"Stores the data, status, and block information of a sequence.\n\n    The sequence is constructed from the LLMInputs instance passed\n    in through the `inputs` constructor argument.\n\n    For encoder/decoder models, LLMInputs encapsulates both a\n    decoder and encoder prompt, creating an ambiguity about which\n    prompt to construct the sequence from. The `from_decoder_prompt`\n    constructor argument signals whether to construct the Sequence\n    from the LLMInputs decoder prompt, or encoder prompt.\n\n    Args:\n        seq_id: The ID of the sequence.\n        inputs: The inputs of the sequence.\n        block_size: The block size of the sequence. Should be the same as the\n            block size used by the block manager and cache engine.\n        eos_token_id: The end-of-sequence (EOS) token id recognized by this LLM.\n        lora_request: LoRA request.\n        prompt_adapter_request: Prompt Adapter request.\n        from_decoder_prompt: Construct Sequence from LLMInputs decoder prompt\n                             (True) or encoder prompt (False.) Must be True\n                             for decoder-only model.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        seq_id: int,\n        inputs: \"LLMInputs\",\n        block_size: int,\n        eos_token_id: Optional[int] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        from_decoder_prompt: bool = True,\n    ) -> None:\n        self.seq_id = seq_id\n        self.inputs = inputs\n        self.block_size = block_size\n        self.eos_token_id = eos_token_id\n        self.lora_request = lora_request\n        self.prompt_adapter_request = prompt_adapter_request\n        self.from_decoder_prompt = from_decoder_prompt\n        self._prompt: Optional[str] = None\n        self._prompt_token_ids: Optional[List[int]] = None\n\n        # For decoder-only models, a Sequence is constructed\n        # from an LLMInputs instance (the `inputs` arg.)\n        #\n        # For encoder/decoder models the same `inputs`\n        # instance could be utilized to construct either an\n        # encoder sequence or a decoder sequence, because\n        # `LLMInputs` has both decoder- and encoder-oriented\n        # member variables (i.e. it encapsulates both an encoder\n        # and a decoder prompt.) The decision of which type of sequence\n        # to generate is determined by the `from_decoder_prompt` argument.\n        #\n        # When constructing a encoder sequence\n        # (`from_decoder_prompt` False) it matters that\n        # the `LLMInputs` instance stored in `inputs` is valid\n        # in the sense that its encoder-related member variables are\n        # populated; below, an exception is raised if this is\n        # not the case.\n        #\n        # When constructing a decoder sequence (`from_decoder_prompt` True)\n        # it does not matter whether `inputs` has its encoder-related\n        # member variables populated.\n        if not (from_decoder_prompt\n                or is_valid_encoder_decoder_llm_inputs(inputs)):\n            raise ValueError(\"Cannot extract encoder input prompt from \"\n                             f\"invalid input {inputs}; did you forget the \"\n                             \"encoder input prompt fields?\")\n\n        self.data = SequenceData(\n            array(VLLM_TOKEN_ID_ARRAY_TYPE, self.prompt_token_ids))\n        self.output_logprobs: SampleLogprobs = []\n        self.output_text = \"\"\n\n        self.status = SequenceStatus.WAITING\n        self.stop_reason: Union[int, str, None] = None\n\n        # Used for incremental detokenization\n        self.prefix_offset = 0\n        self.read_offset = 0\n        # Input + output tokens\n        self.tokens: Optional[List[str]] = None\n\n    @property\n    def n_blocks(self) -> int:\n        return (self.get_len() + self.block_size - 1) // self.block_size\n\n    @property\n    def prompt(self) -> Optional[str]:\n        if self._prompt is not None:\n            # Reuse precomputed prompt string\n            return self._prompt\n\n        # Select decoder or encoder input prompt str,\n        # as appropriate\n        prompt_key: str = (\"prompt\"\n                           if self.from_decoder_prompt else \"encoder_prompt\")\n\n        # Cache prompt\n        self._prompt = cast(Optional[str], self.inputs.get(prompt_key))\n        return self._prompt\n\n    @property\n    def prompt_token_ids(self) -> List[int]:\n        if self._prompt_token_ids is not None:\n            # Reuse precomputed prompt token ids\n            return self._prompt_token_ids\n\n        # Select decoder or encoder input prompt\n        # token ids, as appropriate\n        prompt_token_ids_key: str = (\"prompt_token_ids\"\n                                     if self.from_decoder_prompt else\n                                     \"encoder_prompt_token_ids\")\n\n        # Cache computed prompt token ids\n        self._prompt_token_ids = cast(List[int],\n                                      self.inputs.get(prompt_token_ids_key))\n        return self._prompt_token_ids\n\n    @property\n    def multi_modal_data(self) -> \"MultiModalDataDict\":\n        return self.inputs.get(\"multi_modal_data\") or {}\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    def get_output_text_to_return(self, buffer_length: int):\n        # We return the full output text if the sequence is finished.\n        truncate = buffer_length and not self.is_finished()\n        return self.output_text[:-buffer_length] if truncate else (\n            self.output_text)\n\n    def hash_of_block(self, logical_idx: int) -> int:\n        # TODO This can produce incorrect hash when block size > prompt size\n\n        # Compute the number of tokens in the sequence\n        # TODO: The current hashing function is O(L^2). We should optimize\n        # this in the future.\n        num_tokens = self.num_hashed_tokens_of_block(logical_idx)\n        hashed_tokens = self.data.get_prefix_token_ids(num_tokens)\n        return hash((hashed_tokens, self.lora_int_id))\n\n    def num_hashed_tokens_of_block(self, logical_idx: int):\n        return logical_idx * self.block_size + self.block_size\n\n    def reset_state_for_recompute(self):\n        \"\"\"Reset the sequence states for recomputation.\"\"\"\n        self.data.reset_state_for_recompute()\n\n    def append_token_id(\n        self,\n        token_id: int,\n        logprobs: Dict[int, Logprob],\n    ) -> None:\n        assert token_id in logprobs\n        self.output_logprobs.append(logprobs)\n        self.data.append_token_id(token_id, logprobs[token_id].logprob)\n\n    def get_len(self) -> int:\n        return self.data.get_len()\n\n    def get_prompt_len(self) -> int:\n        return self.data.get_prompt_len()\n\n    def get_output_len(self) -> int:\n        return self.data.get_output_len()\n\n    def get_token_ids(self) -> List[int]:\n        return self.data.get_token_ids()\n\n    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n        return self.data.get_prompt_token_ids()\n\n    def get_last_token_id(self) -> int:\n        return self.data.get_last_token_id()\n\n    def get_output_token_ids(self) -> Tuple[int, ...]:\n        return self.data.get_output_token_ids()\n\n    def get_cumulative_logprob(self) -> float:\n        return self.data.cumulative_logprob\n\n    def get_beam_search_score(self,\n                              length_penalty: float = 1.0,\n                              seq_len: Optional[int] = None,\n                              eos_token_id: Optional[int] = None) -> float:\n        \"\"\"Calculate the beam search score with length penalty.\n\n        Adapted from\n\n        https://github.com/huggingface/transformers/blob/ccb92be23def445f2afdea94c31286f84b89eb5b/src/transformers/generation/beam_search.py#L938\n        \"\"\"\n        if seq_len is None:\n            seq_len = self.get_len()\n            # NOTE: HF implementation does not count the EOS token\n            # towards the length, we align with that here for testing.\n            if (eos_token_id is not None\n                    and self.get_last_token_id() == eos_token_id):\n                seq_len -= 1\n        return self.get_cumulative_logprob() / (seq_len**length_penalty)\n\n    def is_finished(self) -> bool:\n        return SequenceStatus.is_finished(self.status)\n\n    def fork(self, new_seq_id: int) -> \"Sequence\":\n        new_seq = copy.deepcopy(self)\n        new_seq.seq_id = new_seq_id\n        return new_seq\n\n    def get_num_new_tokens(self) -> int:\n        \"\"\"Get the number of new tokens to be computed.\n\n        Returns:\n            The new number of tokens to be computed. I.e., 1 for decode, or\n            the remaining prompt size for prefill.\n        \"\"\"\n        if self.data.stage == SequenceStage.DECODE:\n            return 1\n        return self.data.get_num_uncomputed_tokens()\n\n    def is_prefill(self) -> bool:\n        return self.data.stage == SequenceStage.PREFILL\n\n    def __repr__(self) -> str:\n        return (f\"Sequence(seq_id={self.seq_id}, \"\n                f\"status={self.status.name}, \"\n                f\"num_blocks={self.n_blocks}, \")\n\n\nclass SequenceGroupState(msgspec.Struct,\n                         omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"Mutable state tied to a specific sequence group\"\"\"\n\n    # for multi-step decoding\n    num_steps: int = 1\n    current_step: int = 0\n\n    @property\n    def remaining_steps(self) -> int:\n        return self.num_steps - self.current_step\n\n\nclass SequenceGroup:\n    \"\"\"A group of sequences that are generated from the same prompt.\n\n    Args:\n        request_id: The ID of the request.\n        seqs: The list of sequences.\n        sampling_params: The sampling parameters used to generate the outputs.\n        arrival_time: The arrival time of the request.\n        lora_request: LoRA request.\n        embeddings: The embeddings vectors of the prompt of the sequence group\n            for an embedding model.\n        pooling_params: The pooling parameters used to generate the pooling\n            for an embedding model.\n        encoder_seq: Optional, the single encoder sequence. Should be None\n                     unless you are working with an encoder/decoder model.\n        trace_headers: OpenTelemetry trace headers.\n        prompt_adapter_request: Prompt Adapter request.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: str,\n        seqs: List[Sequence],\n        arrival_time: float,\n        sampling_params: Optional[SamplingParams] = None,\n        lora_request: Optional[LoRARequest]\n```\n# Function Signature: def get_max_num_running_seqs(self) -> int:\n# Natural Language Annotations: \"\"\"The maximum number of sequences running in parallel in the remaining\n        lifetime of the request.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_max_num_running_seqs(self", "  ```\ndef get_max_num_running_seqs(self", "  ```\ndef get_max_num_running_seqs(self"]}, "vllm-vllm/utils.py-cuda_device_count_stateless": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/utils.py:\n```\nimport argparse\nimport asyncio\nimport contextlib\nimport datetime\nimport enum\nimport gc\nimport os\nimport socket\nimport subprocess\nimport sys\nimport tempfile\nimport threading\nimport uuid\nimport warnings\nfrom asyncio import FIRST_COMPLETED, ensure_future\nfrom functools import lru_cache, partial, wraps\nfrom platform import uname\nfrom typing import (Any, AsyncGenerator, Awaitable, Callable, Dict, Generic,\n                    Hashable, List, Literal, Optional, OrderedDict, Set, Tuple,\n                    Type, TypeVar, Union, overload)\nfrom uuid import uuid4\n\nimport numpy as np\nimport numpy.typing as npt\nimport psutil\nimport torch\nimport torch.types\nfrom typing_extensions import ParamSpec, TypeIs, assert_never\n\nimport vllm.envs as envs\nfrom vllm.logger import enable_trace_function_call, init_logger\n\nlogger = init_logger(__name__)\n\n# Exception strings for non-implemented encoder/decoder scenarios\n\nSTR_NOT_IMPL_ENC_DEC_SWA = \\\n    \"Sliding window attention for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_PREFIX_CACHE = \\\n    \"Prefix caching for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL = \\\n    \"Chunked prefill for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP = (\n    \"Models with logits_soft_cap \"\n    \"require FlashInfer backend, which is \"\n    \"currently not supported for encoder/decoder \"\n    \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_LORA = (\"LoRA is currently not currently \"\n                             \"supported with encoder/decoder \"\n                             \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_PP = (\"Pipeline parallelism is not \"\n                           \"currently supported with \"\n                           \"encoder/decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_MM = (\"Multimodal is not currently \"\n                           \"supported with encoder/decoder \"\n                           \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_SPEC_DEC = (\"Speculative decoding is not \"\n                                 \"currently supported with encoder/\"\n                                 \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_CUDAGRAPH = (\"CUDAGraph is not \"\n                                  \"currently supported with encoder/\"\n                                  \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_BACKEND = (\"XFormers is the only backend \"\n                                \"currently supported with encoder/\"\n                                \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER = (\"Prompt adapters are not \"\n                                       \"currently supported with encoder/\"\n                                       \"decoder models.\")\n\n# Efficiently import all enc/dec error strings\n# rather than having to import all of the above\nSTR_NOT_IMPL_ENC_DEC_ERR_STRS = {\n    \"STR_NOT_IMPL_ENC_DEC_SWA\": STR_NOT_IMPL_ENC_DEC_SWA,\n    \"STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\": STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n    \"STR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL\":\n    STR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL,\n    \"STR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP\": STR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP,\n    \"STR_NOT_IMPL_ENC_DEC_LORA\": STR_NOT_IMPL_ENC_DEC_LORA,\n    \"STR_NOT_IMPL_ENC_DEC_PP\": STR_NOT_IMPL_ENC_DEC_PP,\n    \"STR_NOT_IMPL_ENC_DEC_MM\": STR_NOT_IMPL_ENC_DEC_MM,\n    \"STR_NOT_IMPL_ENC_DEC_SPEC_DEC\": STR_NOT_IMPL_ENC_DEC_SPEC_DEC,\n    \"STR_NOT_IMPL_ENC_DEC_CUDA_GRAPH\": STR_NOT_IMPL_ENC_DEC_CUDAGRAPH,\n    \"STR_NOT_IMPL_ENC_DEC_BACKEND\": STR_NOT_IMPL_ENC_DEC_BACKEND,\n    \"STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER\": STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER,\n}\n\n# Constants related to forcing the attention backend selection\n\n# String name of register which may be set in order to\n# force auto-selection of attention backend by Attention\n# wrapper\nSTR_BACKEND_ENV_VAR: str = \"VLLM_ATTENTION_BACKEND\"\n\n# Possible string values of STR_BACKEND_ENV_VAR\n# register, corresponding to possible backends\nSTR_FLASHINFER_ATTN_VAL: str = \"FLASHINFER\"\nSTR_TORCH_SDPA_ATTN_VAL: str = \"TORCH_SDPA\"\nSTR_ROCM_FLASH_ATTN_VAL: str = \"ROCM_FLASH\"\nSTR_XFORMERS_ATTN_VAL: str = \"XFORMERS\"\nSTR_FLASH_ATTN_VAL: str = \"FLASH_ATTN\"\nSTR_INVALID_VAL: str = \"INVALID\"\n\nGiB_bytes = 1 << 30\n\"\"\"The number of bytes in one gibibyte (GiB).\"\"\"\n\nSTR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.half,\n    \"bfloat16\": torch.bfloat16,\n    \"float\": torch.float,\n    \"fp8\": torch.uint8,\n    \"fp8_e4m3\": torch.uint8,\n    \"fp8_e5m2\": torch.uint8,\n}\n\nTORCH_DTYPE_TO_NUMPY_DTYPE = {\n    torch.float16: np.float16,\n    torch.float32: np.float32,\n    torch.float64: np.float64,\n    torch.uint8: np.uint8,\n    torch.int32: np.int32,\n    torch.int64: np.int64,\n}\n\nP = ParamSpec('P')\nK = TypeVar(\"K\")\nT = TypeVar(\"T\")\nU = TypeVar(\"U\")\n\n\nclass _Sentinel:\n    ...\n\n\nALL_PINNED_SENTINEL = _Sentinel()\n\n\nclass Device(enum.Enum):\n    GPU = enum.auto()\n    CPU = enum.auto()\n\n\nclass Counter:\n\n    def __init__(self, start: int = 0) -> None:\n        self.counter = start\n\n    def __next__(self) -> int:\n        i = self.counter\n        self.counter += 1\n        return i\n\n    def reset(self) -> None:\n        self.counter = 0\n\n\nclass LRUCache(Generic[T]):\n\n    def __init__(self, capacity: int):\n        self.cache: OrderedDict[Hashable, T] = OrderedDict()\n        self.pinned_items: Set[Hashable] = set()\n        self.capacity = capacity\n\n    def __contains__(self, key: Hashable) -> bool:\n        return key in self.cache\n\n    def __len__(self) -> int:\n        return len(self.cache)\n\n    def __getitem__(self, key: Hashable) -> T:\n        value = self.cache[key]  # Raise KeyError if not exists\n        self.cache.move_to_end(key)\n        return value\n\n    def __setitem__(self, key: Hashable, value: T) -> None:\n        self.put(key, value)\n\n    def __delitem__(self, key: Hashable) -> None:\n        self.pop(key)\n\n    def touch(self, key: Hashable) -> None:\n        self.cache.move_to_end(key)\n\n    def get(self,\n            key: Hashable,\n            default_value: Optional[T] = None) -> Optional[T]:\n        value: Optional[T]\n        if key in self.cache:\n            value = self.cache[key]\n            self.cache.move_to_end(key)\n        else:\n            value = default_value\n        return value\n\n    def put(self, key: Hashable, value: T) -> None:\n        self.cache[key] = value\n        self.cache.move_to_end(key)\n        self._remove_old_if_needed()\n\n    def pin(self, key: Hashable) -> None:\n        \"\"\"\n        Pins a key in the cache preventing it from being\n        evicted in the LRU order.\n        \"\"\"\n        if key not in self.cache:\n            raise ValueError(f\"Cannot pin key: {key} not in cache.\")\n        self.pinned_items.add(key)\n\n    def _unpin(self, key: Hashable) -> None:\n        self.pinned_items.remove(key)\n\n    def _on_remove(self, key: Hashable, value: Optional[T]):\n        pass\n\n    def remove_oldest(self, remove_pinned=False):\n        if not self.cache:\n            return\n\n        if not remove_pinned:\n            # pop the oldest item in the cache that is not pinned\n            lru_key = next(\n                (key for key in self.cache if key not in self.pinned_items),\n                ALL_PINNED_SENTINEL)\n            if lru_key is ALL_PINNED_SENTINEL:\n                raise RuntimeError(\"All items are pinned, \"\n                                   \"cannot remove oldest from the cache.\")\n        else:\n            lru_key = next(iter(self.cache))\n        self.pop(lru_key)\n\n    def _remove_old_if_needed(self) -> None:\n        while len(self.cache) > self.capacity:\n            self.remove_oldest()\n\n    def pop(self,\n            key: Hashable,\n            default_value: Optional[T] = None) -> Optional[T]:\n        run_on_remove = key in self.cache\n        value: Optional[T] = self.cache.pop(key, default_value)\n        # remove from pinned items\n        if key in self.pinned_items:\n            self._unpin(key)\n        if run_on_remove:\n            self._on_remove(key, value)\n        return value\n\n    def clear(self):\n        while len(self.cache) > 0:\n            self.remove_oldest(remove_pinned=True)\n        self.cache.clear()\n\n\nclass PyObjectCache:\n    \"\"\"Used to cache python objects to avoid object allocations \n    across scheduler iterations.\n    \"\"\"\n\n    def __init__(self, obj_builder):\n        self._obj_builder = obj_builder\n        self._index = 0\n\n        self._obj_cache = []\n        for _ in range(128):\n            self._obj_cache.append(self._obj_builder())\n\n    def _grow_cache(self):\n        # Double the size of the cache\n        num_objs = len(self._obj_cache)\n        for _ in range(num_objs):\n            self._obj_cache.append(self._obj_builder())\n\n    def get_object(self):\n        \"\"\"Returns a pre-allocated cached object. If there is not enough \n        objects, then the cache size will double.\n        \"\"\"\n        if self._index >= len(self._obj_cache):\n            self._grow_cache()\n            assert self._index < len(self._obj_cache)\n\n        obj = self._obj_cache[self._index]\n        self._index += 1\n\n        return obj\n\n    def reset(self):\n        \"\"\"Makes all cached-objects available for the next scheduler iteration.\n        \"\"\"\n        self._index = 0\n\n\ndef is_hip() -> bool:\n    return torch.version.hip is not None\n\n\n@lru_cache(maxsize=None)\ndef is_cpu() -> bool:\n    from importlib.metadata import PackageNotFoundError, version\n    try:\n        return \"cpu\" in version(\"vllm\")\n    except PackageNotFoundError:\n        return False\n\n\n@lru_cache(maxsize=None)\ndef is_openvino() -> bool:\n    from importlib.metadata import PackageNotFoundError, version\n    try:\n        return \"openvino\" in version(\"vllm\")\n    except PackageNotFoundError:\n        return False\n\n\n@lru_cache(maxsize=None)\ndef is_neuron() -> bool:\n    try:\n        import transformers_neuronx\n    except ImportError:\n        transformers_neuronx = None\n    return transformers_neuronx is not None\n\n\n@lru_cache(maxsize=None)\ndef is_xpu() -> bool:\n    from importlib.metadata import PackageNotFoundError, version\n    try:\n        is_xpu_flag = \"xpu\" in version(\"vllm\")\n    except PackageNotFoundError:\n        return False\n    # vllm is not build with xpu\n    if not is_xpu_flag:\n        return False\n    try:\n        import intel_extension_for_pytorch as ipex  # noqa: F401\n        _import_ipex = True\n    except ImportError as e:\n        logger.warning(\"Import Error for IPEX: %s\", e.msg)\n        _import_ipex = False\n    # ipex dependency is not ready\n    if not _import_ipex:\n        logger.warning(\"not found ipex lib\")\n        return False\n    return hasattr(torch, \"xpu\") and torch.xpu.is_available()\n\n\n@lru_cache(maxsize=None)\ndef get_max_shared_memory_bytes(gpu: int = 0) -> int:\n    \"\"\"Returns the maximum shared memory per thread block in bytes.\"\"\"\n    from vllm import _custom_ops as ops\n    max_shared_mem = (\n        ops.get_max_shared_memory_per_block_device_attribute(gpu))\n    # value 0 will cause MAX_SEQ_LEN become negative and test_attention.py\n    # will fail\n    assert max_shared_mem > 0, \"max_shared_mem can not be zero\"\n    return int(max_shared_mem)\n\n\ndef get_cpu_memory() -> int:\n    \"\"\"Returns the total CPU memory of the node in bytes.\"\"\"\n    return psutil.virtual_memory().total\n\n\ndef random_uuid() -> str:\n    return str(uuid.uuid4().hex)\n\n\n@lru_cache(maxsize=None)\ndef get_vllm_instance_id() -> str:\n    \"\"\"\n    If the environment variable VLLM_INSTANCE_ID is set, return it.\n    Otherwise, return a random UUID.\n    Instance id represents an instance of the VLLM. All processes in the same\n    instance should have the same instance id.\n    \"\"\"\n    return envs.VLLM_INSTANCE_ID or f\"vllm-instance-{random_uuid()}\"\n\n\n@lru_cache(maxsize=None)\ndef in_wsl() -> bool:\n    # Reference: https://github.com/microsoft/WSL/issues/4071\n    return \"microsoft\" in \" \".join(uname()).lower()\n\n\ndef make_async(func: Callable[P, T]) -> Callable[P, Awaitable[T]]:\n    \"\"\"Take a blocking function, and run it on in an executor thread.\n\n    This function prevents the blocking function from blocking the\n    asyncio event loop.\n    The code in this function needs to be thread safe.\n    \"\"\"\n\n    def _async_wrapper(*args: P.args, **kwargs: P.kwargs) -> asyncio.Future:\n        loop = asyncio.get_event_loop()\n        p_func = partial(func, *args, **kwargs)\n        return loop.run_in_executor(executor=None, func=p_func)\n\n    return _async_wrapper\n\n\nasync def iterate_with_cancellation(\n    iterator: AsyncGenerator[T, None],\n    is_cancelled: Callable[[], Awaitable[bool]],\n) -> AsyncGenerator[T, None]:\n    \"\"\"Convert async iterator into one that polls the provided function\n    at least once per second to check for client cancellation.\n    \"\"\"\n\n    # Can use anext() in python >= 3.10\n    awaits = [ensure_future(iterator.__anext__())]\n    while True:\n        done, pending = await asyncio.wait(awaits, timeout=1)\n        if await is_cancelled():\n            with contextlib.suppress(BaseException):\n                awaits[0].cancel()\n                await iterator.aclose()\n            raise asyncio.CancelledError(\"client cancelled\")\n        if done:\n            try:\n                item = await awaits[0]\n                awaits[0] = ensure_future(iterator.__anext__())\n                yield item\n            except StopAsyncIteration:\n                # we are done\n                return\n\n\nasync def merge_async_iterators(\n    *iterators: AsyncGenerator[T, None],\n    is_cancelled: Optional[Callable[[], Awaitable[bool]]] = None,\n) -> AsyncGenerator[Tuple[int, T], None]:\n    \"\"\"Merge multiple asynchronous iterators into a single iterator.\n\n    This method handle the case where some iterators finish before others.\n    When it yields, it yields a tuple (i, item) where i is the index of the\n    iterator that yields the item.\n\n    It also optionally polls a provided function at least once per second\n    to check for client cancellation.\n    \"\"\"\n\n    # Can use anext() in python >= 3.10\n    awaits = {\n        ensure_future(pair[1].__anext__()): pair\n        for pair in enumerate(iterators)\n    }\n    timeout = None if is_cancelled is None else 1\n    try:\n        while awaits:\n            done, pending = await asyncio.wait(awaits.keys(),\n                                               return_when=FIRST_COMPLETED,\n                                               timeout=timeout)\n            if is_cancelled is not None and await is_cancelled():\n                raise asyncio.CancelledError(\"client cancelled\")\n            for d in done:\n                pair = awaits.pop(d)\n                try:\n                    item = await d\n                    i, it = pair\n                    awaits[ensure_future(it.__anext__())] = pair\n                    yield i, item\n                except StopAsyncIteration:\n                    pass\n    finally:\n        # Cancel any remaining iterators\n        for f, (_, it) in awaits.items():\n            with contextlib.suppress(BaseException):\n                f.cancel()\n                await it.aclose()\n\n\ndef get_ip() -> str:\n    host_ip = envs.VLLM_HOST_IP\n    if host_ip:\n        return host_ip\n\n    # IP is not set, try to get it from the network interface\n\n    # try ipv4\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        s.connect((\"8.8.8.8\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    # try ipv6\n    try:\n        s = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n        # Google's public DNS server, see\n        # https://developers.google.com/speed/public-dns/docs/using#addresses\n        s.connect((\"2001:4860:4860::8888\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    warnings.warn(\n        \"Failed to get the IP address, using 0.0.0.0 by default.\"\n        \"The value can be set by the environment variable\"\n        \" VLLM_HOST_IP or HOST_IP.\",\n        stacklevel=2)\n    return \"0.0.0.0\"\n\n\ndef get_distributed_init_method(ip: str, port: int) -> str:\n    # Brackets are not permitted in ipv4 addresses,\n    # see https://github.com/python/cpython/issues/103848\n    return f\"tcp://[{ip}]:{port}\" if \":\" in ip else f\"tcp://{ip}:{port}\"\n\n\ndef get_open_zmq_ipc_path() -> str:\n    base_rpc_path = envs.VLLM_RPC_BASE_PATH\n    return f\"ipc://{base_rpc_path}/{uuid4()}\"\n\n\ndef get_open_port() -> int:\n    port = envs.VLLM_PORT\n    if port is not None:\n        while True:\n            try:\n                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                    s.bind((\"\", port))\n                    return port\n            except OSError:\n                port += 1  # Increment port number if already in use\n                logger.info(\"Port %d is already in use, trying port %d\",\n                            port - 1, port)\n    # try ipv4\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n    except OSError:\n        # try ipv6\n        with socket.socket(socket.AF_INET6, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n\n\ndef update_environment_variables(envs: Dict[str, str]):\n    for k, v in envs.items():\n        if k in os.environ and os.environ[k] != v:\n            logger.warning(\n                \"Overwriting environment variable %s \"\n                \"from '%s' to '%s'\", k, os.environ[k], v)\n        os.environ[k] = v\n\n\ndef chunk_list(lst: List[T], chunk_size: int):\n    \"\"\"Yield successive chunk_size chunks from lst.\"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i:i + chunk_size]\n\n\ndef cdiv(a: int, b: int) -> int:\n    \"\"\"Ceiling division.\"\"\"\n    return -(a // -b)\n\n\ndef _generate_random_fp8(\n    tensor: torch.Tensor,\n    low: float,\n    high: float,\n) -> None:\n    # NOTE(zhaoyang): Due to NaN and Inf representation for fp8 data type,\n    # it may occur Inf or NaN if we directly use torch.randint\n    # to generate random data for fp8 data.\n    # For example, s.11111.00 in fp8e5m2 format represents Inf.\n    #     | E4M3        | E5M2\n    #-----|-------------|-------------------\n    # Inf | N/A         | s.11111.00\n    # NaN | s.1111.111  | s.11111.{01,10,11}\n    from vllm import _custom_ops as ops\n    tensor_tmp = torch.empty_like(tensor, dtype=torch.float16)\n    tensor_tmp.uniform_(low, high)\n    ops.convert_fp8(tensor, tensor_tmp)\n    del tensor_tmp\n\n\ndef get_kv_cache_torch_dtype(\n        cache_dtype: Optional[Union[str, torch.dtype]],\n        model_dtype: Optional[Union[str, torch.dtype]] = None) -> torch.dtype:\n    if isinstance(cache_dtype, str):\n        if cache_dtype == \"auto\":\n            if isinstance(model_dtype, str):\n                torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[model_dtype]\n            elif isinstance(model_dtype, torch.dtype):\n                torch_dtype = model_dtype\n            else:\n                raise ValueError(f\"Invalid model dtype: {model_dtype}\")\n        elif cache_dtype in [\"half\", \"bfloat16\", \"float\"]:\n            torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_dtype]\n        elif cache_dtype == \"fp8\":\n            torch_dtype = torch.uint8\n        else:\n            raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    elif isinstance(cache_dtype, torch.dtype):\n        torch_dtype = cache_dtype\n    else:\n        raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    return torch_dtype\n\n\ndef create_kv_caches_with_random_flash(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n    model_dtype: Optional[Union[str, torch.dtype]] = None,\n    seed: int = 0,\n    device: Optional[str] = \"cuda\",\n) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    torch.random.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\n    torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n    key_value_cache_shape = (num_blocks, 2, block_size, num_heads, head_size)\n    scale = head_size**-0.5\n\n    key_caches: List[torch.Tensor] = []\n    value_caches: List[torch.Tensor] = []\n\n    for _ in range(num_layers):\n        key_value_cache = torch.empty(size=key_value_cache_shape,\n                                      dtype=torch_dtype,\n                                      device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            key_value_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(key_value_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support key cache of type {cache_dtype}\")\n        key_caches.append(key_value_cache[:, 0])\n        value_caches.append(key_value_cache[:, 1])\n    return key_caches, value_caches\n\n\ndef create_kv_caches_with_random(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n   \n```\n# Function Signature: def cuda_device_count_stateless() -> int:\n# Natural Language Annotations: \"\"\"Get number of CUDA devices, caching based on the value of\n    CUDA_VISIBLE_DEVICES at the time of call.\n    \n    This should be used instead of torch.cuda.device_count()\n    unless CUDA_VISIBLE_DEVICES has already been set to the desired\n    value.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef cuda_device_count_stateless() -> int", "  ```\ndef cuda_device_count_stateless() -> int", "  ```\ndef cuda_device_count_stateless() -> int"]}, "vllm-vllm/config.py-get_sliding_window": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/config.py:\n```\nimport enum\nimport json\nfrom dataclasses import dataclass, field, fields\nfrom typing import (TYPE_CHECKING, ClassVar, List, Mapping, Optional, Tuple,\n                    Type, Union)\n\nimport torch\nfrom transformers import PretrainedConfig\n\nimport vllm.envs as envs\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\nfrom vllm.model_executor.models import ModelRegistry\nfrom vllm.platforms import current_platform\nfrom vllm.tracing import is_otel_installed\nfrom vllm.transformers_utils.config import get_config, get_hf_text_config\nfrom vllm.utils import (STR_NOT_IMPL_ENC_DEC_CUDAGRAPH, GiB_bytes,\n                        cuda_device_count_stateless, get_cpu_memory, is_cpu,\n                        is_hip, is_neuron, is_openvino, is_xpu,\n                        print_warning_once)\n\nif TYPE_CHECKING:\n    from ray.util.placement_group import PlacementGroup\n\n    from vllm.executor.executor_base import ExecutorBase\n    from vllm.model_executor.model_loader.loader import BaseModelLoader\n    from vllm.transformers_utils.tokenizer_group.base_tokenizer_group import (\n        BaseTokenizerGroup)\n\nlogger = init_logger(__name__)\n\n_EMBEDDING_MODEL_MAX_NUM_BATCHED_TOKENS = 32768\n\n_PP_SUPPORTED_MODELS = [\n    \"AquilaModel\",\n    \"AquilaForCausalLM\",\n    \"DeepseekV2ForCausalLM\",\n    \"InternLMForCausalLM\",\n    \"JAISLMHeadModel\",\n    \"LlamaForCausalLM\",\n    \"LLaMAForCausalLM\",\n    \"MistralForCausalLM\",\n    \"Phi3ForCausalLM\",\n    \"GPT2LMHeadModel\",\n    \"MixtralForCausalLM\",\n    \"NemotronForCausalLM\",\n    \"Qwen2ForCausalLM\",\n    \"Qwen2MoeForCausalLM\",\n    \"QWenLMHeadModel\",\n]\n\n\nclass ModelConfig:\n    \"\"\"Configuration for the model.\n\n    Args:\n        model: Name or path of the huggingface model to use.\n            It is also used as the content for `model_name` tag in metrics \n            output when `served_model_name` is not specified. \n        tokenizer: Name or path of the huggingface tokenizer to use.\n        tokenizer_mode: Tokenizer mode. \"auto\" will use the fast tokenizer if\n            available, and \"slow\" will always use the slow tokenizer.\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\n            downloading the model and tokenizer.\n        dtype: Data type for model weights and activations. The \"auto\" option\n            will use FP16 precision for FP32 and FP16 models, and BF16 precision\n            for BF16 models.\n        seed: Random seed for reproducibility.\n        revision: The specific model version to use. It can be a branch name,\n            a tag name, or a commit id. If unspecified, will use the default\n            version.\n        code_revision: The specific revision to use for the model code on\n            Hugging Face Hub. It can be a branch name, a tag name, or a\n            commit id. If unspecified, will use the default version.\n        rope_scaling: Dictionary containing the scaling configuration for the\n            RoPE embeddings. When using this flag, don't update\n            `max_position_embeddings` to the expected new maximum.\n        tokenizer_revision: The specific tokenizer version to use. It can be a\n            branch name, a tag name, or a commit id. If unspecified, will use\n            the default version.\n        max_model_len: Maximum length of a sequence (including prompt and\n            output). If None, will be derived from the model.\n        quantization: Quantization method that was used to quantize the model\n            weights. If None, we assume the model weights are not quantized.\n        quantization_param_path: Path to JSON file containing scaling factors.\n            Used to load KV cache scaling factors into the model when KV cache\n            type is FP8_E4M3 on ROCm (AMD GPU). In the future these will also\n            be used to load activation and weight scaling factors when the\n            model dtype is FP8_E4M3 on ROCm.\n        enforce_eager: Whether to enforce eager execution. If True, we will\n            disable CUDA graph and always execute the model in eager mode.\n            If False, we will use CUDA graph and eager execution in hybrid.\n            If None, the user did not specify, so default to False -\n            except for encoder/decoder models, which currently require\n            eager mode.\n        max_context_len_to_capture: Maximum context len covered by CUDA graphs.\n            When a sequence has context length larger than this, we fall back\n            to eager mode (DEPRECATED. Use max_seq_len_to_capture instead).\n        max_seq_len_to_capture: Maximum sequence len covered by CUDA graphs.\n            When a sequence has context length larger than this, we fall back\n            to eager mode\n        disable_sliding_window: Whether to disable sliding window. If True,\n            we will disable the sliding window functionality of the model.\n            If the model does not support sliding window, this argument is\n            ignored.\n        skip_tokenizer_init: If true, skip initialization of tokenizer and\n            detokenizer.\n        served_model_name: The model name used in metrics tag `model_name`,\n            matches the model name exposed via the APIs. If multiple model \n            names provided, the first name will be used. If not specified, \n            the model name will be the same as `model`.\n        limit_mm_per_prompt: Maximum number of data instances per modality \n            per prompt. Only applicable for multimodal models.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        tokenizer: str,\n        tokenizer_mode: str,\n        trust_remote_code: bool,\n        dtype: Union[str, torch.dtype],\n        seed: int,\n        revision: Optional[str] = None,\n        code_revision: Optional[str] = None,\n        rope_scaling: Optional[dict] = None,\n        rope_theta: Optional[float] = None,\n        tokenizer_revision: Optional[str] = None,\n        max_model_len: Optional[int] = None,\n        quantization: Optional[str] = None,\n        quantization_param_path: Optional[str] = None,\n        enforce_eager: Optional[bool] = None,\n        max_context_len_to_capture: Optional[int] = None,\n        max_seq_len_to_capture: Optional[int] = None,\n        max_logprobs: int = 20,\n        disable_sliding_window: bool = False,\n        skip_tokenizer_init: bool = False,\n        served_model_name: Optional[Union[str, List[str]]] = None,\n        limit_mm_per_prompt: Optional[Mapping[str, int]] = None,\n    ) -> None:\n        self.model = model\n        self.tokenizer = tokenizer\n        self.tokenizer_mode = tokenizer_mode\n        self.trust_remote_code = trust_remote_code\n        self.seed = seed\n        self.revision = revision\n        self.code_revision = code_revision\n        self.rope_scaling = rope_scaling\n        self.rope_theta = rope_theta\n        # The tokenizer version is consistent with the model version by default.\n        if tokenizer_revision is None:\n            self.tokenizer_revision = revision\n        else:\n            self.tokenizer_revision = tokenizer_revision\n        self.quantization = quantization\n        self.quantization_param_path = quantization_param_path\n        self.enforce_eager = enforce_eager\n        if max_context_len_to_capture is not None:\n            raise ValueError(\"`max_context_len_to_capture` is deprecated. \"\n                             \"Use `max_seq_len_to_capture` instead.\")\n        self.max_seq_len_to_capture = max_seq_len_to_capture\n        self.max_logprobs = max_logprobs\n        self.disable_sliding_window = disable_sliding_window\n        self.skip_tokenizer_init = skip_tokenizer_init\n\n        self.hf_config = get_config(self.model, trust_remote_code, revision,\n                                    code_revision, rope_scaling, rope_theta)\n        self.hf_text_config = get_hf_text_config(self.hf_config)\n        self.dtype = _get_and_verify_dtype(self.hf_text_config, dtype)\n\n        # Choose a default enforce_eager value if the user did not specify\n        # a value (enforce_eager is None)\n        if getattr(self.hf_config, 'is_encoder_decoder', False):\n            if self.enforce_eager is None:\n                # *Only for encoder/decoder models* and\n                # *only if enforce_eager is unset*, override\n                # to enforce_eager=True\n                #\n                # Add a logger message since it is *somewhat* non-intuitive that\n                # enforce_eager is True when the user has not specified its\n                # value.\n                logger.info(\"Forcing enforce_eager == True because \"\n                            \"enforce_eager setting was unspecified and \"\n                            \"CUDAGraph is not supported with encoder/ \"\n                            \"decoder models.\")\n                self.enforce_eager = True\n\n            if not self.enforce_eager:\n                # Eager mode explicitly disabled by user for an encoder/\n                # decoder model; however CUDAGRAPH + encoder/decoder is\n                # not currently supported\n                raise ValueError(STR_NOT_IMPL_ENC_DEC_CUDAGRAPH)\n        elif self.enforce_eager is None:\n            # *Only for decoder-only models*, enforce_eager\n            # defaults to False if unset. This is intuitive\n            # so no logging message needed.\n            self.enforce_eager = False\n\n        if (not self.disable_sliding_window\n                and self.hf_text_config.model_type == \"gemma2\"\n                and self.hf_text_config.sliding_window is not None):\n            print_warning_once(\n                \"Gemma 2 uses sliding window attention for every odd layer, \"\n                \"which is currently not supported by vLLM. Disabling sliding \"\n                \"window and capping the max length to the sliding window size \"\n                f\"({self.hf_text_config.sliding_window}).\")\n            self.disable_sliding_window = True\n\n        self.max_model_len = _get_and_verify_max_len(\n            hf_config=self.hf_text_config,\n            max_model_len=max_model_len,\n            disable_sliding_window=self.disable_sliding_window,\n            sliding_window_len=self.get_hf_config_sliding_window())\n        self.served_model_name = get_served_model_name(model,\n                                                       served_model_name)\n        self.multimodal_config = self._init_multimodal_config(\n            limit_mm_per_prompt)\n        if not self.skip_tokenizer_init:\n            self._verify_tokenizer_mode()\n        self._verify_embedding_mode()\n        self._verify_quantization()\n        self._verify_cuda_graph()\n\n    def _init_multimodal_config(\n        self, limit_mm_per_prompt: Optional[Mapping[str, int]]\n    ) -> Optional[\"MultiModalConfig\"]:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        if any(\n                ModelRegistry.is_multimodal_model(arch)\n                for arch in architectures):\n            return MultiModalConfig(limit_per_prompt=limit_mm_per_prompt or {})\n        else:\n            if limit_mm_per_prompt:\n                raise ValueError(\n                    \"limit_mm_per_prompt is only supported for multimodal \"\n                    \"models.\")\n            return None\n\n    def _verify_tokenizer_mode(self) -> None:\n        tokenizer_mode = self.tokenizer_mode.lower()\n        if tokenizer_mode not in [\"auto\", \"slow\"]:\n            raise ValueError(\n                f\"Unknown tokenizer mode: {self.tokenizer_mode}. Must be \"\n                \"either 'auto' or 'slow'.\")\n        self.tokenizer_mode = tokenizer_mode\n\n    def _verify_embedding_mode(self) -> None:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        self.embedding_mode = any(\n            ModelRegistry.is_embedding_model(arch) for arch in architectures)\n\n    def _parse_quant_hf_config(self):\n        quant_cfg = getattr(self.hf_config, \"quantization_config\", None)\n        if quant_cfg is None:\n            # compressed-tensors uses a \"compression_config\" key\n            quant_cfg = getattr(self.hf_config, \"compression_config\", None)\n        return quant_cfg\n\n    def _verify_quantization(self) -> None:\n        supported_quantization = [*QUANTIZATION_METHODS]\n        rocm_supported_quantization = [\"gptq\", \"squeezellm\", \"fp8\"]\n        optimized_quantization_methods = [\n            \"fp8\", \"marlin\", \"gptq_marlin_24\", \"gptq_marlin\", \"awq_marlin\",\n            \"fbgemm_fp8\", \"compressed_tensors\", \"compressed-tensors\",\n            \"experts_int8\"\n        ]\n        tpu_supported_quantization = [\"tpu_int8\"]\n        if self.quantization is not None:\n            self.quantization = self.quantization.lower()\n\n        # Parse quantization method from the HF model config, if available.\n        quant_cfg = self._parse_quant_hf_config()\n\n        if quant_cfg is not None:\n            quant_method = quant_cfg.get(\"quant_method\", \"\").lower()\n\n            # Detect which checkpoint is it\n            for _, method in QUANTIZATION_METHODS.items():\n                quantization_override = method.override_quantization_method(\n                    quant_cfg, self.quantization)\n                if quantization_override:\n                    quant_method = quantization_override\n                    self.quantization = quantization_override\n                    break\n\n            # Verify quantization configurations.\n            if self.quantization is None:\n                self.quantization = quant_method\n            elif self.quantization != quant_method:\n                raise ValueError(\n                    \"Quantization method specified in the model config \"\n                    f\"({quant_method}) does not match the quantization \"\n                    f\"method specified in the `quantization` argument \"\n                    f\"({self.quantization}).\")\n\n        if self.quantization is not None:\n            if self.quantization not in supported_quantization:\n                raise ValueError(\n                    f\"Unknown quantization method: {self.quantization}. Must \"\n                    f\"be one of {supported_quantization}.\")\n            if is_hip(\n            ) and self.quantization not in rocm_supported_quantization:\n                raise ValueError(\n                    f\"{self.quantization} quantization is currently not \"\n                    f\"supported in ROCm.\")\n            if current_platform.is_tpu(\n            ) and self.quantization not in tpu_supported_quantization:\n                raise ValueError(\n                    f\"{self.quantization} quantization is currently not \"\n                    f\"supported in TPU Backend.\")\n            if self.quantization not in optimized_quantization_methods:\n                logger.warning(\n                    \"%s quantization is not fully \"\n                    \"optimized yet. The speed can be slower than \"\n                    \"non-quantized models.\", self.quantization)\n\n    def _verify_cuda_graph(self) -> None:\n        if self.max_seq_len_to_capture is None:\n            self.max_seq_len_to_capture = self.max_model_len\n        self.max_seq_len_to_capture = min(self.max_seq_len_to_capture,\n                                          self.max_model_len)\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_num_attention_heads = getattr(self.hf_text_config,\n                                            \"num_attention_heads\", 0)\n        tensor_parallel_size = parallel_config.tensor_parallel_size\n        if total_num_attention_heads % tensor_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of attention heads ({total_num_attention_heads})\"\n                \" must be divisible by tensor parallel size \"\n                f\"({tensor_parallel_size}).\")\n\n        pipeline_parallel_size = parallel_config.pipeline_parallel_size\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        if not all(arch in _PP_SUPPORTED_MODELS\n                   for arch in architectures) and pipeline_parallel_size > 1:\n            raise NotImplementedError(\n                \"Pipeline parallelism is only supported for the following \"\n                f\" architectures: {_PP_SUPPORTED_MODELS}.\")\n\n        if self.quantization == \"bitsandbytes\" and (\n                parallel_config.tensor_parallel_size > 1\n                or parallel_config.pipeline_parallel_size > 1):\n            raise ValueError(\n                \"BitAndBytes quantization with TP or PP is not supported yet.\")\n\n        if self.quantization == \"bitsandbytes\" and self.enforce_eager is False:\n            logger.warning(\"CUDA graph is not supported on BitAndBytes yet, \"\n                           \"fallback to the eager mode.\")\n            self.enforce_eager = True\n\n    def get_hf_config_sliding_window(self) -> Optional[int]:\n        \"\"\"Get the sliding window size, or None if disabled.\"\"\"\n\n        # Some models, like Qwen2 and Qwen1.5, use `use_sliding_window` in\n        # addition to sliding window size. We check if that field is present\n        # and if it's False, return None.\n        if (hasattr(self.hf_text_config, \"use_sliding_window\")\n                and not self.hf_text_config.use_sliding_window):\n            return None\n        return getattr(self.hf_text_config, \"sliding_window\", None)\n\n\n\n\n\n\n\n\n\n\n    def get_vocab_size(self) -> int:\n        return self.hf_text_config.vocab_size\n\n    def get_hidden_size(self) -> int:\n        return self.hf_text_config.hidden_size\n\n    def get_head_size(self) -> int:\n        # TODO remove hard code\n        if hasattr(self.hf_text_config, \"model_type\"\n                   ) and self.hf_text_config.model_type == 'deepseek_v2':\n            # FlashAttention supports only head_size 32, 64, 128, 256,\n            # we need to pad head_size 192 to 256\n            return 256\n        if hasattr(self.hf_text_config, \"head_dim\"):\n            return self.hf_text_config.head_dim\n        # FIXME(woosuk): This may not be true for all models.\n        return (self.hf_text_config.hidden_size //\n                self.hf_text_config.num_attention_heads)\n\n    def get_total_num_kv_heads(self) -> int:\n        \"\"\"Returns the total number of KV heads.\"\"\"\n        # For GPTBigCode & Falcon:\n        # NOTE: for falcon, when new_decoder_architecture is True, the\n        # multi_query flag is ignored and we use n_head_kv for the number of\n        # KV heads.\n        falcon_model_types = [\"falcon\", \"RefinedWeb\", \"RefinedWebModel\"]\n        new_decoder_arch_falcon = (\n            self.hf_config.model_type in falcon_model_types\n            and getattr(self.hf_config, \"new_decoder_architecture\", False))\n        if not new_decoder_arch_falcon and getattr(self.hf_text_config,\n                                                   \"multi_query\", False):\n            # Multi-query attention, only one KV head.\n            # Currently, tensor parallelism is not supported in this case.\n            return 1\n\n        # For DBRX and MPT\n        if self.hf_config.model_type == \"mpt\":\n            if \"kv_n_heads\" in self.hf_config.attn_config:\n                return self.hf_config.attn_config[\"kv_n_heads\"]\n            return self.hf_config.num_attention_heads\n        if self.hf_config.model_type == \"dbrx\":\n            return getattr(self.hf_config.attn_config, \"kv_n_heads\",\n                           self.hf_config.num_attention_heads)\n\n        attributes = [\n            # For Falcon:\n            \"n_head_kv\",\n            \"num_kv_heads\",\n            # For LLaMA-2:\n            \"num_key_value_heads\",\n            # For ChatGLM:\n            \"multi_query_group_num\",\n        ]\n        for attr in attributes:\n            num_kv_heads = getattr(self.hf_text_config, attr, None)\n            if num_kv_heads is not None:\n                return num_kv_heads\n\n        # For non-grouped-query attention models, the number of KV heads is\n        # equal to the number of attention heads.\n        return self.hf_text_config.num_attention_heads\n\n    def get_num_kv_heads(self, parallel_config: \"ParallelConfig\") -> int:\n        \"\"\"Returns the number of KV heads per GPU.\"\"\"\n        total_num_kv_heads = self.get_total_num_kv_heads()\n        # If tensor parallelism is used, we divide the number of KV heads by\n        # the tensor parallel size. We will replicate the KV heads in the\n        # case where the number of KV heads is smaller than the tensor\n        # parallel size so each GPU has at least one KV head.\n        return max(1,\n                   total_num_kv_heads // parallel_config.tensor_parallel_size)\n\n    def get_num_attention_heads(self,\n                                parallel_config: \"ParallelConfig\") -> int:\n        num_heads = getattr(self.hf_text_config, \"num_attention_heads\", 0)\n        return num_heads // parallel_config.tensor_parallel_size\n\n    def get_num_layers(self, parallel_config: \"ParallelConfig\") -> int:\n        from vllm.distributed.utils import get_pp_indices\n        total_num_hidden_layers = getattr(self.hf_text_config,\n                                          \"num_hidden_layers\", 0)\n        pp_rank = parallel_config.rank // parallel_config.tensor_parallel_size\n        pp_size = parallel_config.pipeline_parallel_size\n        start, end = get_pp_indices(total_num_hidden_layers, pp_rank, pp_size)\n        return end - start\n\n    def contains_seqlen_agnostic_layers(\n            self, parallel_config: \"ParallelConfig\") -> bool:\n        \"\"\"True for Mamba/SSM models (Jamba)\"\"\"\n        return self._get_num_seqlen_agnostic_layers(parallel_config) > 0\n\n    def get_layers_block_type(self,\n                              parallel_config: \"ParallelConfig\") -> List[str]:\n        num_layers = self.get_num_layers(parallel_config)\n        # Transformers supports layers_block_type @property\n        return getattr(self.hf_config, \"layers_block_type\",\n                       [\"attention\"] * num_layers)\n\n    def get_num_attention_layers(self,\n                                 parallel_config: \"ParallelConfig\") -> int:\n        return len([\n            t for t in self.get_layers_block_type(parallel_config)\n            if t == \"attention\"\n        ])\n\n    def _get_num_seqlen_agnostic_layers(\n            self, parallel_config: \"ParallelConfig\") -> int:\n        return len([\n            t for t in self.get_layers_block_type(parallel_config)\n            if t != \"attention\"\n        ])\n\n    def get_multimodal_config(self) -> \"MultiModalConfig\":\n        \"\"\"\n        Get the multimodal configuration of the model.\n\n        Raises:\n            ValueError: If the model is not multimodal.\n        \"\"\"\n        if self.multimodal_config is None:\n            raise ValueError(\"The model is not multimodal.\")\n\n        return self.multimodal_config\n\n    @property\n    def is_encoder_decoder_model(self) -> bool:\n        \"\"\"Extract the HF encoder/decoder model flag.\"\"\"\n        return getattr(self.hf_config, \"is_encoder_decoder\", False)\n\n    @property\n    def is_embedding_model(self) -> bool:\n        \"\"\"Extract the embedding model flag.\"\"\"\n        return self.embedding_mode\n\n\nclass CacheConfig:\n    \"\"\"Configuration for the KV cache.\n\n    Args:\n        block_size: Size of a cache block in number of tokens.\n        gpu_memory_utilization: Fraction of GPU memory to use for the\n            vLLM execution.\n        swap_space: Size of the CPU swap space per GPU (in GiB).\n        cache_dtype: Data type for kv cache storage.\n        num_gpu_blocks_override: Number of GPU blocks to use. This overrides the\n            profiled num_gpu_blocks if specified. Does nothing if None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        gpu_memory_utilization: float,\n        swap_space: float,\n        cache_dtype: str,\n        num_gpu_blocks_override: Optional[int] = None,\n        sliding_window\n```\n# Function Signature: def get_sliding_window(self) -> Optional[int]:\n# Natural Language Annotations: \"\"\"Get the sliding window size, or None if disabled.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_sliding_window(self) -> Optional[", "  ```\ndef get_sliding_window(self) -> Optional[", "  ```\ndef get_sliding_window(self) -> Optional["]}, "vllm-vllm/transformers_utils/detokenizer.py-decode_prompt_logprobs_inplace": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/transformers_utils/detokenizer.py:\n```\nfrom typing import Dict, List, Optional, Tuple\n\nfrom vllm.sequence import Logprob, SamplingParams, Sequence, SequenceGroup\n\nfrom .tokenizer import AnyTokenizer\nfrom .tokenizer_group import BaseTokenizerGroup\n\n# Used eg. for marking rejected tokens in spec decoding.\nINVALID_TOKEN_ID = -1\n\n\nclass Detokenizer:\n    \"\"\"Provides methods to decode the output of a model into text.\"\"\"\n\n    def __init__(self, tokenizer_group: BaseTokenizerGroup):\n        self.tokenizer_group = tokenizer_group\n\n    def get_tokenizer_for_seq(self, sequence: Sequence) -> AnyTokenizer:\n        \"\"\"Returns the HF tokenizer to use for a given sequence.\"\"\"\n        return self.tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef _replace_none_with_empty(tokens: List[Optional[str]]):\n    for i, token in enumerate(tokens):\n        if token is None:\n            tokens[i] = \"\"\n\n\ndef _convert_tokens_to_string_with_added_encoders(\n    tokenizer: AnyTokenizer,\n    output_tokens: List[str],\n    skip_special_tokens: bool,\n    spaces_between_special_tokens: bool,\n) -> str:\n    # Adapted from\n    # https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/tokenization_utils.py#L921\n    # NOTE(woosuk): The following code is slow because it runs a for loop over\n    # the output_tokens. In Python, running a for loop over a list can be slow\n    # even when the loop body is very simple.\n    sub_texts: List[str] = []\n    current_sub_text: List[str] = []\n    all_special_tokens = set(tokenizer.all_special_tokens)\n    for token in output_tokens:\n        if skip_special_tokens and token in all_special_tokens:\n            continue\n        if token in tokenizer.get_added_vocab():\n            if current_sub_text:\n                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n                sub_texts.append(sub_text)\n                current_sub_text = []\n            sub_texts.append(token)\n        else:\n            current_sub_text.append(token)\n    if current_sub_text:\n        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n        sub_texts.append(sub_text)\n    if spaces_between_special_tokens:\n        return \" \".join(sub_texts)\n    else:\n        return \"\".join(sub_texts)\n\n\n# 5 is an arbitrary value that should work for all\n# tokenizers (bigger = more conservative).\nINITIAL_INCREMENTAL_DETOKENIZATION_OFFSET = 5\n\n\ndef convert_prompt_ids_to_tokens(\n    tokenizer: AnyTokenizer,\n    prompt_ids: List[int],\n    skip_special_tokens: bool = False,\n) -> Tuple[List[str], int, int]:\n    \"\"\"Converts the prompt ids to tokens and returns the tokens and offsets\n    for incremental detokenization.\n\n    Note that not all tokens are converted to strings. Only the tokens that\n    are necessary for incremental detokenization are converted to strings.\n    \"\"\"\n    # We do not need to convert the whole prompt to tokens.\n    # Offset a little more in case we have special tokens.\n    new_tokens = tokenizer.convert_ids_to_tokens(\n        prompt_ids[-INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET - 2:],\n        skip_special_tokens=skip_special_tokens)\n    read_offset = len(new_tokens)\n    prefix_offset = max(\n        read_offset - INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET, 0)\n    # This is required to guard against out-of-vocab prompt token ids\n    _replace_none_with_empty(new_tokens)\n    return new_tokens, prefix_offset, read_offset\n\n\n# Based on\n# https://github.com/huggingface/text-generation-inference/blob/v0.9.4/server/text_generation_server/models/model.py#L62C9-L62C15\n# under Apache 2.0 license\ndef detokenize_incrementally(\n    tokenizer: AnyTokenizer,\n    all_input_ids: List[int],\n    prev_tokens: Optional[List[str]],\n    prefix_offset: int,\n    read_offset: int,\n    skip_special_tokens: bool = False,\n    spaces_between_special_tokens: bool = True,\n) -> Tuple[List[str], str, int, int]:\n    \"\"\"Detokenizes the input ids incrementally and returns the new tokens\n    and the new text.\n\n    If `prev_tokens` is None, this function will convert the input ids to\n    tokens and return the tokens and the new text. Otherwise, it will return the\n    new tokens and the new text.\n\n    This function will also return the new prefix offset and the new read\n    offset to be used in the next iteration.\n\n    The offsets are necessary to defeat cleanup algorithms in the decode which\n    decide to add a space or not depending on the surrounding ids.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        all_input_ids: The input ids. The last id is the new token id.\n        prev_tokens: The previous tokens. If None, this function will convert\n            the input ids to tokens and return the tokens and the new text.\n        prefix_offset: The prefix offset.\n        read_offset: The read offset.\n        skip_special_tokens: Whether to skip special tokens.\n        spaces_between_special_tokens: Whether to add spaces between special\n            tokens.\n    \"\"\"\n    new_token_id = all_input_ids[-1]\n    # This is the first iteration for this sequence\n    is_first_iter = prev_tokens is None\n    if is_first_iter:\n        (prev_tokens, prefix_offset,\n         read_offset) = convert_prompt_ids_to_tokens(\n             tokenizer,\n             all_input_ids[:-1],\n             skip_special_tokens=skip_special_tokens)\n    assert prev_tokens is not None\n\n    # If the new token id is out of bounds, return an empty string.\n    if new_token_id >= len(tokenizer):\n        new_tokens = [\"\"]\n    else:\n        # Put new_token_id in a list so skip_special_tokens is respected\n        new_tokens = tokenizer.convert_ids_to_tokens(\n            [new_token_id], skip_special_tokens=skip_special_tokens)\n        if isinstance(new_tokens, str):\n            new_tokens = [new_tokens]\n    output_tokens = prev_tokens + new_tokens\n\n    # If this is the first iteration, return all tokens.\n    if is_first_iter:\n        new_tokens = output_tokens\n\n    # The prefix text is necessary only to defeat cleanup algorithms in\n    # the decode which decide to add a space or not depending on the\n    # surrounding ids.\n    if tokenizer.is_fast or not tokenizer.get_added_vocab():\n        prefix_text = tokenizer.convert_tokens_to_string(\n            output_tokens[prefix_offset:read_offset])\n        new_text = tokenizer.convert_tokens_to_string(\n            output_tokens[prefix_offset:])\n    else:\n        prefix_text = _convert_tokens_to_string_with_added_encoders(\n            tokenizer,\n            output_tokens[prefix_offset:read_offset],\n            skip_special_tokens=skip_special_tokens,\n            spaces_between_special_tokens=spaces_between_special_tokens,\n        )\n        new_text = _convert_tokens_to_string_with_added_encoders(\n            tokenizer,\n            output_tokens[prefix_offset:],\n            skip_special_tokens=skip_special_tokens,\n            spaces_between_special_tokens=spaces_between_special_tokens,\n        )\n\n    if len(new_text) <= len(prefix_text) or new_text.endswith(\"\ufffd\"):\n        # utf-8 char at the end means it's a potential unfinished byte sequence\n        # from byte fallback tokenization.\n        # If it's in the middle, it's probably a real invalid id generated\n        # by the model\n        return new_tokens, \"\", prefix_offset, read_offset\n\n    new_text = new_text[len(prefix_text):]\n    return new_tokens, new_text, read_offset, len(output_tokens)\n\n```\n# Function Signature: def decode_prompt_logprobs_inplace(self, seq_group: SequenceGroup,\n# Natural Language Annotations: \"\"\"Decodes the logprobs for the prompt of a sequence group.\n\n        Args:\n            seq_group: The sequence group to decode.\n            prompt_logprobs: The logprobs to decode.\n            position_offset: Offset of the first index of the logprobs \n                relative to the start of the sequence (for chunked prefill).\n        \n        Returns:\n            The prompt logprobs with the decoded tokens.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef decode_prompt_logprobs_inplace(", "  ```\ndef decode_prompt_logprobs_inplace(", "  ```\ndef decode_prompt_logprobs_inplace("]}, "vllm-vllm/transformers_utils/detokenizer.py-decode_sequence_inplace": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/transformers_utils/detokenizer.py:\n```\nfrom typing import Dict, List, Optional, Tuple\n\nfrom vllm.sequence import Logprob, SamplingParams, Sequence, SequenceGroup\n\nfrom .tokenizer import AnyTokenizer\nfrom .tokenizer_group import BaseTokenizerGroup\n\n# Used eg. for marking rejected tokens in spec decoding.\nINVALID_TOKEN_ID = -1\n\n\nclass Detokenizer:\n    \"\"\"Provides methods to decode the output of a model into text.\"\"\"\n\n    def __init__(self, tokenizer_group: BaseTokenizerGroup):\n        self.tokenizer_group = tokenizer_group\n\n    def get_tokenizer_for_seq(self, sequence: Sequence) -> AnyTokenizer:\n        \"\"\"Returns the HF tokenizer to use for a given sequence.\"\"\"\n        return self.tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef _replace_none_with_empty(tokens: List[Optional[str]]):\n    for i, token in enumerate(tokens):\n        if token is None:\n            tokens[i] = \"\"\n\n\ndef _convert_tokens_to_string_with_added_encoders(\n    tokenizer: AnyTokenizer,\n    output_tokens: List[str],\n    skip_special_tokens: bool,\n    spaces_between_special_tokens: bool,\n) -> str:\n    # Adapted from\n    # https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/tokenization_utils.py#L921\n    # NOTE(woosuk): The following code is slow because it runs a for loop over\n    # the output_tokens. In Python, running a for loop over a list can be slow\n    # even when the loop body is very simple.\n    sub_texts: List[str] = []\n    current_sub_text: List[str] = []\n    all_special_tokens = set(tokenizer.all_special_tokens)\n    for token in output_tokens:\n        if skip_special_tokens and token in all_special_tokens:\n            continue\n        if token in tokenizer.get_added_vocab():\n            if current_sub_text:\n                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n                sub_texts.append(sub_text)\n                current_sub_text = []\n            sub_texts.append(token)\n        else:\n            current_sub_text.append(token)\n    if current_sub_text:\n        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n        sub_texts.append(sub_text)\n    if spaces_between_special_tokens:\n        return \" \".join(sub_texts)\n    else:\n        return \"\".join(sub_texts)\n\n\n# 5 is an arbitrary value that should work for all\n# tokenizers (bigger = more conservative).\nINITIAL_INCREMENTAL_DETOKENIZATION_OFFSET = 5\n\n\ndef convert_prompt_ids_to_tokens(\n    tokenizer: AnyTokenizer,\n    prompt_ids: List[int],\n    skip_special_tokens: bool = False,\n) -> Tuple[List[str], int, int]:\n    \"\"\"Converts the prompt ids to tokens and returns the tokens and offsets\n    for incremental detokenization.\n\n    Note that not all tokens are converted to strings. Only the tokens that\n    are necessary for incremental detokenization are converted to strings.\n    \"\"\"\n    # We do not need to convert the whole prompt to tokens.\n    # Offset a little more in case we have special tokens.\n    new_tokens = tokenizer.convert_ids_to_tokens(\n        prompt_ids[-INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET - 2:],\n        skip_special_tokens=skip_special_tokens)\n    read_offset = len(new_tokens)\n    prefix_offset = max(\n        read_offset - INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET, 0)\n    # This is required to guard against out-of-vocab prompt token ids\n    _replace_none_with_empty(new_tokens)\n    return new_tokens, prefix_offset, read_offset\n\n\n# Based on\n# https://github.com/huggingface/text-generation-inference/blob/v0.9.4/server/text_generation_server/models/model.py#L62C9-L62C15\n# under Apache 2.0 license\ndef detokenize_incrementally(\n    tokenizer: AnyTokenizer,\n    all_input_ids: List[int],\n    prev_tokens: Optional[List[str]],\n    prefix_offset: int,\n    read_offset: int,\n    skip_special_tokens: bool = False,\n    spaces_between_special_tokens: bool = True,\n) -> Tuple[List[str], str, int, int]:\n    \"\"\"Detokenizes the input ids incrementally and returns the new tokens\n    and the new text.\n\n    If `prev_tokens` is None, this function will convert the input ids to\n    tokens and return the tokens and the new text. Otherwise, it will return the\n    new tokens and the new text.\n\n    This function will also return the new prefix offset and the new read\n    offset to be used in the next iteration.\n\n    The offsets are necessary to defeat cleanup algorithms in the decode which\n    decide to add a space or not depending on the surrounding ids.\n\n    Args:\n        tokenizer: The tokenizer to use.\n        all_input_ids: The input ids. The last id is the new token id.\n        prev_tokens: The previous tokens. If None, this function will convert\n            the input ids to tokens and return the tokens and the new text.\n        prefix_offset: The prefix offset.\n        read_offset: The read offset.\n        skip_special_tokens: Whether to skip special tokens.\n        spaces_between_special_tokens: Whether to add spaces between special\n            tokens.\n    \"\"\"\n    new_token_id = all_input_ids[-1]\n    # This is the first iteration for this sequence\n    is_first_iter = prev_tokens is None\n    if is_first_iter:\n        (prev_tokens, prefix_offset,\n         read_offset) = convert_prompt_ids_to_tokens(\n             tokenizer,\n             all_input_ids[:-1],\n             skip_special_tokens=skip_special_tokens)\n    assert prev_tokens is not None\n\n    # If the new token id is out of bounds, return an empty string.\n    if new_token_id >= len(tokenizer):\n        new_tokens = [\"\"]\n    else:\n        # Put new_token_id in a list so skip_special_tokens is respected\n        new_tokens = tokenizer.convert_ids_to_tokens(\n            [new_token_id], skip_special_tokens=skip_special_tokens)\n        if isinstance(new_tokens, str):\n            new_tokens = [new_tokens]\n    output_tokens = prev_tokens + new_tokens\n\n    # If this is the first iteration, return all tokens.\n    if is_first_iter:\n        new_tokens = output_tokens\n\n    # The prefix text is necessary only to defeat cleanup algorithms in\n    # the decode which decide to add a space or not depending on the\n    # surrounding ids.\n    if tokenizer.is_fast or not tokenizer.get_added_vocab():\n        prefix_text = tokenizer.convert_tokens_to_string(\n            output_tokens[prefix_offset:read_offset])\n        new_text = tokenizer.convert_tokens_to_string(\n            output_tokens[prefix_offset:])\n    else:\n        prefix_text = _convert_tokens_to_string_with_added_encoders(\n            tokenizer,\n            output_tokens[prefix_offset:read_offset],\n            skip_special_tokens=skip_special_tokens,\n            spaces_between_special_tokens=spaces_between_special_tokens,\n        )\n        new_text = _convert_tokens_to_string_with_added_encoders(\n            tokenizer,\n            output_tokens[prefix_offset:],\n            skip_special_tokens=skip_special_tokens,\n            spaces_between_special_tokens=spaces_between_special_tokens,\n        )\n\n    if len(new_text) <= len(prefix_text) or new_text.endswith(\"\ufffd\"):\n        # utf-8 char at the end means it's a potential unfinished byte sequence\n        # from byte fallback tokenization.\n        # If it's in the middle, it's probably a real invalid id generated\n        # by the model\n        return new_tokens, \"\", prefix_offset, read_offset\n\n    new_text = new_text[len(prefix_text):]\n    return new_tokens, new_text, read_offset, len(output_tokens)\n\n```\n# Function Signature: def decode_sequence_inplace(self, seq: Sequence,\n# Natural Language Annotations: \"\"\"Decodes the new token for a sequence. In-place operation.\n\n        Args:\n            seq: The sequence to decode.\n            prms: The sampling parameters used to generate the sequence.\n\n        Returns:\n            The number of characters added to the output text.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport numpy as np\n\nfrom vllm.utils.", "  ```\ndef decode_sequence_inplace(self, seq: Se", "  ```\ndef Detokenizer:\n    \"\"\"Decodes the new token"]}, "vllm-vllm/transformers_utils/tokenizer.py-get_tokenizer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/transformers_utils/tokenizer.py:\n```\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport huggingface_hub\nfrom transformers import (AutoTokenizer, PreTrainedTokenizer,\n                          PreTrainedTokenizerFast)\n\nfrom vllm.envs import VLLM_USE_MODELSCOPE\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.transformers_utils.tokenizers import BaichuanTokenizer\nfrom vllm.utils import make_async\n\nlogger = init_logger(__name__)\n\nAnyTokenizer = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n\n\ndef get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n    \"\"\"Get tokenizer with cached properties.\n\n    This will patch the tokenizer object in place.\n\n    By default, transformers will recompute multiple tokenizer properties\n    each time they are called, leading to a significant slowdown. This\n    function caches these properties for faster access.\"\"\"\n\n    tokenizer_all_special_ids = set(tokenizer.all_special_ids)\n    tokenizer_all_special_tokens_extended = (\n        tokenizer.all_special_tokens_extended)\n    tokenizer_all_special_tokens = set(tokenizer.all_special_tokens)\n    tokenizer_len = len(tokenizer)\n\n    class CachedTokenizer(tokenizer.__class__):  # type: ignore\n\n        @property\n        def all_special_ids(self):\n            return tokenizer_all_special_ids\n\n        @property\n        def all_special_tokens(self):\n            return tokenizer_all_special_tokens\n\n        @property\n        def all_special_tokens_extended(self):\n            return tokenizer_all_special_tokens_extended\n\n        def __len__(self):\n            return tokenizer_len\n\n    CachedTokenizer.__name__ = f\"Cached{tokenizer.__class__.__name__}\"\n\n    tokenizer.__class__ = CachedTokenizer\n    return tokenizer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef get_lora_tokenizer(lora_request: LoRARequest, *args,\n                       **kwargs) -> Optional[AnyTokenizer]:\n    if lora_request is None:\n        return None\n    try:\n        tokenizer = get_tokenizer(lora_request.lora_path, *args, **kwargs)\n    except OSError as e:\n        # No tokenizer was found in the LoRA folder,\n        # use base model tokenizer\n        logger.warning(\n            \"No tokenizer found in %s, using base model tokenizer instead. \"\n            \"(Exception: %s)\", lora_request.lora_path, e)\n        tokenizer = None\n    return tokenizer\n\n\nget_lora_tokenizer_async = make_async(get_lora_tokenizer)\n\n```\n# Function Signature: def get_tokenizer(\n# Natural Language Annotations: \"\"\"Gets a tokenizer for the given model name via HuggingFace or ModelScope.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_tokenizer(model_name: str, use", "  ```\ndef get_tokenizer(\n    model_name: str", "  ```\ndef get_tokenizer(model_name: str, use"]}, "vllm-vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py-encode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py:\n```\nimport asyncio\nimport os\nfrom typing import List, Optional\n\ntry:\n    from ray.exceptions import ActorDiedError  # type: ignore\nexcept ImportError:\n    # For older versions of Ray\n    from ray.exceptions import RayActorError as ActorDiedError  # type: ignore\nfrom ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n\nfrom vllm.config import TokenizerPoolConfig\nfrom vllm.executor.ray_utils import ray\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\n\nfrom .base_tokenizer_group import BaseTokenizerGroup\nfrom .tokenizer_group import TokenizerGroup\n\nlogger = init_logger(__name__)\n\n\nclass RayTokenizerGroupPool(BaseTokenizerGroup):\n    \"\"\"A Ray-based pool of TokenizerGroups for async tokenization.\"\"\"\n\n    # Class to use for workers making up the pool.\n    _worker_cls = TokenizerGroup\n\n    @classmethod\n    def from_config(cls, tokenizer_pool_config: Optional[TokenizerPoolConfig],\n                    **init_kwargs) -> \"RayTokenizerGroupPool\":\n        if not tokenizer_pool_config:\n            raise ValueError(\"tokenizer_pool_config must not be None.\")\n        ray_actor_options = (tokenizer_pool_config.extra_config or {\n            \"num_cpus\": 0\n        })\n        ray_actor_options.setdefault(\n            \"scheduling_strategy\",\n            NodeAffinitySchedulingStrategy(\n                node_id=ray.get_runtime_context().get_node_id(), soft=True))\n\n        # Carry over the env vars to the actors.\n        # This is necessary for API keys and such.\n        ray_actor_options.setdefault(\"runtime_env\", {})\n        _carry_over_env_vars_to_runtime_env(ray_actor_options[\"runtime_env\"])\n\n        init_kwargs[\"num_actors\"] = tokenizer_pool_config.pool_size\n        init_kwargs[\"ray_actor_options\"] = ray_actor_options\n\n        return cls(**init_kwargs)\n\n    def __init__(self, tokenizer_id: str, enable_lora: bool, max_num_seqs: int,\n                 max_input_length: Optional[int], num_actors: int,\n                 ray_actor_options: dict, **tokenizer_config):\n        # Store a local copy of the TokenizerGroup for quick access\n        # to underlying HF tokenizers.\n        self._tokenizer_config = {\n            \"tokenizer_id\": tokenizer_id,\n            \"enable_lora\": enable_lora,\n            \"max_num_seqs\": max_num_seqs,\n            \"max_input_length\": max_input_length,\n            **tokenizer_config\n        }\n        self._local_tokenizer_group = self._worker_cls(\n            **self._tokenizer_config, )\n\n        self._ray_tokenizer_group_cls = ray.remote(\n            self._worker_cls).options(**ray_actor_options)  # type: ignore\n        self.tokenizer_actors = [self._init_actor() for _ in range(num_actors)]\n        self._idle_actors: Optional[asyncio.Queue] = None\n\n        # If set, actor is unhealthy. Will reraise on the next\n        # check_health call.\n        self._exception: Optional[ActorDiedError] = None\n\n    def _init_actor(self) -> ray.ObjectRef:\n        return self._ray_tokenizer_group_cls.remote(**self._tokenizer_config)\n\n    @property\n    def pool_size(self) -> int:\n        return len(self.tokenizer_actors)\n\n    def ping(self):\n        return ray.get([\n            actor.ping.remote()  # type: ignore\n            for actor in self.tokenizer_actors\n        ])\n\n    def _ensure_queue_initialized(self):\n        if self._idle_actors is None:\n            self._idle_actors = asyncio.Queue()\n            for actor in self.tokenizer_actors:\n                self._idle_actors.put_nowait(actor)\n\n    def _finalize_encode(self, actor: ray.ObjectRef,\n                         original_actor: ray.ObjectRef, actor_is_alive: bool):\n        assert self._idle_actors is not None\n        # Cleanup the dead actor.\n        if not actor_is_alive or original_actor is not actor:\n            self.tokenizer_actors.remove(original_actor)\n        if actor_is_alive:\n            # Put the actor back in the queue.\n            # This is done in a finally block to ensure that the actor is\n            # always put back in the queue, even if an exception/cancellation\n            # is raised.\n            self._idle_actors.put_nowait(actor)\n            # Add back the new actor.\n            if original_actor is not actor:\n                self.tokenizer_actors.append(actor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_max_input_len(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        \"\"\"Get the maximum input length for the LoRA request.\"\"\"\n        return self._local_tokenizer_group.get_max_input_len(lora_request)\n\n    def get_lora_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self._local_tokenizer_group.get_lora_tokenizer(lora_request)\n\n    async def get_lora_tokenizer_async(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return await self._local_tokenizer_group.get_lora_tokenizer_async(\n            lora_request)\n\n    def check_health(self):\n        if self._exception:\n            raise RuntimeError(\n                \"TokenizerGroupPool is unhealthy.\") from self._exception\n\n\ndef _carry_over_env_vars_to_runtime_env(runtime_env: dict) -> None:\n    \"\"\"Copy over all current process environment variables to the runtime_env.\n\n    The variables in runtime_env will take precedence over the current process\n    environment variables.\n\n    runtime_env will be modified in place.\"\"\"\n    env_vars = os.environ.copy()\n    runtime_env.setdefault(\"env_vars\", {})\n    env_vars.update(runtime_env[\"env_vars\"])\n    runtime_env[\"env_vars\"] = env_vars\n\n```\n# Function Signature: def encode(self,\n# Natural Language Annotations: \"\"\"Encode a prompt using the tokenizer group.\n\n        We pick an idle actor and use it to encode the prompt.\n        The actor is then put back in the queue for future use.\n        This is blocking.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nclass RayTokenizerGroupPool(BaseTokenizerGroup):\n", "  ```\ndef encode(self, prompt: str, lora_request", "  ```\ndef encode(self, prompt: str) -> List[str"]}, "vllm-vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py-encode_async": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py:\n```\nimport asyncio\nimport os\nfrom typing import List, Optional\n\ntry:\n    from ray.exceptions import ActorDiedError  # type: ignore\nexcept ImportError:\n    # For older versions of Ray\n    from ray.exceptions import RayActorError as ActorDiedError  # type: ignore\nfrom ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n\nfrom vllm.config import TokenizerPoolConfig\nfrom vllm.executor.ray_utils import ray\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\n\nfrom .base_tokenizer_group import BaseTokenizerGroup\nfrom .tokenizer_group import TokenizerGroup\n\nlogger = init_logger(__name__)\n\n\nclass RayTokenizerGroupPool(BaseTokenizerGroup):\n    \"\"\"A Ray-based pool of TokenizerGroups for async tokenization.\"\"\"\n\n    # Class to use for workers making up the pool.\n    _worker_cls = TokenizerGroup\n\n    @classmethod\n    def from_config(cls, tokenizer_pool_config: Optional[TokenizerPoolConfig],\n                    **init_kwargs) -> \"RayTokenizerGroupPool\":\n        if not tokenizer_pool_config:\n            raise ValueError(\"tokenizer_pool_config must not be None.\")\n        ray_actor_options = (tokenizer_pool_config.extra_config or {\n            \"num_cpus\": 0\n        })\n        ray_actor_options.setdefault(\n            \"scheduling_strategy\",\n            NodeAffinitySchedulingStrategy(\n                node_id=ray.get_runtime_context().get_node_id(), soft=True))\n\n        # Carry over the env vars to the actors.\n        # This is necessary for API keys and such.\n        ray_actor_options.setdefault(\"runtime_env\", {})\n        _carry_over_env_vars_to_runtime_env(ray_actor_options[\"runtime_env\"])\n\n        init_kwargs[\"num_actors\"] = tokenizer_pool_config.pool_size\n        init_kwargs[\"ray_actor_options\"] = ray_actor_options\n\n        return cls(**init_kwargs)\n\n    def __init__(self, tokenizer_id: str, enable_lora: bool, max_num_seqs: int,\n                 max_input_length: Optional[int], num_actors: int,\n                 ray_actor_options: dict, **tokenizer_config):\n        # Store a local copy of the TokenizerGroup for quick access\n        # to underlying HF tokenizers.\n        self._tokenizer_config = {\n            \"tokenizer_id\": tokenizer_id,\n            \"enable_lora\": enable_lora,\n            \"max_num_seqs\": max_num_seqs,\n            \"max_input_length\": max_input_length,\n            **tokenizer_config\n        }\n        self._local_tokenizer_group = self._worker_cls(\n            **self._tokenizer_config, )\n\n        self._ray_tokenizer_group_cls = ray.remote(\n            self._worker_cls).options(**ray_actor_options)  # type: ignore\n        self.tokenizer_actors = [self._init_actor() for _ in range(num_actors)]\n        self._idle_actors: Optional[asyncio.Queue] = None\n\n        # If set, actor is unhealthy. Will reraise on the next\n        # check_health call.\n        self._exception: Optional[ActorDiedError] = None\n\n    def _init_actor(self) -> ray.ObjectRef:\n        return self._ray_tokenizer_group_cls.remote(**self._tokenizer_config)\n\n    @property\n    def pool_size(self) -> int:\n        return len(self.tokenizer_actors)\n\n    def ping(self):\n        return ray.get([\n            actor.ping.remote()  # type: ignore\n            for actor in self.tokenizer_actors\n        ])\n\n    def _ensure_queue_initialized(self):\n        if self._idle_actors is None:\n            self._idle_actors = asyncio.Queue()\n            for actor in self.tokenizer_actors:\n                self._idle_actors.put_nowait(actor)\n\n    def _finalize_encode(self, actor: ray.ObjectRef,\n                         original_actor: ray.ObjectRef, actor_is_alive: bool):\n        assert self._idle_actors is not None\n        # Cleanup the dead actor.\n        if not actor_is_alive or original_actor is not actor:\n            self.tokenizer_actors.remove(original_actor)\n        if actor_is_alive:\n            # Put the actor back in the queue.\n            # This is done in a finally block to ensure that the actor is\n            # always put back in the queue, even if an exception/cancellation\n            # is raised.\n            self._idle_actors.put_nowait(actor)\n            # Add back the new actor.\n            if original_actor is not actor:\n                self.tokenizer_actors.append(actor)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_max_input_len(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        \"\"\"Get the maximum input length for the LoRA request.\"\"\"\n        return self._local_tokenizer_group.get_max_input_len(lora_request)\n\n    def get_lora_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self._local_tokenizer_group.get_lora_tokenizer(lora_request)\n\n    async def get_lora_tokenizer_async(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return await self._local_tokenizer_group.get_lora_tokenizer_async(\n            lora_request)\n\n    def check_health(self):\n        if self._exception:\n            raise RuntimeError(\n                \"TokenizerGroupPool is unhealthy.\") from self._exception\n\n\ndef _carry_over_env_vars_to_runtime_env(runtime_env: dict) -> None:\n    \"\"\"Copy over all current process environment variables to the runtime_env.\n\n    The variables in runtime_env will take precedence over the current process\n    environment variables.\n\n    runtime_env will be modified in place.\"\"\"\n    env_vars = os.environ.copy()\n    runtime_env.setdefault(\"env_vars\", {})\n    env_vars.update(runtime_env[\"env_vars\"])\n    runtime_env[\"env_vars\"] = env_vars\n\n```\n# Function Signature: async def encode_async(\n# Natural Language Annotations: \"\"\"Encode a prompt using the tokenizer group.\n\n        We pick an idle actor and use it to encode the prompt.\n        If there are no idle actors, we wait until one becomes\n        available.\n        The actor is then put back in the queue for future use.\n        This is non-blocking.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport asyncio\nimport os\nfrom typing import List,", "  ```\nasync def encode_async(self, prompt: str, **", "  ```\nasync def encode_async(self, prompt: str, max"]}, "vllm-vllm/attention/selector.py-which_attn_to_use": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/attention/selector.py:\n```\nimport enum\nimport os\nfrom contextlib import contextmanager\nfrom functools import lru_cache\nfrom typing import Generator, Optional, Type\n\nimport torch\n\nimport vllm.envs as envs\nfrom vllm.attention.backends.abstract import AttentionBackend\nfrom vllm.logger import init_logger\nfrom vllm.platforms import current_platform\nfrom vllm.utils import STR_BACKEND_ENV_VAR, is_cpu, is_hip, is_openvino, is_xpu\n\nlogger = init_logger(__name__)\n\n\nclass _Backend(enum.Enum):\n    FLASH_ATTN = enum.auto()\n    XFORMERS = enum.auto()\n    ROCM_FLASH = enum.auto()\n    TORCH_SDPA = enum.auto()\n    OPENVINO = enum.auto()\n    FLASHINFER = enum.auto()\n    PALLAS = enum.auto()\n    IPEX = enum.auto()\n\n\ndef backend_name_to_enum(backend_name: str) -> _Backend:\n    assert backend_name is not None\n\n    backend_members = _Backend.__members__\n    if backend_name not in backend_members:\n        raise ValueError(f\"Invalid attention backend '{backend_name}'. \"\n                         f\"Available backends: {', '.join(backend_members)} \"\n                         \"(case-sensitive).\")\n\n    return _Backend[backend_name]\n\n\ndef get_env_variable_attn_backend() -> Optional[_Backend]:\n    '''\n    Get the backend override specified by the vLLM attention\n    backend environment variable, if one is specified.\n\n    Returns:\n\n    * _Backend enum value if an override is specified\n    * None otherwise\n    '''\n    backend_name = os.environ.get(STR_BACKEND_ENV_VAR)\n    return (None\n            if backend_name is None else backend_name_to_enum(backend_name))\n\n\n# Global state allows a particular choice of backend\n# to be forced, overriding the logic which auto-selects\n# a backend based on system & workload configuration\n# (default behavior if this variable is None)\n#\n# THIS SELECTION TAKES PRECEDENCE OVER THE\n# VLLM ATTENTION BACKEND ENVIRONMENT VARIABLE\nforced_attn_backend: Optional[_Backend] = None\n\n\ndef global_force_attn_backend(attn_backend: Optional[_Backend]) -> None:\n    '''\n    Force all attention operations to use a specified backend.\n\n    Passing `None` for the argument re-enables automatic\n    backend selection.,\n\n    Arguments:\n\n    * attn_backend: backend selection (None to revert to auto)\n    '''\n    global forced_attn_backend\n    forced_attn_backend = attn_backend\n\n\ndef get_global_forced_attn_backend() -> Optional[_Backend]:\n    '''\n    Get the currently-forced choice of attention backend,\n    or None if auto-selection is currently enabled.\n    '''\n    return forced_attn_backend\n\n\n@lru_cache(maxsize=None)\ndef get_attn_backend(\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int,\n    sliding_window: Optional[int],\n    dtype: torch.dtype,\n    kv_cache_dtype: Optional[str],\n    block_size: int,\n    is_blocksparse: bool = False,\n) -> Type[AttentionBackend]:\n    \"\"\"Selects which attention backend to use and lazily imports it.\"\"\"\n\n    if is_blocksparse:\n        logger.info(\"Using BlocksparseFlashAttention backend.\")\n        from vllm.attention.backends.blocksparse_attn import (\n            BlocksparseFlashAttentionBackend)\n        return BlocksparseFlashAttentionBackend\n\n    backend = which_attn_to_use(num_heads, head_size, num_kv_heads,\n                                sliding_window, dtype, kv_cache_dtype,\n                                block_size)\n    if backend == _Backend.FLASH_ATTN:\n        from vllm.attention.backends.flash_attn import (  # noqa: F401\n            FlashAttentionBackend)\n        return FlashAttentionBackend\n    if backend == _Backend.XFORMERS:\n        logger.info(\"Using XFormers backend.\")\n        from vllm.attention.backends.xformers import (  # noqa: F401\n            XFormersBackend)\n        return XFormersBackend\n    elif backend == _Backend.ROCM_FLASH:\n        logger.info(\"Using ROCmFlashAttention backend.\")\n        from vllm.attention.backends.rocm_flash_attn import (  # noqa: F401\n            ROCmFlashAttentionBackend)\n        return ROCmFlashAttentionBackend\n    elif backend == _Backend.TORCH_SDPA:\n        assert is_cpu(), RuntimeError(\n            \"Torch SDPA backend is only used for the CPU device.\")\n        logger.info(\"Using Torch SDPA backend.\")\n        from vllm.attention.backends.torch_sdpa import TorchSDPABackend\n        return TorchSDPABackend\n    elif backend == _Backend.OPENVINO:\n        logger.info(\"Using OpenVINO Attention backend.\")\n        from vllm.attention.backends.openvino import OpenVINOAttentionBackend\n        return OpenVINOAttentionBackend\n    elif backend == _Backend.IPEX:\n        assert is_xpu(), RuntimeError(\n            \"IPEX attention backend is only used for the XPU device.\")\n        logger.info(\"Using IPEX attention backend.\")\n        from vllm.attention.backends.ipex_attn import IpexAttnBackend\n        return IpexAttnBackend\n    elif backend == _Backend.FLASHINFER:\n        logger.info(\"Using Flashinfer backend.\")\n        from vllm.attention.backends.flashinfer import FlashInferBackend\n        return FlashInferBackend\n    elif backend == _Backend.PALLAS:\n        logger.info(\"Using Pallas backend.\")\n        from vllm.attention.backends.pallas import PallasAttentionBackend\n        return PallasAttentionBackend\n    else:\n        raise ValueError(\"Invalid attention backend.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@contextmanager\ndef global_force_attn_backend_context_manager(\n        attn_backend: _Backend) -> Generator[None, None, None]:\n    '''\n    Globally force a vLLM attention backend override within a\n    context manager, reverting the global attention backend\n    override to its prior state upon exiting the context\n    manager.\n\n    Arguments:\n\n    * attn_backend: attention backend to force\n\n    Returns:\n\n    * Generator\n    '''\n\n    # Save the current state of the global backend override (if any)\n    original_value = get_global_forced_attn_backend()\n\n    # Globally force the new backend override\n    global_force_attn_backend(attn_backend)\n\n    # Yield control back to the enclosed code block\n    try:\n        yield\n    finally:\n        # Revert the original global backend override, if any\n        global_force_attn_backend(original_value)\n\n```\n# Function Signature: def which_attn_to_use(\n# Natural Language Annotations: \"\"\"Returns which flash attention backend to use.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef which_attn_to_use(\n    num", "  ```\ndef which_attn_to_use(\n    num", "  ```\ndef which_attn_to_use(\n    num"]}, "vllm-vllm/spec_decode/top1_proposer.py-get_spec_proposals": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/spec_decode/top1_proposer.py:\n```\nfrom typing import List, Optional, Set, Tuple\n\nimport torch\n\nfrom vllm.sequence import (ExecuteModelRequest, SamplerOutput,\n                           SequenceGroupMetadata)\nfrom vllm.spec_decode.interfaces import (SpeculativeProposals,\n                                         SpeculativeProposer)\nfrom vllm.spec_decode.proposer_worker_base import ProposerWorkerBase\nfrom vllm.spec_decode.util import sampler_output_to_torch\n\n\nclass Top1Proposer(SpeculativeProposer):\n    \"\"\"Helper class which separates out sequences which would exceed the max\n    model length when speculated upon.\n\n    This allows combinations of models such as JackFram/llama-68m draft with\n    meta-llama/Llama2-13b-chat-hf, as llama-68m has max_position_embeddings of\n    2048 while Llama2-13b has max_position_embeddings of 4096.\n\n    We treat the sequences which exceed the proposal draft model length as\n    \"non-spec sequences\". Essentially they skip the draft model and go through\n    normal decoding in the target model.\n\n    Currently, only proposal_lens of 0 and k are supported, where k is a global\n    batch proposal length. In the future vLLM should support per-sequence\n    proposal lengths.\n    \"\"\"\n\n    def __init__(\n        self,\n        worker: ProposerWorkerBase,\n        device: str,\n        vocab_size: int,\n        max_proposal_len: Optional[int] = None,\n    ):\n        self._worker = worker\n        self._device = device\n        self.max_proposal_len = max_proposal_len\n        self._vocab_size = vocab_size\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _split_by_proposal_len(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        proposal_len: int,\n    ) -> Tuple[List[int], List[SequenceGroupMetadata], List[int]]:\n        \"\"\"Split sequences by two groups:\n        1. Sequences with non-zero proposal length.\n        2. Sequences with zero proposal length (due to disabled speculation\n        or exceed the maximum model length).\n        \"\"\"\n\n        proposal_lens: List[int] = []\n        nonzero_proposal_len_seqs: List[SequenceGroupMetadata] = []\n        nonzero_proposal_len_indices: List[int] = []\n        for i, seq_group_metadata in enumerate(seq_group_metadata_list):\n            # The speculative decoding for this request has been disabled\n            # (e.g. due to high traffic).\n            if seq_group_metadata.num_speculative_tokens == 0:\n                proposal_lens.append(0)\n                continue\n\n            seq_data = next(iter(seq_group_metadata.seq_data.values()))\n            seq_len = seq_data.get_len()\n\n            # Currently only proposal lens of 0 or the global batch proposal len\n            # are supported.\n            # If max_proposal_len is defined, then we shall no exceed this\n            # quota for nonzero_proposal\n            new_k = 0\n            if (self.max_proposal_len is None\n                    or seq_len + proposal_len < self.max_proposal_len):\n                new_k = proposal_len\n                nonzero_proposal_len_seqs.append(seq_group_metadata)\n                nonzero_proposal_len_indices.append(i)\n            proposal_lens.append(new_k)\n            seq_group_metadata.num_speculative_tokens = new_k\n\n        return (\n            proposal_lens,\n            nonzero_proposal_len_seqs,\n            nonzero_proposal_len_indices,\n        )\n\n    @staticmethod\n    def _remove_no_proposal_seqs(proposal_lens, maybe_sampler_output,\n                                 nonzero_proposal_len_indices, transposed):\n        \"\"\"Remove sequences from nonzero_proposal_len_indices and reset\n        their proposal_len to 0 the draft worker does not provide a proposal\n        (maybe_sampler_output=None). This can avoid scoring overheads.\n        \"\"\"\n\n        # If maybe_sampler_output is None, then the draft worker did not\n        # provide a proposal for any sequence and thus no action needed.\n        # Also we do not support transposed maybe_sampler_output for now\n        # because it seems not straightforward for draft workers outputting\n        # transposed sampler outputs to handle the case of no proposal.\n        if maybe_sampler_output is None or transposed:\n            return (proposal_lens, maybe_sampler_output,\n                    nonzero_proposal_len_indices)\n\n        new_proposal_lens: List[int] = []\n        new_nonzero_proposal_len_indices: List[int] = []\n        new_maybe_sampler_output: List[SamplerOutput] = []\n        nonzero_proposal_len_idx_ptr = 0\n        seq_idx = 0\n        while seq_idx < len(\n                proposal_lens) and nonzero_proposal_len_idx_ptr < len(\n                    nonzero_proposal_len_indices):\n            if seq_idx < nonzero_proposal_len_indices[\n                    nonzero_proposal_len_idx_ptr]:\n                # Sequence is not in the original nonzero_proposal_len_indices,\n                # meaning that it has a proposal length of 0 before sending to\n                # the draft worker.\n                assert proposal_lens[seq_idx] == 0\n                new_proposal_lens.append(0)\n            else:\n                # Sequence is in the original nonzero_proposal_len_indices\n                if maybe_sampler_output[nonzero_proposal_len_idx_ptr] is None:\n                    # but does not have a proposal from the draft worker.\n                    new_proposal_lens.append(0)\n                else:\n                    # and has a proposal from the draft worker. Add it to the\n                    # new nonzero proposal list and keep the sampler output.\n                    new_proposal_lens.append(proposal_lens[seq_idx])\n                    new_nonzero_proposal_len_indices.append(seq_idx)\n                    new_maybe_sampler_output.append(\n                        maybe_sampler_output[nonzero_proposal_len_idx_ptr])\n                nonzero_proposal_len_idx_ptr += 1\n            seq_idx += 1\n\n        # The remaining sequences should have proposal length of 0.\n        new_proposal_lens.extend(proposal_lens[seq_idx:])\n\n        # We assume sampler_output will not be a list of all Nones.\n        # In this case this function should not be called.\n        assert new_maybe_sampler_output\n        return (new_proposal_lens, new_maybe_sampler_output,\n                new_nonzero_proposal_len_indices)\n\n    def _merge_outputs(\n        self,\n        batch_size: int,\n        proposal_len: int,\n        maybe_sampler_output: Optional[List[SamplerOutput]],\n        proposal_lens: List[int],\n        nonzero_proposal_len_indices: List[int],\n        sampler_transposed: bool,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"After speculations are produced, merge the speculation results with\n        the skipped sequences.\n        \"\"\"\n        if maybe_sampler_output is None:\n            # If no speculative tokens, the sampler output will be None.\n            # In this case we return empty proposals.\n            proposal_tokens = torch.tensor(-1,\n                                           dtype=torch.long,\n                                           device=self._device).expand(\n                                               batch_size, proposal_len)\n            proposal_probs = torch.tensor(0,\n                                          dtype=torch.float32,\n                                          device=self._device).expand(\n                                              batch_size, proposal_len,\n                                              self._vocab_size)\n            proposal_lens_tensor = torch.tensor(0,\n                                                dtype=torch.long,\n                                                device=self._device).expand(\n                                                    len(proposal_lens))\n            return proposal_tokens, proposal_probs, proposal_lens_tensor\n\n        sampler_output = maybe_sampler_output\n        proposal_tokens, proposal_probs, _ = sampler_output_to_torch(\n            sampler_output, sampler_transposed)\n\n        # Now, reformat the output GPU tensors such that each sequence has\n        # a proposal. the proposal can be empty, e.g. [-1, -1, -1]\n\n        entire_proposal_tokens = proposal_tokens.new_full(\n            size=(batch_size, *proposal_tokens.shape[1:]),\n            fill_value=-1,\n        )\n        entire_proposal_tokens[nonzero_proposal_len_indices] = proposal_tokens\n        entire_proposal_probs = proposal_probs.new_zeros(\n            batch_size,\n            *proposal_probs.shape[1:],\n        )\n        entire_proposal_probs[nonzero_proposal_len_indices] = proposal_probs\n\n        proposal_tokens, proposal_probs = (\n            entire_proposal_tokens,\n            entire_proposal_probs,\n        )\n\n        proposal_lens_tensor = torch.zeros(batch_size,\n                                           dtype=torch.long,\n                                           device=self._device)\n        proposal_lens_tensor[nonzero_proposal_len_indices] = proposal_len\n\n        return proposal_tokens, proposal_probs, proposal_lens_tensor\n\n```\n# Function Signature: def get_spec_proposals(\n# Natural Language Annotations: \"\"\"Get speculative proposals given the input batch.\n\n        Sequences which would exceed the max model length are skipped during\n        speculation.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_spec_proposals(\n        self,", "  ```\ndef get_spec_proposals(self, batch_", "  ```\ndef get_spec_proposals(\n    self,"]}, "vllm-vllm/spec_decode/draft_model_runner.py-execute_model": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/spec_decode/draft_model_runner.py:\n```\nfrom typing import List, Optional\n\nimport torch\n\nfrom vllm import _custom_ops as ops\n\ntry:\n    from vllm.attention.backends.flash_attn import FlashAttentionMetadata\nexcept ModuleNotFoundError:\n    # vllm_flash_attn is not installed, use the identical ROCm FA metadata\n    from vllm.attention.backends.rocm_flash_attn import (\n        ROCmFlashAttentionMetadata as FlashAttentionMetadata)\n\ntry:\n    from flashinfer import BatchDecodeWithPagedKVCacheWrapper\n    from flashinfer.decode import CUDAGraphBatchDecodeWithPagedKVCacheWrapper\n    from flashinfer.prefill import BatchPrefillWithPagedKVCacheWrapper\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 256 * 1024 * 1024\nexcept ImportError:\n    BatchDecodeWithPagedKVCacheWrapper = None\n    CUDAGraphBatchDecodeWithPagedKVCacheWrapper = None\n    BatchPrefillWithPagedKVCacheWrapper = None\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 0\n\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig)\nfrom vllm.logger import init_logger\nfrom vllm.multimodal import MultiModalInputs\nfrom vllm.sequence import (ExecuteModelRequest, IntermediateTensors,\n                           SamplerOutput)\nfrom vllm.worker.model_runner import (ModelInputForGPUWithSamplingMetadata,\n                                      ModelRunner)\n\nlogger = init_logger(__name__)\n\n# A flag to enable debug prints for the updated input tensors\n# before each step.\ndebug_advance_input = False\n# A flag to allow GPU advance step for draft model runner.\n# Set to False for debugging.\nallow_gpu_advance_step = True\n\n\nclass TP1DraftModelRunner(ModelRunner):\n    \"\"\"Specialized model runner for speculative decoding draft model.\n    Since the draft model always execute k forward passes consecutively to\n    generate k speculative tokens in a single speculative decoding step,\n    we could get rid of most CPU-GPU synchronization and data transfer\n    overheads by keeping model input and output tensors on GPU all the time.\n\n    TODOs:\n    1. Currently supports only flash-attn, add support for other attn_backends.\n    2. Support TP > 1 (this requires some designs because we do not expect\n       any broadcasting inside execute_model).\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        kv_cache_dtype: Optional[str] = \"auto\",\n        is_driver_worker: bool = False,\n        prompt_adapter_config: Optional[PromptAdapterConfig] = None,\n        return_hidden_states: bool = False,\n        observability_config: Optional[ObservabilityConfig] = None,\n    ):\n        if return_hidden_states:\n            raise ValueError(\n                \"return_hidden_states is not supported for TP1DraftModelRunner.\"\n            )\n\n        super().__init__(\n            model_config=model_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            cache_config=cache_config,\n            load_config=load_config,\n            lora_config=lora_config,\n            kv_cache_dtype=kv_cache_dtype,\n            is_driver_worker=is_driver_worker,\n            prompt_adapter_config=prompt_adapter_config,\n            return_hidden_states=return_hidden_states,\n            observability_config=observability_config,\n        )\n\n        self.flashinfer_decode_workspace_buffer = None\n        self.flashinfer_decode_wrapper = None\n        self.flashinfer_prefill_workspace_buffer = None\n        self.flashinfer_prefill_wrapper = None\n\n    def _update_sampling_metadata(self, sampling_metadata, num_seqs,\n                                  num_queries):\n\n        assert sampling_metadata.num_prompts == 0\n        assert len(sampling_metadata.seq_groups) == num_queries\n        assert sampling_metadata.selected_token_indices.shape == (\n            num_queries, )\n        # assert sampling_metadata.categorized_sample_indices == TODO: Add if needed # noqa: E501\n\n        # Verify that all sequences are decodes\n        for i in range(num_queries):\n            seq_group = sampling_metadata.seq_groups[i]\n\n            assert seq_group.is_prompt is False  # No prompt\n            assert seq_group.prompt_logprob_indices == []  # No prompt\n            assert seq_group.sample_indices == [i]  # Simple\n            assert seq_group.seq_len is None  # Decode\n            assert seq_group.query_len is None  # Decode\n\n    def _gpu_advance_step(\n            self, model_input: ModelInputForGPUWithSamplingMetadata,\n            last_output: SamplerOutput\n    ) -> ModelInputForGPUWithSamplingMetadata:\n        # Currently, we expect \"decode mode\" only\n        assert not model_input.is_prompt\n\n        # Get num_seqs\n        num_seqs = len(model_input.seq_lens)\n        num_queries = len(model_input.query_lens)\n\n        # Get output tokens GPU tensor\n        sampled_token_ids = last_output.sampled_token_ids\n        assert sampled_token_ids is not None\n\n        # Update attn_metadata\n        attn_metadata = model_input.attn_metadata\n        assert isinstance(attn_metadata, FlashAttentionMetadata)\n        attn_metadata.advance_step(num_seqs, num_queries)\n\n        # Update GPU tensors\n        ops.advance_step(num_seqs=num_seqs,\n                         num_queries=num_queries,\n                         block_size=self.block_size,\n                         input_tokens=model_input.input_tokens,\n                         sampled_token_ids=sampled_token_ids,\n                         input_positions=model_input.input_positions,\n                         seq_lens=attn_metadata.seq_lens_tensor,\n                         slot_mapping=attn_metadata.slot_mapping,\n                         block_tables=attn_metadata.block_tables)\n\n        # Update sampling_metadata\n        sampling_metadata = model_input.sampling_metadata\n        self._update_sampling_metadata(sampling_metadata, num_seqs,\n                                       num_queries)\n\n        # Create new input\n        new_model_input = self._model_input_cls(\n            input_tokens=model_input.input_tokens,\n            input_positions=model_input.input_positions,\n            attn_metadata=attn_metadata,\n            seq_lens=attn_metadata.seq_lens,\n            query_lens=model_input.query_lens,\n            lora_mapping=model_input.lora_mapping,\n            lora_requests=model_input.lora_requests,\n            multi_modal_kwargs=model_input.multi_modal_kwargs,\n            sampling_metadata=model_input.sampling_metadata,\n            is_prompt=False,\n        )\n\n        # Ensure we skip CPU samples\n        assert new_model_input.sampling_metadata.skip_sampler_cpu_output is True\n        # We can reuse sampling tensors since every decode iteration is the same\n        new_model_input.sampling_metadata.reuse_sampling_tensors = True\n\n        if debug_advance_input:\n            logger.debug(\"NEW INPUT: \")\n            logger.debug(\"  input_tokens = %s\", new_model_input.input_tokens)\n            logger.debug(\"  input_positions = %s\",\n                         new_model_input.input_positions)\n            logger.debug(\"  seq_lens = %d\", new_model_input.seq_lens)\n            logger.debug(\"  query_lens = %d\", new_model_input.query_lens)\n            logger.debug(\"  attn_metadata:\")\n            logger.debug(\"    seq_lens_tensor: %s\",\n                         attn_metadata.seq_lens_tensor)\n            logger.debug(\"    slot_mapping: %s\", attn_metadata.slot_mapping)\n            logger.debug(\"    block_tables: %s\", attn_metadata.block_tables)\n\n        return new_model_input\n\n    def supports_gpu_multi_step(self, execute_model_req: ExecuteModelRequest):\n        \"\"\"Determines if draft_model_runner GPU multi-step can be used.\n        Currently required conditions are:\n            1. Only decodes \n            2. Only flash-attn\n            3. No LORA\n            4. No prompt_adapter_config\n        \"\"\"\n        if not allow_gpu_advance_step:\n            return False\n\n        # We allow multi-step GPU only in decode mode\n        for seq_group in execute_model_req.seq_group_metadata_list:\n            if seq_group.is_prompt:\n                return False\n\n        # TODO: Add support for other attn backends\n        if self.attn_backend.get_name() != \"flash-attn\":\n            return False\n\n        # TODO: Add support for LORA\n        if self.lora_config:\n            return False\n\n        # TODO: Add soft-tuning prompt adapter support\n        if self.prompt_adapter_config:\n            return False\n\n        return True\n\n    @torch.inference_mode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def execute_model(\n# Natural Language Annotations: \"\"\"Executes num_steps forward passes with advacement of input tensors \n        on the GPU. Look at supports_gpu_multi_step(..) for pre-conditions.\n\n        Optimizations used:\n            1. Input tensors are updated on the GPU directly\n            2. Skips GPU=>CPU serialization of sampler outputs (we don't need \n                them since we do batch expansion later that uses GPU outputs)\n            3. Reuses sampling tensors (since we run only decodes and they have\n                a repeating sampling logic)\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef execute_model(self, execute_model_req,", "  ```\ndef execute_model(self, execute_model_req:", "  ```\ndef execute_model(self, execute_model_req:"]}, "vllm-vllm/spec_decode/spec_decode_worker.py-init_device": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/spec_decode/spec_decode_worker.py:\n```\nfrom collections import defaultdict\nfrom functools import cached_property\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\nimport torch\n\nfrom vllm.config import ParallelConfig, SpeculativeConfig\nfrom vllm.distributed.communication_op import broadcast_tensor_dict\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.rejection_sampler import RejectionSampler\nfrom vllm.model_executor.layers.spec_decode_base_sampler import (\n    SpecDecodeBaseSampler, SpecDecodeStochasticBaseSampler)\nfrom vllm.model_executor.layers.typical_acceptance_sampler import (\n    TypicalAcceptanceSampler)\nfrom vllm.sequence import (CompletionSequenceGroupOutput, ExecuteModelRequest,\n                           HiddenStates, SamplerOutput, SequenceGroupMetadata,\n                           get_all_seq_ids, get_all_seq_ids_and_request_ids)\nfrom vllm.spec_decode.batch_expansion import BatchExpansionTop1Scorer\nfrom vllm.spec_decode.draft_model_runner import TP1DraftModelRunner\nfrom vllm.spec_decode.interfaces import (SpeculativeProposals,\n                                         SpeculativeScorer, SpeculativeScores)\nfrom vllm.spec_decode.medusa_worker import MedusaWorker\nfrom vllm.spec_decode.metrics import AsyncMetricsCollector\nfrom vllm.spec_decode.mlp_speculator_worker import MLPSpeculatorWorker\nfrom vllm.spec_decode.multi_step_worker import MultiStepWorker\nfrom vllm.spec_decode.ngram_worker import NGramWorker\nfrom vllm.spec_decode.proposer_worker_base import ProposerWorkerBase\nfrom vllm.spec_decode.smaller_tp_proposer_worker import SmallerTpProposerWorker\nfrom vllm.spec_decode.target_model_runner import TargetModelRunner\nfrom vllm.spec_decode.util import (Timer, create_sequence_group_output,\n                                   get_all_num_logprobs,\n                                   get_sampled_token_logprobs, nvtx_range,\n                                   split_batch_by_proposal_len)\nfrom vllm.worker.worker import Worker\nfrom vllm.worker.worker_base import LoraNotSupportedWorkerBase, WorkerBase\n\nlogger = init_logger(__name__)\n\n\ndef create_spec_worker(*args, **kwargs) -> \"SpecDecodeWorker\":\n    \"\"\"Helper method that is the entrypoint for Executors which use\n    WorkerWrapper. It constructs a SpecDecodeWorker from the speculative config.\n    \"\"\"\n    assert \"speculative_config\" in kwargs\n    speculative_config: SpeculativeConfig = kwargs.get(\"speculative_config\")\n    assert speculative_config is not None\n\n    draft_worker_kwargs = kwargs.copy()\n\n    kwargs[\"model_runner_cls\"] = TargetModelRunner\n    target_worker = Worker(*args, **kwargs)\n    # Set the disable_logprobs variable in the TargetModelRunner instance\n    # as per its value specified in the SpeculativeConfig.\n    target_worker.model_runner.disable_logprobs =\\\n         speculative_config.disable_logprobs\n\n    # Override draft-model specific worker args.\n    draft_worker_kwargs.update(\n        model_config=speculative_config.draft_model_config,\n        parallel_config=speculative_config.draft_parallel_config,\n        ngram_prompt_lookup_max=speculative_config.ngram_prompt_lookup_max,\n        ngram_prompt_lookup_min=speculative_config.ngram_prompt_lookup_min,\n        # TODO allow draft-model specific load config.\n        #load_config=load_config,\n    )\n\n    spec_decode_worker = SpecDecodeWorker.create_worker(\n        scorer_worker=target_worker,\n        draft_worker_kwargs=draft_worker_kwargs,\n        disable_by_batch_size=speculative_config.\n        speculative_disable_by_batch_size,\n        draft_token_acceptance_method=speculative_config.\n        draft_token_acceptance_method,\n        typical_acceptance_sampler_posterior_threshold=speculative_config.\n        typical_acceptance_sampler_posterior_threshold,\n        typical_acceptance_sampler_posterior_alpha=speculative_config.\n        typical_acceptance_sampler_posterior_alpha,\n        disable_logprobs=speculative_config.disable_logprobs,\n        disable_log_stats=speculative_config.disable_log_stats,\n    )\n\n    return spec_decode_worker\n\n\nclass SpecDecodeWorker(LoraNotSupportedWorkerBase):\n    \"\"\"Worker which implements speculative decoding.\n\n    Speculative decoding reduces decoding per-token latency by using a proposal\n    method, such as a small draft model, to speculate ahead of a larger LLM. The\n    probabilities of the speculative tokens are then determined by the larger\n    LLM, after which some verification routine determines which (if any) of the\n    speculative tokens are accepted by the larger LLM.\n\n    See https://github.com/vllm-project/vllm/pull/2188 and\n    https://github.com/vllm-project/vllm/pull/3103 for more info.\n\n    The current implementation has the following limitations:\n    * Only draft-model proposal is implemented (contributions for more forms are\n        welcome!).\n    * Only top-1 proposal and scoring are implemented. Tree-attention is left as\n        future work.\n    * All sequences in a batch must have the same proposal length, or zero. This\n        can be improved by having per-sequence speculation in the future.\n    * The scoring forward pass is done without an MQA kernel, which is\n        suboptimal especially as the batch size, proposal length, and sequence\n        lengths grow. Contributions to add a MQA scoring are welcome once\n        correctness tests pass.\n        More info here https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit.\n    \"\"\"\n\n    @classmethod\n    def create_worker(\n        cls,\n        scorer_worker: Worker,\n        draft_worker_kwargs: Dict[str, Any],\n        disable_by_batch_size: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: float,\n        typical_acceptance_sampler_posterior_alpha: float,\n        disable_logprobs: bool,\n        disable_log_stats: bool,\n    ) -> \"SpecDecodeWorker\":\n\n        allow_zero_draft_token_step = True\n        ngram_prompt_lookup_max = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_max\"))\n        ngram_prompt_lookup_min = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_min\"))\n        if ngram_prompt_lookup_max > 0:\n            proposer_worker = NGramWorker(**draft_worker_kwargs)\n            proposer_worker.set_ngram_window_size(ngram_prompt_lookup_min,\n                                                  ngram_prompt_lookup_max)\n        else:\n            draft_parallel_config: ParallelConfig = draft_worker_kwargs[\n                'parallel_config']\n            draft_tp = draft_parallel_config.tensor_parallel_size\n            target_tp = scorer_worker.parallel_config.tensor_parallel_size\n\n            if draft_worker_kwargs[\n                    \"model_config\"].hf_config.model_type == \"mlp_speculator\":\n                proposer_worker = MLPSpeculatorWorker(**draft_worker_kwargs)\n            elif draft_worker_kwargs[\n                    \"model_config\"].hf_config.model_type == \"medusa\":\n                proposer_worker = MedusaWorker(**draft_worker_kwargs)\n            else:\n                if draft_tp == 1:\n                    draft_worker_kwargs[\n                        \"model_runner_cls\"] = TP1DraftModelRunner\n                else:\n                    allow_zero_draft_token_step = False\n                proposer_worker = MultiStepWorker(**draft_worker_kwargs)\n\n            proposer_worker = SmallerTpProposerWorker.maybe_wrap_worker(\n                proposer_worker, draft_tp, target_tp)\n\n        logger.info(\"Configuring SpecDecodeWorker with proposer=%s\",\n                    type(proposer_worker))\n\n        spec_decode_sampler: SpecDecodeBaseSampler = None\n        if draft_token_acceptance_method == \"rejection_sampler\":\n            spec_decode_sampler = RejectionSampler(\n                disable_bonus_tokens=False, )\n        elif draft_token_acceptance_method == \"typical_acceptance_sampler\":\n            spec_decode_sampler = TypicalAcceptanceSampler(\n                disable_bonus_tokens=False,\n                posterior_threshold=\\\n                    typical_acceptance_sampler_posterior_threshold,\n                posterior_alpha=typical_acceptance_sampler_posterior_alpha,\n            )\n        logger.info(\"Configuring SpecDecodeWorker with sampler=%s\",\n                    type(spec_decode_sampler))\n\n        return SpecDecodeWorker(\n            proposer_worker,\n            scorer_worker,\n            disable_logprobs=disable_logprobs,\n            disable_log_stats=disable_log_stats,\n            disable_by_batch_size=disable_by_batch_size,\n            spec_decode_sampler=spec_decode_sampler,\n            allow_zero_draft_token_step=allow_zero_draft_token_step)\n\n    def __init__(\n        self,\n        proposer_worker: ProposerWorkerBase,\n        scorer_worker: WorkerBase,\n        spec_decode_sampler: SpecDecodeBaseSampler,\n        disable_logprobs: bool = False,\n        disable_log_stats: bool = False,\n        metrics_collector: Optional[AsyncMetricsCollector] = None,\n        disable_by_batch_size: Optional[int] = None,\n        allow_zero_draft_token_step: Optional[bool] = True,\n    ):\n        \"\"\"\n        Create a SpecDecodeWorker.\n\n        Args:\n            proposer_worker: A worker that can produce speculative tokens for\n                sequences.\n            scorer_worker: A worker that produces probabilities of speculative\n                tokens according to some base model. Typically a vanilla vLLM\n                Worker.\n            spec_decode_sampler: A Torch module used to perform acceptance\n                sampling of the draft tokens in the verification step of\n                speculative decoding. Currently we support two different \n                types of sampler namely RejectionSampler and\n                TypicalAcceptanceSampler. 'spec_decode_sampler' is either an\n                instance of RejectionSampler or TypicalAcceptanceSampler.\n            disable_logprobs: If set to True, token log probabilities will\n                not be output in both the draft worker and the target worker.\n                If set to False, log probabilities will be output by both.\n            disable_log_stats: If set to True, disable periodic printing of\n                speculative stage times.\n            disable_by_batch_size: If the batch size is larger than this,\n                disable speculative decoding for new incoming requests.\n            metrics_collector: Helper class for collecting metrics; can be set\n                for testing purposes.\n            allow_zero_draft_token_step: whether to allow a step where the draft\n                model generates no draft token; should disallow when the tp of\n                draft model is larger than 1 (TODO: #5814)\n        \"\"\"\n        self.proposer_worker = proposer_worker\n        self.scorer_worker = scorer_worker\n        scorer_runner = getattr(self.scorer_worker, \"model_runner\", None)\n        self.generators = scorer_runner.get_generators(\n        ) if scorer_runner else None\n        self.disable_by_batch_size = disable_by_batch_size or float(\"inf\")\n        self.spec_decode_sampler = spec_decode_sampler\n        self._allow_zero_draft_token_step = allow_zero_draft_token_step\n        self._metrics = AsyncMetricsCollector(\n            self.spec_decode_sampler\n        ) if metrics_collector is None else metrics_collector\n        # Tracks the sequence IDs that received a bonus token ID in\n        # their last forward pass. Needed only if KV cache is being\n        # used for token generation such as in the case of MultiStepWorker.\n        self._seq_with_bonus_token_in_last_step: Set[int] = set()\n        # Tracks the currently active request ids and the sequence IDs\n        # corresponding to them\n        self._request_id_seq_id_mapping: Dict[str, Set[int]] = defaultdict(set)\n        # Tracks if the proposer worker uses the KV cache or not.\n\n        self.probs_dtype = self.spec_decode_sampler.probs_dtype\n        self.token_id_dtype = self.spec_decode_sampler.token_id_dtype\n        # Lazy initialization.\n        self.scorer: SpeculativeScorer\n\n        # Hidden states from target model to pass to proposer\n        # in the subsequent step.\n        self.previous_hidden_states: Optional[HiddenStates] = None\n        self._disable_logprobs = disable_logprobs\n        self._disable_log_stats = disable_log_stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def load_model(self, *args, **kwargs):\n        pass\n\n    def _configure_model_sampler_for_spec_decode(self):\n        \"\"\"Configure model sampler to emit GPU tensors. This allows spec decode\n        to keep data on device without transferring to CPU and serializing,\n        which significantly reduces overhead of sampling during verification.\n\n        NOTE(cade): This breaks abstraction boundaries pretty badly. The better\n        design is to have the \"move to CPU and serialize\" sampling decision be\n        done outside of the model/sampler; this way the \"last-mile\" worker\n        object which interfaces with the scheduler can serialize and incur the\n        performance hit as necessary. This allows us to run the worker several\n        iterations in a row without incurring the \"move to CPU and serialize\"\n        performance penalty.\n\n        Since this requires a large change to vLLM, we defer it to later and\n        temporarily accept this broken abstraction boundary.\n\n        NOTE(cade): This will require a special check if the proposer worker\n        does not have a sampler (e.g. ngram speculation).\n        \"\"\"\n        (self.scorer_worker.model_runner.model.sampler.include_gpu_probs_tensor\n         ) = True\n        (self.scorer_worker.model_runner.model.sampler.\n         should_modify_greedy_probs_inplace) = True\n        self.proposer_worker.set_include_gpu_probs_tensor()\n        self.proposer_worker.set_should_modify_greedy_probs_inplace()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"Initialize the cache engine of the scorer and proposer workers.\n        \"\"\"\n        self.scorer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                            num_cpu_blocks=num_cpu_blocks)\n        self.proposer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                              num_cpu_blocks=num_cpu_blocks)\n\n    @torch.inference_mode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @torch.inference_mode()\n    def start_worker_execution_loop(self) -> None:\n        \"\"\"Execute model loop to perform speculative decoding\n        in parallel worker.\"\"\"\n        while self._run_non_driver_rank():\n            pass\n\n    def _should_disable_all_speculation(\n            self, execute_model_req: ExecuteModelRequest) -> bool:\n        # When the batch size is too large, disable speculative decoding\n        # to stop trading off throughput for latency.\n        disable_all_speculation = (execute_model_req.running_queue_size >=\n                                   self.disable_by_batch_size)\n\n        return disable_all_speculation\n\n    def _maybe_disable_speculative_tokens(\n            self, disable_all_speculation: bool,\n            seq_group_metadata_list: List[SequenceGroupMetadata]) -> None:\n        if not disable_all_speculation:\n            return\n\n        for seq_group_metadata in seq_group_metadata_list:\n            # Once num_speculative_tokens is set to 0, the spec decode\n            # of this request will be disabled forever.\n            # TODO(comaniac): We currently store spec decoding specific\n            # state in the global data structure, but we should maintain\n            # this state within spec decode worker.\n            seq_group_metadata.num_speculative_tokens = 0\n\n    def _serialize_sampler_output_no_logprobs(\n            self, execute_model_req: ExecuteModelRequest,\n            sampler_output: SamplerOutput) -> SamplerOutput:\n        \"\"\"\n        Creates and returns a `SamplerOutput` with only the sampled token IDs \n        being serialized to CPU & populated in `CompletionSequenceGroupOutput`.\n        All other parameters in `CompletionSequenceGroupOutput` related to log \n        probabilities are skipped.\n\n        Args:\n            execute_model_req (ExecuteModelRequest): The model request that\n            was executed.\n            sampler_output (SamplerOutput): The output from the sampler with\n            only GPU tensors populated.\n\n        Returns:\n            SamplerOutput: A new `SamplerOutput` instance containing a list of \n            `CompletionSequenceGroupOutput` objects with only sampled token\n            IDs populated.\n        \"\"\"\n        seq_ids = get_all_seq_ids(execute_model_req.seq_group_metadata_list)\n        sampled_token_ids_list = sampler_output.sampled_token_ids.tolist()\n        completion_seq_group_output_list: List[\n            CompletionSequenceGroupOutput] = []\n        for index, seq_id in enumerate(seq_ids):\n            completion_seq_group_output_list.append(\n                create_sequence_group_output(\n                    token_id=sampled_token_ids_list[index][0],\n                    token_id_logprob_rank=-1,\n                    token_id_logprob=0.0,\n                    seq_id=seq_id,\n                    topk_token_ids=[],\n                    topk_logprobs=[],\n                ))\n        return SamplerOutput(outputs=completion_seq_group_output_list)\n\n    @nvtx_range(\"spec_decode_worker._run_no_spec\")\n    def _run_no_spec(self, execute_model_req: ExecuteModelRequest,\n                     skip_proposer: bool) -> List[SamplerOutput]:\n        \"\"\"Run a single generation step without any speculation. The input is\n        sent to the proposer and scorer model so that the KV cache is consistent\n        between the two. When skip_proposer is True, the proposer model is\n        not called, meaning that the kv-cache in proposer for requests is not\n        updated, so they cannot enable spec decode in the rest decoding.\n        \"\"\"\n        if not skip_proposer:\n            self.proposer_worker.execute_model(execute_model_req)\n\n        sampler_output = self.scorer_worker.execute_model(execute_model_req)\n        assert len(sampler_output) == 1\n        sampler_output = sampler_output[0]\n\n        # Store hidden states from target model execution.\n        hidden_states = sampler_output.hidden_states\n        if hidden_states is not None:\n            if self.previous_hidden_states is None:\n                self.previous_hidden_states = HiddenStates(\n                    execute_model_req.seq_group_metadata_list, hidden_states)\n            else:\n                self.previous_hidden_states.update(\n                    execute_model_req.seq_group_metadata_list, hidden_states)\n\n        sampler_output_to_return = (self._serialize_sampler_output_no_logprobs(\n            execute_model_req=execute_model_req, sampler_output=sampler_output)\n                                    if self._disable_logprobs else\n                                    sampler_output)\n\n        # Clear device tensors from sampler output. This reduces communication\n        # overhead when the engine runs in a different process than the workers.\n        sampler_output.sampled_token_probs = None\n        sampler_output.sampled_token_ids = None\n        sampler_output.logprobs = None\n        return [sampler_output_to_return]\n\n    def _run_non_driver_rank(self) -> bool:\n        \"\"\"Run proposer and verifier model in non-driver workers. This is used\n        for both speculation cases (num_lookahead_slots>0) and non-speculation\n        cases (e.g. prefill).\n\n        Returns True if there are remaining sequences to process.\n        \"\"\"\n        assert self.rank != self._driver_rank\n\n        data = broadcast_tensor_dict(src=self._driver_rank)\n        if not data:\n            return False\n        num_lookahead_slots = data[\"num_lookahead_slots\"]\n\n        # Even if num_lookahead_slots is zero, we want to run the proposer model\n        # as it may have KV.\n        #\n        # We run the proposer once per lookahead slot. In the future we should\n        # delegate how many times it runs to the proposer.\n        for _ in range(max(num_lookahead_slots, 1)):\n            self.proposer_worker.execute_model()\n\n        self.scorer_worker.execute_model()\n        return True\n\n    @nvtx_range(\"spec_decode_worker._run_speculative_decoding_step\")\n    def _run_speculative_decoding_step(\n            self, execute_model_req: ExecuteModelRequest,\n            num_lookahead_slots: int) -> List[SamplerOutput]:\n        \"\"\"Execute a single step of speculative decoding.\n\n        This invokes the proposer worker to get k speculative tokens for each\n        sequence, then scores each speculative token using the scoring worker.\n\n        Returns a list of SamplerOutput, each containing a single token per\n        sequence.\n        \"\"\"\n        assert num_lookahead_slots == execute_model_req.num_lookahead_slots\n\n        # Pass last hidden states from target model to proposer\n        execute_model_req.previous_hidden_states = self.previous_hidden_states\n        self.previous_hidden_states = None\n\n        with Timer() as proposal_timer:\n            # Generate proposals using draft worker.\n            proposals = self.proposer_worker.get_spec_proposals(\n                execute_model_req, self._seq_with_bonus_token_in_last_step)\n\n        if not self._allow_zero_draft_token_step and proposals.no_proposals:\n            #TODO: Fix it #5814\n            raise RuntimeError(\"Cannot handle cases where distributed draft \"\n                               \"workers generate no tokens\")\n\n        with Timer() as scoring_timer:\n            proposal_scores = self.scorer.score_proposals(\n                execute_model_req,\n                proposals,\n            )\n\n        with Timer() as verification_timer:\n            accepted_token_ids, target_logprobs = self._verify_tokens(\n                execute_model_req.seq_group_metadata_list, proposal_scores,\n                proposals, execute_model_req.num_lookahead_slots)\n\n        stage_times = (proposal_timer.elapsed_time_ms / num_lookahead_slots,\n                       scoring_timer.elapsed_time_ms,\n                       verification_timer.elapsed_time_ms)\n\n        return self._create_output_sampler_list(\n            execute_model_req.seq_group_metadata_list,\n            accepted_token_ids,\n            target_logprobs=target_logprobs,\n            k=execute_model_req.num_lookahead_slots,\n            stage_times=stage_times)\n\n    @nvtx_range(\"spec_decode_worker._verify_tokens\")\n    def _verify_tokens(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        proposal_scores: SpeculativeScores,\n        proposals: SpeculativeProposals,\n        max_proposal_len: int,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Determine which speculative tokens are accepted using the\n        probabilities of each token according to the proposer and scorer models.\n\n        Returns a tuple of Tensors, one for the accepted token ids and one for\n        the logprobs according to the scoring model.\n        \"\"\"\n        proposal_lens_list = proposals.proposal_lens.tolist()\n\n        # vLLM currently only supports proposal lens equal to zero or the batch\n        # proposal len. This adds some complexity (splitting the batch into spec\n        # and non spec sequences) and should be removed in the future. It can be\n        # done by supporting per-sequence proposal lens.\n        _, spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=False)\n        _, non_spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=True)\n        original_indices = spec_indices + non_spec_indices\n\n        # Get probabilities of target model, excluding bonus token.\n        proposal_verifier_probs = proposal_scores.probs[spec_indices, :-1]\n\n        # Get non-speculative sampled tokens from target model.\n        non_spec_token_ids = proposal_scores.token_ids[non_spec_indices]\n\n        # Get bonus tokens from target model.\n        bonus_token_ids = proposal_scores.token_ids[spec_indices, -1:]\n\n        # Get probabilities according to proposal method.\n        proposal_probs = proposals.proposal_probs[spec_indices]\n\n        # Get proposed tokens.\n        proposal_token_ids = proposals.proposal_token_ids[spec_indices]\n\n        # Sampler arguments\n        sampler_extra_kwargs: Dict[str, Any] = {}\n        if self.generators and isinstance(self.spec_decode_sampler,\n                                          SpecDecodeStochasticBaseSampler):\n            sampler_extra_kwargs[\"seeded_seqs\"] = {\n                idx: self.generators[sgm.request_id]\n                for idx, sgm in enumerate(seq_group_metadata_list)\n                if sgm.sampling_params.seed is not None\n            }\n\n        accepted_token_ids = self.spec_decode_sampler(\n            target_probs=proposal_verifier_probs,\n            bonus_token_ids=bonus_token_ids,\n            draft_probs=proposal_probs,\n            draft_token_ids=proposal_token_ids,\n            **sampler_extra_kwargs,\n        )\n\n        # Append output tokens from non-speculative sequences to\n        # the accepted token ids tensor.\n        non_spec_token_ids = non_spec_token_ids.expand(-1, max_proposal_len +\n                                                       1).clone()\n        non_spec_token_ids[:, 1:] = -1\n        accepted_token_ids = torch.cat(\n            [accepted_token_ids, non_spec_token_ids])\n        log\n```\n# Function Signature: def init_device(self) -> None:\n# Natural Language Annotations: \"\"\"Initialize both scorer and proposer models.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef init_device(self):\n    self.scorer", "  ```\ndef init_device(self) -> None:\n    \"\"\"", "  ```\ndef init_device(self) -> None:\n    self"]}, "vllm-vllm/spec_decode/spec_decode_worker.py-determine_num_available_blocks": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/spec_decode/spec_decode_worker.py:\n```\nfrom collections import defaultdict\nfrom functools import cached_property\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\nimport torch\n\nfrom vllm.config import ParallelConfig, SpeculativeConfig\nfrom vllm.distributed.communication_op import broadcast_tensor_dict\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.rejection_sampler import RejectionSampler\nfrom vllm.model_executor.layers.spec_decode_base_sampler import (\n    SpecDecodeBaseSampler, SpecDecodeStochasticBaseSampler)\nfrom vllm.model_executor.layers.typical_acceptance_sampler import (\n    TypicalAcceptanceSampler)\nfrom vllm.sequence import (CompletionSequenceGroupOutput, ExecuteModelRequest,\n                           HiddenStates, SamplerOutput, SequenceGroupMetadata,\n                           get_all_seq_ids, get_all_seq_ids_and_request_ids)\nfrom vllm.spec_decode.batch_expansion import BatchExpansionTop1Scorer\nfrom vllm.spec_decode.draft_model_runner import TP1DraftModelRunner\nfrom vllm.spec_decode.interfaces import (SpeculativeProposals,\n                                         SpeculativeScorer, SpeculativeScores)\nfrom vllm.spec_decode.medusa_worker import MedusaWorker\nfrom vllm.spec_decode.metrics import AsyncMetricsCollector\nfrom vllm.spec_decode.mlp_speculator_worker import MLPSpeculatorWorker\nfrom vllm.spec_decode.multi_step_worker import MultiStepWorker\nfrom vllm.spec_decode.ngram_worker import NGramWorker\nfrom vllm.spec_decode.proposer_worker_base import ProposerWorkerBase\nfrom vllm.spec_decode.smaller_tp_proposer_worker import SmallerTpProposerWorker\nfrom vllm.spec_decode.target_model_runner import TargetModelRunner\nfrom vllm.spec_decode.util import (Timer, create_sequence_group_output,\n                                   get_all_num_logprobs,\n                                   get_sampled_token_logprobs, nvtx_range,\n                                   split_batch_by_proposal_len)\nfrom vllm.worker.worker import Worker\nfrom vllm.worker.worker_base import LoraNotSupportedWorkerBase, WorkerBase\n\nlogger = init_logger(__name__)\n\n\ndef create_spec_worker(*args, **kwargs) -> \"SpecDecodeWorker\":\n    \"\"\"Helper method that is the entrypoint for Executors which use\n    WorkerWrapper. It constructs a SpecDecodeWorker from the speculative config.\n    \"\"\"\n    assert \"speculative_config\" in kwargs\n    speculative_config: SpeculativeConfig = kwargs.get(\"speculative_config\")\n    assert speculative_config is not None\n\n    draft_worker_kwargs = kwargs.copy()\n\n    kwargs[\"model_runner_cls\"] = TargetModelRunner\n    target_worker = Worker(*args, **kwargs)\n    # Set the disable_logprobs variable in the TargetModelRunner instance\n    # as per its value specified in the SpeculativeConfig.\n    target_worker.model_runner.disable_logprobs =\\\n         speculative_config.disable_logprobs\n\n    # Override draft-model specific worker args.\n    draft_worker_kwargs.update(\n        model_config=speculative_config.draft_model_config,\n        parallel_config=speculative_config.draft_parallel_config,\n        ngram_prompt_lookup_max=speculative_config.ngram_prompt_lookup_max,\n        ngram_prompt_lookup_min=speculative_config.ngram_prompt_lookup_min,\n        # TODO allow draft-model specific load config.\n        #load_config=load_config,\n    )\n\n    spec_decode_worker = SpecDecodeWorker.create_worker(\n        scorer_worker=target_worker,\n        draft_worker_kwargs=draft_worker_kwargs,\n        disable_by_batch_size=speculative_config.\n        speculative_disable_by_batch_size,\n        draft_token_acceptance_method=speculative_config.\n        draft_token_acceptance_method,\n        typical_acceptance_sampler_posterior_threshold=speculative_config.\n        typical_acceptance_sampler_posterior_threshold,\n        typical_acceptance_sampler_posterior_alpha=speculative_config.\n        typical_acceptance_sampler_posterior_alpha,\n        disable_logprobs=speculative_config.disable_logprobs,\n        disable_log_stats=speculative_config.disable_log_stats,\n    )\n\n    return spec_decode_worker\n\n\nclass SpecDecodeWorker(LoraNotSupportedWorkerBase):\n    \"\"\"Worker which implements speculative decoding.\n\n    Speculative decoding reduces decoding per-token latency by using a proposal\n    method, such as a small draft model, to speculate ahead of a larger LLM. The\n    probabilities of the speculative tokens are then determined by the larger\n    LLM, after which some verification routine determines which (if any) of the\n    speculative tokens are accepted by the larger LLM.\n\n    See https://github.com/vllm-project/vllm/pull/2188 and\n    https://github.com/vllm-project/vllm/pull/3103 for more info.\n\n    The current implementation has the following limitations:\n    * Only draft-model proposal is implemented (contributions for more forms are\n        welcome!).\n    * Only top-1 proposal and scoring are implemented. Tree-attention is left as\n        future work.\n    * All sequences in a batch must have the same proposal length, or zero. This\n        can be improved by having per-sequence speculation in the future.\n    * The scoring forward pass is done without an MQA kernel, which is\n        suboptimal especially as the batch size, proposal length, and sequence\n        lengths grow. Contributions to add a MQA scoring are welcome once\n        correctness tests pass.\n        More info here https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit.\n    \"\"\"\n\n    @classmethod\n    def create_worker(\n        cls,\n        scorer_worker: Worker,\n        draft_worker_kwargs: Dict[str, Any],\n        disable_by_batch_size: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: float,\n        typical_acceptance_sampler_posterior_alpha: float,\n        disable_logprobs: bool,\n        disable_log_stats: bool,\n    ) -> \"SpecDecodeWorker\":\n\n        allow_zero_draft_token_step = True\n        ngram_prompt_lookup_max = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_max\"))\n        ngram_prompt_lookup_min = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_min\"))\n        if ngram_prompt_lookup_max > 0:\n            proposer_worker = NGramWorker(**draft_worker_kwargs)\n            proposer_worker.set_ngram_window_size(ngram_prompt_lookup_min,\n                                                  ngram_prompt_lookup_max)\n        else:\n            draft_parallel_config: ParallelConfig = draft_worker_kwargs[\n                'parallel_config']\n            draft_tp = draft_parallel_config.tensor_parallel_size\n            target_tp = scorer_worker.parallel_config.tensor_parallel_size\n\n            if draft_worker_kwargs[\n                    \"model_config\"].hf_config.model_type == \"mlp_speculator\":\n                proposer_worker = MLPSpeculatorWorker(**draft_worker_kwargs)\n            elif draft_worker_kwargs[\n                    \"model_config\"].hf_config.model_type == \"medusa\":\n                proposer_worker = MedusaWorker(**draft_worker_kwargs)\n            else:\n                if draft_tp == 1:\n                    draft_worker_kwargs[\n                        \"model_runner_cls\"] = TP1DraftModelRunner\n                else:\n                    allow_zero_draft_token_step = False\n                proposer_worker = MultiStepWorker(**draft_worker_kwargs)\n\n            proposer_worker = SmallerTpProposerWorker.maybe_wrap_worker(\n                proposer_worker, draft_tp, target_tp)\n\n        logger.info(\"Configuring SpecDecodeWorker with proposer=%s\",\n                    type(proposer_worker))\n\n        spec_decode_sampler: SpecDecodeBaseSampler = None\n        if draft_token_acceptance_method == \"rejection_sampler\":\n            spec_decode_sampler = RejectionSampler(\n                disable_bonus_tokens=False, )\n        elif draft_token_acceptance_method == \"typical_acceptance_sampler\":\n            spec_decode_sampler = TypicalAcceptanceSampler(\n                disable_bonus_tokens=False,\n                posterior_threshold=\\\n                    typical_acceptance_sampler_posterior_threshold,\n                posterior_alpha=typical_acceptance_sampler_posterior_alpha,\n            )\n        logger.info(\"Configuring SpecDecodeWorker with sampler=%s\",\n                    type(spec_decode_sampler))\n\n        return SpecDecodeWorker(\n            proposer_worker,\n            scorer_worker,\n            disable_logprobs=disable_logprobs,\n            disable_log_stats=disable_log_stats,\n            disable_by_batch_size=disable_by_batch_size,\n            spec_decode_sampler=spec_decode_sampler,\n            allow_zero_draft_token_step=allow_zero_draft_token_step)\n\n    def __init__(\n        self,\n        proposer_worker: ProposerWorkerBase,\n        scorer_worker: WorkerBase,\n        spec_decode_sampler: SpecDecodeBaseSampler,\n        disable_logprobs: bool = False,\n        disable_log_stats: bool = False,\n        metrics_collector: Optional[AsyncMetricsCollector] = None,\n        disable_by_batch_size: Optional[int] = None,\n        allow_zero_draft_token_step: Optional[bool] = True,\n    ):\n        \"\"\"\n        Create a SpecDecodeWorker.\n\n        Args:\n            proposer_worker: A worker that can produce speculative tokens for\n                sequences.\n            scorer_worker: A worker that produces probabilities of speculative\n                tokens according to some base model. Typically a vanilla vLLM\n                Worker.\n            spec_decode_sampler: A Torch module used to perform acceptance\n                sampling of the draft tokens in the verification step of\n                speculative decoding. Currently we support two different \n                types of sampler namely RejectionSampler and\n                TypicalAcceptanceSampler. 'spec_decode_sampler' is either an\n                instance of RejectionSampler or TypicalAcceptanceSampler.\n            disable_logprobs: If set to True, token log probabilities will\n                not be output in both the draft worker and the target worker.\n                If set to False, log probabilities will be output by both.\n            disable_log_stats: If set to True, disable periodic printing of\n                speculative stage times.\n            disable_by_batch_size: If the batch size is larger than this,\n                disable speculative decoding for new incoming requests.\n            metrics_collector: Helper class for collecting metrics; can be set\n                for testing purposes.\n            allow_zero_draft_token_step: whether to allow a step where the draft\n                model generates no draft token; should disallow when the tp of\n                draft model is larger than 1 (TODO: #5814)\n        \"\"\"\n        self.proposer_worker = proposer_worker\n        self.scorer_worker = scorer_worker\n        scorer_runner = getattr(self.scorer_worker, \"model_runner\", None)\n        self.generators = scorer_runner.get_generators(\n        ) if scorer_runner else None\n        self.disable_by_batch_size = disable_by_batch_size or float(\"inf\")\n        self.spec_decode_sampler = spec_decode_sampler\n        self._allow_zero_draft_token_step = allow_zero_draft_token_step\n        self._metrics = AsyncMetricsCollector(\n            self.spec_decode_sampler\n        ) if metrics_collector is None else metrics_collector\n        # Tracks the sequence IDs that received a bonus token ID in\n        # their last forward pass. Needed only if KV cache is being\n        # used for token generation such as in the case of MultiStepWorker.\n        self._seq_with_bonus_token_in_last_step: Set[int] = set()\n        # Tracks the currently active request ids and the sequence IDs\n        # corresponding to them\n        self._request_id_seq_id_mapping: Dict[str, Set[int]] = defaultdict(set)\n        # Tracks if the proposer worker uses the KV cache or not.\n\n        self.probs_dtype = self.spec_decode_sampler.probs_dtype\n        self.token_id_dtype = self.spec_decode_sampler.token_id_dtype\n        # Lazy initialization.\n        self.scorer: SpeculativeScorer\n\n        # Hidden states from target model to pass to proposer\n        # in the subsequent step.\n        self.previous_hidden_states: Optional[HiddenStates] = None\n        self._disable_logprobs = disable_logprobs\n        self._disable_log_stats = disable_log_stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def load_model(self, *args, **kwargs):\n        pass\n\n    def _configure_model_sampler_for_spec_decode(self):\n        \"\"\"Configure model sampler to emit GPU tensors. This allows spec decode\n        to keep data on device without transferring to CPU and serializing,\n        which significantly reduces overhead of sampling during verification.\n\n        NOTE(cade): This breaks abstraction boundaries pretty badly. The better\n        design is to have the \"move to CPU and serialize\" sampling decision be\n        done outside of the model/sampler; this way the \"last-mile\" worker\n        object which interfaces with the scheduler can serialize and incur the\n        performance hit as necessary. This allows us to run the worker several\n        iterations in a row without incurring the \"move to CPU and serialize\"\n        performance penalty.\n\n        Since this requires a large change to vLLM, we defer it to later and\n        temporarily accept this broken abstraction boundary.\n\n        NOTE(cade): This will require a special check if the proposer worker\n        does not have a sampler (e.g. ngram speculation).\n        \"\"\"\n        (self.scorer_worker.model_runner.model.sampler.include_gpu_probs_tensor\n         ) = True\n        (self.scorer_worker.model_runner.model.sampler.\n         should_modify_greedy_probs_inplace) = True\n        self.proposer_worker.set_include_gpu_probs_tensor()\n        self.proposer_worker.set_should_modify_greedy_probs_inplace()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"Initialize the cache engine of the scorer and proposer workers.\n        \"\"\"\n        self.scorer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                            num_cpu_blocks=num_cpu_blocks)\n        self.proposer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                              num_cpu_blocks=num_cpu_blocks)\n\n    @torch.inference_mode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @torch.inference_mode()\n    def start_worker_execution_loop(self) -> None:\n        \"\"\"Execute model loop to perform speculative decoding\n        in parallel worker.\"\"\"\n        while self._run_non_driver_rank():\n            pass\n\n    def _should_disable_all_speculation(\n            self, execute_model_req: ExecuteModelRequest) -> bool:\n        # When the batch size is too large, disable speculative decoding\n        # to stop trading off throughput for latency.\n        disable_all_speculation = (execute_model_req.running_queue_size >=\n                                   self.disable_by_batch_size)\n\n        return disable_all_speculation\n\n    def _maybe_disable_speculative_tokens(\n            self, disable_all_speculation: bool,\n            seq_group_metadata_list: List[SequenceGroupMetadata]) -> None:\n        if not disable_all_speculation:\n            return\n\n        for seq_group_metadata in seq_group_metadata_list:\n            # Once num_speculative_tokens is set to 0, the spec decode\n            # of this request will be disabled forever.\n            # TODO(comaniac): We currently store spec decoding specific\n            # state in the global data structure, but we should maintain\n            # this state within spec decode worker.\n            seq_group_metadata.num_speculative_tokens = 0\n\n    def _serialize_sampler_output_no_logprobs(\n            self, execute_model_req: ExecuteModelRequest,\n            sampler_output: SamplerOutput) -> SamplerOutput:\n        \"\"\"\n        Creates and returns a `SamplerOutput` with only the sampled token IDs \n        being serialized to CPU & populated in `CompletionSequenceGroupOutput`.\n        All other parameters in `CompletionSequenceGroupOutput` related to log \n        probabilities are skipped.\n\n        Args:\n            execute_model_req (ExecuteModelRequest): The model request that\n            was executed.\n            sampler_output (SamplerOutput): The output from the sampler with\n            only GPU tensors populated.\n\n        Returns:\n            SamplerOutput: A new `SamplerOutput` instance containing a list of \n            `CompletionSequenceGroupOutput` objects with only sampled token\n            IDs populated.\n        \"\"\"\n        seq_ids = get_all_seq_ids(execute_model_req.seq_group_metadata_list)\n        sampled_token_ids_list = sampler_output.sampled_token_ids.tolist()\n        completion_seq_group_output_list: List[\n            CompletionSequenceGroupOutput] = []\n        for index, seq_id in enumerate(seq_ids):\n            completion_seq_group_output_list.append(\n                create_sequence_group_output(\n                    token_id=sampled_token_ids_list[index][0],\n                    token_id_logprob_rank=-1,\n                    token_id_logprob=0.0,\n                    seq_id=seq_id,\n                    topk_token_ids=[],\n                    topk_logprobs=[],\n                ))\n        return SamplerOutput(outputs=completion_seq_group_output_list)\n\n    @nvtx_range(\"spec_decode_worker._run_no_spec\")\n    def _run_no_spec(self, execute_model_req: ExecuteModelRequest,\n                     skip_proposer: bool) -> List[SamplerOutput]:\n        \"\"\"Run a single generation step without any speculation. The input is\n        sent to the proposer and scorer model so that the KV cache is consistent\n        between the two. When skip_proposer is True, the proposer model is\n        not called, meaning that the kv-cache in proposer for requests is not\n        updated, so they cannot enable spec decode in the rest decoding.\n        \"\"\"\n        if not skip_proposer:\n            self.proposer_worker.execute_model(execute_model_req)\n\n        sampler_output = self.scorer_worker.execute_model(execute_model_req)\n        assert len(sampler_output) == 1\n        sampler_output = sampler_output[0]\n\n        # Store hidden states from target model execution.\n        hidden_states = sampler_output.hidden_states\n        if hidden_states is not None:\n            if self.previous_hidden_states is None:\n                self.previous_hidden_states = HiddenStates(\n                    execute_model_req.seq_group_metadata_list, hidden_states)\n            else:\n                self.previous_hidden_states.update(\n                    execute_model_req.seq_group_metadata_list, hidden_states)\n\n        sampler_output_to_return = (self._serialize_sampler_output_no_logprobs(\n            execute_model_req=execute_model_req, sampler_output=sampler_output)\n                                    if self._disable_logprobs else\n                                    sampler_output)\n\n        # Clear device tensors from sampler output. This reduces communication\n        # overhead when the engine runs in a different process than the workers.\n        sampler_output.sampled_token_probs = None\n        sampler_output.sampled_token_ids = None\n        sampler_output.logprobs = None\n        return [sampler_output_to_return]\n\n    def _run_non_driver_rank(self) -> bool:\n        \"\"\"Run proposer and verifier model in non-driver workers. This is used\n        for both speculation cases (num_lookahead_slots>0) and non-speculation\n        cases (e.g. prefill).\n\n        Returns True if there are remaining sequences to process.\n        \"\"\"\n        assert self.rank != self._driver_rank\n\n        data = broadcast_tensor_dict(src=self._driver_rank)\n        if not data:\n            return False\n        num_lookahead_slots = data[\"num_lookahead_slots\"]\n\n        # Even if num_lookahead_slots is zero, we want to run the proposer model\n        # as it may have KV.\n        #\n        # We run the proposer once per lookahead slot. In the future we should\n        # delegate how many times it runs to the proposer.\n        for _ in range(max(num_lookahead_slots, 1)):\n            self.proposer_worker.execute_model()\n\n        self.scorer_worker.execute_model()\n        return True\n\n    @nvtx_range(\"spec_decode_worker._run_speculative_decoding_step\")\n    def _run_speculative_decoding_step(\n            self, execute_model_req: ExecuteModelRequest,\n            num_lookahead_slots: int) -> List[SamplerOutput]:\n        \"\"\"Execute a single step of speculative decoding.\n\n        This invokes the proposer worker to get k speculative tokens for each\n        sequence, then scores each speculative token using the scoring worker.\n\n        Returns a list of SamplerOutput, each containing a single token per\n        sequence.\n        \"\"\"\n        assert num_lookahead_slots == execute_model_req.num_lookahead_slots\n\n        # Pass last hidden states from target model to proposer\n        execute_model_req.previous_hidden_states = self.previous_hidden_states\n        self.previous_hidden_states = None\n\n        with Timer() as proposal_timer:\n            # Generate proposals using draft worker.\n            proposals = self.proposer_worker.get_spec_proposals(\n                execute_model_req, self._seq_with_bonus_token_in_last_step)\n\n        if not self._allow_zero_draft_token_step and proposals.no_proposals:\n            #TODO: Fix it #5814\n            raise RuntimeError(\"Cannot handle cases where distributed draft \"\n                               \"workers generate no tokens\")\n\n        with Timer() as scoring_timer:\n            proposal_scores = self.scorer.score_proposals(\n                execute_model_req,\n                proposals,\n            )\n\n        with Timer() as verification_timer:\n            accepted_token_ids, target_logprobs = self._verify_tokens(\n                execute_model_req.seq_group_metadata_list, proposal_scores,\n                proposals, execute_model_req.num_lookahead_slots)\n\n        stage_times = (proposal_timer.elapsed_time_ms / num_lookahead_slots,\n                       scoring_timer.elapsed_time_ms,\n                       verification_timer.elapsed_time_ms)\n\n        return self._create_output_sampler_list(\n            execute_model_req.seq_group_metadata_list,\n            accepted_token_ids,\n            target_logprobs=target_logprobs,\n            k=execute_model_req.num_lookahead_slots,\n            stage_times=stage_times)\n\n    @nvtx_range(\"spec_decode_worker._verify_tokens\")\n    def _verify_tokens(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        proposal_scores: SpeculativeScores,\n        proposals: SpeculativeProposals,\n        max_proposal_len: int,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Determine which speculative tokens are accepted using the\n        probabilities of each token according to the proposer and scorer models.\n\n        Returns a tuple of Tensors, one for the accepted token ids and one for\n        the logprobs according to the scoring model.\n        \"\"\"\n        proposal_lens_list = proposals.proposal_lens.tolist()\n\n        # vLLM currently only supports proposal lens equal to zero or the batch\n        # proposal len. This adds some complexity (splitting the batch into spec\n        # and non spec sequences) and should be removed in the future. It can be\n        # done by supporting per-sequence proposal lens.\n        _, spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=False)\n        _, non_spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=True)\n        original_indices = spec_indices + non_spec_indices\n\n        # Get probabilities of target model, excluding bonus token.\n        proposal_verifier_probs = proposal_scores.probs[spec_indices, :-1]\n\n        # Get non-speculative sampled tokens from target model.\n        non_spec_token_ids = proposal_scores.token_ids[non_spec_indices]\n\n        # Get bonus tokens from target model.\n        bonus_token_ids = proposal_scores.token_ids[spec_indices, -1:]\n\n        # Get probabilities according to proposal method.\n        proposal_probs = proposals.proposal_probs[spec_indices]\n\n        # Get proposed tokens.\n        proposal_token_ids = proposals.proposal_token_ids[spec_indices]\n\n        # Sampler arguments\n        sampler_extra_kwargs: Dict[str, Any] = {}\n        if self.generators and isinstance(self.spec_decode_sampler,\n                                          SpecDecodeStochasticBaseSampler):\n            sampler_extra_kwargs[\"seeded_seqs\"] = {\n                idx: self.generators[sgm.request_id]\n                for idx, sgm in enumerate(seq_group_metadata_list)\n                if sgm.sampling_params.seed is not None\n            }\n\n        accepted_token_ids = self.spec_decode_sampler(\n            target_probs=proposal_verifier_probs,\n            bonus_token_ids=bonus_token_ids,\n            draft_probs=proposal_probs,\n            draft_token_ids=proposal_token_ids,\n            **sampler_extra_kwargs,\n        )\n\n        # Append output tokens from non-speculative sequences to\n        # the accepted token ids tensor.\n        non_spec_token_ids = non_spec_token_ids.expand(-1, max_proposal_len +\n                                                       1).clone()\n        non_spec_token_ids[:, 1:] = -1\n        accepted_token_ids = torch.cat(\n            [accepted_token_ids, non_spec_token_ids])\n        log\n```\n# Function Signature: def determine_num_available_blocks(self) -> Tuple[int, int]:\n# Natural Language Annotations: \"\"\"Determine the number of cache blocks to use.\n\n        This is done by profiling the scorer model (which is typically the\n        larger of the two). Then the total memory which would be used by the\n        scorer cache is divided evenly between the proposer and scorer model KV,\n        such that the number of blocks is equal in both KV caches.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef determine_num_available_blocks(self) ->", "  ```\ndef determine_num_available_blocks(self) ->", "  ```\ndef determine_num_available_blocks(self):\n"]}, "vllm-vllm/spec_decode/spec_decode_worker.py-execute_model": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/spec_decode/spec_decode_worker.py:\n```\nfrom collections import defaultdict\nfrom functools import cached_property\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\nimport torch\n\nfrom vllm.config import ParallelConfig, SpeculativeConfig\nfrom vllm.distributed.communication_op import broadcast_tensor_dict\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.rejection_sampler import RejectionSampler\nfrom vllm.model_executor.layers.spec_decode_base_sampler import (\n    SpecDecodeBaseSampler, SpecDecodeStochasticBaseSampler)\nfrom vllm.model_executor.layers.typical_acceptance_sampler import (\n    TypicalAcceptanceSampler)\nfrom vllm.sequence import (CompletionSequenceGroupOutput, ExecuteModelRequest,\n                           HiddenStates, SamplerOutput, SequenceGroupMetadata,\n                           get_all_seq_ids, get_all_seq_ids_and_request_ids)\nfrom vllm.spec_decode.batch_expansion import BatchExpansionTop1Scorer\nfrom vllm.spec_decode.draft_model_runner import TP1DraftModelRunner\nfrom vllm.spec_decode.interfaces import (SpeculativeProposals,\n                                         SpeculativeScorer, SpeculativeScores)\nfrom vllm.spec_decode.medusa_worker import MedusaWorker\nfrom vllm.spec_decode.metrics import AsyncMetricsCollector\nfrom vllm.spec_decode.mlp_speculator_worker import MLPSpeculatorWorker\nfrom vllm.spec_decode.multi_step_worker import MultiStepWorker\nfrom vllm.spec_decode.ngram_worker import NGramWorker\nfrom vllm.spec_decode.proposer_worker_base import ProposerWorkerBase\nfrom vllm.spec_decode.smaller_tp_proposer_worker import SmallerTpProposerWorker\nfrom vllm.spec_decode.target_model_runner import TargetModelRunner\nfrom vllm.spec_decode.util import (Timer, create_sequence_group_output,\n                                   get_all_num_logprobs,\n                                   get_sampled_token_logprobs, nvtx_range,\n                                   split_batch_by_proposal_len)\nfrom vllm.worker.worker import Worker\nfrom vllm.worker.worker_base import LoraNotSupportedWorkerBase, WorkerBase\n\nlogger = init_logger(__name__)\n\n\ndef create_spec_worker(*args, **kwargs) -> \"SpecDecodeWorker\":\n    \"\"\"Helper method that is the entrypoint for Executors which use\n    WorkerWrapper. It constructs a SpecDecodeWorker from the speculative config.\n    \"\"\"\n    assert \"speculative_config\" in kwargs\n    speculative_config: SpeculativeConfig = kwargs.get(\"speculative_config\")\n    assert speculative_config is not None\n\n    draft_worker_kwargs = kwargs.copy()\n\n    kwargs[\"model_runner_cls\"] = TargetModelRunner\n    target_worker = Worker(*args, **kwargs)\n    # Set the disable_logprobs variable in the TargetModelRunner instance\n    # as per its value specified in the SpeculativeConfig.\n    target_worker.model_runner.disable_logprobs =\\\n         speculative_config.disable_logprobs\n\n    # Override draft-model specific worker args.\n    draft_worker_kwargs.update(\n        model_config=speculative_config.draft_model_config,\n        parallel_config=speculative_config.draft_parallel_config,\n        ngram_prompt_lookup_max=speculative_config.ngram_prompt_lookup_max,\n        ngram_prompt_lookup_min=speculative_config.ngram_prompt_lookup_min,\n        # TODO allow draft-model specific load config.\n        #load_config=load_config,\n    )\n\n    spec_decode_worker = SpecDecodeWorker.create_worker(\n        scorer_worker=target_worker,\n        draft_worker_kwargs=draft_worker_kwargs,\n        disable_by_batch_size=speculative_config.\n        speculative_disable_by_batch_size,\n        draft_token_acceptance_method=speculative_config.\n        draft_token_acceptance_method,\n        typical_acceptance_sampler_posterior_threshold=speculative_config.\n        typical_acceptance_sampler_posterior_threshold,\n        typical_acceptance_sampler_posterior_alpha=speculative_config.\n        typical_acceptance_sampler_posterior_alpha,\n        disable_logprobs=speculative_config.disable_logprobs,\n        disable_log_stats=speculative_config.disable_log_stats,\n    )\n\n    return spec_decode_worker\n\n\nclass SpecDecodeWorker(LoraNotSupportedWorkerBase):\n    \"\"\"Worker which implements speculative decoding.\n\n    Speculative decoding reduces decoding per-token latency by using a proposal\n    method, such as a small draft model, to speculate ahead of a larger LLM. The\n    probabilities of the speculative tokens are then determined by the larger\n    LLM, after which some verification routine determines which (if any) of the\n    speculative tokens are accepted by the larger LLM.\n\n    See https://github.com/vllm-project/vllm/pull/2188 and\n    https://github.com/vllm-project/vllm/pull/3103 for more info.\n\n    The current implementation has the following limitations:\n    * Only draft-model proposal is implemented (contributions for more forms are\n        welcome!).\n    * Only top-1 proposal and scoring are implemented. Tree-attention is left as\n        future work.\n    * All sequences in a batch must have the same proposal length, or zero. This\n        can be improved by having per-sequence speculation in the future.\n    * The scoring forward pass is done without an MQA kernel, which is\n        suboptimal especially as the batch size, proposal length, and sequence\n        lengths grow. Contributions to add a MQA scoring are welcome once\n        correctness tests pass.\n        More info here https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit.\n    \"\"\"\n\n    @classmethod\n    def create_worker(\n        cls,\n        scorer_worker: Worker,\n        draft_worker_kwargs: Dict[str, Any],\n        disable_by_batch_size: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: float,\n        typical_acceptance_sampler_posterior_alpha: float,\n        disable_logprobs: bool,\n        disable_log_stats: bool,\n    ) -> \"SpecDecodeWorker\":\n\n        allow_zero_draft_token_step = True\n        ngram_prompt_lookup_max = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_max\"))\n        ngram_prompt_lookup_min = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_min\"))\n        if ngram_prompt_lookup_max > 0:\n            proposer_worker = NGramWorker(**draft_worker_kwargs)\n            proposer_worker.set_ngram_window_size(ngram_prompt_lookup_min,\n                                                  ngram_prompt_lookup_max)\n        else:\n            draft_parallel_config: ParallelConfig = draft_worker_kwargs[\n                'parallel_config']\n            draft_tp = draft_parallel_config.tensor_parallel_size\n            target_tp = scorer_worker.parallel_config.tensor_parallel_size\n\n            if draft_worker_kwargs[\n                    \"model_config\"].hf_config.model_type == \"mlp_speculator\":\n                proposer_worker = MLPSpeculatorWorker(**draft_worker_kwargs)\n            elif draft_worker_kwargs[\n                    \"model_config\"].hf_config.model_type == \"medusa\":\n                proposer_worker = MedusaWorker(**draft_worker_kwargs)\n            else:\n                if draft_tp == 1:\n                    draft_worker_kwargs[\n                        \"model_runner_cls\"] = TP1DraftModelRunner\n                else:\n                    allow_zero_draft_token_step = False\n                proposer_worker = MultiStepWorker(**draft_worker_kwargs)\n\n            proposer_worker = SmallerTpProposerWorker.maybe_wrap_worker(\n                proposer_worker, draft_tp, target_tp)\n\n        logger.info(\"Configuring SpecDecodeWorker with proposer=%s\",\n                    type(proposer_worker))\n\n        spec_decode_sampler: SpecDecodeBaseSampler = None\n        if draft_token_acceptance_method == \"rejection_sampler\":\n            spec_decode_sampler = RejectionSampler(\n                disable_bonus_tokens=False, )\n        elif draft_token_acceptance_method == \"typical_acceptance_sampler\":\n            spec_decode_sampler = TypicalAcceptanceSampler(\n                disable_bonus_tokens=False,\n                posterior_threshold=\\\n                    typical_acceptance_sampler_posterior_threshold,\n                posterior_alpha=typical_acceptance_sampler_posterior_alpha,\n            )\n        logger.info(\"Configuring SpecDecodeWorker with sampler=%s\",\n                    type(spec_decode_sampler))\n\n        return SpecDecodeWorker(\n            proposer_worker,\n            scorer_worker,\n            disable_logprobs=disable_logprobs,\n            disable_log_stats=disable_log_stats,\n            disable_by_batch_size=disable_by_batch_size,\n            spec_decode_sampler=spec_decode_sampler,\n            allow_zero_draft_token_step=allow_zero_draft_token_step)\n\n    def __init__(\n        self,\n        proposer_worker: ProposerWorkerBase,\n        scorer_worker: WorkerBase,\n        spec_decode_sampler: SpecDecodeBaseSampler,\n        disable_logprobs: bool = False,\n        disable_log_stats: bool = False,\n        metrics_collector: Optional[AsyncMetricsCollector] = None,\n        disable_by_batch_size: Optional[int] = None,\n        allow_zero_draft_token_step: Optional[bool] = True,\n    ):\n        \"\"\"\n        Create a SpecDecodeWorker.\n\n        Args:\n            proposer_worker: A worker that can produce speculative tokens for\n                sequences.\n            scorer_worker: A worker that produces probabilities of speculative\n                tokens according to some base model. Typically a vanilla vLLM\n                Worker.\n            spec_decode_sampler: A Torch module used to perform acceptance\n                sampling of the draft tokens in the verification step of\n                speculative decoding. Currently we support two different \n                types of sampler namely RejectionSampler and\n                TypicalAcceptanceSampler. 'spec_decode_sampler' is either an\n                instance of RejectionSampler or TypicalAcceptanceSampler.\n            disable_logprobs: If set to True, token log probabilities will\n                not be output in both the draft worker and the target worker.\n                If set to False, log probabilities will be output by both.\n            disable_log_stats: If set to True, disable periodic printing of\n                speculative stage times.\n            disable_by_batch_size: If the batch size is larger than this,\n                disable speculative decoding for new incoming requests.\n            metrics_collector: Helper class for collecting metrics; can be set\n                for testing purposes.\n            allow_zero_draft_token_step: whether to allow a step where the draft\n                model generates no draft token; should disallow when the tp of\n                draft model is larger than 1 (TODO: #5814)\n        \"\"\"\n        self.proposer_worker = proposer_worker\n        self.scorer_worker = scorer_worker\n        scorer_runner = getattr(self.scorer_worker, \"model_runner\", None)\n        self.generators = scorer_runner.get_generators(\n        ) if scorer_runner else None\n        self.disable_by_batch_size = disable_by_batch_size or float(\"inf\")\n        self.spec_decode_sampler = spec_decode_sampler\n        self._allow_zero_draft_token_step = allow_zero_draft_token_step\n        self._metrics = AsyncMetricsCollector(\n            self.spec_decode_sampler\n        ) if metrics_collector is None else metrics_collector\n        # Tracks the sequence IDs that received a bonus token ID in\n        # their last forward pass. Needed only if KV cache is being\n        # used for token generation such as in the case of MultiStepWorker.\n        self._seq_with_bonus_token_in_last_step: Set[int] = set()\n        # Tracks the currently active request ids and the sequence IDs\n        # corresponding to them\n        self._request_id_seq_id_mapping: Dict[str, Set[int]] = defaultdict(set)\n        # Tracks if the proposer worker uses the KV cache or not.\n\n        self.probs_dtype = self.spec_decode_sampler.probs_dtype\n        self.token_id_dtype = self.spec_decode_sampler.token_id_dtype\n        # Lazy initialization.\n        self.scorer: SpeculativeScorer\n\n        # Hidden states from target model to pass to proposer\n        # in the subsequent step.\n        self.previous_hidden_states: Optional[HiddenStates] = None\n        self._disable_logprobs = disable_logprobs\n        self._disable_log_stats = disable_log_stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def load_model(self, *args, **kwargs):\n        pass\n\n    def _configure_model_sampler_for_spec_decode(self):\n        \"\"\"Configure model sampler to emit GPU tensors. This allows spec decode\n        to keep data on device without transferring to CPU and serializing,\n        which significantly reduces overhead of sampling during verification.\n\n        NOTE(cade): This breaks abstraction boundaries pretty badly. The better\n        design is to have the \"move to CPU and serialize\" sampling decision be\n        done outside of the model/sampler; this way the \"last-mile\" worker\n        object which interfaces with the scheduler can serialize and incur the\n        performance hit as necessary. This allows us to run the worker several\n        iterations in a row without incurring the \"move to CPU and serialize\"\n        performance penalty.\n\n        Since this requires a large change to vLLM, we defer it to later and\n        temporarily accept this broken abstraction boundary.\n\n        NOTE(cade): This will require a special check if the proposer worker\n        does not have a sampler (e.g. ngram speculation).\n        \"\"\"\n        (self.scorer_worker.model_runner.model.sampler.include_gpu_probs_tensor\n         ) = True\n        (self.scorer_worker.model_runner.model.sampler.\n         should_modify_greedy_probs_inplace) = True\n        self.proposer_worker.set_include_gpu_probs_tensor()\n        self.proposer_worker.set_should_modify_greedy_probs_inplace()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"Initialize the cache engine of the scorer and proposer workers.\n        \"\"\"\n        self.scorer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                            num_cpu_blocks=num_cpu_blocks)\n        self.proposer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                              num_cpu_blocks=num_cpu_blocks)\n\n    @torch.inference_mode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @torch.inference_mode()\n    def start_worker_execution_loop(self) -> None:\n        \"\"\"Execute model loop to perform speculative decoding\n        in parallel worker.\"\"\"\n        while self._run_non_driver_rank():\n            pass\n\n    def _should_disable_all_speculation(\n            self, execute_model_req: ExecuteModelRequest) -> bool:\n        # When the batch size is too large, disable speculative decoding\n        # to stop trading off throughput for latency.\n        disable_all_speculation = (execute_model_req.running_queue_size >=\n                                   self.disable_by_batch_size)\n\n        return disable_all_speculation\n\n    def _maybe_disable_speculative_tokens(\n            self, disable_all_speculation: bool,\n            seq_group_metadata_list: List[SequenceGroupMetadata]) -> None:\n        if not disable_all_speculation:\n            return\n\n        for seq_group_metadata in seq_group_metadata_list:\n            # Once num_speculative_tokens is set to 0, the spec decode\n            # of this request will be disabled forever.\n            # TODO(comaniac): We currently store spec decoding specific\n            # state in the global data structure, but we should maintain\n            # this state within spec decode worker.\n            seq_group_metadata.num_speculative_tokens = 0\n\n    def _serialize_sampler_output_no_logprobs(\n            self, execute_model_req: ExecuteModelRequest,\n            sampler_output: SamplerOutput) -> SamplerOutput:\n        \"\"\"\n        Creates and returns a `SamplerOutput` with only the sampled token IDs \n        being serialized to CPU & populated in `CompletionSequenceGroupOutput`.\n        All other parameters in `CompletionSequenceGroupOutput` related to log \n        probabilities are skipped.\n\n        Args:\n            execute_model_req (ExecuteModelRequest): The model request that\n            was executed.\n            sampler_output (SamplerOutput): The output from the sampler with\n            only GPU tensors populated.\n\n        Returns:\n            SamplerOutput: A new `SamplerOutput` instance containing a list of \n            `CompletionSequenceGroupOutput` objects with only sampled token\n            IDs populated.\n        \"\"\"\n        seq_ids = get_all_seq_ids(execute_model_req.seq_group_metadata_list)\n        sampled_token_ids_list = sampler_output.sampled_token_ids.tolist()\n        completion_seq_group_output_list: List[\n            CompletionSequenceGroupOutput] = []\n        for index, seq_id in enumerate(seq_ids):\n            completion_seq_group_output_list.append(\n                create_sequence_group_output(\n                    token_id=sampled_token_ids_list[index][0],\n                    token_id_logprob_rank=-1,\n                    token_id_logprob=0.0,\n                    seq_id=seq_id,\n                    topk_token_ids=[],\n                    topk_logprobs=[],\n                ))\n        return SamplerOutput(outputs=completion_seq_group_output_list)\n\n    @nvtx_range(\"spec_decode_worker._run_no_spec\")\n    def _run_no_spec(self, execute_model_req: ExecuteModelRequest,\n                     skip_proposer: bool) -> List[SamplerOutput]:\n        \"\"\"Run a single generation step without any speculation. The input is\n        sent to the proposer and scorer model so that the KV cache is consistent\n        between the two. When skip_proposer is True, the proposer model is\n        not called, meaning that the kv-cache in proposer for requests is not\n        updated, so they cannot enable spec decode in the rest decoding.\n        \"\"\"\n        if not skip_proposer:\n            self.proposer_worker.execute_model(execute_model_req)\n\n        sampler_output = self.scorer_worker.execute_model(execute_model_req)\n        assert len(sampler_output) == 1\n        sampler_output = sampler_output[0]\n\n        # Store hidden states from target model execution.\n        hidden_states = sampler_output.hidden_states\n        if hidden_states is not None:\n            if self.previous_hidden_states is None:\n                self.previous_hidden_states = HiddenStates(\n                    execute_model_req.seq_group_metadata_list, hidden_states)\n            else:\n                self.previous_hidden_states.update(\n                    execute_model_req.seq_group_metadata_list, hidden_states)\n\n        sampler_output_to_return = (self._serialize_sampler_output_no_logprobs(\n            execute_model_req=execute_model_req, sampler_output=sampler_output)\n                                    if self._disable_logprobs else\n                                    sampler_output)\n\n        # Clear device tensors from sampler output. This reduces communication\n        # overhead when the engine runs in a different process than the workers.\n        sampler_output.sampled_token_probs = None\n        sampler_output.sampled_token_ids = None\n        sampler_output.logprobs = None\n        return [sampler_output_to_return]\n\n    def _run_non_driver_rank(self) -> bool:\n        \"\"\"Run proposer and verifier model in non-driver workers. This is used\n        for both speculation cases (num_lookahead_slots>0) and non-speculation\n        cases (e.g. prefill).\n\n        Returns True if there are remaining sequences to process.\n        \"\"\"\n        assert self.rank != self._driver_rank\n\n        data = broadcast_tensor_dict(src=self._driver_rank)\n        if not data:\n            return False\n        num_lookahead_slots = data[\"num_lookahead_slots\"]\n\n        # Even if num_lookahead_slots is zero, we want to run the proposer model\n        # as it may have KV.\n        #\n        # We run the proposer once per lookahead slot. In the future we should\n        # delegate how many times it runs to the proposer.\n        for _ in range(max(num_lookahead_slots, 1)):\n            self.proposer_worker.execute_model()\n\n        self.scorer_worker.execute_model()\n        return True\n\n    @nvtx_range(\"spec_decode_worker._run_speculative_decoding_step\")\n    def _run_speculative_decoding_step(\n            self, execute_model_req: ExecuteModelRequest,\n            num_lookahead_slots: int) -> List[SamplerOutput]:\n        \"\"\"Execute a single step of speculative decoding.\n\n        This invokes the proposer worker to get k speculative tokens for each\n        sequence, then scores each speculative token using the scoring worker.\n\n        Returns a list of SamplerOutput, each containing a single token per\n        sequence.\n        \"\"\"\n        assert num_lookahead_slots == execute_model_req.num_lookahead_slots\n\n        # Pass last hidden states from target model to proposer\n        execute_model_req.previous_hidden_states = self.previous_hidden_states\n        self.previous_hidden_states = None\n\n        with Timer() as proposal_timer:\n            # Generate proposals using draft worker.\n            proposals = self.proposer_worker.get_spec_proposals(\n                execute_model_req, self._seq_with_bonus_token_in_last_step)\n\n        if not self._allow_zero_draft_token_step and proposals.no_proposals:\n            #TODO: Fix it #5814\n            raise RuntimeError(\"Cannot handle cases where distributed draft \"\n                               \"workers generate no tokens\")\n\n        with Timer() as scoring_timer:\n            proposal_scores = self.scorer.score_proposals(\n                execute_model_req,\n                proposals,\n            )\n\n        with Timer() as verification_timer:\n            accepted_token_ids, target_logprobs = self._verify_tokens(\n                execute_model_req.seq_group_metadata_list, proposal_scores,\n                proposals, execute_model_req.num_lookahead_slots)\n\n        stage_times = (proposal_timer.elapsed_time_ms / num_lookahead_slots,\n                       scoring_timer.elapsed_time_ms,\n                       verification_timer.elapsed_time_ms)\n\n        return self._create_output_sampler_list(\n            execute_model_req.seq_group_metadata_list,\n            accepted_token_ids,\n            target_logprobs=target_logprobs,\n            k=execute_model_req.num_lookahead_slots,\n            stage_times=stage_times)\n\n    @nvtx_range(\"spec_decode_worker._verify_tokens\")\n    def _verify_tokens(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        proposal_scores: SpeculativeScores,\n        proposals: SpeculativeProposals,\n        max_proposal_len: int,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Determine which speculative tokens are accepted using the\n        probabilities of each token according to the proposer and scorer models.\n\n        Returns a tuple of Tensors, one for the accepted token ids and one for\n        the logprobs according to the scoring model.\n        \"\"\"\n        proposal_lens_list = proposals.proposal_lens.tolist()\n\n        # vLLM currently only supports proposal lens equal to zero or the batch\n        # proposal len. This adds some complexity (splitting the batch into spec\n        # and non spec sequences) and should be removed in the future. It can be\n        # done by supporting per-sequence proposal lens.\n        _, spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=False)\n        _, non_spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=True)\n        original_indices = spec_indices + non_spec_indices\n\n        # Get probabilities of target model, excluding bonus token.\n        proposal_verifier_probs = proposal_scores.probs[spec_indices, :-1]\n\n        # Get non-speculative sampled tokens from target model.\n        non_spec_token_ids = proposal_scores.token_ids[non_spec_indices]\n\n        # Get bonus tokens from target model.\n        bonus_token_ids = proposal_scores.token_ids[spec_indices, -1:]\n\n        # Get probabilities according to proposal method.\n        proposal_probs = proposals.proposal_probs[spec_indices]\n\n        # Get proposed tokens.\n        proposal_token_ids = proposals.proposal_token_ids[spec_indices]\n\n        # Sampler arguments\n        sampler_extra_kwargs: Dict[str, Any] = {}\n        if self.generators and isinstance(self.spec_decode_sampler,\n                                          SpecDecodeStochasticBaseSampler):\n            sampler_extra_kwargs[\"seeded_seqs\"] = {\n                idx: self.generators[sgm.request_id]\n                for idx, sgm in enumerate(seq_group_metadata_list)\n                if sgm.sampling_params.seed is not None\n            }\n\n        accepted_token_ids = self.spec_decode_sampler(\n            target_probs=proposal_verifier_probs,\n            bonus_token_ids=bonus_token_ids,\n            draft_probs=proposal_probs,\n            draft_token_ids=proposal_token_ids,\n            **sampler_extra_kwargs,\n        )\n\n        # Append output tokens from non-speculative sequences to\n        # the accepted token ids tensor.\n        non_spec_token_ids = non_spec_token_ids.expand(-1, max_proposal_len +\n                                                       1).clone()\n        non_spec_token_ids[:, 1:] = -1\n        accepted_token_ids = torch.cat(\n            [accepted_token_ids, non_spec_token_ids])\n        log\n```\n# Function Signature: def execute_model(\n# Natural Language Annotations: \"\"\"Perform speculative decoding on the input batch.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef execute_model(\n    self,\n    execute_", "  ```\ndef execute_model(self, execute_model_req:", "  ```\ndef execute_model(\n    self,\n    execute_"]}, "vllm-vllm/spec_decode/spec_decode_worker.py-_create_output_sampler_list": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/spec_decode/spec_decode_worker.py:\n```\nfrom collections import defaultdict\nfrom functools import cached_property\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\nimport torch\n\nfrom vllm.config import ParallelConfig, SpeculativeConfig\nfrom vllm.distributed.communication_op import broadcast_tensor_dict\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.rejection_sampler import RejectionSampler\nfrom vllm.model_executor.layers.spec_decode_base_sampler import (\n    SpecDecodeBaseSampler, SpecDecodeStochasticBaseSampler)\nfrom vllm.model_executor.layers.typical_acceptance_sampler import (\n    TypicalAcceptanceSampler)\nfrom vllm.sequence import (CompletionSequenceGroupOutput, ExecuteModelRequest,\n                           HiddenStates, SamplerOutput, SequenceGroupMetadata,\n                           get_all_seq_ids, get_all_seq_ids_and_request_ids)\nfrom vllm.spec_decode.batch_expansion import BatchExpansionTop1Scorer\nfrom vllm.spec_decode.draft_model_runner import TP1DraftModelRunner\nfrom vllm.spec_decode.interfaces import (SpeculativeProposals,\n                                         SpeculativeScorer, SpeculativeScores)\nfrom vllm.spec_decode.medusa_worker import MedusaWorker\nfrom vllm.spec_decode.metrics import AsyncMetricsCollector\nfrom vllm.spec_decode.mlp_speculator_worker import MLPSpeculatorWorker\nfrom vllm.spec_decode.multi_step_worker import MultiStepWorker\nfrom vllm.spec_decode.ngram_worker import NGramWorker\nfrom vllm.spec_decode.proposer_worker_base import ProposerWorkerBase\nfrom vllm.spec_decode.smaller_tp_proposer_worker import SmallerTpProposerWorker\nfrom vllm.spec_decode.target_model_runner import TargetModelRunner\nfrom vllm.spec_decode.util import (Timer, create_sequence_group_output,\n                                   get_all_num_logprobs,\n                                   get_sampled_token_logprobs, nvtx_range,\n                                   split_batch_by_proposal_len)\nfrom vllm.worker.worker import Worker\nfrom vllm.worker.worker_base import LoraNotSupportedWorkerBase, WorkerBase\n\nlogger = init_logger(__name__)\n\n\ndef create_spec_worker(*args, **kwargs) -> \"SpecDecodeWorker\":\n    \"\"\"Helper method that is the entrypoint for Executors which use\n    WorkerWrapper. It constructs a SpecDecodeWorker from the speculative config.\n    \"\"\"\n    assert \"speculative_config\" in kwargs\n    speculative_config: SpeculativeConfig = kwargs.get(\"speculative_config\")\n    assert speculative_config is not None\n\n    draft_worker_kwargs = kwargs.copy()\n\n    kwargs[\"model_runner_cls\"] = TargetModelRunner\n    target_worker = Worker(*args, **kwargs)\n    # Set the disable_logprobs variable in the TargetModelRunner instance\n    # as per its value specified in the SpeculativeConfig.\n    target_worker.model_runner.disable_logprobs =\\\n         speculative_config.disable_logprobs\n\n    # Override draft-model specific worker args.\n    draft_worker_kwargs.update(\n        model_config=speculative_config.draft_model_config,\n        parallel_config=speculative_config.draft_parallel_config,\n        ngram_prompt_lookup_max=speculative_config.ngram_prompt_lookup_max,\n        ngram_prompt_lookup_min=speculative_config.ngram_prompt_lookup_min,\n        # TODO allow draft-model specific load config.\n        #load_config=load_config,\n    )\n\n    spec_decode_worker = SpecDecodeWorker.create_worker(\n        scorer_worker=target_worker,\n        draft_worker_kwargs=draft_worker_kwargs,\n        disable_by_batch_size=speculative_config.\n        speculative_disable_by_batch_size,\n        draft_token_acceptance_method=speculative_config.\n        draft_token_acceptance_method,\n        typical_acceptance_sampler_posterior_threshold=speculative_config.\n        typical_acceptance_sampler_posterior_threshold,\n        typical_acceptance_sampler_posterior_alpha=speculative_config.\n        typical_acceptance_sampler_posterior_alpha,\n        disable_logprobs=speculative_config.disable_logprobs,\n        disable_log_stats=speculative_config.disable_log_stats,\n    )\n\n    return spec_decode_worker\n\n\nclass SpecDecodeWorker(LoraNotSupportedWorkerBase):\n    \"\"\"Worker which implements speculative decoding.\n\n    Speculative decoding reduces decoding per-token latency by using a proposal\n    method, such as a small draft model, to speculate ahead of a larger LLM. The\n    probabilities of the speculative tokens are then determined by the larger\n    LLM, after which some verification routine determines which (if any) of the\n    speculative tokens are accepted by the larger LLM.\n\n    See https://github.com/vllm-project/vllm/pull/2188 and\n    https://github.com/vllm-project/vllm/pull/3103 for more info.\n\n    The current implementation has the following limitations:\n    * Only draft-model proposal is implemented (contributions for more forms are\n        welcome!).\n    * Only top-1 proposal and scoring are implemented. Tree-attention is left as\n        future work.\n    * All sequences in a batch must have the same proposal length, or zero. This\n        can be improved by having per-sequence speculation in the future.\n    * The scoring forward pass is done without an MQA kernel, which is\n        suboptimal especially as the batch size, proposal length, and sequence\n        lengths grow. Contributions to add a MQA scoring are welcome once\n        correctness tests pass.\n        More info here https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit.\n    \"\"\"\n\n    @classmethod\n    def create_worker(\n        cls,\n        scorer_worker: Worker,\n        draft_worker_kwargs: Dict[str, Any],\n        disable_by_batch_size: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: float,\n        typical_acceptance_sampler_posterior_alpha: float,\n        disable_logprobs: bool,\n        disable_log_stats: bool,\n    ) -> \"SpecDecodeWorker\":\n\n        allow_zero_draft_token_step = True\n        ngram_prompt_lookup_max = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_max\"))\n        ngram_prompt_lookup_min = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_min\"))\n        if ngram_prompt_lookup_max > 0:\n            proposer_worker = NGramWorker(**draft_worker_kwargs)\n            proposer_worker.set_ngram_window_size(ngram_prompt_lookup_min,\n                                                  ngram_prompt_lookup_max)\n        else:\n            draft_parallel_config: ParallelConfig = draft_worker_kwargs[\n                'parallel_config']\n            draft_tp = draft_parallel_config.tensor_parallel_size\n            target_tp = scorer_worker.parallel_config.tensor_parallel_size\n\n            if draft_worker_kwargs[\n                    \"model_config\"].hf_config.model_type == \"mlp_speculator\":\n                proposer_worker = MLPSpeculatorWorker(**draft_worker_kwargs)\n            elif draft_worker_kwargs[\n                    \"model_config\"].hf_config.model_type == \"medusa\":\n                proposer_worker = MedusaWorker(**draft_worker_kwargs)\n            else:\n                if draft_tp == 1:\n                    draft_worker_kwargs[\n                        \"model_runner_cls\"] = TP1DraftModelRunner\n                else:\n                    allow_zero_draft_token_step = False\n                proposer_worker = MultiStepWorker(**draft_worker_kwargs)\n\n            proposer_worker = SmallerTpProposerWorker.maybe_wrap_worker(\n                proposer_worker, draft_tp, target_tp)\n\n        logger.info(\"Configuring SpecDecodeWorker with proposer=%s\",\n                    type(proposer_worker))\n\n        spec_decode_sampler: SpecDecodeBaseSampler = None\n        if draft_token_acceptance_method == \"rejection_sampler\":\n            spec_decode_sampler = RejectionSampler(\n                disable_bonus_tokens=False, )\n        elif draft_token_acceptance_method == \"typical_acceptance_sampler\":\n            spec_decode_sampler = TypicalAcceptanceSampler(\n                disable_bonus_tokens=False,\n                posterior_threshold=\\\n                    typical_acceptance_sampler_posterior_threshold,\n                posterior_alpha=typical_acceptance_sampler_posterior_alpha,\n            )\n        logger.info(\"Configuring SpecDecodeWorker with sampler=%s\",\n                    type(spec_decode_sampler))\n\n        return SpecDecodeWorker(\n            proposer_worker,\n            scorer_worker,\n            disable_logprobs=disable_logprobs,\n            disable_log_stats=disable_log_stats,\n            disable_by_batch_size=disable_by_batch_size,\n            spec_decode_sampler=spec_decode_sampler,\n            allow_zero_draft_token_step=allow_zero_draft_token_step)\n\n    def __init__(\n        self,\n        proposer_worker: ProposerWorkerBase,\n        scorer_worker: WorkerBase,\n        spec_decode_sampler: SpecDecodeBaseSampler,\n        disable_logprobs: bool = False,\n        disable_log_stats: bool = False,\n        metrics_collector: Optional[AsyncMetricsCollector] = None,\n        disable_by_batch_size: Optional[int] = None,\n        allow_zero_draft_token_step: Optional[bool] = True,\n    ):\n        \"\"\"\n        Create a SpecDecodeWorker.\n\n        Args:\n            proposer_worker: A worker that can produce speculative tokens for\n                sequences.\n            scorer_worker: A worker that produces probabilities of speculative\n                tokens according to some base model. Typically a vanilla vLLM\n                Worker.\n            spec_decode_sampler: A Torch module used to perform acceptance\n                sampling of the draft tokens in the verification step of\n                speculative decoding. Currently we support two different \n                types of sampler namely RejectionSampler and\n                TypicalAcceptanceSampler. 'spec_decode_sampler' is either an\n                instance of RejectionSampler or TypicalAcceptanceSampler.\n            disable_logprobs: If set to True, token log probabilities will\n                not be output in both the draft worker and the target worker.\n                If set to False, log probabilities will be output by both.\n            disable_log_stats: If set to True, disable periodic printing of\n                speculative stage times.\n            disable_by_batch_size: If the batch size is larger than this,\n                disable speculative decoding for new incoming requests.\n            metrics_collector: Helper class for collecting metrics; can be set\n                for testing purposes.\n            allow_zero_draft_token_step: whether to allow a step where the draft\n                model generates no draft token; should disallow when the tp of\n                draft model is larger than 1 (TODO: #5814)\n        \"\"\"\n        self.proposer_worker = proposer_worker\n        self.scorer_worker = scorer_worker\n        scorer_runner = getattr(self.scorer_worker, \"model_runner\", None)\n        self.generators = scorer_runner.get_generators(\n        ) if scorer_runner else None\n        self.disable_by_batch_size = disable_by_batch_size or float(\"inf\")\n        self.spec_decode_sampler = spec_decode_sampler\n        self._allow_zero_draft_token_step = allow_zero_draft_token_step\n        self._metrics = AsyncMetricsCollector(\n            self.spec_decode_sampler\n        ) if metrics_collector is None else metrics_collector\n        # Tracks the sequence IDs that received a bonus token ID in\n        # their last forward pass. Needed only if KV cache is being\n        # used for token generation such as in the case of MultiStepWorker.\n        self._seq_with_bonus_token_in_last_step: Set[int] = set()\n        # Tracks the currently active request ids and the sequence IDs\n        # corresponding to them\n        self._request_id_seq_id_mapping: Dict[str, Set[int]] = defaultdict(set)\n        # Tracks if the proposer worker uses the KV cache or not.\n\n        self.probs_dtype = self.spec_decode_sampler.probs_dtype\n        self.token_id_dtype = self.spec_decode_sampler.token_id_dtype\n        # Lazy initialization.\n        self.scorer: SpeculativeScorer\n\n        # Hidden states from target model to pass to proposer\n        # in the subsequent step.\n        self.previous_hidden_states: Optional[HiddenStates] = None\n        self._disable_logprobs = disable_logprobs\n        self._disable_log_stats = disable_log_stats\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def load_model(self, *args, **kwargs):\n        pass\n\n    def _configure_model_sampler_for_spec_decode(self):\n        \"\"\"Configure model sampler to emit GPU tensors. This allows spec decode\n        to keep data on device without transferring to CPU and serializing,\n        which significantly reduces overhead of sampling during verification.\n\n        NOTE(cade): This breaks abstraction boundaries pretty badly. The better\n        design is to have the \"move to CPU and serialize\" sampling decision be\n        done outside of the model/sampler; this way the \"last-mile\" worker\n        object which interfaces with the scheduler can serialize and incur the\n        performance hit as necessary. This allows us to run the worker several\n        iterations in a row without incurring the \"move to CPU and serialize\"\n        performance penalty.\n\n        Since this requires a large change to vLLM, we defer it to later and\n        temporarily accept this broken abstraction boundary.\n\n        NOTE(cade): This will require a special check if the proposer worker\n        does not have a sampler (e.g. ngram speculation).\n        \"\"\"\n        (self.scorer_worker.model_runner.model.sampler.include_gpu_probs_tensor\n         ) = True\n        (self.scorer_worker.model_runner.model.sampler.\n         should_modify_greedy_probs_inplace) = True\n        self.proposer_worker.set_include_gpu_probs_tensor()\n        self.proposer_worker.set_should_modify_greedy_probs_inplace()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"Initialize the cache engine of the scorer and proposer workers.\n        \"\"\"\n        self.scorer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                            num_cpu_blocks=num_cpu_blocks)\n        self.proposer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                              num_cpu_blocks=num_cpu_blocks)\n\n    @torch.inference_mode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @torch.inference_mode()\n    def start_worker_execution_loop(self) -> None:\n        \"\"\"Execute model loop to perform speculative decoding\n        in parallel worker.\"\"\"\n        while self._run_non_driver_rank():\n            pass\n\n    def _should_disable_all_speculation(\n            self, execute_model_req: ExecuteModelRequest) -> bool:\n        # When the batch size is too large, disable speculative decoding\n        # to stop trading off throughput for latency.\n        disable_all_speculation = (execute_model_req.running_queue_size >=\n                                   self.disable_by_batch_size)\n\n        return disable_all_speculation\n\n    def _maybe_disable_speculative_tokens(\n            self, disable_all_speculation: bool,\n            seq_group_metadata_list: List[SequenceGroupMetadata]) -> None:\n        if not disable_all_speculation:\n            return\n\n        for seq_group_metadata in seq_group_metadata_list:\n            # Once num_speculative_tokens is set to 0, the spec decode\n            # of this request will be disabled forever.\n            # TODO(comaniac): We currently store spec decoding specific\n            # state in the global data structure, but we should maintain\n            # this state within spec decode worker.\n            seq_group_metadata.num_speculative_tokens = 0\n\n    def _serialize_sampler_output_no_logprobs(\n            self, execute_model_req: ExecuteModelRequest,\n            sampler_output: SamplerOutput) -> SamplerOutput:\n        \"\"\"\n        Creates and returns a `SamplerOutput` with only the sampled token IDs \n        being serialized to CPU & populated in `CompletionSequenceGroupOutput`.\n        All other parameters in `CompletionSequenceGroupOutput` related to log \n        probabilities are skipped.\n\n        Args:\n            execute_model_req (ExecuteModelRequest): The model request that\n            was executed.\n            sampler_output (SamplerOutput): The output from the sampler with\n            only GPU tensors populated.\n\n        Returns:\n            SamplerOutput: A new `SamplerOutput` instance containing a list of \n            `CompletionSequenceGroupOutput` objects with only sampled token\n            IDs populated.\n        \"\"\"\n        seq_ids = get_all_seq_ids(execute_model_req.seq_group_metadata_list)\n        sampled_token_ids_list = sampler_output.sampled_token_ids.tolist()\n        completion_seq_group_output_list: List[\n            CompletionSequenceGroupOutput] = []\n        for index, seq_id in enumerate(seq_ids):\n            completion_seq_group_output_list.append(\n                create_sequence_group_output(\n                    token_id=sampled_token_ids_list[index][0],\n                    token_id_logprob_rank=-1,\n                    token_id_logprob=0.0,\n                    seq_id=seq_id,\n                    topk_token_ids=[],\n                    topk_logprobs=[],\n                ))\n        return SamplerOutput(outputs=completion_seq_group_output_list)\n\n    @nvtx_range(\"spec_decode_worker._run_no_spec\")\n    def _run_no_spec(self, execute_model_req: ExecuteModelRequest,\n                     skip_proposer: bool) -> List[SamplerOutput]:\n        \"\"\"Run a single generation step without any speculation. The input is\n        sent to the proposer and scorer model so that the KV cache is consistent\n        between the two. When skip_proposer is True, the proposer model is\n        not called, meaning that the kv-cache in proposer for requests is not\n        updated, so they cannot enable spec decode in the rest decoding.\n        \"\"\"\n        if not skip_proposer:\n            self.proposer_worker.execute_model(execute_model_req)\n\n        sampler_output = self.scorer_worker.execute_model(execute_model_req)\n        assert len(sampler_output) == 1\n        sampler_output = sampler_output[0]\n\n        # Store hidden states from target model execution.\n        hidden_states = sampler_output.hidden_states\n        if hidden_states is not None:\n            if self.previous_hidden_states is None:\n                self.previous_hidden_states = HiddenStates(\n                    execute_model_req.seq_group_metadata_list, hidden_states)\n            else:\n                self.previous_hidden_states.update(\n                    execute_model_req.seq_group_metadata_list, hidden_states)\n\n        sampler_output_to_return = (self._serialize_sampler_output_no_logprobs(\n            execute_model_req=execute_model_req, sampler_output=sampler_output)\n                                    if self._disable_logprobs else\n                                    sampler_output)\n\n        # Clear device tensors from sampler output. This reduces communication\n        # overhead when the engine runs in a different process than the workers.\n        sampler_output.sampled_token_probs = None\n        sampler_output.sampled_token_ids = None\n        sampler_output.logprobs = None\n        return [sampler_output_to_return]\n\n    def _run_non_driver_rank(self) -> bool:\n        \"\"\"Run proposer and verifier model in non-driver workers. This is used\n        for both speculation cases (num_lookahead_slots>0) and non-speculation\n        cases (e.g. prefill).\n\n        Returns True if there are remaining sequences to process.\n        \"\"\"\n        assert self.rank != self._driver_rank\n\n        data = broadcast_tensor_dict(src=self._driver_rank)\n        if not data:\n            return False\n        num_lookahead_slots = data[\"num_lookahead_slots\"]\n\n        # Even if num_lookahead_slots is zero, we want to run the proposer model\n        # as it may have KV.\n        #\n        # We run the proposer once per lookahead slot. In the future we should\n        # delegate how many times it runs to the proposer.\n        for _ in range(max(num_lookahead_slots, 1)):\n            self.proposer_worker.execute_model()\n\n        self.scorer_worker.execute_model()\n        return True\n\n    @nvtx_range(\"spec_decode_worker._run_speculative_decoding_step\")\n    def _run_speculative_decoding_step(\n            self, execute_model_req: ExecuteModelRequest,\n            num_lookahead_slots: int) -> List[SamplerOutput]:\n        \"\"\"Execute a single step of speculative decoding.\n\n        This invokes the proposer worker to get k speculative tokens for each\n        sequence, then scores each speculative token using the scoring worker.\n\n        Returns a list of SamplerOutput, each containing a single token per\n        sequence.\n        \"\"\"\n        assert num_lookahead_slots == execute_model_req.num_lookahead_slots\n\n        # Pass last hidden states from target model to proposer\n        execute_model_req.previous_hidden_states = self.previous_hidden_states\n        self.previous_hidden_states = None\n\n        with Timer() as proposal_timer:\n            # Generate proposals using draft worker.\n            proposals = self.proposer_worker.get_spec_proposals(\n                execute_model_req, self._seq_with_bonus_token_in_last_step)\n\n        if not self._allow_zero_draft_token_step and proposals.no_proposals:\n            #TODO: Fix it #5814\n            raise RuntimeError(\"Cannot handle cases where distributed draft \"\n                               \"workers generate no tokens\")\n\n        with Timer() as scoring_timer:\n            proposal_scores = self.scorer.score_proposals(\n                execute_model_req,\n                proposals,\n            )\n\n        with Timer() as verification_timer:\n            accepted_token_ids, target_logprobs = self._verify_tokens(\n                execute_model_req.seq_group_metadata_list, proposal_scores,\n                proposals, execute_model_req.num_lookahead_slots)\n\n        stage_times = (proposal_timer.elapsed_time_ms / num_lookahead_slots,\n                       scoring_timer.elapsed_time_ms,\n                       verification_timer.elapsed_time_ms)\n\n        return self._create_output_sampler_list(\n            execute_model_req.seq_group_metadata_list,\n            accepted_token_ids,\n            target_logprobs=target_logprobs,\n            k=execute_model_req.num_lookahead_slots,\n            stage_times=stage_times)\n\n    @nvtx_range(\"spec_decode_worker._verify_tokens\")\n    def _verify_tokens(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        proposal_scores: SpeculativeScores,\n        proposals: SpeculativeProposals,\n        max_proposal_len: int,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Determine which speculative tokens are accepted using the\n        probabilities of each token according to the proposer and scorer models.\n\n        Returns a tuple of Tensors, one for the accepted token ids and one for\n        the logprobs according to the scoring model.\n        \"\"\"\n        proposal_lens_list = proposals.proposal_lens.tolist()\n\n        # vLLM currently only supports proposal lens equal to zero or the batch\n        # proposal len. This adds some complexity (splitting the batch into spec\n        # and non spec sequences) and should be removed in the future. It can be\n        # done by supporting per-sequence proposal lens.\n        _, spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=False)\n        _, non_spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=True)\n        original_indices = spec_indices + non_spec_indices\n\n        # Get probabilities of target model, excluding bonus token.\n        proposal_verifier_probs = proposal_scores.probs[spec_indices, :-1]\n\n        # Get non-speculative sampled tokens from target model.\n        non_spec_token_ids = proposal_scores.token_ids[non_spec_indices]\n\n        # Get bonus tokens from target model.\n        bonus_token_ids = proposal_scores.token_ids[spec_indices, -1:]\n\n        # Get probabilities according to proposal method.\n        proposal_probs = proposals.proposal_probs[spec_indices]\n\n        # Get proposed tokens.\n        proposal_token_ids = proposals.proposal_token_ids[spec_indices]\n\n        # Sampler arguments\n        sampler_extra_kwargs: Dict[str, Any] = {}\n        if self.generators and isinstance(self.spec_decode_sampler,\n                                          SpecDecodeStochasticBaseSampler):\n            sampler_extra_kwargs[\"seeded_seqs\"] = {\n                idx: self.generators[sgm.request_id]\n                for idx, sgm in enumerate(seq_group_metadata_list)\n                if sgm.sampling_params.seed is not None\n            }\n\n        accepted_token_ids = self.spec_decode_sampler(\n            target_probs=proposal_verifier_probs,\n            bonus_token_ids=bonus_token_ids,\n            draft_probs=proposal_probs,\n            draft_token_ids=proposal_token_ids,\n            **sampler_extra_kwargs,\n        )\n\n        # Append output tokens from non-speculative sequences to\n        # the accepted token ids tensor.\n        non_spec_token_ids = non_spec_token_ids.expand(-1, max_proposal_len +\n                                                       1).clone()\n        non_spec_token_ids[:, 1:] = -1\n        accepted_token_ids = torch.cat(\n            [accepted_token_ids, non_spec_token_ids])\n        log\n```\n# Function Signature: def _create_output_sampler_list(\n# Natural Language Annotations: \"\"\"Given the accepted token ids, create a list of SamplerOutput.\n\n        The output is padded with -1 tokens such that each sequence has\n        the same number of outputs.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _create_output_sampler_list(self,", "  ```\ndef _create_output_sampler_list(\n   ", "  ```\ndef _create_output_sampler_list(\n       "]}, "vllm-vllm/spec_decode/multi_step_worker.py-sampler_output": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/spec_decode/multi_step_worker.py:\n```\nimport copy\nimport weakref\nfrom typing import Dict, List, Set, Tuple\n\nimport torch\n\nfrom vllm.sequence import (ExecuteModelRequest, SamplerOutput, SequenceData,\n                           SequenceGroupMetadata)\nfrom vllm.spec_decode.draft_model_runner import TP1DraftModelRunner\nfrom vllm.spec_decode.interfaces import (SpeculativeProposals,\n                                         SpeculativeProposer)\nfrom vllm.spec_decode.proposer_worker_base import ProposerWorkerBase\nfrom vllm.spec_decode.top1_proposer import Top1Proposer\nfrom vllm.worker.worker import Worker\n\n\nclass MultiStepWorker(Worker, ProposerWorkerBase):\n    \"\"\"The MultiStepWorker is equivalent to a Worker except that it allows\n    multiple forward passes in a single call, assuming the scheduler has\n    allocated enough space to store the additional KV. This reduces overhead\n    by invoking the scheduler less.\n\n    The MultiStepWorker does not support cache swap operations, or beam search.\n    Cache swap operations do not require large modifications. On the other hand,\n    beam search requires memory allocations during sequence forks and thus\n    requires more thought for MultiStepWorker support.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Lazy initialization list.\n        self._proposer: SpeculativeProposer\n\n    def init_device(self) -> None:\n        super().init_device()\n\n        self._proposer = Top1Proposer(\n            weakref.proxy(self),  # type: ignore[arg-type]\n            self.device,\n            self.vocab_size,\n            max_proposal_len=self.max_model_len,\n        )\n\n    def set_include_gpu_probs_tensor(self) -> None:\n        # Need include_gpu_probs_tensor for MultiStepWorker\n        self.model_runner.model.sampler.include_gpu_probs_tensor = True\n\n    def set_should_modify_greedy_probs_inplace(self) -> None:\n        self.model_runner.model.sampler.should_modify_greedy_probs_inplace = (\n            True)\n\n    @torch.inference_mode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @staticmethod\n    def _expand_execute_model_request(\n        execute_model_req: ExecuteModelRequest,\n        seq_with_bonus_token_in_last_step: set,\n    ) -> Tuple[ExecuteModelRequest, List[int]]:\n        \"\"\"\n        Expands the execute model request based on sequences with bonus\n        tokens.\n\n        For each sequence with a bonus token, this method creates a new\n        sequence without the bonus token and adds it to the execute model\n        request. The original sequence groups are also retained. The indices\n        of the original sequence groups are returned for further processing.\n\n        Args:\n            execute_model_req (ExecuteModelRequest): The original execute\n            model request.\n            seq_with_bonus_token_in_last_step (set): Set of sequence IDs that \n            contain bonus tokens.\n\n        Returns:\n            Tuple[ExecuteModelRequest, List[int]]: The updated execute model\n            request with expanded sequences and a list of indices corresponding\n            to the original sequence groups.\n        \"\"\"\n        updated_seq_group_metadata_list: List[SequenceGroupMetadata] = []\n        updated_execute_model_req = execute_model_req.clone(\n            updated_seq_group_metadata_list)\n        indices_of_original_sequence_groups = []\n        for seq_group in execute_model_req.seq_group_metadata_list:\n            seq_group_has_bonus_tokens = False\n            for seq_id, _ in seq_group.seq_data.items():\n                # Identify sequences with bonus tokens in the sequence group.\n                if seq_id in seq_with_bonus_token_in_last_step:\n                    seq_group_has_bonus_tokens = True\n                    break\n            if seq_group_has_bonus_tokens:\n                #Create new sequences without the last bonus token. These new\n                # sequence have the same sequence id as the original sequence.\n                # We create a new sequence group and add them there.\n                updated_seq_group_without_bonus_token  = \\\n                    MultiStepWorker._copy_seq_metadata_excluding_last_token(\n                        seq_group, seq_with_bonus_token_in_last_step)\n                updated_seq_group_metadata_list.append(\n                    updated_seq_group_without_bonus_token)\n            # Add the original sequence group.\n            updated_seq_group_metadata_list.append(\n                MultiStepWorker._shallow_copy_seq_group_metadata(seq_group))\n            # Record the index of the original sequence group.\n            indices_of_original_sequence_groups.append(\n                len(updated_seq_group_metadata_list) - 1)\n\n        updated_execute_model_req.seq_group_metadata_list =\\\n            updated_seq_group_metadata_list\n        return updated_execute_model_req, indices_of_original_sequence_groups\n\n    @staticmethod\n    def _filter_model_output(\n            expanded_batch_outputs: List[SamplerOutput],\n            output_indices_to_retain: List[int]) -> List[SamplerOutput]:\n        \"\"\"\n        Filters the model output to include only the specified sequence\n        outputs. This method contracts the expanded batch output from the\n        model to retain the outputs of only those sequences indicated by the\n        provided indices.\n\n        Args:\n            expanded_batch_output (List[SamplerOutput]): The expanded output\n                batch from the model.\n            output_indices_to_retain (List[int]): Indices of the model outputs\n                to retain.\n\n        Returns:\n            List[SamplerOutput]: A list containing the filtered model \n            outputs for the specified indices.\n        \"\"\"\n        return [\n            SamplerOutput(\n                outputs=[\n                    expanded_batch_output.outputs[i]\n                    for i in output_indices_to_retain\n                ] if len(expanded_batch_output.outputs) > 0 else [],\n                sampled_token_probs=(\n                    expanded_batch_output.\n                    sampled_token_probs[output_indices_to_retain]\n                    if expanded_batch_output.sampled_token_probs is not None\n                    else None),\n                logprobs=(\n                    expanded_batch_output.logprobs[output_indices_to_retain]\n                    if expanded_batch_output.logprobs is not None else None),\n                sampled_token_ids=(expanded_batch_output.\n                                   sampled_token_ids[output_indices_to_retain]\n                                   if expanded_batch_output.sampled_token_ids\n                                   is not None else None))\n            for expanded_batch_output in expanded_batch_outputs\n        ]\n\n    def get_spec_proposals(\n        self,\n        execute_model_req: ExecuteModelRequest,\n        seq_ids_with_bonus_token_in_last_step: set,\n    ) -> SpeculativeProposals:\n        \"\"\"Produce speculations given an input batch of sequences. The number of\n        speculative tokens per sequence is determined by max_proposal_len.\n        \"\"\"\n        return self._proposer.get_spec_proposals(\n            execute_model_req, seq_ids_with_bonus_token_in_last_step)\n\n    @staticmethod\n    def _append_new_tokens(\n            model_output: List[SamplerOutput],\n            seq_group_metadata_list: List[SequenceGroupMetadata]) -> None:\n        \"\"\"Given model output from a single run, append the tokens to the\n        sequences. This is normally done outside of the worker, but it is\n        required if the worker is to perform multiple forward passes.\n        \"\"\"\n        for seq_group_metadata, sequence_group_outputs in zip(\n                seq_group_metadata_list, model_output):\n            seq_group_metadata.is_prompt = False\n\n            for seq_output in sequence_group_outputs.samples:\n                # NOTE: Beam search is not supported, so we can assume that\n                # parent_seq_id == seq_id.\n                seq = seq_group_metadata.seq_data[seq_output.parent_seq_id]\n\n                token_id = seq_output.output_token\n                token_logprob = seq_output.logprobs[token_id]\n\n                seq.append_token_id(token_id, token_logprob.logprob)\n                seq.update_num_computed_tokens(1)\n\n    @staticmethod\n    def _shallow_copy_seq_group_metadata(\n        seq_group_metadata: SequenceGroupMetadata, ) -> SequenceGroupMetadata:\n        \"\"\"Copy input data structures to remove side-effects when input data\n        structures are shared with other modules.\n\n        Helpful when the vLLM scheduler runs in the same process as the worker.\n        The alternative is deep-copying (or other form of deep copy); this has\n        performance downsides.\n        \"\"\"\n        # Shallow-copy the SequenceGroupMetadata. This allows us to\n        # append tokens and change is_prompt without external side-effects.\n        # We must shallow-copy seq_group_metadata as is_prompt could change.\n        new_seq_group_metadata = copy.copy(seq_group_metadata)\n\n        # We must shallow-copy seq_data as we will append token ids\n        new_seq_data: Dict[int, SequenceData] = {}\n        for seq_id, old_seq_data in seq_group_metadata.seq_data.items():\n            new_seq_data[seq_id] = copy.copy(old_seq_data)\n            new_seq_data[seq_id].output_token_ids =\\\n                old_seq_data.output_token_ids[:]\n\n        new_seq_group_metadata.seq_data = new_seq_data\n        return new_seq_group_metadata\n\n    @staticmethod\n    def _copy_seq_metadata_excluding_last_token(\n        seq_group_metadata: SequenceGroupMetadata,\n        seq_ids_to_copy: Set[int],\n    ) -> SequenceGroupMetadata:\n        \"\"\"\n        Creates a shallow copy of the given SequenceGroupMetadata, retaining\n        only the sequence IDs specified in seq_ids_to_copy. For each of these\n        sequence IDs, all output_token_ids except the last one are copied.\n        Sequence IDs not in seq_ids_to_copy are excluded from the copy.\n        \n        Parameters:\n        seq_group_metadata (SequenceGroupMetadata): The original sequence\n            group metadata.\n        seq_ids_to_copy (Set[int]): The set of sequence IDs to include in the\n            copy.\n        \n        Returns:\n        SequenceGroupMetadata: A shallow copy of the sequence group metadata\n            with the specified modifications.\n        \"\"\"\n        # Shallow-copy the SequenceGroupMetadata.\n        new_seq_group_metadata = copy.copy(seq_group_metadata)\n        # Shallow-copy seq_data and modify the output_token_ids.\n        new_seq_data: Dict[int, SequenceData] = {}\n        for seq_id, old_seq_data in seq_group_metadata.seq_data.items():\n            if (seq_id in seq_ids_to_copy):\n                new_seq_data[seq_id] = copy.copy(old_seq_data)\n                # Copy all the output token ids except the last.\n                # Also reduce num_computed_tokens by 1 since we are not\n                # including the last output token.\n                # NOTE: num_computed_tokens is not directly used by the\n                # speculative decoding workers, as it is only relevant for\n                # chunked prefill, which is disabled for speculative decoding.\n                # However, to maintain consistency in num_computed_tokens,\n                # we update it here.\n                new_seq_data[seq_id].output_token_ids =\\\n                    old_seq_data.output_token_ids[:-1]\n                new_seq_data[seq_id].update_num_computed_tokens(-1)\n        new_seq_group_metadata.seq_data = new_seq_data\n        return new_seq_group_metadata\n\n    def _assert_enough_kv_space(\n            self, seq_group_metadata_list: List[SequenceGroupMetadata],\n            num_steps: int) -> None:\n        \"\"\"Assert there are enough physical blocks per sequence to store the\n        current KV plus additional KV from num_steps tokens.\n        \"\"\"\n        assert self.model_runner.block_size is not None\n        for seq_group_metadata in seq_group_metadata_list:\n            # Only one seq_id is guaranteed because there is no beam search.\n            seq_id = list(seq_group_metadata.seq_data.keys())[0]\n            seq = seq_group_metadata.seq_data[seq_id]\n\n            # After num_steps, the seq len will be the current seq len\n            # plus one token per step.\n            final_seq_len = seq.get_len() + num_steps\n\n            # We will have final_seq_len - 1 KV because vLLM saves KV for a\n            # token in the iteration after the token was generated.\n            required_num_kv_slots = final_seq_len - 1\n\n            # The allocated number of kv slots is the number of allocated blocks\n            # times the number of slots of block.\n            number_physical_blocks = len(\n                seq_group_metadata.block_tables[seq_id])\n            allocated_kv_slots = (number_physical_blocks *\n                                  self.model_runner.block_size)\n\n            if required_num_kv_slots > allocated_kv_slots:\n                request_id = seq_group_metadata.request_id\n                raise ValueError(\n                    \"The worker attempted to run \"\n                    f\"{num_steps} times but found insufficient KV space for \"\n                    f\"{request_id=} {seq_id=}. ({allocated_kv_slots=} \"\n                    f\"{required_num_kv_slots=}).\")\n\n    def _raise_if_unsupported(\n        self,\n        execute_model_req: ExecuteModelRequest,\n    ) -> None:\n        \"\"\"MultiStepWorker does not yet implement support for cache swap\n        operations or beam search.\n        \"\"\"\n        if any([\n                execute_model_req.blocks_to_swap_in,\n                execute_model_req.blocks_to_swap_out,\n                execute_model_req.blocks_to_copy\n        ]):\n            raise NotImplementedError(\n                \"MultiStepWorker does not support cache operations\")\n\n        if any(\n                len(seq_group_metadata.seq_data.keys()) != 1\n                for seq_group_metadata in\n                execute_model_req.seq_group_metadata_list):\n            raise NotImplementedError(\n                \"MultiStepWorker does not support beam search.\")\n\n```\n# Function Signature: def sampler_output(\n# Natural Language Annotations: \"\"\"Run the model forward pass sample_len times. Returns the list of\n        sampler output, one per model forward pass, along with indicator of\n        whether torch tensor in sampler output need to be transposed in latter\n        sampler_output_to_torch logic.\n\n        For multi step worker, this indicator shall be True.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef sampler_output(self, execute_model_req", "  ```\ndef sampler_output(self, num_steps: int", "  ```\ndef sampler_output(self, execute_model_req"]}, "vllm-vllm/model_executor/model_loader/loader.py-_filter_subtensors": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/model_executor/model_loader/loader.py:\n```\n# ruff: noqa: SIM117\nimport collections\nimport copy\nimport fnmatch\nimport glob\nimport json\nimport math\nimport os\nfrom abc import ABC, abstractmethod\nfrom contextlib import contextmanager\nfrom typing import Any, Dict, Generator, List, Optional, Tuple, Type\n\nimport gguf\nimport huggingface_hub\nimport numpy as np\nimport torch\nfrom huggingface_hub import HfApi, hf_hub_download\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM, PretrainedConfig\n\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoadFormat,\n                         LoRAConfig, ModelConfig, MultiModalConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.envs import VLLM_USE_MODELSCOPE\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization.base_config import (\n    QuantizationConfig)\nfrom vllm.model_executor.model_loader.tensorizer import (\n    TensorizerConfig, is_vllm_tensorized, load_with_tensorizer,\n    serialize_vllm_model, tensorizer_weights_iterator)\nfrom vllm.model_executor.model_loader.utils import (get_model_architecture,\n                                                    set_default_torch_dtype)\nfrom vllm.model_executor.model_loader.weight_utils import (\n    download_safetensors_index_file_from_hf, download_weights_from_hf,\n    filter_duplicate_safetensors_files, filter_files_not_needed_for_inference,\n    get_gguf_extra_tensor_names, get_quant_config, gguf_quant_weights_iterator,\n    initialize_dummy_weights, np_cache_weights_iterator, pt_weights_iterator,\n    safetensors_weights_iterator)\nfrom vllm.model_executor.models.interfaces import (has_inner_state,\n                                                   supports_lora,\n                                                   supports_multimodal)\nfrom vllm.model_executor.utils import set_weight_attrs\nfrom vllm.platforms import current_platform\nfrom vllm.utils import is_pin_memory_available\n\n\n@contextmanager\ndef device_loading_context(module: torch.nn.Module,\n                           target_device: torch.device):\n    if target_device.type == \"cpu\":\n        # If target is CPU, no need to move anything\n        yield module\n        return\n\n    original_device_states: Dict[str, torch.device] = {}\n\n    # Store original device states and move parameters to GPU if they're on CPU\n    for name, p in module.named_parameters():\n        if p.device.type == \"cpu\":\n            original_device_states[name] = p.device\n            p.data = p.data.to(target_device)\n        # Parameters already on target device are not touched\n\n    try:\n        yield module\n\n    finally:\n        # Restore parameters to their original devices, ignoring new parameters\n        pin_memory = is_pin_memory_available()\n        for name, p in module.named_parameters():\n            if name in original_device_states:\n                original_device: torch.device = original_device_states[name]\n                if original_device.type == \"cpu\":\n                    # `torch.empty_like` does not support `pin_memory` argument\n                    cpu_data = torch.empty_strided(size=p.data.size(),\n                                                   stride=p.data.stride(),\n                                                   dtype=p.data.dtype,\n                                                   layout=p.data.layout,\n                                                   device=\"cpu\",\n                                                   pin_memory=pin_memory)\n                    cpu_data.copy_(p.data)\n                    p.data = cpu_data\n                else:\n                    p.data = p.data.to(original_device)\n        # New parameters or parameters already on target device are untouched\n\n\nlogger = init_logger(__name__)\n\n\ndef _get_quantization_config(\n        model_config: ModelConfig,\n        load_config: LoadConfig) -> Optional[QuantizationConfig]:\n    \"\"\"Get the quantization config.\"\"\"\n    if model_config.quantization is not None:\n        quant_config = get_quant_config(model_config, load_config)\n        if not current_platform.is_tpu():\n            capability = current_platform.get_device_capability()\n            capability = capability[0] * 10 + capability[1]\n            if capability < quant_config.get_min_capability():\n                raise ValueError(\n                    f\"The quantization method {model_config.quantization} \"\n                    \"is not supported for the current GPU. \"\n                    f\"Minimum capability: {quant_config.get_min_capability()}. \"\n                    f\"Current capability: {capability}.\")\n        supported_dtypes = quant_config.get_supported_act_dtypes()\n        if model_config.dtype not in supported_dtypes:\n            raise ValueError(\n                f\"{model_config.dtype} is not supported for quantization \"\n                f\"method {model_config.quantization}. Supported dtypes: \"\n                f\"{supported_dtypes}\")\n        return quant_config\n    return None\n\n\ndef _get_model_initialization_kwargs(\n        model_class: Type[nn.Module],\n        lora_config: Optional[LoRAConfig],\n        multimodal_config: Optional[MultiModalConfig],\n        scheduler_config: Optional[SchedulerConfig] = None) -> Dict[str, Any]:\n    \"\"\"Get extra kwargs for model initialization.\"\"\"\n    extra_kwargs: Dict[str, Any] = {}\n\n    if supports_lora(model_class):\n        # lora_config=None is used to disable LoRA\n        extra_kwargs[\"lora_config\"] = lora_config\n    elif lora_config:\n        raise ValueError(\n            f\"Model {model_class.__name__} does not support LoRA, \"\n            \"but LoRA is enabled. Support for this model may \"\n            \"be added in the future. If this is important to you, \"\n            \"please open an issue on github.\")\n\n    if supports_multimodal(model_class):\n        assert multimodal_config is not None\n\n        extra_kwargs[\"multimodal_config\"] = multimodal_config\n\n    if has_inner_state(model_class) and scheduler_config:\n        extra_kwargs[\"scheduler_config\"] = scheduler_config\n\n    return extra_kwargs\n\n\ndef build_model(model_class: Type[nn.Module], hf_config: PretrainedConfig,\n                cache_config: Optional[CacheConfig],\n                quant_config: Optional[QuantizationConfig], *,\n                lora_config: Optional[LoRAConfig],\n                multimodal_config: Optional[MultiModalConfig],\n                scheduler_config: Optional[SchedulerConfig]) -> nn.Module:\n    extra_kwargs = _get_model_initialization_kwargs(model_class, lora_config,\n                                                    multimodal_config,\n                                                    scheduler_config)\n\n    return model_class(config=hf_config,\n                       cache_config=cache_config,\n                       quant_config=quant_config,\n                       **extra_kwargs)\n\n\ndef _initialize_model(\n        model_config: ModelConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        cache_config: CacheConfig,\n        scheduler_config: Optional[SchedulerConfig] = None) -> nn.Module:\n    \"\"\"Initialize a model with the given configurations.\"\"\"\n    model_class, _ = get_model_architecture(model_config)\n\n    return build_model(\n        model_class,\n        model_config.hf_config,\n        cache_config=cache_config,\n        quant_config=_get_quantization_config(model_config, load_config),\n        lora_config=lora_config,\n        multimodal_config=model_config.multimodal_config,\n        scheduler_config=scheduler_config,\n    )\n\n\nclass BaseModelLoader(ABC):\n    \"\"\"Base class for model loaders.\"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        self.load_config = load_config\n\n    @abstractmethod\n    def load_model(self, *, model_config: ModelConfig,\n                   device_config: DeviceConfig,\n                   lora_config: Optional[LoRAConfig],\n                   parallel_config: ParallelConfig,\n                   scheduler_config: SchedulerConfig,\n                   cache_config: CacheConfig) -> nn.Module:\n        \"\"\"Load a model with the given configurations.\"\"\"\n        ...\n\n\nclass DefaultModelLoader(BaseModelLoader):\n    \"\"\"Model loader that can load different file types from disk.\"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        if load_config.model_loader_extra_config:\n            raise ValueError(f\"Model loader extra config is not supported for \"\n                             f\"load format {load_config.load_format}\")\n\n    def _maybe_download_from_modelscope(\n            self, model: str, revision: Optional[str]) -> Optional[str]:\n        \"\"\"Download model from ModelScope hub if VLLM_USE_MODELSCOPE is True.\n\n        Returns the path to the downloaded model, or None if the model is not\n        downloaded from ModelScope.\"\"\"\n        if VLLM_USE_MODELSCOPE:\n            # download model from ModelScope hub,\n            # lazy import so that modelscope is not required for normal use.\n            # pylint: disable=C.\n            from modelscope.hub.snapshot_download import snapshot_download\n\n            if not os.path.exists(model):\n                model_path = snapshot_download(\n                    model_id=model,\n                    cache_dir=self.load_config.download_dir,\n                    local_files_only=huggingface_hub.constants.HF_HUB_OFFLINE,\n                    revision=revision,\n                    ignore_file_pattern=self.load_config.ignore_patterns,\n                )\n            else:\n                model_path = model\n            return model_path\n        return None\n\n    def _prepare_weights(self, model_name_or_path: str,\n                         revision: Optional[str],\n                         fall_back_to_pt: bool) -> Tuple[str, List[str], bool]:\n        \"\"\"Prepare weights for the model.\n\n        If the model is not local, it will be downloaded.\"\"\"\n        model_name_or_path = self._maybe_download_from_modelscope(\n            model_name_or_path, revision) or model_name_or_path\n\n        is_local = os.path.isdir(model_name_or_path)\n        load_format = self.load_config.load_format\n        use_safetensors = False\n        # Some quantized models use .pt files for storing the weights.\n        if load_format == LoadFormat.AUTO:\n            allow_patterns = [\"*.safetensors\", \"*.bin\"]\n        elif load_format == LoadFormat.SAFETENSORS:\n            use_safetensors = True\n            allow_patterns = [\"*.safetensors\"]\n        elif load_format == LoadFormat.PT:\n            allow_patterns = [\"*.pt\"]\n        elif load_format == LoadFormat.NPCACHE:\n            allow_patterns = [\"*.bin\"]\n        else:\n            raise ValueError(f\"Unknown load_format: {load_format}\")\n\n        if fall_back_to_pt:\n            allow_patterns += [\"*.pt\"]\n\n        if not is_local:\n            hf_folder = download_weights_from_hf(\n                model_name_or_path,\n                self.load_config.download_dir,\n                allow_patterns,\n                revision,\n                ignore_patterns=self.load_config.ignore_patterns,\n            )\n        else:\n            hf_folder = model_name_or_path\n\n        hf_weights_files: List[str] = []\n        for pattern in allow_patterns:\n            hf_weights_files += glob.glob(os.path.join(hf_folder, pattern))\n            if len(hf_weights_files) > 0:\n                if pattern == \"*.safetensors\":\n                    use_safetensors = True\n                break\n\n        if use_safetensors:\n            # For models like Mistral-7B-Instruct-v0.3\n            # there are both sharded safetensors files and a consolidated\n            # safetensors file. Using both breaks.\n            # Here, we download the `model.safetensors.index.json` and filter\n            # any files not found in the index.\n            if not is_local:\n                download_safetensors_index_file_from_hf(\n                    model_name_or_path, self.load_config.download_dir,\n                    revision)\n            hf_weights_files = filter_duplicate_safetensors_files(\n                hf_weights_files, hf_folder)\n        else:\n            hf_weights_files = filter_files_not_needed_for_inference(\n                hf_weights_files)\n\n        if len(hf_weights_files) == 0:\n            raise RuntimeError(\n                f\"Cannot find any model weights with `{model_name_or_path}`\")\n\n        return hf_folder, hf_weights_files, use_safetensors\n\n    def _get_weights_iterator(\n        self, model_name_or_path: str, revision: Optional[str],\n        fall_back_to_pt: bool\n    ) -> Generator[Tuple[str, torch.Tensor], None, None]:\n        \"\"\"Get an iterator for the model weights based on the load format.\"\"\"\n        hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n            model_name_or_path, revision, fall_back_to_pt)\n        if self.load_config.load_format == LoadFormat.NPCACHE:\n            # Currently np_cache only support *.bin checkpoints\n            assert use_safetensors is False\n            weights_iterator = np_cache_weights_iterator(\n                model_name_or_path, self.load_config.download_dir, hf_folder,\n                hf_weights_files)\n        elif use_safetensors:\n            weights_iterator = safetensors_weights_iterator(hf_weights_files)\n        else:\n            weights_iterator = pt_weights_iterator(hf_weights_files)\n\n        if current_platform.is_tpu():\n            # In PyTorch XLA, we should call `xm.mark_step` frequently so that\n            # not too many ops are accumulated in the XLA program.\n            import torch_xla.core.xla_model as xm\n\n            def _xla_weights_iterator(iterator: Generator):\n                for weights in iterator:\n                    yield weights\n                    xm.mark_step()\n\n            weights_iterator = _xla_weights_iterator(weights_iterator)\n        return weights_iterator\n\n    def load_model(self, *, model_config: ModelConfig,\n                   device_config: DeviceConfig,\n                   lora_config: Optional[LoRAConfig],\n                   parallel_config: ParallelConfig,\n                   scheduler_config: SchedulerConfig,\n                   cache_config: CacheConfig) -> nn.Module:\n        target_device = torch.device(device_config.device)\n        with set_default_torch_dtype(model_config.dtype):\n            with target_device:\n                model = _initialize_model(model_config, self.load_config,\n                                          lora_config, cache_config,\n                                          scheduler_config)\n            model.load_weights(\n                self._get_weights_iterator(model_config.model,\n                                           model_config.revision,\n                                           fall_back_to_pt=getattr(\n                                               model,\n                                               \"fall_back_to_pt_during_load\",\n                                               True)), )\n\n            for _, module in model.named_modules():\n                quant_method = getattr(module, \"quant_method\", None)\n                if quant_method is not None:\n                    # When quant methods need to process weights after loading\n                    # (for repacking, quantizing, etc), they expect parameters\n                    # to be on the global target device. This scope is for the\n                    # case where cpu offloading is used, where we will move the\n                    # parameters onto device for processing and back off after.\n                    with device_loading_context(module, target_device):\n                        quant_method.process_weights_after_loading(module)\n        return model.eval()\n\n\nclass DummyModelLoader(BaseModelLoader):\n    \"\"\"Model loader that will set model weights to random values.\"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        if load_config.model_loader_extra_config:\n            raise ValueError(f\"Model loader extra config is not supported for \"\n                             f\"load format {load_config.load_format}\")\n\n    def load_model(self, *, model_config: ModelConfig,\n                   device_config: DeviceConfig,\n                   lora_config: Optional[LoRAConfig],\n                   parallel_config: ParallelConfig,\n                   scheduler_config: SchedulerConfig,\n                   cache_config: CacheConfig) -> nn.Module:\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model = _initialize_model(model_config, self.load_config,\n                                          lora_config, cache_config,\n                                          scheduler_config)\n            # NOTE(woosuk): For accurate performance evaluation, we assign\n            # random values to the weights.\n            initialize_dummy_weights(model)\n        return model.eval()\n\n\nclass TensorizerLoader(BaseModelLoader):\n    \"\"\"Model loader using CoreWeave's tensorizer library.\"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        if isinstance(load_config.model_loader_extra_config, TensorizerConfig):\n            self.tensorizer_config = load_config.model_loader_extra_config\n        else:\n            self.tensorizer_config = TensorizerConfig(\n                **load_config.model_loader_extra_config)\n\n    def _verify_config(self, model_config: ModelConfig,\n                       parallel_config: ParallelConfig):\n        self.tensorizer_config.verify_with_model_config(model_config)\n        self.tensorizer_config.verify_with_parallel_config(parallel_config)\n\n    def _get_weights_iterator(\n            self) -> Generator[Tuple[str, torch.Tensor], None, None]:\n        tensorizer_args = self.tensorizer_config._construct_tensorizer_args()\n        return tensorizer_weights_iterator(tensorizer_args)\n\n    def _load_model_serialized_cpu(\n        self,\n        model_config: ModelConfig,\n        device_config: DeviceConfig,\n        lora_config: Optional[LoRAConfig],\n        cache_config: CacheConfig,\n    ) -> nn.Module:\n        \"\"\"Load a serialized model with tensorizer to the CPU.\n\n        This is only necessary when the model isn't vLLM-tensorized (see\n        examples/tensorize_vllm_model.py) This should still be faster than\n        default HuggingFace loading, but will be slower than loading a\n        vLLM-tensorized model.\n        \"\"\"\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model = _initialize_model(model_config, self.load_config,\n                                          lora_config, cache_config)\n\n            model.load_weights(self._get_weights_iterator())\n        return model.eval()\n\n    def _load_model_serialized(\n        self,\n        model_config: ModelConfig,\n        device_config: DeviceConfig,\n        lora_config: Optional[LoRAConfig],\n        cache_config: CacheConfig,\n    ) -> nn.Module:\n        \"\"\"Load a serialized model with tensorizer.\n\n        Expects a vLLM-tensorized model. See the\n        examples/tensorize_vllm_model.py example script\n        for serializing vLLM models.\"\"\"\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model_class = get_model_architecture(model_config)[0]\n                quant_config = _get_quantization_config(\n                    model_config, self.load_config)\n                extra_kwargs = _get_model_initialization_kwargs(\n                    model_class, lora_config, model_config.multimodal_config)\n                extra_kwargs[\"quant_config\"] = quant_config\n                extra_kwargs[\"cache_config\"] = cache_config\n\n                tensorizer_config = copy.copy(self.tensorizer_config)\n                tensorizer_config.model_class = model_class\n                tensorizer_config.hf_config = model_config.hf_config\n                tensorizer_config.dtype = model_config.dtype\n\n                model = load_with_tensorizer(tensorizer_config, **extra_kwargs)\n        return model.eval()\n\n    def load_model(self, *, model_config: ModelConfig,\n                   device_config: DeviceConfig,\n                   lora_config: Optional[LoRAConfig],\n                   parallel_config: ParallelConfig,\n                   scheduler_config: SchedulerConfig,\n                   cache_config: CacheConfig) -> nn.Module:\n        self._verify_config(model_config, parallel_config)\n\n        if parallel_config.tensor_parallel_size > 1:\n            from vllm.distributed import get_tensor_model_parallel_rank\n            self.tensorizer_config.tensorizer_uri = \\\n                self.tensorizer_config.tensorizer_uri \\\n                    % get_tensor_model_parallel_rank()\n\n        if is_vllm_tensorized(self.tensorizer_config):\n            return self._load_model_serialized(model_config, device_config,\n                                               lora_config, cache_config)\n        return self._load_model_serialized_cpu(model_config, device_config,\n                                               lora_config, cache_config)\n\n    @staticmethod\n    def save_model(\n        model: torch.nn.Module,\n        tensorizer_config: TensorizerConfig,\n    ) -> None:\n        serialize_vllm_model(\n            model=model,\n            tensorizer_config=tensorizer_config,\n        )\n\n\nclass ShardedStateLoader(BaseModelLoader):\n    \"\"\"\n    Model loader that directly loads each worker's model state dict, which\n    enables a fast load path for large tensor-parallel models where each worker\n    only needs to read its own shard rather than the entire checkpoint. See\n    `examples/save_sharded_state.py` for creating a sharded checkpoint.\n    \"\"\"\n\n    DEFAULT_PATTERN = \"model-rank-{rank}-part-{part}.safetensors\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        extra_config = ({} if load_config.model_loader_extra_config is None\n                        else load_config.model_loader_extra_config.copy())\n        self.pattern = extra_config.pop(\"pattern\", self.DEFAULT_PATTERN)\n        if extra_config:\n            raise ValueError(f\"Unexpected extra config keys for load format \"\n                             f\"{load_config.load_format}: \"\n                             f\"{load_config.model_loader_extra_config.keys()}\")\n\n    @staticmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _prepare_weights(self, model_name_or_path: str,\n                         revision: Optional[str]):\n        if os.path.isdir(model_name_or_path):\n            return model_name_or_path\n        else:\n            allow_patterns = [\"*.safetensors\"]\n            return download_weights_from_hf(\n                model_name_or_path,\n                self.load_config.download_dir,\n                allow_patterns,\n                revision,\n                ignore_patterns=self.load_config.ignore_patterns,\n            )\n\n    def load_model(self, *, model_config: ModelConfig,\n                   device_config: DeviceConfig,\n                   lora_config: Optional[LoRAConfig],\n                   parallel_config: ParallelConfig,\n                   scheduler_config: SchedulerConfig,\n                   cache_config: CacheConfig) -> nn.Module:\n        from safetensors.torch import safe_open\n\n        from vllm.distributed import get_tensor_model_parallel_rank\n\n        local_model_path = self._prepare_weights(model_config.model,\n                                                 model_config.revision)\n\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model = _initialize_model(model_config, self.load_config,\n                                          lora_config, cache_config)\n            rank = get_tensor_model_parallel_rank()\n            pattern = os.path.join(\n                local_model_path,\n                self.pattern.format(rank=rank, part=\"*\"),\n            )\n            filepaths = glob.glob(pattern)\n            if not filepaths:\n                # TODO: support un-sharded checkpoints too\n                raise ValueError(\n                    f\"Could not find checkpoint files '{pattern}', only \"\n                    f\"pre-sharded checkpoints are currently supported!\")\n            state_dict = self._filter_subtensors(model.state_dict())\n            for path in filepaths:\n                with safe_open(path, framework=\"pt\") as f:\n                    for key in f.keys():  # noqa: SIM118\n                        tensor = f.get_tensor(key)\n                        # If loading with LoRA enabled, additional padding may\n                        # be added to certain parameters. We only load into a\n                        # narrowed view of the parameter data.\n                        param_data = state_dict[key].data\n                        param_shape = state_dict[key].shape\n                        for dim, size in enumerate(tensor.shape):\n                            if size < param_shape[dim]:\n                                param_data = param_data.narrow(dim, 0, size)\n                        if tensor.shape != param_shape:\n                            logger.warning(\n                                \"loading tensor of shape %s into \"\n                                \"parameter '%s' of shape %s\", tensor.shape,\n                                key, param_shape)\n                        param_data.copy_(tensor)\n                        state_dict.pop(key)\n            if state_dict:\n                raise ValueError(\n                    f\"Missing keys {tuple(state_dict)} in loaded state!\")\n        return model.eval()\n\n    @staticmethod\n    def save_model(\n        model: torch.nn.Module,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        from safetensors.torch import save_file\n\n        from vllm.distributed import get_tensor_model_parallel_rank\n        if pattern is None:\n            pattern = ShardedStateLoader.DEFAULT_PATTERN\n        rank = get_tensor_model_parallel_rank()\n        part_idx = 0\n        total_size = 0\n        state_dict = ShardedStateLoader._filter_subtensors(model.state_dict())\n        state_dict_part: Dict[str, torch.Tensor] = {}\n        for key, tensor in state_dict.items():\n            param_size = tensor.nelement() * tensor.element_size()\n            if max_size is not None and total_size + param_size > max_size:\n                filename = pattern.format(rank=rank, part=part_idx)\n                save_file(\n                    state_dict_part,\n                    os.path\n```\n# Function Signature: def _filter_subtensors(\n# Natural Language Annotations: \"\"\"\n        Filter out all tensors that share the same memory or a subset of the\n        memory of another tensor.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _filter_subtensors(\n    state_", "  ```\ndef _filter_subtensors(state_dict:", "  ```\ndef _filter_subtensors(state_dict:"]}, "vllm-vllm/model_executor/model_loader/weight_utils.py-download_weights_from_hf": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/model_executor/model_loader/weight_utils.py:\n```\n\"\"\"Utilities for downloading and initializing model weights.\"\"\"\nimport fnmatch\nimport glob\nimport hashlib\nimport json\nimport os\nimport tempfile\nfrom collections import defaultdict\nfrom typing import Any, Dict, Generator, Iterable, List, Optional, Tuple, Union\n\nimport filelock\nimport gguf\nimport huggingface_hub.constants\nimport numpy as np\nimport torch\nfrom huggingface_hub import HfFileSystem, hf_hub_download, snapshot_download\nfrom safetensors.torch import load_file, safe_open, save_file\nfrom tqdm.auto import tqdm\nfrom transformers.utils import SAFE_WEIGHTS_INDEX_NAME\n\nfrom vllm.config import LoadConfig, ModelConfig\nfrom vllm.distributed import get_tensor_model_parallel_rank\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import (QuantizationConfig,\n                                                     get_quantization_config)\nfrom vllm.model_executor.layers.quantization.schema import QuantParamSchema\nfrom vllm.platforms import current_platform\nfrom vllm.utils import print_warning_once\n\nlogger = init_logger(__name__)\n\n# use system-level temp directory for file locks, so that multiple users\n# can share the same lock without error.\n# lock files in the temp directory will be automatically deleted when the\n# system reboots, so users will not complain about annoying lock files\ntemp_dir = tempfile.gettempdir()\n\n\ndef enable_hf_transfer():\n    \"\"\"automatically activates hf_transfer\n    \"\"\"\n    if \"HF_HUB_ENABLE_HF_TRANSFER\" not in os.environ:\n        try:\n            # enable hf hub transfer if available\n            import hf_transfer  # type: ignore # noqa\n            huggingface_hub.constants.HF_HUB_ENABLE_HF_TRANSFER = True\n        except ImportError:\n            pass\n\n\nenable_hf_transfer()\n\n\nclass DisabledTqdm(tqdm):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs, disable=True)\n\n\ndef get_lock(model_name_or_path: str, cache_dir: Optional[str] = None):\n    lock_dir = cache_dir or temp_dir\n    os.makedirs(os.path.dirname(lock_dir), exist_ok=True)\n    model_name = model_name_or_path.replace(\"/\", \"-\")\n    hash_name = hashlib.sha256(model_name.encode()).hexdigest()\n    # add hash to avoid conflict with old users' lock files\n    lock_file_name = hash_name + model_name + \".lock\"\n    # mode 0o666 is required for the filelock to be shared across users\n    lock = filelock.FileLock(os.path.join(lock_dir, lock_file_name),\n                             mode=0o666)\n    return lock\n\n\ndef _shared_pointers(tensors):\n    ptrs = defaultdict(list)\n    for k, v in tensors.items():\n        ptrs[v.data_ptr()].append(k)\n    failing = []\n    for _, names in ptrs.items():\n        if len(names) > 1:\n            failing.append(names)\n    return failing\n\n\ndef convert_bin_to_safetensor_file(\n    pt_filename: str,\n    sf_filename: str,\n) -> None:\n    loaded = torch.load(pt_filename, map_location=\"cpu\")\n    if \"state_dict\" in loaded:\n        loaded = loaded[\"state_dict\"]\n    shared = _shared_pointers(loaded)\n    for shared_weights in shared:\n        for name in shared_weights[1:]:\n            loaded.pop(name)\n\n    # For tensors to be contiguous\n    loaded = {k: v.contiguous() for k, v in loaded.items()}\n\n    dirname = os.path.dirname(sf_filename)\n    os.makedirs(dirname, exist_ok=True)\n    save_file(loaded, sf_filename, metadata={\"format\": \"pt\"})\n\n    # check file size\n    sf_size = os.stat(sf_filename).st_size\n    pt_size = os.stat(pt_filename).st_size\n    if (sf_size - pt_size) / pt_size > 0.01:\n        raise RuntimeError(f\"\"\"The file size different is more than 1%:\n         - {sf_filename}: {sf_size}\n         - {pt_filename}: {pt_size}\n         \"\"\")\n\n    # check if the tensors are the same\n    reloaded = load_file(sf_filename)\n    for k in loaded:\n        pt_tensor = loaded[k]\n        sf_tensor = reloaded[k]\n        if not torch.equal(pt_tensor, sf_tensor):\n            raise RuntimeError(f\"The output tensors do not match for key {k}\")\n\n\n# TODO(woosuk): Move this to other place.\ndef get_quant_config(model_config: ModelConfig,\n                     load_config: LoadConfig) -> QuantizationConfig:\n\n    quant_cls = get_quantization_config(model_config.quantization)\n\n    # GGUF doesn't have config file\n    if model_config.quantization == \"gguf\":\n        return quant_cls.from_config({})\n\n    # Read the quantization config from the HF model config, if available.\n    hf_quant_config = getattr(model_config.hf_config, \"quantization_config\",\n                              None)\n    if hf_quant_config is None:\n        # compressed-tensors uses a compressions_config\n        hf_quant_config = getattr(model_config.hf_config, \"compression_config\",\n                                  None)\n    if hf_quant_config is not None:\n        return quant_cls.from_config(hf_quant_config)\n    # In case of bitsandbytes/QLoRA, get quant config from the adapter model.\n    if model_config.quantization == \"bitsandbytes\":\n        if (not load_config.model_loader_extra_config\n                or \"qlora_adapter_name_or_path\"\n                not in load_config.model_loader_extra_config):\n            return quant_cls.from_config({\"adapter_name_or_path\": \"\"})\n        model_name_or_path = load_config.model_loader_extra_config[\n            \"qlora_adapter_name_or_path\"]\n\n    else:\n        model_name_or_path = model_config.model\n    is_local = os.path.isdir(model_name_or_path)\n    if not is_local:\n        # Download the config files.\n        with get_lock(model_name_or_path, load_config.download_dir):\n            hf_folder = snapshot_download(\n                model_name_or_path,\n                revision=model_config.revision,\n                allow_patterns=\"*.json\",\n                cache_dir=load_config.download_dir,\n                local_files_only=huggingface_hub.constants.HF_HUB_OFFLINE,\n                tqdm_class=DisabledTqdm,\n            )\n    else:\n        hf_folder = model_name_or_path\n\n    possible_config_filenames = quant_cls.get_config_filenames()\n\n    # If the quantization config is not found, use the default config.\n    if not possible_config_filenames:\n        return quant_cls()\n\n    config_files = glob.glob(os.path.join(hf_folder, \"*.json\"))\n\n    quant_config_files = [\n        f for f in config_files if any(\n            f.endswith(x) for x in possible_config_filenames)\n    ]\n    if len(quant_config_files) == 0:\n        raise ValueError(\n            f\"Cannot find the config file for {model_config.quantization}\")\n    if len(quant_config_files) > 1:\n        raise ValueError(\n            f\"Found multiple config files for {model_config.quantization}: \"\n            f\"{quant_config_files}\")\n\n    quant_config_file = quant_config_files[0]\n    with open(quant_config_file, \"r\") as f:\n        config = json.load(f)\n\n        if model_config.quantization == \"bitsandbytes\":\n            config[\"adapter_name_or_path\"] = model_name_or_path\n\n    return quant_cls.from_config(config)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef download_safetensors_index_file_from_hf(\n    model_name_or_path: str,\n    cache_dir: Optional[str],\n    revision: Optional[str] = None,\n) -> None:\n    \"\"\"Download hf safetensors index file from Hugging Face Hub.\n\n    Args:\n        model_name_or_path (str): The model name or path.\n        cache_dir (Optional[str]): The cache directory to store the model\n            weights. If None, will use HF defaults.\n        revision (Optional[str]): The revision of the model.\n    \"\"\"\n    # Use file lock to prevent multiple processes from\n    # downloading the same model weights at the same time.\n    with get_lock(model_name_or_path, cache_dir):\n        try:\n            # Download the safetensors index file.\n            hf_hub_download(\n                repo_id=model_name_or_path,\n                filename=SAFE_WEIGHTS_INDEX_NAME,\n                cache_dir=cache_dir,\n                revision=revision,\n                local_files_only=huggingface_hub.constants.HF_HUB_OFFLINE,\n            )\n        # If file not found on remote or locally, we should not fail since\n        # only some models will have SAFE_WEIGHTS_INDEX_NAME.\n        except huggingface_hub.utils.EntryNotFoundError:\n            logger.info(\"No %s found in remote.\", SAFE_WEIGHTS_INDEX_NAME)\n        except huggingface_hub.utils.LocalEntryNotFoundError:\n            logger.info(\"No %s found in local cache.\", SAFE_WEIGHTS_INDEX_NAME)\n\n\n# For models like Mistral-7B-v0.3, there are both sharded\n# safetensors files and a consolidated safetensors file.\n# Passing both of these to the weight loader functionality breaks.\n# So, we use the SAFE_WEIGHTS_INDEX_NAME to\n# look up which safetensors files should be used.\ndef filter_duplicate_safetensors_files(hf_weights_files: List[str],\n                                       hf_folder: str) -> List[str]:\n    # model.safetensors.index.json is a mapping from keys in the\n    # torch state_dict to safetensors file holding that weight.\n    index_file_name = os.path.join(hf_folder, SAFE_WEIGHTS_INDEX_NAME)\n    if not os.path.isfile(index_file_name):\n        return hf_weights_files\n\n    # Iterate through the weight_map (weight_name: safetensors files)\n    # to identify weights that we should use.\n    with open(index_file_name) as index_file:\n        weight_map = json.load(index_file)[\"weight_map\"]\n    weight_files_in_index = set()\n    for weight_name in weight_map:\n        weight_files_in_index.add(\n            os.path.join(hf_folder, weight_map[weight_name]))\n    # Filter out any fields that are not found in the index file.\n    hf_weights_files = [\n        f for f in hf_weights_files if f in weight_files_in_index\n    ]\n    return hf_weights_files\n\n\ndef filter_files_not_needed_for_inference(\n        hf_weights_files: List[str]) -> List[str]:\n    \"\"\"\n    Exclude files that are not needed for inference.\n\n    See https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/trainer.py#L227-L233\n    \"\"\"\n    blacklist = [\n        \"training_args.bin\",\n        \"optimizer.bin\",\n        \"optimizer.pt\",\n        \"scheduler.pt\",\n        \"scaler.pt\",\n    ]\n    hf_weights_files = [\n        f for f in hf_weights_files\n        if not any(f.endswith(x) for x in blacklist)\n    ]\n    return hf_weights_files\n\n\n# explicitly use pure text format, with a newline at the end\n# this makes it impossible to see the animation in the progress bar\n# but will avoid messing up with ray or multiprocessing, which wraps\n# each line of output with some prefix.\n_BAR_FORMAT = \"{desc}: {percentage:3.0f}% Completed | {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\\n\"  # noqa: E501\n\n\ndef np_cache_weights_iterator(\n    model_name_or_path: str, cache_dir: Optional[str], hf_folder: str,\n    hf_weights_files: List[str]\n) -> Generator[Tuple[str, torch.Tensor], None, None]:\n    \"\"\"Iterate over the weights in the model np files.\n\n    Will dump the model weights to numpy files if they are not already dumped.\n    \"\"\"\n    enable_tqdm = not torch.distributed.is_initialized(\n    ) or torch.distributed.get_rank() == 0\n    # Convert the model weights from torch tensors to numpy arrays for\n    # faster loading.\n    np_folder = os.path.join(hf_folder, \"np\")\n    os.makedirs(np_folder, exist_ok=True)\n    weight_names_file = os.path.join(np_folder, \"weight_names.json\")\n    # Use file lock to prevent multiple processes from\n    # dumping the same model weights to numpy at the same time.\n    with get_lock(model_name_or_path, cache_dir):\n        if not os.path.exists(weight_names_file):\n            weight_names: List[str] = []\n            for bin_file in tqdm(\n                    hf_weights_files,\n                    desc=\"Loading np_cache checkpoint shards\",\n                    disable=not enable_tqdm,\n                    bar_format=_BAR_FORMAT,\n            ):\n                state = torch.load(bin_file, map_location=\"cpu\")\n                for name, param in state.items():\n                    param_path = os.path.join(np_folder, name)\n                    with open(param_path, \"wb\") as f:\n                        np.save(f, param.cpu().detach().numpy())\n                    weight_names.append(name)\n            with open(weight_names_file, \"w\") as f:\n                json.dump(weight_names, f)\n\n    with open(weight_names_file, \"r\") as f:\n        weight_names = json.load(f)\n\n    for name in weight_names:\n        param_path = os.path.join(np_folder, name)\n        with open(param_path, \"rb\") as f:\n            param = np.load(f)\n        yield name, torch.from_numpy(param)\n\n\ndef safetensors_weights_iterator(\n    hf_weights_files: List[str]\n) -> Generator[Tuple[str, torch.Tensor], None, None]:\n    \"\"\"Iterate over the weights in the model safetensor files.\"\"\"\n    enable_tqdm = not torch.distributed.is_initialized(\n    ) or torch.distributed.get_rank() == 0\n    for st_file in tqdm(\n            hf_weights_files,\n            desc=\"Loading safetensors checkpoint shards\",\n            disable=not enable_tqdm,\n            bar_format=_BAR_FORMAT,\n    ):\n        with safe_open(st_file, framework=\"pt\") as f:\n            for name in f.keys():  # noqa: SIM118\n                param = f.get_tensor(name)\n                yield name, param\n\n\ndef pt_weights_iterator(\n    hf_weights_files: List[str]\n) -> Generator[Tuple[str, torch.Tensor], None, None]:\n    \"\"\"Iterate over the weights in the model bin/pt files.\"\"\"\n    enable_tqdm = not torch.distributed.is_initialized(\n    ) or torch.distributed.get_rank() == 0\n    for bin_file in tqdm(\n            hf_weights_files,\n            desc=\"Loading pt checkpoint shards\",\n            disable=not enable_tqdm,\n            bar_format=_BAR_FORMAT,\n    ):\n        state = torch.load(bin_file, map_location=\"cpu\")\n        for name, param in state.items():\n            yield name, param\n        del state\n        torch.cuda.empty_cache()\n\n\ndef get_gguf_extra_tensor_names(\n        gguf_file: str, gguf_to_hf_name_map: Dict[str, str]) -> List[str]:\n    reader = gguf.GGUFReader(gguf_file)\n    expected_gguf_keys = set(gguf_to_hf_name_map.keys())\n    exact_gguf_keys = set([tensor.name for tensor in reader.tensors])\n    extra_keys = expected_gguf_keys - exact_gguf_keys\n    return [gguf_to_hf_name_map[key] for key in extra_keys]\n\n\ndef gguf_quant_weights_iterator(\n    gguf_file: str, gguf_to_hf_name_map: Dict[str, str]\n) -> Generator[Tuple[str, torch.Tensor], None, None]:\n    \"\"\"\n    Iterate over the quant weights in the model gguf files and convert\n    them to torch tensors\n    \"\"\"\n\n    reader = gguf.GGUFReader(gguf_file)\n\n    for tensor in reader.tensors:\n        if tensor.name in gguf_to_hf_name_map:\n            weight_type = tensor.tensor_type\n            name = gguf_to_hf_name_map[tensor.name]\n\n            if weight_type.name != \"F32\":\n                weight_type_name = name.replace(\"weight\", \"qweight_type\")\n                weight_type = torch.tensor(weight_type)\n                yield weight_type_name, weight_type\n\n    for tensor in reader.tensors:\n        if tensor.name in gguf_to_hf_name_map:\n            weight = tensor.data\n            weight_type = tensor.tensor_type\n            name = gguf_to_hf_name_map[tensor.name]\n\n            if weight_type.name != \"F32\":\n                name = name.replace(\"weight\", \"qweight\")\n            param = torch.tensor(weight)\n            yield name, param\n\n\ndef kv_cache_scales_loader(\n        filename: str, tp_rank: int, tp_size: int, num_hidden_layers: int,\n        model_type: Optional[str]) -> Iterable[Tuple[int, float]]:\n    \"\"\"\n    A simple utility to read in KV cache scaling factors that have been\n    previously serialized to disk. Used by the model to populate the appropriate\n    KV cache scaling factors. The serialization should represent a dictionary\n    whose keys are the TP ranks and values are another dictionary mapping layers\n    to their KV cache scaling factors.\n    Keep this function in sync with the output of examples/fp8/extract_scales.py\n    \"\"\"\n    try:\n        with open(filename) as f:\n            context = {\n                \"model_type\": model_type,\n                \"num_hidden_layers\": num_hidden_layers,\n                \"tp_rank\": tp_rank,\n                \"tp_size\": tp_size,\n            }\n            schema_dct = json.load(f)\n            schema = QuantParamSchema.model_validate(schema_dct,\n                                                     context=context)\n            layer_scales_map = schema.kv_cache.scaling_factor[tp_rank]\n            return layer_scales_map.items()\n\n    except FileNotFoundError:\n        logger.error(\"File or directory '%s' not found.\", filename)\n    except json.JSONDecodeError:\n        logger.error(\"Error decoding JSON in file '%s'.\", filename)\n    except Exception as e:\n        logger.error(\"An error occurred while reading '%s': %s\", filename, e)\n    # This section is reached if and only if any of the excepts are hit\n    # Return an empty iterable (list) => no KV cache scales are loaded\n    # which ultimately defaults to 1.0 scales\n    logger.warning(\n        \"Defaulting to KV cache scaling factors = 1.0 for all \"\n        \"layers in TP rank %d as an error occurred during loading.\", tp_rank)\n    return []\n\n\ndef convert_pyslice_to_tensor(x: Any) -> torch.Tensor:\n    \"\"\"convert PySafeSlice object from safetensors to torch.Tensor\n\n    PySafeSlice object supports indexing, which is done before loading the\n    actual tensor and can reduce the amount of memory being read into the\n    memory. However, it does not support more advanced functionalities\n    like `.view()` or `.t()`. Therefore, if we need to modify the loaded\n    tensor with these more complicated operators, we need to convert to\n    tensor first.\n    \"\"\"\n    if not isinstance(x, torch.Tensor):\n        x = x[:]\n    return x\n\n\ndef default_weight_loader(param: torch.Tensor,\n                          loaded_weight: torch.Tensor) -> None:\n    \"\"\"Default weight loader.\"\"\"\n    try:\n        if param.numel() == 1 and loaded_weight.numel() == 1:\n            # Sometimes scalar values aren't considered tensors with shapes\n            # so if both param and loaded_weight are a scalar,\n            # \"broadcast\" instead of copy\n            param.data.fill_(loaded_weight.item())\n        else:\n            assert param.size() == loaded_weight.size(), (\n                f\"Attempted to load weight ({loaded_weight.size()}) \"\n                f\"into parameter ({param.size()})\")\n\n            param.data.copy_(loaded_weight)\n    except Exception:\n        # NOTE: This exception is added for the purpose of setting breakpoint to\n        # debug weight loading issues.\n        raise\n\n\ndef row_parallel_weight_loader(param: torch.Tensor,\n                               loaded_weight: torch.Tensor) -> None:\n    \"\"\"Load weights that are row-parallelized.\"\"\"\n    tp_rank = get_tensor_model_parallel_rank()\n    shard_dim = 0 if param.dim() != 1 else None\n\n    if shard_dim is not None:\n        shard_size = param.data.shape[shard_dim]\n        start_idx = tp_rank * shard_size\n        loaded_weight = loaded_weight.narrow(shard_dim, start_idx, shard_size)\n\n    return default_weight_loader(param, loaded_weight)\n\n\ndef initialize_dummy_weights(\n    model: torch.nn.Module,\n    low: float = -1e-3,\n    high: float = 1e-3,\n    seed: int = 1234,\n) -> None:\n    \"\"\"Initialize model weights with random values.\n\n    The model weights must be randomly initialized for accurate performance\n    measurements. Additionally, the model weights should not cause NaNs in the\n    forward pass. We empirically found that initializing the weights with\n    values between -1e-3 and 1e-3 works well for most models.\n\n    We use per-parameter random seed, so that dummy weights are consistent,\n    even if the model is partitioned across multiple devices. When the seed\n    is fixed, the random values generated by this function only depends on\n    the parameter's number of elements and its data type.\n    \"\"\"\n    for param in model.state_dict().values():\n        if torch.is_floating_point(param):\n            if current_platform.is_tpu():\n                # XLA device does not support torch.Generator()\n                param.uniform_(low, high)\n                continue\n\n            generator = torch.Generator(device=param.data.device)\n            generator.manual_seed(seed)\n            if torch.finfo(param.data.dtype).bits < 16:\n                # uniform_ doesn't support < 16-bit datatypes (FP8)\n                dtype = param.data.dtype\n                tmp_param = param.data.to(torch.float16)\n                tmp_param = tmp_param.uniform_(low, high,\n                                               generator=generator).to(dtype)\n                param.data.copy_(tmp_param)\n            else:\n                param.uniform_(low, high, generator=generator)\n\n\ndef maybe_remap_kv_scale_name(name: str, params_dict: dict) -> Optional[str]:\n    \"\"\"Remap the name of FP8 k/v_scale parameters.\n\n    This function handles the remapping of FP8 k/v_scale parameter names.\n    It detects if the given name ends with a suffix and attempts to remap\n    it to the expected name format in the model. If the remapped name is not\n    found in the params_dict, a warning is printed and None is returned.\n\n    Args:\n        name (str): The original loaded checkpoint parameter name.\n        params_dict (dict): Dictionary containing the model's named parameters.\n\n    Returns:\n        str: The remapped parameter name if successful, or the original name\n             if no remapping is needed.\n        None: If the remapped name is not found in params_dict.\n    \"\"\"\n    if name.endswith(\".kv_scale\"):\n        print_warning_once(\n            \"DEPRECATED. Found kv_scale in the checkpoint. \"\n            \"This format is deprecated in favor of separate k_scale and \"\n            \"v_scale tensors and will be removed in a future release. \"\n            \"Functionally, we will remap kv_scale to k_scale and duplicate \"\n            \"k_scale to v_scale\")\n        # NOTE: we remap the deprecated kv_scale to k_scale\n        remapped_name = name.replace(\".kv_scale\", \".attn.k_scale\")\n        if remapped_name not in params_dict:\n            print_warning_once(\n                f\"Found kv_scale in the checkpoint (e.g. {name}), \"\n                \"but not found the expected name in the model \"\n                f\"(e.g. {remapped_name}). kv_scale is \"\n                \"not loaded.\")\n            return None\n        return remapped_name\n\n    possible_scale_names = [\".k_scale\", \".v_scale\"]\n    for scale_name in possible_scale_names:\n        if name.endswith(scale_name):\n            remapped_name = name.replace(scale_name, f\".attn{scale_name}\")\n            if remapped_name not in params_dict:\n                print_warning_once(\n                    f\"Found {scale_name\n```\n# Function Signature: def download_weights_from_hf(\n# Natural Language Annotations: \"\"\"Download model weights from Hugging Face Hub.\n\n    Args:\n        model_name_or_path (str): The model name or path.\n        cache_dir (Optional[str]): The cache directory to store the model\n            weights. If None, will use HF defaults.\n        allow_patterns (List[str]): The allowed patterns for the\n            weight files. Files matched by any of the patterns will be\n            downloaded.\n        revision (Optional[str]): The revision of the model.\n        ignore_patterns (Optional[Union[str, List[str]]]): The patterns to\n            filter out the weight files. Files matched by any of the patterns\n            will be ignored.\n\n    Returns:\n        str: The path to the downloaded model weights.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef download_weights_from_hf(\n   ", "  ```\nfrom vllm.model_executor.model_loader", "  ```\ndef download_weights_from_hf(\n   "]}, "vllm-vllm/worker/worker.py-initialize_cache": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/worker/worker.py:\n```\n\"\"\"A GPU worker class.\"\"\"\nimport gc\nimport os\nfrom typing import Dict, List, Optional, Set, Tuple, Type, Union\n\nimport torch\nimport torch.distributed\n\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig,\n                         SpeculativeConfig)\nfrom vllm.distributed import (ensure_model_parallel_initialized,\n                              init_distributed_environment,\n                              set_custom_all_reduce)\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor import set_random_seed\nfrom vllm.model_executor.model_loader.tensorizer import TensorizerConfig\nfrom vllm.platforms import current_platform\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sequence import (ExecuteModelRequest, IntermediateTensors,\n                           SamplerOutput, SequenceGroupMetadata,\n                           SequenceGroupMetadataDelta)\nfrom vllm.worker.cache_engine import CacheEngine\nfrom vllm.worker.embedding_model_runner import EmbeddingModelRunner\nfrom vllm.worker.enc_dec_model_runner import EncoderDecoderModelRunner\nfrom vllm.worker.model_runner import GPUModelRunnerBase, ModelRunner\nfrom vllm.worker.worker_base import LocalOrDistributedWorkerBase, WorkerInput\n\n\nclass Worker(LocalOrDistributedWorkerBase):\n    \"\"\"A worker class that executes (a partition of) the model on a GPU.\n\n    Each worker is associated with a single GPU. The worker is responsible for\n    maintaining the KV cache and executing the model on the GPU. In case of\n    distributed inference, each worker is assigned a partition of the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        local_rank: int,\n        rank: int,\n        distributed_init_method: str,\n        lora_config: Optional[LoRAConfig] = None,\n        speculative_config: Optional[SpeculativeConfig] = None,\n        prompt_adapter_config: Optional[PromptAdapterConfig] = None,\n        is_driver_worker: bool = False,\n        model_runner_cls: Optional[Type[GPUModelRunnerBase]] = None,\n        observability_config: Optional[ObservabilityConfig] = None,\n    ) -> None:\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.parallel_config.rank = rank\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.cache_config = cache_config\n        self.local_rank = local_rank\n        self.rank = rank\n        self.distributed_init_method = distributed_init_method\n        self.lora_config = lora_config\n        self.load_config = load_config\n        self.prompt_adapter_config = prompt_adapter_config\n        self.is_driver_worker = is_driver_worker\n        if parallel_config and is_driver_worker:\n            assert rank % parallel_config.tensor_parallel_size == 0, \\\n                   \"Driver worker should be rank 0 of tensor parallel group.\"\n        if self.model_config.trust_remote_code:\n            # note: lazy import to avoid importing torch before initializing\n            from vllm.utils import init_cached_hf_modules\n            init_cached_hf_modules()\n        self.observability_config = observability_config\n\n        # Return hidden states from target model if the draft model is an\n        # mlp_speculator\n        speculative_args = {} if speculative_config is None \\\n            or (speculative_config.draft_model_config.model ==\n                model_config.model) \\\n            or (speculative_config.draft_model_config.hf_config.model_type\n                not in [\"medusa\", \"mlp_speculator\"]) \\\n                    else {\"return_hidden_states\": True}\n\n        ModelRunnerClass: Type[GPUModelRunnerBase] = ModelRunner\n        if model_runner_cls is not None:\n            ModelRunnerClass = model_runner_cls\n        elif self._is_embedding_model():\n            ModelRunnerClass = EmbeddingModelRunner\n        elif self._is_encoder_decoder_model():\n            ModelRunnerClass = EncoderDecoderModelRunner\n        self.model_runner: GPUModelRunnerBase = ModelRunnerClass(\n            model_config,\n            parallel_config,\n            scheduler_config,\n            device_config,\n            cache_config,\n            load_config=load_config,\n            lora_config=self.lora_config,\n            kv_cache_dtype=self.cache_config.cache_dtype,\n            is_driver_worker=is_driver_worker,\n            prompt_adapter_config=prompt_adapter_config,\n            observability_config=observability_config,\n            **speculative_args,\n        )\n        # Uninitialized cache engine. Will be initialized by\n        # initialize_cache.\n        self.cache_engine: List[CacheEngine]\n        # Initialize gpu_cache as embedding models don't initialize kv_caches\n        self.gpu_cache: Optional[List[List[torch.Tensor]]] = None\n        self._seq_group_metadata_cache: Dict[str, SequenceGroupMetadata] = {}\n\n    def _is_encoder_decoder_model(self):\n        return self.model_config.is_encoder_decoder_model\n\n    def _is_embedding_model(self):\n        return self.model_config.is_embedding_model\n\n    def init_device(self) -> None:\n        if self.device_config.device.type == \"cuda\":\n            # torch.distributed.all_reduce does not free the input tensor until\n            # the synchronization point. This causes the memory usage to grow\n            # as the number of all_reduce calls increases. This env var disables\n            # this behavior.\n            # Related issue:\n            # https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573\n            os.environ[\"TORCH_NCCL_AVOID_RECORD_STREAMS\"] = \"1\"\n\n            # This env var set by Ray causes exceptions with graph building.\n            os.environ.pop(\"NCCL_ASYNC_ERROR_HANDLING\", None)\n            self.device = torch.device(f\"cuda:{self.local_rank}\")\n            torch.cuda.set_device(self.device)\n\n            _check_if_gpu_supports_dtype(self.model_config.dtype)\n            torch.cuda.empty_cache()\n            self.init_gpu_memory = torch.cuda.mem_get_info()[0]\n        else:\n            raise RuntimeError(\n                f\"Not support device type: {self.device_config.device}\")\n        # Initialize the distributed environment.\n        init_worker_distributed_environment(self.parallel_config, self.rank,\n                                            self.distributed_init_method,\n                                            self.local_rank)\n        # Set random seed.\n        set_random_seed(self.model_config.seed)\n\n    def load_model(self):\n        self.model_runner.load_model()\n\n    def save_sharded_state(\n        self,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        self.model_runner.save_sharded_state(\n            path,\n            pattern=pattern,\n            max_size=max_size,\n        )\n\n    def save_tensorized_model(\n        self,\n        tensorizer_config: TensorizerConfig,\n    ) -> None:\n        self.model_runner.save_tensorized_model(\n            tensorizer_config=tensorizer_config, )\n\n    @torch.inference_mode()\n    def determine_num_available_blocks(self) -> Tuple[int, int]:\n        \"\"\"Profiles the peak memory usage of the model to determine how many\n        KV blocks may be allocated without OOMs.\n\n        The engine will first conduct a profiling of the existing memory usage.\n        Then, it calculate the maximum possible number of GPU and CPU blocks\n        that can be allocated with the remaining free memory.\n\n        .. tip::\n            You may limit the usage of GPU memory\n            by adjusting the `gpu_memory_utilization` parameter.\n        \"\"\"\n        # Profile the memory usage of the model and get the maximum number of\n        # cache blocks that can be allocated with the remaining free memory.\n        torch.cuda.empty_cache()\n\n        # Execute a forward pass with dummy inputs to profile the memory usage\n        # of the model.\n        self.model_runner.profile_run()\n\n        # Calculate the number of blocks that can be allocated with the\n        # profiled peak memory.\n        torch.cuda.synchronize()\n        free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n        # NOTE(woosuk): Here we assume that the other processes using the same\n        # GPU did not change their memory usage during the profiling.\n        peak_memory = self.init_gpu_memory - free_gpu_memory\n        assert peak_memory > 0, (\n            \"Error in memory profiling. \"\n            f\"Initial free memory {self.init_gpu_memory}, current free memory\"\n            f\" {free_gpu_memory}. This happens when the GPU memory was \"\n            \"not properly cleaned up before initializing the vLLM instance.\")\n\n        cache_block_size = self.get_cache_block_size_bytes()\n        num_gpu_blocks = int(\n            (total_gpu_memory * self.cache_config.gpu_memory_utilization -\n             peak_memory) // cache_block_size)\n        num_cpu_blocks = int(self.cache_config.swap_space_bytes //\n                             cache_block_size)\n        num_gpu_blocks = max(num_gpu_blocks, 0)\n        num_cpu_blocks = max(num_cpu_blocks, 0)\n        if self.model_runner.lora_manager:\n            self.model_runner.remove_all_loras()\n        gc.collect()\n        torch.cuda.empty_cache()\n        return num_gpu_blocks, num_cpu_blocks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _init_cache_engine(self):\n        assert self.cache_config.num_gpu_blocks is not None\n        self.cache_engine = [\n            CacheEngine(self.cache_config, self.model_config,\n                        self.parallel_config, self.device_config)\n            for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n        self.gpu_cache = [\n            self.cache_engine[ve].gpu_cache\n            for ve in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n    def _warm_up_model(self) -> None:\n        if not self.model_config.enforce_eager:\n            self.model_runner.capture_model(self.gpu_cache)\n        # Reset the seed to ensure that the random state is not affected by\n        # the model initialization and profiling.\n        set_random_seed(self.model_config.seed)\n\n    @property\n    def do_metadata_broadcast(self) -> bool:\n        return self.parallel_config.tensor_parallel_size > 1\n\n    @property\n    def kv_cache(self) -> Optional[List[List[torch.Tensor]]]:\n        return self.gpu_cache\n\n    @torch.inference_mode()\n    def prepare_worker_input(\n            self, execute_model_req: ExecuteModelRequest) -> WorkerInput:\n        virtual_engine = execute_model_req.virtual_engine\n        num_steps = execute_model_req.num_steps\n        num_seq_groups = len(execute_model_req.seq_group_metadata_list)\n        # `blocks_to_swap_in` and `blocks_to_swap_out` are cpu tensors.\n        # they contain parameters to launch cudamemcpyasync.\n        blocks_to_swap_in = torch.tensor(execute_model_req.blocks_to_swap_in,\n                                         device=\"cpu\",\n                                         dtype=torch.int64).view(-1, 2)\n        blocks_to_swap_out = torch.tensor(execute_model_req.blocks_to_swap_out,\n                                          device=\"cpu\",\n                                          dtype=torch.int64).view(-1, 2)\n        # `blocks_to_copy` is a gpu tensor. The src and tgt of\n        # blocks to copy are in the same device, and `blocks_to_copy`\n        # can be used directly within cuda kernels.\n        blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,\n                                      device=self.device,\n                                      dtype=torch.int64).view(-1, 2)\n\n        return WorkerInput(\n            num_seq_groups=num_seq_groups,\n            blocks_to_swap_in=blocks_to_swap_in,\n            blocks_to_swap_out=blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            virtual_engine=virtual_engine,\n            num_steps=num_steps,\n        )\n\n    @torch.inference_mode()\n    def execute_worker(self, worker_input: WorkerInput) -> None:\n        virtual_engine = worker_input.virtual_engine\n        # Issue cache operations.\n        if (worker_input.blocks_to_swap_in is not None\n                and worker_input.blocks_to_swap_in.numel() > 0):\n            self.cache_engine[virtual_engine].swap_in(\n                worker_input.blocks_to_swap_in)\n        if (worker_input.blocks_to_swap_out is not None\n                and worker_input.blocks_to_swap_out.numel() > 0):\n            self.cache_engine[virtual_engine].swap_out(\n                worker_input.blocks_to_swap_out)\n        if (worker_input.blocks_to_copy is not None\n                and worker_input.blocks_to_copy.numel() > 0):\n            self.cache_engine[virtual_engine].copy(worker_input.blocks_to_copy)\n\n    def _get_cached_seq_group_metadata(\n            self,\n            seq_group_metadata_list: List[Union[SequenceGroupMetadata,\n                                                SequenceGroupMetadataDelta]],\n            finished_request_ids: List[str]) -> List[SequenceGroupMetadata]:\n        \"\"\"Return a list of cached Sequence Group Metadata after updating its\n        state.\n\n        It is used because scheduler only sends delta to workers to reduce\n        the data payload size. The function also cleans up cache based on\n        a given `finished_request_ids`.\n        \"\"\"\n        new_seq_group_metadata_list = []\n        for metadata_or_delta in seq_group_metadata_list:\n            request_id = metadata_or_delta.request_id\n            if request_id not in self._seq_group_metadata_cache:\n                # The first prefill.\n                assert isinstance(metadata_or_delta, SequenceGroupMetadata)\n                self._seq_group_metadata_cache[request_id] = metadata_or_delta\n            else:\n                # The first prefill is already cached.\n                if isinstance(metadata_or_delta, SequenceGroupMetadataDelta):\n                    self._seq_group_metadata_cache[request_id].apply_delta(\n                        metadata_or_delta)\n                else:\n                    # If metadata snapshot is sent again, it is\n                    # preempted. Reset the cache because we need to start\n                    # from scratch.\n                    assert isinstance(metadata_or_delta, SequenceGroupMetadata)\n                    self._seq_group_metadata_cache[\n                        request_id] = metadata_or_delta\n\n            new_seq_group_metadata_list.append(\n                self._seq_group_metadata_cache[request_id])\n\n        # Clean up finished ids\n        for finished_id in finished_request_ids:\n            del self._seq_group_metadata_cache[finished_id]\n\n        return new_seq_group_metadata_list\n\n    def _execute_model_spmd(\n        self,\n        execute_model_req: ExecuteModelRequest,\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n    ) -> Optional[List[SamplerOutput]]:\n        if execute_model_req is not None:\n            new_seq_group_metadata_list = self._get_cached_seq_group_metadata(\n                execute_model_req.seq_group_metadata_list,\n                execute_model_req.finished_requests_ids)\n\n            execute_model_req.seq_group_metadata_list = (\n                new_seq_group_metadata_list)\n        output = super()._execute_model_spmd(execute_model_req,\n                                             intermediate_tensors)\n        return output\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.model_runner.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.model_runner.remove_lora(lora_id)\n\n    def pin_lora(self, lora_id: int) -> bool:\n        return self.model_runner.pin_lora(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        return self.model_runner.list_loras()\n\n    def add_prompt_adapter(\n            self, prompt_adapter_request: PromptAdapterRequest) -> bool:\n        return self.model_runner.add_prompt_adapter(prompt_adapter_request)\n\n    def remove_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        return self.model_runner.remove_lora(prompt_adapter_id)\n\n    def pin_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        return self.model_runner.pin_prompt_adapter(prompt_adapter_id)\n\n    def list_prompt_adapters(self) -> Set[int]:\n        return self.model_runner.list_prompt_adapters()\n\n    @property\n    def max_model_len(self) -> int:\n        return self.model_config.max_model_len\n\n    @property\n    def vocab_size(self) -> int:\n        return self.model_runner.vocab_size\n\n    def get_cache_block_size_bytes(self) -> int:\n        \"\"\"Get the size of the KV cache block size in bytes.\n        \"\"\"\n        return CacheEngine.get_cache_block_size(self.cache_config,\n                                                self.model_config,\n                                                self.parallel_config)\n\n\ndef init_worker_distributed_environment(\n    parallel_config: ParallelConfig,\n    rank: int,\n    distributed_init_method: Optional[str] = None,\n    local_rank: int = -1,\n) -> None:\n    \"\"\"Initialize the distributed environment.\"\"\"\n    set_custom_all_reduce(not parallel_config.disable_custom_all_reduce)\n\n    init_distributed_environment(parallel_config.world_size, rank,\n                                 distributed_init_method, local_rank)\n\n    ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\n                                      parallel_config.pipeline_parallel_size)\n\n\ndef _check_if_gpu_supports_dtype(torch_dtype: torch.dtype):\n    # Check if the GPU supports the dtype.\n    if torch_dtype == torch.bfloat16:\n        compute_capability = current_platform.get_device_capability()\n        if compute_capability[0] < 8:\n            gpu_name = current_platform.get_device_name()\n            raise ValueError(\n                \"Bfloat16 is only supported on GPUs with compute capability \"\n                f\"of at least 8.0. Your {gpu_name} GPU has compute capability \"\n                f\"{compute_capability[0]}.{compute_capability[1]}. \"\n                \"You can use float16 instead by explicitly setting the\"\n                \"`dtype` flag in CLI, for example: --dtype=half.\")\n\n\ndef raise_if_cache_size_invalid(num_gpu_blocks, block_size,\n                                max_model_len) -> None:\n    if num_gpu_blocks <= 0:\n        raise ValueError(\"No available memory for the cache blocks. \"\n                         \"Try increasing `gpu_memory_utilization` when \"\n                         \"initializing the engine.\")\n    max_seq_len = block_size * num_gpu_blocks\n    if max_model_len > max_seq_len:\n        raise ValueError(\n            f\"The model's max seq len ({max_model_len}) \"\n            \"is larger than the maximum number of tokens that can be \"\n            f\"stored in KV cache ({max_seq_len}). Try increasing \"\n            \"`gpu_memory_utilization` or decreasing `max_model_len` when \"\n            \"initializing the engine.\")\n\n```\n# Function Signature: def initialize_cache(self, num_gpu_blocks: int,\n# Natural Language Annotations: \"\"\"Allocate GPU and CPU KV cache with the specified number of blocks.\n\n        This also warms up the model, which may record CUDA graphs.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef initialize_cache(self, num_gpu_block", "  ```\ndef initialize_cache(self, num_gpu_block", "  ```\ndef initialize_cache(self, num_gpu_block"]}, "vllm-vllm/worker/model_runner.py-prepare_model_input": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/worker/model_runner.py:\n```\nimport dataclasses\nimport gc\nimport itertools\nimport time\nimport warnings\nimport weakref\nfrom dataclasses import dataclass\nfrom typing import (TYPE_CHECKING, Any, Dict, List, Optional, Set, Tuple, Type,\n                    TypeVar, Union)\n\nimport numpy as np\nimport torch\nimport torch.distributed\nimport torch.nn as nn\n\ntry:\n    from flashinfer import BatchDecodeWithPagedKVCacheWrapper\n    from flashinfer.decode import CUDAGraphBatchDecodeWithPagedKVCacheWrapper\n    from flashinfer.prefill import BatchPrefillWithPagedKVCacheWrapper\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 256 * 1024 * 1024\nexcept ImportError:\n    BatchDecodeWithPagedKVCacheWrapper = None\n    CUDAGraphBatchDecodeWithPagedKVCacheWrapper = None\n    BatchPrefillWithPagedKVCacheWrapper = None\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 0\n\nimport vllm.envs as envs\nfrom vllm.attention import AttentionMetadata, get_attn_backend\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig)\nfrom vllm.distributed import get_pp_group\nfrom vllm.distributed.parallel_state import graph_capture\nfrom vllm.inputs import INPUT_REGISTRY, InputRegistry\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import LoRAMapping\nfrom vllm.lora.request import LoRARequest\nfrom vllm.lora.worker_manager import LRUCacheWorkerLoRAManager\nfrom vllm.model_executor import SamplingMetadata, SamplingMetadataCache\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.model_executor.model_loader.tensorizer import TensorizerConfig\nfrom vllm.model_executor.models.interfaces import (supports_lora,\n                                                   supports_multimodal)\nfrom vllm.model_executor.models.utils import set_cpu_offload_max_bytes\nfrom vllm.multimodal import (MULTIMODAL_REGISTRY, BatchedTensorInputs,\n                             MultiModalInputs, MultiModalRegistry)\nfrom vllm.prompt_adapter.layers import PromptAdapterMapping\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.prompt_adapter.worker_manager import (\n    LRUCacheWorkerPromptAdapterManager)\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (IntermediateTensors, SamplerOutput,\n                           SequenceGroupMetadata)\nfrom vllm.utils import (CudaMemoryProfiler, PyObjectCache, async_tensor_h2d,\n                        flatten_2d_lists, get_kv_cache_torch_dtype, is_hip,\n                        is_pin_memory_available)\nfrom vllm.worker.model_runner_base import (\n    ModelRunnerBase, ModelRunnerInputBase, ModelRunnerInputBuilderBase,\n    _add_attn_metadata_broadcastable_dict,\n    _add_sampling_metadata_broadcastable_dict,\n    _init_attn_metadata_from_tensor_dict,\n    _init_sampling_metadata_from_tensor_dict)\n\nif TYPE_CHECKING:\n    from vllm.attention.backends.abstract import AttentionBackend\n\nlogger = init_logger(__name__)\n\n_PAD_SLOT_ID = -1\nLORA_WARMUP_RANK = 8\n_BATCH_SIZE_ALIGNMENT = 8\n# Capture graphs for token size 1, 2, 4, 8, 16, 24, 32, 40, ..., 256.\n# NOTE: _get_graph_batch_size needs to be updated if this list is changed.\n_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [\n    _BATCH_SIZE_ALIGNMENT * i for i in range(1, 33)\n]\n_NUM_WARMUP_ITERS = 2\n\nTModelInputForGPU = TypeVar('TModelInputForGPU', bound=\"ModelInputForGPU\")\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPU(ModelRunnerInputBase):\n    \"\"\"\n    This base class contains metadata needed for the base model forward pass\n    but not metadata for possible additional steps, e.g., sampling. Model\n    runners that run additional steps should subclass this method to add\n    additional fields.\n    \"\"\"\n    input_tokens: Optional[torch.Tensor] = None\n    input_positions: Optional[torch.Tensor] = None\n    seq_lens: Optional[List[int]] = None\n    query_lens: Optional[List[int]] = None\n    lora_mapping: Optional[\"LoRAMapping\"] = None\n    lora_requests: Optional[Set[LoRARequest]] = None\n    attn_metadata: Optional[\"AttentionMetadata\"] = None\n    prompt_adapter_mapping: Optional[PromptAdapterMapping] = None\n    prompt_adapter_requests: Optional[Set[PromptAdapterRequest]] = None\n    multi_modal_kwargs: Optional[BatchedTensorInputs] = None\n    request_ids_to_seq_ids: Optional[Dict[str, List[int]]] = None\n    finished_requests_ids: Optional[List[str]] = None\n    virtual_engine: int = 0\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls: Type[TModelInputForGPU],\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> TModelInputForGPU:\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):\n    \"\"\"\n    Used by the ModelRunner.\n    \"\"\"\n    sampling_metadata: Optional[\"SamplingMetadata\"] = None\n    # Used for speculative decoding. We do not broadcast it because it is only\n    # used by the driver worker.\n    is_prompt: Optional[bool] = None\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        _add_sampling_metadata_broadcastable_dict(tensor_dict,\n                                                  self.sampling_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls,\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> \"ModelInputForGPUWithSamplingMetadata\":\n        tensor_dict = _init_sampling_metadata_from_tensor_dict(tensor_dict)\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\nclass ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n    \"\"\"Build ModelInputForGPU from SequenceGroupMetadata.\"\"\"\n\n    # Note: ideally we would be using a dataclass(kw_only=True)\n    # here, so that this can be subclassed easily,\n    # but kw_only is not supported in python<3.10.\n    class InterDataForSeqGroup:\n        \"\"\"Intermediate data for the current sequence group.\"\"\"\n\n        def simple_reinit(self):\n            self.input_tokens[0].clear()  # type: ignore\n            self.input_positions[0].clear()  # type: ignore\n            self.seq_lens[0] = 0  # type: ignore\n            self.orig_seq_lens[0] = 0  # type: ignore\n            self.query_lens[0] = 0  # type: ignore\n            self.context_lens[0] = 0  # type: ignore\n            self.curr_sliding_window_blocks[0] = 0  # type: ignore\n            self.lora_index_mapping.clear()  # type: ignore\n            self.lora_prompt_mapping.clear()  # type: ignore\n            self.lora_requests.clear()  # type: ignore\n            self.prompt_adapter_index_mapping.clear()  # type: ignore\n            self.prompt_adapter_prompt_mapping.clear()  # type: ignore\n\n        def __init__(\n            self,\n            *,\n            # From sequence group metadata.\n            request_id: str,\n            seq_ids: List[int],\n            is_prompt: bool,\n            block_tables: Optional[Dict[int, List[int]]],\n            computed_block_nums: List[int],\n            n_seqs: int = 0,\n\n            # Input tokens and positions.\n            input_tokens: Optional[List[List[int]]] = None,\n            input_positions: Optional[List[List[int]]] = None,\n\n            # The sequence length (may be capped to the sliding window).\n            seq_lens: Optional[List[int]] = None,\n            # The original sequence length (before applying sliding window).\n            # This is used to compute slot mapping.\n            orig_seq_lens: Optional[List[int]] = None,\n            # The query length.\n            query_lens: Optional[List[int]] = None,\n            # The number of tokens that are already computed.\n            context_lens: Optional[List[int]] = None,\n            # The current sliding window block.\n            curr_sliding_window_blocks: Optional[List[int]] = None,\n\n            # LoRA inputs.\n            lora_index_mapping: Optional[List[List[int]]] = None,\n            lora_prompt_mapping: Optional[List[List[int]]] = None,\n            lora_requests: Optional[Set[LoRARequest]] = None,\n\n            # Prompt adapter inputs.\n            prompt_adapter_index_mapping: Optional[List[int]] = None,\n            prompt_adapter_prompt_mapping: Optional[List[int]] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n\n            # Multi-modal inputs.\n            multi_modal_inputs: Optional[MultiModalInputs] = None,\n\n            # Whether the prefix cache is hit (prefill only).\n            prefix_cache_hit: bool = False,\n            reinit: bool = False,\n            reinit_use_defaults: bool = False,\n        ):\n            if reinit:\n                assert len(self.seq_ids) == len(seq_ids)  # type: ignore\n                for i, seq_id in enumerate(seq_ids):\n                    self.seq_ids[i] = seq_id  # type: ignore\n            else:\n                self.seq_ids = seq_ids\n\n            self.request_id = request_id\n            self.is_prompt = is_prompt\n            self.block_tables = block_tables\n            self.computed_block_nums = computed_block_nums\n            self.n_seqs = n_seqs\n\n            if reinit:\n                if len(self.seq_ids) == 1 and reinit_use_defaults:\n                    self.simple_reinit()\n                else:\n                    if input_tokens:\n                        self.input_tokens = input_tokens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.input_tokens[seq_id].clear()\n\n                    if input_positions:\n                        self.input_positions = input_positions\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.input_positions[seq_id].clear()\n\n                    if seq_lens:\n                        self.seq_lens = seq_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.seq_lens[seq_id] = 0\n\n                    if orig_seq_lens:\n                        self.orig_seq_lens = orig_seq_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.orig_seq_lens[seq_id] = 0\n\n                    if query_lens:\n                        self.query_lens = query_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.query_lens[seq_id] = 0\n\n                    if context_lens:\n                        self.context_lens = context_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.context_lens[seq_id] = 0\n\n                    if curr_sliding_window_blocks:\n                        self.curr_sliding_window_blocks = \\\n                            curr_sliding_window_blocks\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.curr_sliding_window_blocks[seq_id] = 0\n\n                    if lora_index_mapping:\n                        self.lora_index_mapping = lora_index_mapping\n                    else:\n                        self.lora_index_mapping.clear()\n\n                    if lora_prompt_mapping:\n                        self.lora_prompt_mapping = lora_prompt_mapping\n                    else:\n                        self.lora_prompt_mapping.clear()\n\n                    if lora_requests:\n                        self.lora_requests = lora_requests\n                    else:\n                        self.lora_requests.clear()\n\n                    if prompt_adapter_index_mapping:\n                        self.prompt_adapter_index_mapping = \\\n                            prompt_adapter_index_mapping\n                    else:\n                        self.prompt_adapter_index_mapping.clear()\n\n                    if prompt_adapter_prompt_mapping:\n                        self.prompt_adapter_prompt_mapping = \\\n                            prompt_adapter_prompt_mapping\n                    else:\n                        self.prompt_adapter_prompt_mapping.clear()\n\n            else:\n                self.input_tokens = input_tokens or []\n                self.input_positions = input_positions or []\n                self.seq_lens = seq_lens or []\n                self.orig_seq_lens = orig_seq_lens or []\n                self.query_lens = query_lens or []\n                self.context_lens = context_lens or []\n                self.curr_sliding_window_blocks = \\\n                    curr_sliding_window_blocks or []\n\n                self.lora_index_mapping = lora_index_mapping or []\n                self.lora_prompt_mapping = lora_prompt_mapping or []\n                self.lora_requests = lora_requests or set()\n\n                self.prompt_adapter_index_mapping = (\n                    prompt_adapter_index_mapping or [])\n                self.prompt_adapter_prompt_mapping = (\n                    prompt_adapter_prompt_mapping or [])\n\n            self.prompt_adapter_request = prompt_adapter_request\n            self.multi_modal_inputs = multi_modal_inputs\n            self.prefix_cache_hit = prefix_cache_hit\n\n            self.n_seqs = len(self.seq_ids)\n\n            if not reinit:\n                self.__post_init__()\n\n        def __post_init__(self):\n            self.n_seqs = len(self.seq_ids)\n\n            self.input_tokens = [[] for _ in range(self.n_seqs)]\n            self.input_positions = [[] for _ in range(self.n_seqs)]\n            self.seq_lens = [0] * self.n_seqs\n            self.orig_seq_lens = [0] * self.n_seqs\n            self.query_lens = [0] * self.n_seqs\n            self.context_lens = [0] * self.n_seqs\n            self.curr_sliding_window_blocks = [0] * self.n_seqs\n\n            self.lora_index_mapping = []\n            self.lora_prompt_mapping = []\n\n    def gen_inter_data_builder(self, num_seqs: int):\n        return lambda: ModelInputForGPUBuilder.InterDataForSeqGroup(\n            request_id=\"\",\n            seq_ids=[0] * num_seqs,\n            is_prompt=True,\n            block_tables=None,\n            computed_block_nums=[])\n\n    def init_cached_inter_data(self, *args, **kwargs):\n        assert len(args) == 0\n        assert \"seq_ids\" in kwargs\n        seq_ids = kwargs[\"seq_ids\"]\n        num_seqs = len(seq_ids)\n\n        # The inter-data cache is per model_runner\n        inter_data_cache = self.runner.inter_data_cache\n        if num_seqs not in inter_data_cache:\n            inter_data_cache[num_seqs] = PyObjectCache(\n                self.gen_inter_data_builder(num_seqs))\n\n        obj = inter_data_cache[num_seqs].get_object()\n        obj.__init__(*args, **kwargs)\n        return obj\n\n    def reset_cached_inter_data(self):\n        for cache in self.runner.inter_data_cache.values():\n            cache.reset()\n\n    def __init__(self,\n                 runner: \"GPUModelRunnerBase\",\n                 finished_requests_ids: Optional[List[str]] = None):\n        super().__init__()\n        # Compute functions for each sequence in a sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_compute_fns = [\n            self._compute_lens,\n            self._compute_for_prefix_cache_hit,\n            self._compute_for_sliding_window,\n            self._compute_lora_input,\n        ]\n        # Compute functions for each sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_group_compute_fns = [\n            self._compute_prompt_adapter_input,\n            self._compute_multi_modal_input,\n        ]\n\n        self.runner = runner\n        self.model_input_cls = self.runner._model_input_cls\n        self.attn_backend = self.runner.attn_backend\n        self.scheduler_config = self.runner.scheduler_config\n        self.sliding_window = self.runner.sliding_window\n        self.block_size = self.runner.block_size\n        self.enable_lora = self.runner.lora_config is not None\n        self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                      is not None)\n        self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n        self.finished_requests_ids = finished_requests_ids\n        self.decode_only = True\n\n        # Intermediate data (data in CPU before going to GPU) for\n        # the current sequence group.\n        self.inter_data_list: List[\n            ModelInputForGPUBuilder.InterDataForSeqGroup] = []\n\n        # Attention metadata inputs.\n        self.attn_metadata_builder = self.attn_backend.make_metadata_builder(\n            weakref.proxy(self))\n\n        # Engine/Model configurations.\n        self.chunked_prefill_enabled = (\n            self.scheduler_config is not None\n            and self.scheduler_config.chunked_prefill_enabled)\n        if self.sliding_window is not None:\n            self.sliding_window_blocks = (\n                self.sliding_window + self.block_size - 1) // self.block_size\n            self.block_aligned_sliding_window = \\\n                self.sliding_window_blocks * self.block_size\n\n    def _compute_lens(self, inter_data: InterDataForSeqGroup, seq_idx: int,\n                      seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Compute context length, sequence length and tokens\n        for the given sequence data.\n        \"\"\"\n        seq_data = seq_group_metadata.seq_data[inter_data.seq_ids[seq_idx]]\n        token_chunk_size = seq_group_metadata.token_chunk_size\n\n        # Compute context length (the number of tokens that are\n        # already computed) and sequence length (total number of tokens).\n        seq_len = seq_data.get_len()\n        if inter_data.is_prompt:\n            context_len = seq_data.get_num_computed_tokens()\n        else:\n            # get_num_computed_tokens is incorrect for spec decoding.\n            # So, we should have a special logic here.\n            # TODO(sang): Fix it.\n            context_len = seq_len - 1\n        seq_len = min(seq_len, context_len + token_chunk_size)\n\n        # Compute tokens.\n        if inter_data.is_prompt:\n            tokens = seq_data.get_token_ids()\n            if context_len != 0 or seq_len < len(tokens):\n                tokens = tokens[context_len:seq_len]\n        else:\n            # Optimization. get_token_ids requires the entire copy of\n            # tokens.\n            tokens = seq_data.get_last_token_id()\n\n        inter_data.seq_lens[seq_idx] = seq_len\n        inter_data.orig_seq_lens[seq_idx] = seq_len\n        inter_data.context_lens[seq_idx] = context_len\n\n        if isinstance(tokens, list):\n            inter_data.input_tokens[seq_idx].extend(tokens)\n        else:\n            inter_data.input_tokens[seq_idx].append(tokens)\n\n        if (seq_len - context_len) == 1:\n            inter_data.input_positions[seq_idx].append(seq_len - 1)\n        else:\n            inter_data.input_positions[seq_idx].extend(\n                range(context_len, seq_len))\n\n        inter_data.query_lens[\n            seq_idx] = seq_len - context_len if inter_data.is_prompt else 1\n\n    def _compute_for_prefix_cache_hit(\n            self, inter_data: InterDataForSeqGroup, seq_idx: int,\n            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Check if hit prefix cache (i.e., some blocks are already computed).\n        If hit, update input tokens and positions to only compute the\n        remaining blocks.\n        \"\"\"\n        computed_block_nums = inter_data.computed_block_nums\n\n        # Note that prefix caching does not support sliding window.\n        prefix_cache_hit = (computed_block_nums is not None\n                            and len(computed_block_nums) > 0\n                            and self.sliding_window is None\n                            and inter_data.is_prompt)\n        inter_data.prefix_cache_hit = prefix_cache_hit\n        if self.chunked_prefill_enabled and prefix_cache_hit:\n            raise RuntimeError(\n                \"chunked prefill cannot be used with prefix caching now.\")\n\n        # If prefix cache is hit, advance context length to bypass\n        # hit blocks. Accordingly, input tokens, position and query length\n        # have to be updated.\n        if prefix_cache_hit:\n            assert computed_block_nums is not None\n            context_len = len(computed_block_nums) * self.block_size\n            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n                seq_idx][context_len:]\n            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n                seq_idx][context_len:]\n            inter_data.context_lens[seq_idx] = context_len\n            inter_data.query_lens[\n                seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n\n    def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,\n                                    seq_idx: int,\n                                    seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Update seq_len and curr_sliding_window_block for the given\n        sequence data (only required by decoding) if sliding window is enabled.\n        \"\"\"\n        curr_sliding_window_block = 0\n        sliding_seq_len = inter_data.seq_lens[seq_idx]\n        if not inter_data.is_prompt and self.sliding_window is not None:\n            # TODO(sang): This is a hack to make sliding window work with\n            # paged attn. We can remove it if we make paged attn kernel\n            # to properly handle slinding window attn.\n            curr_sliding_window_block = self.sliding_window_blocks\n            if self.scheduler_config.use_v2_block_manager:\n                # number of elements in last block\n                suff_len = inter_data.seq_lens[seq_idx] % self.block_size\n                sliding_seq_len = min(\n                    inter_data.seq_lens[seq_idx],\n                    self.block_aligned_sliding_window + suff_len)\n                if suff_len > 0:\n                    curr_sliding_window_block += 1\n            else:\n                sliding_seq_len = min(inter_data.seq_lens[seq_idx],\n                                      self.sliding_window)\n\n        inter_data.curr_sliding_window_blocks[\n            seq_idx] = curr_sliding_window_block\n        inter_data.seq_lens[seq_idx] = sliding_seq_len\n\n    def _compute_lora_input(self, inter_data: InterDataForSeqGroup,\n                            seq_idx: int,\n                            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If LoRA is enabled, compute LoRA index and prompt mapping.\"\"\"\n        if not self.enable_lora:\n            return\n\n        lora_id = seq_group_metadata.lora_int_id\n        if lora_id > 0:\n            inter_data.lora_requests.add(seq_group_metadata.lora_request)\n        query_len = inter_data.query_lens[seq_idx]\n        inter_data.lora_index_mapping\n```\n# Function Signature: def prepare_model_input(\n# Natural Language Annotations: \"\"\"Prepare the model input based on a given sequence group, including\n        metadata for the sampling step.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef prepare_model_input(\n    seq_group_", "  ```\ndef prepare_model_input(self,\n                        seq", "  ```\ndef prepare_model_input(self, seq_group_"]}, "vllm-vllm/worker/enc_dec_model_runner.py-prepare_model_input": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/worker/enc_dec_model_runner.py:\n```\nimport dataclasses\nfrom typing import Any, Dict, List, Optional, Tuple, Type, cast\n\nimport torch\nimport torch.distributed\n\nfrom vllm.attention.backends.abstract import (AttentionBackend,\n                                              AttentionMetadata)\nfrom vllm.attention.selector import (_Backend, get_env_variable_attn_backend,\n                                     get_global_forced_attn_backend,\n                                     global_force_attn_backend)\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig)\nfrom vllm.inputs import INPUT_REGISTRY, InputRegistry\nfrom vllm.logger import init_logger\nfrom vllm.model_executor import SamplingMetadata\nfrom vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (IntermediateTensors, PoolerOutput, SamplerOutput,\n                           SequenceGroupMetadata)\nfrom vllm.utils import STR_NOT_IMPL_ENC_DEC_BACKEND, make_tensor_with_pad\nfrom vllm.worker.model_runner import (_PAD_SLOT_ID, GPUModelRunnerBase,\n                                      ModelInputForGPUBuilder,\n                                      ModelInputForGPUWithSamplingMetadata)\nfrom vllm.worker.model_runner_base import (\n    _add_attn_metadata_broadcastable_dict,\n    _add_sampling_metadata_broadcastable_dict)\nfrom vllm.worker.utils import assert_enc_dec_mr_supported_scenario\n\nlogger = init_logger(__name__)\n\n\n@dataclasses.dataclass(frozen=True)\nclass EncoderDecoderModelInput(ModelInputForGPUWithSamplingMetadata):\n    \"\"\"\n    Used by the EncoderDecoderModelRunner.\n    \"\"\"\n    encoder_input_tokens: Optional[torch.Tensor] = None\n    encoder_input_positions: Optional[torch.Tensor] = None\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"encoder_input_tokens\": self.encoder_input_tokens,\n            \"encoder_input_positions\": self.encoder_input_positions,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        _add_sampling_metadata_broadcastable_dict(tensor_dict,\n                                                  self.sampling_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls,\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> \"EncoderDecoderModelInput\":\n        return cast(\n            EncoderDecoderModelInput,\n            super().from_broadcasted_tensor_dict(tensor_dict, attn_backend))\n\n\nclass EncoderDecoderModelRunner(GPUModelRunnerBase[EncoderDecoderModelInput]):\n    _model_input_cls: Type[EncoderDecoderModelInput] = (\n        EncoderDecoderModelInput)\n    _builder_cls: Type[ModelInputForGPUBuilder] = (ModelInputForGPUBuilder)\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        kv_cache_dtype: Optional[str] = \"auto\",\n        is_driver_worker: bool = False,\n        prompt_adapter_config: Optional[PromptAdapterConfig] = None,\n        observability_config: Optional[ObservabilityConfig] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n    ):\n        '''\n        EncoderDecoderModelRunner constructor.\n\n        `lora_config` and `prompt_adapter_config` are\n        unused (since these features are not yet supported for encoder/decoder\n        models) but these arguments are present here for compatibility with \n        the base-class constructor.\n        '''\n\n        self._maybe_force_supported_attention_backend()\n\n        super().__init__(\n            model_config,\n            parallel_config,\n            scheduler_config,\n            device_config,\n            cache_config,\n            load_config,\n            lora_config=None,\n            kv_cache_dtype=kv_cache_dtype,\n            is_driver_worker=is_driver_worker,\n        )\n\n        # Crash for unsupported encoder/scenarios\n        assert_enc_dec_mr_supported_scenario(self)\n\n    def _maybe_force_supported_attention_backend(self):\n        '''\n        Force vLLM to use the XFormers attention backend,\n        which is currently the only supported option.\n        '''\n\n        def raise_backend_err():\n            # The user has specified an attention backend override\n            # which is invalid for encoder/decoder models\n            raise NotImplementedError(STR_NOT_IMPL_ENC_DEC_BACKEND)\n\n        maybe_env_var_forced_backend = get_env_variable_attn_backend()\n        maybe_global_forced_backend = get_global_forced_attn_backend()\n        is_forced_by_global = maybe_global_forced_backend is not None\n        is_forced_by_env_var = maybe_env_var_forced_backend is not None\n\n        if not (is_forced_by_global or is_forced_by_env_var):\n            # The user has not already specified an attention backend\n            # override\n            logger.info(\"EncoderDecoderModelRunner requires \"\n                        \"XFormers backend; overriding backend \"\n                        \"auto-selection and forcing XFormers.\")\n            global_force_attn_backend(_Backend.XFORMERS)\n        elif is_forced_by_global:\n            # Backend override enforced by global variable takes\n            # precedence over vLLM backend environment variable.\n            if maybe_global_forced_backend != _Backend.XFORMERS:\n                raise_backend_err()\n        elif is_forced_by_env_var:\n            # Backend override enforced by vLLM backend\n            # environment variable\n            if maybe_env_var_forced_backend != _Backend.XFORMERS:\n                raise_backend_err()\n\n    def _list_to_int32_tensor(\n        self,\n        _list: List[int],\n    ) -> torch.Tensor:\n        return torch.tensor(_list, dtype=torch.int32, device=self.device)\n\n    def _list_to_long_tensor(\n        self,\n        _list: List[int],\n    ) -> torch.Tensor:\n        return torch.tensor(_list, dtype=torch.long, device=self.device)\n\n    def _empty_int32_tensor(self) -> torch.Tensor:\n        return self._list_to_int32_tensor([])\n\n    def _empty_long_tensor(self) -> torch.Tensor:\n        return self._list_to_long_tensor([])\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        model_input: EncoderDecoderModelInput,\n        kv_caches: List[torch.Tensor],\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        num_steps: int = 1,\n    ) -> Optional[List[PoolerOutput]]:\n        if num_steps > 1:\n            raise ValueError(\"num_steps > 1 is not supported in \"\n                             \"EncoderDecoderModelRunner\")\n\n        model_executable = self.model\n\n        seqlen_agnostic_kwargs = {\n            \"finished_requests_ids\": model_input.finished_requests_ids,\n            \"request_ids_to_seq_ids\": model_input.request_ids_to_seq_ids,\n        } if self.has_seqlen_agnostic else {}\n        hidden_or_intermediate_states = model_executable(\n            input_ids=model_input.input_tokens,\n            positions=model_input.input_positions,\n            encoder_input_ids=model_input.encoder_input_tokens,\n            encoder_positions=model_input.encoder_input_positions,\n            kv_caches=kv_caches,\n            attn_metadata=model_input.attn_metadata,\n            intermediate_tensors=intermediate_tensors,\n            **seqlen_agnostic_kwargs)\n\n        logits = self.model.compute_logits(hidden_or_intermediate_states,\n                                           model_input.sampling_metadata)\n\n        if not self.is_driver_worker:\n            return []\n\n        # Sample the next token.\n        output: SamplerOutput = self.model.sample(\n            logits=logits,\n            sampling_metadata=model_input.sampling_metadata,\n        )\n\n        return [output]\n\n    def make_model_input_from_broadcasted_tensor_dict(\n            self, tensor_dict: Dict[str, Any]) -> EncoderDecoderModelInput:\n        return EncoderDecoderModelInput.from_broadcasted_tensor_dict(\n            tensor_dict,\n            attn_backend=self.attn_backend,\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @torch.inference_mode()\n    def profile_run(self) -> None:\n        # Enable top-k sampling to reflect the accurate memory usage.\n        sampling_params = SamplingParams(top_p=0.99, top_k=self.vocab_size - 1)\n        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens\n        max_num_seqs = self.scheduler_config.max_num_seqs\n\n        # Profile memory usage with max_num_sequences sequences and the total\n        # number of tokens equal to max_num_batched_tokens.\n        seqs: List[SequenceGroupMetadata] = []\n\n        max_mm_tokens = self.mm_registry.get_max_multimodal_tokens(\n            self.model_config)\n        if max_mm_tokens > 0:\n            raise NotImplementedError(\n                \"Multi-modal encoder-decoder models are not supported yet\")\n\n        batch_size = 0\n        for group_id in range(max_num_seqs):\n            seq_len = (max_num_batched_tokens // max_num_seqs +\n                       (group_id < max_num_batched_tokens % max_num_seqs))\n            batch_size += seq_len\n\n            seq_data, _ = self.input_registry \\\n                .dummy_data_for_profiling(self.model_config,\n                                          seq_len,\n                                          self.mm_registry)\n\n            # Having more tokens is over-conservative but otherwise fine\n            assert len(seq_data.prompt_token_ids) >= seq_len, (\n                f\"Expected at least {seq_len} dummy tokens for profiling, \"\n                f\"but got: {len(seq_data.prompt_token_ids)}\")\n\n            seq = SequenceGroupMetadata(\n                request_id=str(group_id),\n                is_prompt=True,\n                seq_data={group_id: seq_data},\n                sampling_params=sampling_params,\n                block_tables=None,\n                encoder_seq_data=seq_data,\n                cross_block_table=None,\n            )\n            seqs.append(seq)\n\n        # Run the model with the dummy inputs.\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\n        kv_caches = [None] * num_layers\n        finished_requests_ids = [seq.request_id for seq in seqs]\n        model_input = self.prepare_model_input(\n            seqs, finished_requests_ids=finished_requests_ids)\n        intermediate_tensors = None\n        self.execute_model(model_input, kv_caches, intermediate_tensors)\n        torch.cuda.synchronize()\n        return\n\n    def _prepare_encoder_model_input_tensors(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        model_input: EncoderDecoderModelInput,\n    ) -> Tuple[AttentionMetadata, Optional[torch.Tensor],\n               Optional[torch.Tensor]]:\n        \"\"\"Helper method to prepare the encoder- and cross-attn-related\n        model inputs based on a given sequence group. These additional inputs\n        are used to augment an already-computed `EncoderDecoderModelInput`\n        data structure which already has decoder-related model inputs\n        populated.\n\n        Sets the following attn_metadata fields:\n        * `num_encoder_tokens`\n        * `encoder_seq_lens`\n        * `encoder_seq_lens_tensor`\n        * `max_encoder_seq_len`\n        * `cross_slot_mapping`\n        * `cross_block_tables`\n\n        Constructs a new model inputs data structure, based on\n        (1) the existing fields in the `model_inputs` argument,\n        and (2) the following additional fields which are\n        computed (or in the case of `attn_metadata`, updated) \n        by this function:\n        * attn_metadata\n        * encoder_input_tokens\n        * encoder_input_positions\n\n        Arguments:\n\n        * seq_group_metadata_list: list of sequence groups for which to\n                                   compute inputs\n        * model_inputs: model inputs data structure with decoder-oriented\n                        fields already computed.\n\n        Return:\n\n        * Updated model inputs data structure\n        \"\"\"\n\n        if len(seq_group_metadata_list) == 0:\n            return (model_input.attn_metadata, None, None)\n\n        # Since we are not supporting chunked prefill either the entire\n        # batch is prefill or it is decode\n        is_prompt = seq_group_metadata_list[0].is_prompt\n\n        # Build encoder inputs\n        encoder_seq_lens: List[int] = []\n        if is_prompt:\n            # Prefill phase.\n            cross_block_tables = self._empty_int32_tensor().view(\n                len(seq_group_metadata_list), -1)\n\n            # Extract input tokens/positions, cross-attention slot-mapping,\n            # & seq len from each sequence group metadata\n            (\n                encoder_input_tokens,\n                encoder_input_positions,\n                cross_slot_mapping,\n            ) = (\n                [],\n                [],\n                [],\n            )\n            for seq_group_metadata in seq_group_metadata_list:\n                # Build seq lens\n                seq_len = seq_group_metadata.encoder_seq_data.get_len()\n                token_ids = seq_group_metadata.encoder_seq_data.get_token_ids()\n                encoder_seq_lens.append(seq_len)\n\n                # Build slot mapping\n                is_profile_run = (seq_group_metadata.block_tables is None)\n                if is_profile_run:\n                    # During memory profiling, the block tables are not\n                    # initialized yet. In this case, we just use a dummy\n                    # slot mapping.\n                    # In embeddings, the block tables are {seq_id: None}.\n                    cross_slot_mapping.extend([_PAD_SLOT_ID] * seq_len)\n                else:\n                    for i in range(0, seq_len):\n                        block_number = seq_group_metadata.cross_block_table[\n                            i // self.block_size]\n                        block_offset = i % self.block_size\n                        slot = block_number * self.block_size + block_offset\n                        cross_slot_mapping.append(slot)\n\n                # Build encoder input tokens\n                encoder_input_tokens.extend(token_ids)\n                encoder_input_positions.extend(list(range(0, seq_len)))\n\n            # Convert tokens/positions & cross-attention\n            # slot-mapping to encoder input tensors\n            encoder_input_tokens_tensor = self._list_to_long_tensor(\n                encoder_input_tokens)\n            encoder_input_positions_tensor = self._list_to_long_tensor(\n                encoder_input_positions)\n            cross_slot_mapping_tensor = self._list_to_long_tensor(\n                cross_slot_mapping)\n\n        else:\n            # Decode phase.\n            encoder_input_tokens_tensor = self._empty_long_tensor()\n            encoder_input_positions_tensor = self._empty_long_tensor()\n            cross_slot_mapping_tensor = self._empty_long_tensor()\n\n            # Extract cross-attention block tables &\n            # seq len from each sequence group metadata.\n            # Cross-attention block tables are empty\n            # during vLLM memory profiling.\n            cross_block_tables = []\n            for seq_group_metadata in seq_group_metadata_list:\n                encoder_seq_lens.append(\n                    seq_group_metadata.encoder_seq_data.get_len())\n                cross_block_table = seq_group_metadata.cross_block_table\n                cross_block_tables.append([] if (\n                    cross_block_table is None) else cross_block_table)\n\n            # Convert cross-attention block tables to encoder input tensor\n            cross_block_tables = make_tensor_with_pad(\n                cross_block_tables,\n                max_len=max(\n                    len(block_table) for block_table in cross_block_tables),\n                pad=0,\n                dtype=torch.int32,\n                device=self.device,\n            )\n\n        # Compute encoder sequence lengths & encoder\n        # sequence starting offset tensors\n        max_encoder_seq_len = max(encoder_seq_lens, default=0)\n        encoder_seq_lens_tensor = self._list_to_int32_tensor(encoder_seq_lens)\n        encoder_seq_start_loc = torch.zeros(encoder_seq_lens_tensor.shape[0] +\n                                            1,\n                                            dtype=torch.int32,\n                                            device=self.device)\n        torch.cumsum(encoder_seq_lens_tensor,\n                     dim=0,\n                     dtype=encoder_seq_start_loc.dtype,\n                     out=encoder_seq_start_loc[1:])\n\n        # Update attention metadata with encoder-oriented attributes\n        attn_metadata = model_input.attn_metadata\n        assert attn_metadata is not None\n        (\n            attn_metadata.num_encoder_tokens,\n            attn_metadata.encoder_seq_lens,\n            attn_metadata.encoder_seq_lens_tensor,\n            attn_metadata.max_encoder_seq_len,\n            attn_metadata.cross_slot_mapping,\n            attn_metadata.cross_block_tables,\n        ) = (\n            sum(encoder_seq_lens),\n            encoder_seq_lens,\n            encoder_seq_lens_tensor,\n            max_encoder_seq_len,\n            cross_slot_mapping_tensor,\n            cross_block_tables,\n        )\n\n        return (attn_metadata, encoder_input_tokens_tensor,\n                encoder_input_positions_tensor)\n\n```\n# Function Signature: def prepare_model_input(\n# Natural Language Annotations: \"\"\"Prepare the model input based on a given sequence group, including\n        metadata for the sampling step.\n\n        Since chunked prefill is not supported for encoder/decoder models,\n        `input_tokens` is assumed to be either entirely prefill tokens or\n        entirely decode tokens.\n\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef prepare_model_input(self,\n                        seq", "  ```\ndef prepare_model_input(\n    self,\n   ", "  ```\ndef prepare_model_input(\n    seq_group_"]}, "vllm-vllm/core/block_manager_v1.py-append_slots": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/block_manager_v1.py:\n```\n\"\"\"A block manager that manages token blocks.\"\"\"\nimport math\nfrom abc import ABC, abstractmethod\nfrom itertools import count, takewhile\nfrom os.path import commonprefix\nfrom typing import Dict, List, Optional\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Tuple\n\nfrom vllm.block import BlockTable, PhysicalTokenBlock\nfrom vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\nfrom vllm.core.evictor_v1 import EvictionPolicy, Evictor, make_evictor\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\n\nlogger = init_logger(__name__)\n\n\nclass BlockAllocatorBase(ABC):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self,\n                 device: Device,\n                 block_size: int,\n                 num_blocks: int,\n                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU):\n        pass\n\n    @abstractmethod\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        pass\n\n    @abstractmethod\n    def free(self, block: PhysicalTokenBlock) -> None:\n        pass\n\n    @abstractmethod\n    def get_num_free_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def get_num_total_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def contains_block(self, block_hash: int) -> bool:\n        pass\n\n    @abstractmethod\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        pass\n\n\nclass CachedBlockAllocator(BlockAllocatorBase):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    def __init__(self,\n                 device: Device,\n                 block_size: int,\n                 num_blocks: int,\n                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU) -> None:\n        self.device = device\n        self.block_size = block_size\n        self.num_blocks = num_blocks\n\n        self.current_num_blocks = 0\n        self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n\n        self.evictor: Evictor = make_evictor(eviction_policy)\n\n        self.default_hash_ctr = count()\n\n    def allocate_block(self, block_hash: int,\n                       num_hashed_tokens: int) -> PhysicalTokenBlock:\n        if self.current_num_blocks == self.num_blocks:\n            block = self.evictor.evict()\n            block.block_hash = block_hash\n            block.num_hashed_tokens = num_hashed_tokens\n            return block\n        block = PhysicalTokenBlock(device=self.device,\n                                   block_number=self.current_num_blocks,\n                                   block_size=self.block_size,\n                                   block_hash=block_hash,\n                                   num_hashed_tokens=num_hashed_tokens)\n        self.current_num_blocks += 1\n        return block\n\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        if block_hash is None:\n            block_hash = next(self.default_hash_ctr)\n        if block_hash in self.evictor:\n            assert block_hash not in self.cached_blocks\n            block = self.evictor.remove(block_hash)\n            assert block.ref_count == 0\n            self.cached_blocks[block_hash] = block\n            block.ref_count += 1\n            assert block.block_hash == block_hash\n            return block\n        if block_hash not in self.cached_blocks:\n            self.cached_blocks[block_hash] = self.allocate_block(\n                block_hash, num_hashed_tokens)\n        block = self.cached_blocks[block_hash]\n        assert block.block_hash == block_hash\n        block.ref_count += 1\n        return block\n\n    def free(self, block: PhysicalTokenBlock) -> None:\n        if block.ref_count == 0:\n            raise ValueError(f\"Double free! {block} is already freed.\")\n        block.ref_count -= 1\n        if block.ref_count == 0:\n            assert block.block_hash not in self.evictor\n            self.evictor.add(block)\n\n            # Remove the block from the cached_blocks\n            del self.cached_blocks[block.block_hash]\n\n    def get_num_free_blocks(self) -> int:\n        return (self.num_blocks - self.current_num_blocks +\n                self.evictor.num_blocks)\n\n    def get_num_total_blocks(self) -> int:\n        return self.num_blocks\n\n    def contains_block(self, block_hash: int) -> bool:\n        return block_hash in self.cached_blocks or block_hash in self.evictor\n\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        # Update the hash of block and the cached_blocks dictionary.\n        assert not self.contains_block(block_hash)\n        old_hash = block.block_hash\n        block.block_hash = block_hash\n        del self.cached_blocks[old_hash]\n        self.cached_blocks[block_hash] = block\n\n\nclass UncachedBlockAllocator(BlockAllocatorBase):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: Device,\n        block_size: int,\n        num_blocks: int,\n    ) -> None:\n        self.device = device\n        self.block_size = block_size\n        self.num_blocks = num_blocks\n\n        # Initialize the free blocks.\n        self.free_blocks: List[PhysicalTokenBlock] = []\n        for i in range(num_blocks):\n            block = PhysicalTokenBlock(device=device,\n                                       block_number=i,\n                                       block_size=block_size,\n                                       block_hash=-1,\n                                       num_hashed_tokens=0)\n            self.free_blocks.append(block)\n\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        if not self.free_blocks:\n            raise ValueError(\"Out of memory! No free blocks are available.\")\n        block = self.free_blocks.pop()\n        block.ref_count = 1\n        return block\n\n    def free(self, block: PhysicalTokenBlock) -> None:\n        if block.ref_count == 0:\n            raise ValueError(f\"Double free! {block} is already freed.\")\n        block.ref_count -= 1\n        if block.ref_count == 0:\n            self.free_blocks.append(block)\n\n    def get_num_free_blocks(self) -> int:\n        return len(self.free_blocks)\n\n    def get_num_total_blocks(self) -> int:\n        return self.num_blocks\n\n    def contains_block(self, block_hash: int) -> bool:\n        raise NotImplementedError(\n            \"Invalid codepath for uncached block allocator.\")\n\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        raise NotImplementedError(\n            \"Invalid codepath for uncached block allocator.\")\n\n\nclass BlockSpaceManagerV1(BlockSpaceManager):\n    \"\"\"Manages the mapping between logical and physical token blocks.\"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        watermark: float = 0.01,\n        sliding_window: Optional[int] = None,\n        enable_caching: bool = False,\n    ) -> None:\n        self.block_size = block_size\n        self.num_total_gpu_blocks = num_gpu_blocks\n        self.num_total_cpu_blocks = num_cpu_blocks\n\n        if enable_caching and sliding_window is not None:\n            raise NotImplementedError(\n                \"Sliding window is not allowed with prefix caching enabled!\")\n\n        self.block_sliding_window = None\n        if sliding_window is not None:\n            # Round up to nearest block size to regularize sliding window\n            # allocation sizes.\n            self.block_sliding_window = math.ceil(sliding_window / block_size)\n\n        self.watermark = watermark\n        assert watermark >= 0.0\n\n        self.enable_caching = enable_caching\n\n        self.watermark_blocks = int(watermark * num_gpu_blocks)\n\n        if self.enable_caching:\n            logger.info(\"Automatic prefix caching is enabled.\")\n            self.gpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n                Device.GPU, block_size, num_gpu_blocks)\n            self.cpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n                Device.CPU, block_size, num_cpu_blocks)\n        else:\n            self.gpu_allocator = UncachedBlockAllocator(\n                Device.GPU, block_size, num_gpu_blocks)\n            self.cpu_allocator = UncachedBlockAllocator(\n                Device.CPU, block_size, num_cpu_blocks)\n        # Mapping: seq_id -> BlockTable.\n        self.block_tables: Dict[int, BlockTable] = {}\n\n        # Mapping: req_id -> BlockTable\n        # Note that each SequenceGroup has a unique\n        # request ID\n        self.cross_block_tables: Dict[str, BlockTable] = {}\n\n    def _get_seq_num_required_blocks(self, seq: Sequence) -> int:\n        return 0 if seq is None else seq.n_blocks\n\n    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n        # FIXME(woosuk): Here we assume that all sequences in the group share\n        # the same prompt. This may not be true for preempted sequences.\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        self_num_required_blocks = self._get_seq_num_required_blocks(\n            seq_group.get_seqs(status=SequenceStatus.WAITING)[0])\n        cross_num_required_blocks = self._get_seq_num_required_blocks(\n            seq_group.get_encoder_seq())\n        num_required_blocks = self_num_required_blocks + \\\n                              cross_num_required_blocks\n\n        if self.block_sliding_window is not None:\n\n            num_required_blocks = min(num_required_blocks,\n                                      self.block_sliding_window)\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n\n        # Use watermark to avoid frequent cache eviction.\n        if (self.num_total_gpu_blocks - num_required_blocks <\n                self.watermark_blocks):\n            return AllocStatus.NEVER\n        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _allocate_sequence(self, \\\n                           seq: Sequence, \\\n                           ref_count: int, \\\n                           is_encoder_decoder: bool = True) -> BlockTable:\n        # Allocate new physical token blocks that will store the prompt tokens.\n        num_prompt_blocks = seq.n_blocks\n\n        block_table: BlockTable = BlockTable()\n        for logical_idx in range(num_prompt_blocks):\n            if (self.block_sliding_window is not None\n                    and logical_idx >= self.block_sliding_window):\n                block = block_table[logical_idx % self.block_sliding_window]\n                # Set the reference counts of the token blocks.\n                block.ref_count = ref_count\n            elif not is_encoder_decoder and self.enable_caching:\n                block = self.gpu_allocator.allocate(\n                    seq.hash_of_block(logical_idx),\n                    seq.num_hashed_tokens_of_block(logical_idx))\n            else:\n                block = self.gpu_allocator.allocate()\n                # Set the reference counts of the token blocks.\n                block.ref_count = ref_count\n            block_table.append(block)\n\n        return block_table\n\n    def allocate(self, seq_group: SequenceGroup) -> None:\n        is_encoder_decoder = seq_group.is_encoder_decoder()\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        # Allocate decoder sequences\n        #\n        # NOTE: Here we assume that all sequences in the group have the same\n        # decoder prompt.\n        wait_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n        seq = wait_seqs[0]\n        block_table: BlockTable = \\\n            self._allocate_sequence(seq,\n                                    seq_group.num_seqs(),\n                                    is_encoder_decoder)\n\n        # Assign the self-attention block tables for each sequence.\n        if len(wait_seqs) == 1:\n            self.block_tables[seq.seq_id] = block_table\n        else:\n            for seq in wait_seqs:\n                self.block_tables[seq.seq_id] = block_table.copy()\n\n        # Allocate encoder sequence\n        if is_encoder_decoder:\n            # A SequenceGroup has only a single encoder sequence (at most),\n            # thus allocate with a ref count of 1\n            block_table = self._allocate_sequence(seq_group.get_encoder_seq(),\n                                                  1, is_encoder_decoder)\n            # Assign the cross-attention block table for the SequenceGroup.\n            self.cross_block_tables[seq_group.request_id] = block_table\n\n    def can_append_slots(self,\n                         seq_group: SequenceGroup,\n                         num_lookahead_slots: int = 0) -> bool:\n        assert (num_lookahead_slots == 0\n                ), \"lookahead allocation not supported in BlockSpaceManagerV1\"\n\n        # Simple heuristic: If there is at least one free block\n        # for each sequence, we can append.\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)\n        return num_seqs <= num_free_gpu_blocks\n\n    def _promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        assert self.enable_caching\n\n        # Compute a new hash for the block so that it can be shared by other\n        # Sequences\n        new_hash = seq.hash_of_block(seq.n_blocks - 1)\n\n        # if new_hash is already in the cached table, then free last_block\n        # and return the cached version\n        if self.gpu_allocator.contains_block(new_hash):\n            self.gpu_allocator.free(last_block)\n            return self.gpu_allocator.allocate(new_hash)\n        else:\n            self.gpu_allocator.update_hash(new_hash, last_block)\n            return last_block\n\n    def _is_last_block_full(\n        self,\n        seq: Sequence,\n    ) -> bool:\n        token_ids_len = seq.data.get_len()\n        return token_ids_len > 0 and token_ids_len % seq.block_size == 0\n\n    def _maybe_promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        if self._is_last_block_full(seq):\n            return self._promote_last_block(seq, last_block)\n        else:\n            return last_block\n\n    def _allocate_last_physical_block(\n        self,\n        seq: Sequence,\n    ) -> PhysicalTokenBlock:\n        # Called before a new block is appended.\n        # This is in charge of allocating a new physical block (to be appended).\n\n        # None if the last block is not full. Otherwise, we set it to the\n        # content hash.\n        if not self.enable_caching:\n            return self.gpu_allocator.allocate()\n        block_hash: Optional[int] = None\n        n_blocks = seq.n_blocks\n        if (self._is_last_block_full(seq)):\n            block_hash = seq.hash_of_block(n_blocks - 1)\n        num_hashed_tokens = seq.num_hashed_tokens_of_block(n_blocks - 1)\n\n        # num_hashed_tokens is used to compute future hashes\n        # (e.g. in the hashing function, it is used to ask the sequence for\n        # prefix tokens)\n        new_block = self.gpu_allocator.allocate(block_hash, num_hashed_tokens)\n\n        # If the block has is None, then the block is not full.\n        # If the block is not full, then we expect it to have a refcount of 1.\n        if block_hash is None:\n            assert new_block.ref_count == 1\n        return new_block\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        # NOTE: fork does not allocate a new physical block.\n        # Thus, it is always safe from OOM.\n        if parent_seq.seq_id not in self.block_tables:\n            # Parent sequence has either been freed or never existed.\n            return\n        src_block_table = self.block_tables[parent_seq.seq_id]\n        self.block_tables[child_seq.seq_id] = src_block_table.copy()\n\n        # When using a sliding window, blocks will be eventually reused.\n        # In this case the block tables will contain repeated blocks.\n        # When forking, we must make sure that each block's `ref_count`\n        # is only incremented by one, so we deduplicate them by wrapping\n        # them in a set.\n        for block in set(src_block_table):\n            block.ref_count += 1\n\n    def _get_physical_blocks(\n            self, seq_group: SequenceGroup) -> List[PhysicalTokenBlock]:\n\n        # NOTE: Here, we assume that the physical blocks are only shared by\n        # the sequences in the same group.\n        request_id = seq_group.request_id\n        blocks: Set[PhysicalTokenBlock] = set()\n        for seq in seq_group.get_seqs():\n            if seq.is_finished():\n                continue\n            blocks.update(self.block_tables[seq.seq_id])\n        # Cross-attention blocks\n        if seq_group.is_encoder_decoder():\n            blocks.update(self.cross_block_tables[request_id])\n        return list(blocks)\n\n    def can_swap_in(self,\n                    seq_group: SequenceGroup,\n                    num_lookahead_slots: int = 0) -> AllocStatus:\n        assert (num_lookahead_slots == 0\n                ), \"BlockSpaceManagerV1 does not support lookahead allocation\"\n\n        blocks = self._get_physical_blocks(seq_group)\n        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)\n        if seq_group.is_encoder_decoder():\n            num_swapped_seqs += 1\n        num_free_blocks = self.gpu_allocator.get_num_free_blocks()\n        # NOTE: Conservatively, we assume that every sequence will allocate\n        # at least one free block right after the swap-in.\n        # NOTE: This should match the logic in can_append_slot().\n        num_required_blocks = len(blocks) + num_swapped_seqs\n        if self.gpu_allocator.get_num_total_blocks() < num_required_blocks:\n            return AllocStatus.NEVER\n        elif num_free_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _swap_block_table(\n            self, block_table: BlockTable, src_allocator: BlockAllocatorBase,\n            dest_allocator: BlockAllocatorBase,\n            mapping: Dict[PhysicalTokenBlock,\n                          PhysicalTokenBlock]) -> BlockTable:\n        new_block_table: BlockTable = BlockTable()\n\n        for from_block in block_table:\n            if from_block in mapping:\n                to_block = mapping[from_block]\n                to_block.ref_count += 1\n            else:\n                to_block = dest_allocator.allocate(\n                    from_block.block_hash, from_block.num_hashed_tokens)\n                mapping[from_block] = to_block\n            new_block_table.append(to_block)\n            # Free the source block swapped in to destination.\n            src_allocator.free(from_block)\n\n        return new_block_table\n\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n\n        request_id = seq_group.request_id\n\n        # CPU block -> GPU block.\n        # dict is efficient in lookup `if cpu_block in mapping`\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            self.block_tables[seq.seq_id] = \\\n                self._swap_block_table(self.block_tables[seq.seq_id],\n                                       self.cpu_allocator, self.gpu_allocator,\n                                       mapping)\n\n        if seq_group.is_encoder_decoder():\n            self.cross_block_tables[request_id] = \\\n                self._swap_block_table(self.cross_block_tables[request_id],\n                                       self.cpu_allocator,\n                                       self.gpu_allocator,\n                                       mapping)\n\n        return [(cpu_block.block_number, gpu_block.block_number)\n                for cpu_block, gpu_block in mapping.items()]\n\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n        blocks = self._get_physical_blocks(seq_group)\n        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()\n\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        request_id = seq_group.request_id\n\n        # GPU block -> CPU block.\n        # dict is efficient in lookup `if gpu_block in mapping`\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            self.block_tables[seq.seq_id] = \\\n                self._swap_block_table(self.block_tables[seq.seq_id],\n                                       self.gpu_allocator, self.cpu_allocator,\n                                       mapping)\n\n        if seq_group.is_encoder_decoder():\n            self.cross_block_tables[request_id] = \\\n                self._swap_block_table(self.cross_block_tables[request_id],\n                                       self.gpu_allocator,\n                                       self.cpu_allocator,\n                                       mapping)\n\n        return [(cpu_block.block_number, gpu_block.block_number)\n                for cpu_block, gpu_block in mapping.items()]\n\n    def _free_block_table(self, block_table: BlockTable) -> None:\n        # when using a sliding window, each seq will only use up\n        # to `self.block_sliding_window` blocks. When freeing\n        # the block table, we must make sure to not free blocks more\n        # than once. If no sliding window is used, there is no block\n        # reuse in the block table, so we must free all blocks.\n        blocks_to_free = (block_table[-self.block_sliding_window:]\n                          if self.block_sliding_window is not None else\n                          block_table)\n        for block in set(blocks_to_free):\n            if block.device == Device.GPU:\n                self.gpu_allocator.free(block)\n            else:\n                self.cpu_allocator.free(block)\n\n    def free(self, seq: Sequence) -> None:\n        if seq.seq_id not in self.block_tables:\n            # Already freed or haven't been scheduled yet.\n            return\n        block_table = self.block_tables[seq.seq_id]\n        self._free_block_table(block_table)\n        del self.block_tables[seq.seq_id]\n\n    def free_cross(self, seq_group: SequenceGroup) -> None:\n        if seq_group.request_id not in self.cross_block_tables:\n            # Already freed or hasn't ben scheduled yet.\n            return\n        block_table = self.cross_block_tables[seq_group.request_id]\n        self._free_block_table(block_table)\n        del self.cross_block_tables[seq_group.request_id]\n\n    def reset(self) -> None:\n        # Free decoder block tables\n        for block_table in self.block_tables.values():\n            self._free_block_table(block_table)\n        self.block_tables.clear()\n        # Free cross-attention block tables\n        for block_table in self.cross_block_tables.values():\n            self._free_block_table(block_table)\n        self.cross_block_tables.clear()\n\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        return self.block_tables[seq.seq_id].ids()\n\n    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n        block_table = self.cross_block_tables[seq_group.request_id]\n        return [block.block_number for block in block_table]\n\n    def get_num_free_gpu_blocks(self) -> int\n```\n# Function Signature: def append_slots(\n# Natural Language Annotations: \"\"\"Allocate a physical slot for a new token.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef append_slots(seq: Sequence) -> Phys", "  ```\ndef append_slots(seq: Sequence, num_", "  ```\ndef append_slots(self, seq_group: Se"]}, "vllm-vllm/core/block_manager_v2.py-can_swap_in": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/block_manager_v2.py:\n```\n\"\"\"A block manager that manages token blocks.\"\"\"\nfrom itertools import chain\nfrom typing import Dict, List, Optional\nfrom typing import Sequence as GenericSequence\nfrom typing import Tuple\n\nfrom vllm.core.block.block_table import BlockTable\nfrom vllm.core.block.cpu_gpu_block_allocator import CpuGpuBlockAllocator\nfrom vllm.core.block.interfaces import Block\nfrom vllm.core.block.prefix_caching_block import (ComputedBlocksTracker,\n                                                  LastAccessBlocksTracker)\nfrom vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\n\nSeqId = int\nEncoderSeqId = str\n\n\nclass BlockSpaceManagerV2(BlockSpaceManager):\n    \"\"\"BlockSpaceManager which manages the allocation of KV cache.\n\n    It owns responsibility for allocation, swapping, allocating memory for\n    autoregressively-generated tokens, and other advanced features such as\n    prefix caching, forking/copy-on-write, and sliding-window memory allocation.\n\n    The current implementation is partial; in particular prefix caching and\n    sliding-window are not feature complete. This class implements the design\n    described in https://github.com/vllm-project/vllm/pull/3492.\n\n    Lookahead slots\n        The block manager has the notion of a \"lookahead slot\". These are slots\n        in the KV cache that are allocated for a sequence. Unlike the other\n        allocated slots, the content of these slots is undefined -- the worker\n        may use the memory allocations in any way.\n\n        In practice, a worker could use these lookahead slots to run multiple\n        forward passes for a single scheduler invocation. Each successive\n        forward pass would write KV activations to the corresponding lookahead\n        slot. This allows low inter-token latency use-cases, where the overhead\n        of continuous batching scheduling is amortized over >1 generated tokens.\n\n        Speculative decoding uses lookahead slots to store KV activations of\n        proposal tokens.\n\n        See https://github.com/vllm-project/vllm/pull/3250 for more information\n        on lookahead scheduling.\n\n    Args:\n        block_size (int): The size of each memory block.\n        num_gpu_blocks (int): The number of memory blocks allocated on GPU.\n        num_cpu_blocks (int): The number of memory blocks allocated on CPU.\n        watermark (float, optional): The threshold used for memory swapping.\n            Defaults to 0.01.\n        sliding_window (Optional[int], optional): The size of the sliding\n            window. Defaults to None.\n        enable_caching (bool, optional): Flag indicating whether caching is\n            enabled. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        watermark: float = 0.01,\n        sliding_window: Optional[int] = None,\n        enable_caching: bool = False,\n    ) -> None:\n        self.block_size = block_size\n        self.num_total_gpu_blocks = num_gpu_blocks\n        self.num_total_cpu_blocks = num_cpu_blocks\n\n        self.sliding_window = sliding_window\n        # max_block_sliding_window is the max number of blocks that need to be\n        # allocated\n        self.max_block_sliding_window = None\n        if sliding_window is not None:\n            # +1 here because // rounds down\n            num_blocks = sliding_window // block_size + 1\n            # +1 here because the last block may not be full,\n            # and so the sequence stretches one more block at the beginning\n            # For example, if sliding_window is 3 and block_size is 4,\n            # we may need 2 blocks when the second block only holds 1 token.\n            self.max_block_sliding_window = num_blocks + 1\n\n        self.watermark = watermark\n        assert watermark >= 0.0\n\n        self.enable_caching = enable_caching\n\n        self.watermark_blocks = int(watermark * num_gpu_blocks)\n\n        self.block_allocator = CpuGpuBlockAllocator.create(\n            allocator_type=\"prefix_caching\" if enable_caching else \"naive\",\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            block_size=block_size,\n        )\n\n        self.block_tables: Dict[SeqId, BlockTable] = {}\n        self.cross_block_tables: Dict[EncoderSeqId, BlockTable] = {}\n\n        self._computed_blocks_tracker = ComputedBlocksTracker(\n            self.block_allocator)\n        self._last_access_blocks_tracker = LastAccessBlocksTracker(\n            self.block_allocator)\n\n    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n        # FIXME(woosuk): Here we assume that all sequences in the group share\n        # the same prompt. This may not be true for preempted sequences.\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]\n        num_required_blocks = BlockTable.get_num_required_blocks(\n            seq.get_token_ids(),\n            block_size=self.block_size,\n        )\n\n        if seq_group.is_encoder_decoder():\n            num_required_blocks += BlockTable.get_num_required_blocks(\n                seq_group.get_encoder_seq().get_token_ids(),\n                block_size=self.block_size,\n            )\n\n        if self.max_block_sliding_window is not None:\n            num_required_blocks = min(num_required_blocks,\n                                      self.max_block_sliding_window)\n\n        num_free_gpu_blocks = self.block_allocator.get_num_free_blocks(\n            device=Device.GPU)\n\n        # Use watermark to avoid frequent cache eviction.\n        if (self.num_total_gpu_blocks - num_required_blocks <\n                self.watermark_blocks):\n            return AllocStatus.NEVER\n        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _allocate_sequence(self, seq: Sequence) -> BlockTable:\n        block_table = BlockTable(\n            block_size=self.block_size,\n            block_allocator=self.block_allocator,\n            max_block_sliding_window=self.max_block_sliding_window,\n        )\n        block_table.allocate(seq.get_token_ids())\n\n        return block_table\n\n    def allocate(self, seq_group: SequenceGroup) -> None:\n\n        # Allocate self-attention block tables for decoder sequences\n        waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n        assert not (set(seq.seq_id for seq in waiting_seqs)\n                    & self.block_tables.keys()), \"block table already exists\"\n\n        # NOTE: Here we assume that all sequences in the group have the same\n        # prompt.\n        seq = waiting_seqs[0]\n        block_table: BlockTable = self._allocate_sequence(seq)\n        self.block_tables[seq.seq_id] = block_table\n\n        # Track seq\n        self._computed_blocks_tracker.add_seq(seq.seq_id)\n        self._last_access_blocks_tracker.add_seq(seq.seq_id)\n\n        # Assign the block table for each sequence.\n        for seq in waiting_seqs[1:]:\n            self.block_tables[seq.seq_id] = block_table.fork()\n\n            # Track seq\n            self._computed_blocks_tracker.add_seq(seq.seq_id)\n            self._last_access_blocks_tracker.add_seq(seq.seq_id)\n\n        # Allocate cross-attention block table for encoder sequence\n        #\n        # NOTE: Here we assume that all sequences in the group have the same\n        # encoder prompt.\n        request_id = seq_group.request_id\n\n        assert (request_id\n                not in self.cross_block_tables), \\\n                \"block table already exists\"\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        if seq_group.is_encoder_decoder():\n            block_table = self._allocate_sequence(seq_group.get_encoder_seq())\n            self.cross_block_tables[request_id] = block_table\n\n    def can_append_slots(self, seq_group: SequenceGroup,\n                         num_lookahead_slots: int) -> bool:\n        \"\"\"Determine if there is enough space in the GPU KV cache to continue\n        generation of the specified sequence group.\n\n        We use a worst-case heuristic: assume each touched block will require a\n        new allocation (either via CoW or new block). We can append slots if the\n        number of touched blocks is less than the number of free blocks.\n\n        \"Lookahead slots\" are slots that are allocated in addition to the slots\n        for known tokens. The contents of the lookahead slots are not defined.\n        This is used by speculative decoding when speculating future tokens.\n        \"\"\"\n\n        num_touched_blocks = 0\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            block_table = self.block_tables[seq.seq_id]\n\n            num_touched_blocks += (\n                block_table.get_num_blocks_touched_by_append_slots(\n                    token_ids=block_table.get_unseen_token_ids(\n                        seq.get_token_ids()),\n                    num_lookahead_slots=num_lookahead_slots,\n                ))\n\n        num_free_gpu_blocks = self.block_allocator.get_num_free_blocks(\n            Device.GPU)\n        return num_touched_blocks <= num_free_gpu_blocks\n\n    def append_slots(\n        self,\n        seq: Sequence,\n        num_lookahead_slots: int,\n    ) -> List[Tuple[int, int]]:\n\n        block_table = self.block_tables[seq.seq_id]\n\n        block_table.append_token_ids(\n            token_ids=block_table.get_unseen_token_ids(seq.get_token_ids()),\n            num_lookahead_slots=num_lookahead_slots,\n            num_computed_slots=seq.data.get_num_computed_tokens(),\n        )\n        # Return any new copy-on-writes.\n        new_cows = self.block_allocator.clear_copy_on_writes()\n        return new_cows\n\n    def free(self, seq: Sequence) -> None:\n        seq_id = seq.seq_id\n\n        if seq_id not in self.block_tables:\n            # Already freed or haven't been scheduled yet.\n            return\n\n        # Update seq block ids with the latest access time\n        self._last_access_blocks_tracker.update_seq_blocks_last_access(\n            seq_id, self.block_tables[seq.seq_id].physical_block_ids)\n\n        # Untrack seq\n        self._last_access_blocks_tracker.remove_seq(seq_id)\n        self._computed_blocks_tracker.remove_seq(seq_id)\n\n        # Free table/blocks\n        self.block_tables[seq_id].free()\n        del self.block_tables[seq_id]\n\n    def free_cross(self, seq_group: SequenceGroup) -> None:\n        request_id = seq_group.request_id\n        if request_id not in self.cross_block_tables:\n            # Already freed or hasn't been scheduled yet.\n            return\n        self.cross_block_tables[request_id].free()\n        del self.cross_block_tables[request_id]\n\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        block_ids = self.block_tables[seq.seq_id].physical_block_ids\n        return block_ids  # type: ignore\n\n    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n        request_id = seq_group.request_id\n        assert request_id in self.cross_block_tables\n        block_ids = self.cross_block_tables[request_id].physical_block_ids\n        assert all(b is not None for b in block_ids)\n        return block_ids  # type: ignore\n\n    def access_all_blocks_in_seq(self, seq: Sequence, now: float):\n        if self.enable_caching:\n            # Record the latest access time for the sequence. The actual update\n            # of the block ids is deferred to the sequence free(..) call, since\n            # only during freeing of block ids, the blocks are actually added to\n            # the evictor (which is when the most updated time is required)\n            # (This avoids expensive calls to mark_blocks_as_accessed(..))\n            self._last_access_blocks_tracker.update_last_access(\n                seq.seq_id, now)\n\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n        # The only need for mark block as computed is for prefix caching,\n        # while currently we could determine whether one block is computed\n        # or not by check whether it has content hash.\n        # So this function is useless for block_v2.\n        pass\n\n    def get_common_computed_block_ids(\n            self, seqs: List[Sequence]) -> GenericSequence[int]:\n        \"\"\"Determine which blocks for which we skip prefill.\n\n        With prefix caching we can skip prefill for previously-generated blocks.\n        Currently, the attention implementation only supports skipping cached\n        blocks if they are a contiguous prefix of cached blocks.\n\n        This method determines which blocks can be safely skipped for all\n        sequences in the sequence group.\n        \"\"\"\n        computed_seq_block_ids = []\n        for seq in seqs:\n            computed_seq_block_ids.append(\n                self._computed_blocks_tracker.\n                get_cached_computed_blocks_and_update(\n                    seq.seq_id,\n                    self.block_tables[seq.seq_id].physical_block_ids))\n\n        # NOTE(sang): This assumes seq_block_ids doesn't contain any None.\n        return self.block_allocator.get_common_computed_block_ids(\n            computed_seq_block_ids)  # type: ignore\n\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        if parent_seq.seq_id not in self.block_tables:\n            # Parent sequence has either been freed or never existed.\n            return\n        src_block_table = self.block_tables[parent_seq.seq_id]\n        self.block_tables[child_seq.seq_id] = src_block_table.fork()\n\n        # Track child seq\n        self._computed_blocks_tracker.add_seq(child_seq.seq_id)\n        self._last_access_blocks_tracker.add_seq(child_seq.seq_id)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        \"\"\"Returns the block id mapping (from CPU to GPU) generated by\n        swapping in the given seq_group with num_lookahead_slots.\n\n        Args:\n            seq_group (SequenceGroup): The sequence group to swap in.\n\n        Returns:\n            List[Tuple[int, int]]: The mapping of swapping block from CPU \n                to GPU.\n        \"\"\"\n        physical_block_id_mapping = []\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            blocks = self.block_tables[seq.seq_id].blocks\n            if len(blocks) == 0:\n                continue\n\n            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n                                                         src_device=Device.CPU,\n                                                         dst_device=Device.GPU)\n\n            # Refresh the block ids of the table (post-swap)\n            self.block_tables[seq.seq_id].update(blocks)\n\n            seq_physical_block_id_mapping = {\n                self.block_allocator.get_physical_block_id(\n                    Device.CPU, cpu_block_id):\n                self.block_allocator.get_physical_block_id(\n                    Device.GPU, gpu_block_id)\n                for cpu_block_id, gpu_block_id in seq_swap_mapping.items()\n            }\n\n            physical_block_id_mapping.extend(\n                list(seq_physical_block_id_mapping.items()))\n\n        return physical_block_id_mapping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        \"\"\"Returns the block id mapping (from GPU to CPU) generated by\n        swapping out the given sequence_group with num_lookahead_slots.\n\n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n\n        Returns:\n            List[Tuple[int, int]]: The mapping of swapping block from \n                GPU to CPU.\n        \"\"\"\n        physical_block_id_mapping = []\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            blocks = self.block_tables[seq.seq_id].blocks\n            if len(blocks) == 0:\n                continue\n\n            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n                                                         src_device=Device.GPU,\n                                                         dst_device=Device.CPU)\n\n            # Refresh the block ids of the table (post-swap)\n            self.block_tables[seq.seq_id].update(blocks)\n\n            seq_physical_block_id_mapping = {\n                self.block_allocator.get_physical_block_id(\n                    Device.GPU, gpu_block_id):\n                self.block_allocator.get_physical_block_id(\n                    Device.CPU, cpu_block_id)\n                for gpu_block_id, cpu_block_id in seq_swap_mapping.items()\n            }\n\n            physical_block_id_mapping.extend(\n                list(seq_physical_block_id_mapping.items()))\n\n        return physical_block_id_mapping\n\n    def get_num_free_gpu_blocks(self) -> int:\n        return self.block_allocator.get_num_free_blocks(Device.GPU)\n\n    def get_num_free_cpu_blocks(self) -> int:\n        return self.block_allocator.get_num_free_blocks(Device.CPU)\n\n    def _can_swap(self,\n                  seq_group: SequenceGroup,\n                  device: Device,\n                  status: SequenceStatus,\n                  num_lookahead_slots: int = 0) -> AllocStatus:\n        \"\"\"Returns the AllocStatus for swapping in/out the given sequence_group \n        on to the 'device'.\n\n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n            device (Device): device to swap the 'seq_group' on.\n            status (SequenceStatus): The status of sequence which is needed\n                for action. RUNNING for swap out and SWAPPED for swap in\n            num_lookahead_slots (int): Number of lookahead slots used in \n                speculative decoding, default to 0.\n\n        Returns:\n            AllocStatus: The AllocStatus for swapping in/out the given \n                sequence_group on to the 'device'.\n        \"\"\"\n        blocks = self._get_blocks_for_swap(seq_group, status)\n        num_blocks_touched = self.block_allocator.get_num_blocks_touched(\n            blocks, device, num_lookahead_slots)\n        watermark_blocks = 0\n        if device == Device.GPU:\n            watermark_blocks = self.watermark_blocks\n        if self.block_allocator.get_num_total_blocks(\n                device) < num_blocks_touched:\n            return AllocStatus.NEVER\n        elif self.block_allocator.get_num_free_blocks(\n                device) - num_blocks_touched >= watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _get_blocks_for_swap(self, seq_group: SequenceGroup,\n                             status: SequenceStatus) -> List[Block]:\n        \"\"\"Returns the list of blocks those are touched by the seq_group\n        \n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n            status (SequenceStatus): The status of sequence which is needed\n                for action. RUNNING for swap out and SWAPPED for swap in\n        \n        Returns:\n            The list of blocks those are touched by the seq_group.\n        \"\"\"\n        blocks: Dict[int, List[Block]] = {}\n        for seq in seq_group.get_seqs(status=status):\n            block_table = self.block_tables[seq.seq_id]\n            if block_table.blocks is not None:\n                blocks[seq.seq_id] = block_table.blocks\n        combined_blocks = list(chain(*blocks.values()))\n        return combined_blocks\n\n```\n# Function Signature: def can_swap_in(self, seq_group: SequenceGroup,\n# Natural Language Annotations: \"\"\"Returns the AllocStatus for the given sequence_group \n        with num_lookahead_slots.\n\n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n            num_lookahead_slots (int): Number of lookahead slots used in \n                speculative decoding, default to 0.\n\n        Returns:\n            AllocStatus: The AllocStatus for the given sequence group.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef can_swap_in(self, seq_group:", "  ```\ndef can_swap_in(self, seq_group:", "  ```\ndef can_swap_in(self, seq_group:"]}, "vllm-vllm/core/block_manager_v2.py-can_swap_out": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/block_manager_v2.py:\n```\n\"\"\"A block manager that manages token blocks.\"\"\"\nfrom itertools import chain\nfrom typing import Dict, List, Optional\nfrom typing import Sequence as GenericSequence\nfrom typing import Tuple\n\nfrom vllm.core.block.block_table import BlockTable\nfrom vllm.core.block.cpu_gpu_block_allocator import CpuGpuBlockAllocator\nfrom vllm.core.block.interfaces import Block\nfrom vllm.core.block.prefix_caching_block import (ComputedBlocksTracker,\n                                                  LastAccessBlocksTracker)\nfrom vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\n\nSeqId = int\nEncoderSeqId = str\n\n\nclass BlockSpaceManagerV2(BlockSpaceManager):\n    \"\"\"BlockSpaceManager which manages the allocation of KV cache.\n\n    It owns responsibility for allocation, swapping, allocating memory for\n    autoregressively-generated tokens, and other advanced features such as\n    prefix caching, forking/copy-on-write, and sliding-window memory allocation.\n\n    The current implementation is partial; in particular prefix caching and\n    sliding-window are not feature complete. This class implements the design\n    described in https://github.com/vllm-project/vllm/pull/3492.\n\n    Lookahead slots\n        The block manager has the notion of a \"lookahead slot\". These are slots\n        in the KV cache that are allocated for a sequence. Unlike the other\n        allocated slots, the content of these slots is undefined -- the worker\n        may use the memory allocations in any way.\n\n        In practice, a worker could use these lookahead slots to run multiple\n        forward passes for a single scheduler invocation. Each successive\n        forward pass would write KV activations to the corresponding lookahead\n        slot. This allows low inter-token latency use-cases, where the overhead\n        of continuous batching scheduling is amortized over >1 generated tokens.\n\n        Speculative decoding uses lookahead slots to store KV activations of\n        proposal tokens.\n\n        See https://github.com/vllm-project/vllm/pull/3250 for more information\n        on lookahead scheduling.\n\n    Args:\n        block_size (int): The size of each memory block.\n        num_gpu_blocks (int): The number of memory blocks allocated on GPU.\n        num_cpu_blocks (int): The number of memory blocks allocated on CPU.\n        watermark (float, optional): The threshold used for memory swapping.\n            Defaults to 0.01.\n        sliding_window (Optional[int], optional): The size of the sliding\n            window. Defaults to None.\n        enable_caching (bool, optional): Flag indicating whether caching is\n            enabled. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        watermark: float = 0.01,\n        sliding_window: Optional[int] = None,\n        enable_caching: bool = False,\n    ) -> None:\n        self.block_size = block_size\n        self.num_total_gpu_blocks = num_gpu_blocks\n        self.num_total_cpu_blocks = num_cpu_blocks\n\n        self.sliding_window = sliding_window\n        # max_block_sliding_window is the max number of blocks that need to be\n        # allocated\n        self.max_block_sliding_window = None\n        if sliding_window is not None:\n            # +1 here because // rounds down\n            num_blocks = sliding_window // block_size + 1\n            # +1 here because the last block may not be full,\n            # and so the sequence stretches one more block at the beginning\n            # For example, if sliding_window is 3 and block_size is 4,\n            # we may need 2 blocks when the second block only holds 1 token.\n            self.max_block_sliding_window = num_blocks + 1\n\n        self.watermark = watermark\n        assert watermark >= 0.0\n\n        self.enable_caching = enable_caching\n\n        self.watermark_blocks = int(watermark * num_gpu_blocks)\n\n        self.block_allocator = CpuGpuBlockAllocator.create(\n            allocator_type=\"prefix_caching\" if enable_caching else \"naive\",\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            block_size=block_size,\n        )\n\n        self.block_tables: Dict[SeqId, BlockTable] = {}\n        self.cross_block_tables: Dict[EncoderSeqId, BlockTable] = {}\n\n        self._computed_blocks_tracker = ComputedBlocksTracker(\n            self.block_allocator)\n        self._last_access_blocks_tracker = LastAccessBlocksTracker(\n            self.block_allocator)\n\n    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n        # FIXME(woosuk): Here we assume that all sequences in the group share\n        # the same prompt. This may not be true for preempted sequences.\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]\n        num_required_blocks = BlockTable.get_num_required_blocks(\n            seq.get_token_ids(),\n            block_size=self.block_size,\n        )\n\n        if seq_group.is_encoder_decoder():\n            num_required_blocks += BlockTable.get_num_required_blocks(\n                seq_group.get_encoder_seq().get_token_ids(),\n                block_size=self.block_size,\n            )\n\n        if self.max_block_sliding_window is not None:\n            num_required_blocks = min(num_required_blocks,\n                                      self.max_block_sliding_window)\n\n        num_free_gpu_blocks = self.block_allocator.get_num_free_blocks(\n            device=Device.GPU)\n\n        # Use watermark to avoid frequent cache eviction.\n        if (self.num_total_gpu_blocks - num_required_blocks <\n                self.watermark_blocks):\n            return AllocStatus.NEVER\n        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _allocate_sequence(self, seq: Sequence) -> BlockTable:\n        block_table = BlockTable(\n            block_size=self.block_size,\n            block_allocator=self.block_allocator,\n            max_block_sliding_window=self.max_block_sliding_window,\n        )\n        block_table.allocate(seq.get_token_ids())\n\n        return block_table\n\n    def allocate(self, seq_group: SequenceGroup) -> None:\n\n        # Allocate self-attention block tables for decoder sequences\n        waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n        assert not (set(seq.seq_id for seq in waiting_seqs)\n                    & self.block_tables.keys()), \"block table already exists\"\n\n        # NOTE: Here we assume that all sequences in the group have the same\n        # prompt.\n        seq = waiting_seqs[0]\n        block_table: BlockTable = self._allocate_sequence(seq)\n        self.block_tables[seq.seq_id] = block_table\n\n        # Track seq\n        self._computed_blocks_tracker.add_seq(seq.seq_id)\n        self._last_access_blocks_tracker.add_seq(seq.seq_id)\n\n        # Assign the block table for each sequence.\n        for seq in waiting_seqs[1:]:\n            self.block_tables[seq.seq_id] = block_table.fork()\n\n            # Track seq\n            self._computed_blocks_tracker.add_seq(seq.seq_id)\n            self._last_access_blocks_tracker.add_seq(seq.seq_id)\n\n        # Allocate cross-attention block table for encoder sequence\n        #\n        # NOTE: Here we assume that all sequences in the group have the same\n        # encoder prompt.\n        request_id = seq_group.request_id\n\n        assert (request_id\n                not in self.cross_block_tables), \\\n                \"block table already exists\"\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        if seq_group.is_encoder_decoder():\n            block_table = self._allocate_sequence(seq_group.get_encoder_seq())\n            self.cross_block_tables[request_id] = block_table\n\n    def can_append_slots(self, seq_group: SequenceGroup,\n                         num_lookahead_slots: int) -> bool:\n        \"\"\"Determine if there is enough space in the GPU KV cache to continue\n        generation of the specified sequence group.\n\n        We use a worst-case heuristic: assume each touched block will require a\n        new allocation (either via CoW or new block). We can append slots if the\n        number of touched blocks is less than the number of free blocks.\n\n        \"Lookahead slots\" are slots that are allocated in addition to the slots\n        for known tokens. The contents of the lookahead slots are not defined.\n        This is used by speculative decoding when speculating future tokens.\n        \"\"\"\n\n        num_touched_blocks = 0\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            block_table = self.block_tables[seq.seq_id]\n\n            num_touched_blocks += (\n                block_table.get_num_blocks_touched_by_append_slots(\n                    token_ids=block_table.get_unseen_token_ids(\n                        seq.get_token_ids()),\n                    num_lookahead_slots=num_lookahead_slots,\n                ))\n\n        num_free_gpu_blocks = self.block_allocator.get_num_free_blocks(\n            Device.GPU)\n        return num_touched_blocks <= num_free_gpu_blocks\n\n    def append_slots(\n        self,\n        seq: Sequence,\n        num_lookahead_slots: int,\n    ) -> List[Tuple[int, int]]:\n\n        block_table = self.block_tables[seq.seq_id]\n\n        block_table.append_token_ids(\n            token_ids=block_table.get_unseen_token_ids(seq.get_token_ids()),\n            num_lookahead_slots=num_lookahead_slots,\n            num_computed_slots=seq.data.get_num_computed_tokens(),\n        )\n        # Return any new copy-on-writes.\n        new_cows = self.block_allocator.clear_copy_on_writes()\n        return new_cows\n\n    def free(self, seq: Sequence) -> None:\n        seq_id = seq.seq_id\n\n        if seq_id not in self.block_tables:\n            # Already freed or haven't been scheduled yet.\n            return\n\n        # Update seq block ids with the latest access time\n        self._last_access_blocks_tracker.update_seq_blocks_last_access(\n            seq_id, self.block_tables[seq.seq_id].physical_block_ids)\n\n        # Untrack seq\n        self._last_access_blocks_tracker.remove_seq(seq_id)\n        self._computed_blocks_tracker.remove_seq(seq_id)\n\n        # Free table/blocks\n        self.block_tables[seq_id].free()\n        del self.block_tables[seq_id]\n\n    def free_cross(self, seq_group: SequenceGroup) -> None:\n        request_id = seq_group.request_id\n        if request_id not in self.cross_block_tables:\n            # Already freed or hasn't been scheduled yet.\n            return\n        self.cross_block_tables[request_id].free()\n        del self.cross_block_tables[request_id]\n\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        block_ids = self.block_tables[seq.seq_id].physical_block_ids\n        return block_ids  # type: ignore\n\n    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n        request_id = seq_group.request_id\n        assert request_id in self.cross_block_tables\n        block_ids = self.cross_block_tables[request_id].physical_block_ids\n        assert all(b is not None for b in block_ids)\n        return block_ids  # type: ignore\n\n    def access_all_blocks_in_seq(self, seq: Sequence, now: float):\n        if self.enable_caching:\n            # Record the latest access time for the sequence. The actual update\n            # of the block ids is deferred to the sequence free(..) call, since\n            # only during freeing of block ids, the blocks are actually added to\n            # the evictor (which is when the most updated time is required)\n            # (This avoids expensive calls to mark_blocks_as_accessed(..))\n            self._last_access_blocks_tracker.update_last_access(\n                seq.seq_id, now)\n\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n        # The only need for mark block as computed is for prefix caching,\n        # while currently we could determine whether one block is computed\n        # or not by check whether it has content hash.\n        # So this function is useless for block_v2.\n        pass\n\n    def get_common_computed_block_ids(\n            self, seqs: List[Sequence]) -> GenericSequence[int]:\n        \"\"\"Determine which blocks for which we skip prefill.\n\n        With prefix caching we can skip prefill for previously-generated blocks.\n        Currently, the attention implementation only supports skipping cached\n        blocks if they are a contiguous prefix of cached blocks.\n\n        This method determines which blocks can be safely skipped for all\n        sequences in the sequence group.\n        \"\"\"\n        computed_seq_block_ids = []\n        for seq in seqs:\n            computed_seq_block_ids.append(\n                self._computed_blocks_tracker.\n                get_cached_computed_blocks_and_update(\n                    seq.seq_id,\n                    self.block_tables[seq.seq_id].physical_block_ids))\n\n        # NOTE(sang): This assumes seq_block_ids doesn't contain any None.\n        return self.block_allocator.get_common_computed_block_ids(\n            computed_seq_block_ids)  # type: ignore\n\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        if parent_seq.seq_id not in self.block_tables:\n            # Parent sequence has either been freed or never existed.\n            return\n        src_block_table = self.block_tables[parent_seq.seq_id]\n        self.block_tables[child_seq.seq_id] = src_block_table.fork()\n\n        # Track child seq\n        self._computed_blocks_tracker.add_seq(child_seq.seq_id)\n        self._last_access_blocks_tracker.add_seq(child_seq.seq_id)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        \"\"\"Returns the block id mapping (from CPU to GPU) generated by\n        swapping in the given seq_group with num_lookahead_slots.\n\n        Args:\n            seq_group (SequenceGroup): The sequence group to swap in.\n\n        Returns:\n            List[Tuple[int, int]]: The mapping of swapping block from CPU \n                to GPU.\n        \"\"\"\n        physical_block_id_mapping = []\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            blocks = self.block_tables[seq.seq_id].blocks\n            if len(blocks) == 0:\n                continue\n\n            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n                                                         src_device=Device.CPU,\n                                                         dst_device=Device.GPU)\n\n            # Refresh the block ids of the table (post-swap)\n            self.block_tables[seq.seq_id].update(blocks)\n\n            seq_physical_block_id_mapping = {\n                self.block_allocator.get_physical_block_id(\n                    Device.CPU, cpu_block_id):\n                self.block_allocator.get_physical_block_id(\n                    Device.GPU, gpu_block_id)\n                for cpu_block_id, gpu_block_id in seq_swap_mapping.items()\n            }\n\n            physical_block_id_mapping.extend(\n                list(seq_physical_block_id_mapping.items()))\n\n        return physical_block_id_mapping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        \"\"\"Returns the block id mapping (from GPU to CPU) generated by\n        swapping out the given sequence_group with num_lookahead_slots.\n\n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n\n        Returns:\n            List[Tuple[int, int]]: The mapping of swapping block from \n                GPU to CPU.\n        \"\"\"\n        physical_block_id_mapping = []\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            blocks = self.block_tables[seq.seq_id].blocks\n            if len(blocks) == 0:\n                continue\n\n            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n                                                         src_device=Device.GPU,\n                                                         dst_device=Device.CPU)\n\n            # Refresh the block ids of the table (post-swap)\n            self.block_tables[seq.seq_id].update(blocks)\n\n            seq_physical_block_id_mapping = {\n                self.block_allocator.get_physical_block_id(\n                    Device.GPU, gpu_block_id):\n                self.block_allocator.get_physical_block_id(\n                    Device.CPU, cpu_block_id)\n                for gpu_block_id, cpu_block_id in seq_swap_mapping.items()\n            }\n\n            physical_block_id_mapping.extend(\n                list(seq_physical_block_id_mapping.items()))\n\n        return physical_block_id_mapping\n\n    def get_num_free_gpu_blocks(self) -> int:\n        return self.block_allocator.get_num_free_blocks(Device.GPU)\n\n    def get_num_free_cpu_blocks(self) -> int:\n        return self.block_allocator.get_num_free_blocks(Device.CPU)\n\n    def _can_swap(self,\n                  seq_group: SequenceGroup,\n                  device: Device,\n                  status: SequenceStatus,\n                  num_lookahead_slots: int = 0) -> AllocStatus:\n        \"\"\"Returns the AllocStatus for swapping in/out the given sequence_group \n        on to the 'device'.\n\n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n            device (Device): device to swap the 'seq_group' on.\n            status (SequenceStatus): The status of sequence which is needed\n                for action. RUNNING for swap out and SWAPPED for swap in\n            num_lookahead_slots (int): Number of lookahead slots used in \n                speculative decoding, default to 0.\n\n        Returns:\n            AllocStatus: The AllocStatus for swapping in/out the given \n                sequence_group on to the 'device'.\n        \"\"\"\n        blocks = self._get_blocks_for_swap(seq_group, status)\n        num_blocks_touched = self.block_allocator.get_num_blocks_touched(\n            blocks, device, num_lookahead_slots)\n        watermark_blocks = 0\n        if device == Device.GPU:\n            watermark_blocks = self.watermark_blocks\n        if self.block_allocator.get_num_total_blocks(\n                device) < num_blocks_touched:\n            return AllocStatus.NEVER\n        elif self.block_allocator.get_num_free_blocks(\n                device) - num_blocks_touched >= watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _get_blocks_for_swap(self, seq_group: SequenceGroup,\n                             status: SequenceStatus) -> List[Block]:\n        \"\"\"Returns the list of blocks those are touched by the seq_group\n        \n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n            status (SequenceStatus): The status of sequence which is needed\n                for action. RUNNING for swap out and SWAPPED for swap in\n        \n        Returns:\n            The list of blocks those are touched by the seq_group.\n        \"\"\"\n        blocks: Dict[int, List[Block]] = {}\n        for seq in seq_group.get_seqs(status=status):\n            block_table = self.block_tables[seq.seq_id]\n            if block_table.blocks is not None:\n                blocks[seq.seq_id] = block_table.blocks\n        combined_blocks = list(chain(*blocks.values()))\n        return combined_blocks\n\n```\n# Function Signature: def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n# Natural Language Annotations: \"\"\"Returns whether we can swap out the given sequence_group \n        with num_lookahead_slots.\n\n        Args:\n            seq_group (SequenceGroup): The sequence group to swap in.\n            num_lookahead_slots (int): Number of lookahead slots used in \n                speculative decoding, default to 0.\n\n        Returns:\n            bool: Whether it's possible to swap out current sequence group.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef can_swap_out(self, seq_group:", "  ```\ndef can_swap_out(self, seq_group:", "  ```\nbool can_swap_out(self, SequenceGroup seq"]}, "vllm-vllm/core/scheduler.py-abort_seq_group": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/scheduler.py:\n```\nimport enum\nimport os\nimport random\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom typing import Deque, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                           SequenceGroupMetadata, SequenceGroupMetadataDelta,\n                           SequenceStatus)\nfrom vllm.utils import PyObjectCache\n\nlogger = init_logger(__name__)\n\n# Test-only. If configured, decode is preempted with\n# ARTIFICIAL_PREEMPTION_PROB% probability.\nENABLE_ARTIFICIAL_PREEMPT = bool(\n    os.getenv(\"VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT\", False))  # noqa\nARTIFICIAL_PREEMPTION_PROB = 0.5\nARTIFICIAL_PREEMPTION_MAX_CNT = 500\n\n\nclass PreemptionMode(enum.Enum):\n    \"\"\"Preemption modes.\n\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\n    and swap them back in when the sequences are resumed.\n    2. Recomputation: Discard the blocks of the preempted sequences and\n    recompute them when the sequences are resumed, treating the sequences as\n    new prompts.\n    \"\"\"\n    SWAP = enum.auto()\n    RECOMPUTE = enum.auto()\n\n\n@dataclass\nclass SchedulingBudget:\n    \"\"\"The available slots for scheduling.\n\n    TODO(sang): Right now, the budget is request_id-aware meaning it can ignore\n    budget update from the same request_id. It is because in normal scheduling\n    path, we update RUNNING num_seqs ahead of time, meaning it could be\n    updated more than once when scheduling RUNNING requests. Since this won't\n    happen if we only have chunked prefill scheduling, we can remove this\n    feature from the API when chunked prefill is enabled by default.\n    \"\"\"\n    token_budget: int\n    max_num_seqs: int\n    _request_ids_num_batched_tokens: Set[str] = field(default_factory=set)\n    _request_ids_num_curr_seqs: Set[str] = field(default_factory=set)\n    _num_batched_tokens: int = 0\n    _num_curr_seqs: int = 0\n\n    def can_schedule(self, *, num_new_tokens: int, num_new_seqs: int):\n        assert num_new_tokens != 0\n        assert num_new_seqs != 0\n        return (self.num_batched_tokens + num_new_tokens <= self.token_budget\n                and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)\n\n    def remaining_token_budget(self):\n        return self.token_budget - self.num_batched_tokens\n\n    def add_num_batched_tokens(self, req_id: str, num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            return\n\n        self._request_ids_num_batched_tokens.add(req_id)\n        self._num_batched_tokens += num_batched_tokens\n\n    def subtract_num_batched_tokens(self, req_id: str,\n                                    num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            self._request_ids_num_batched_tokens.remove(req_id)\n            self._num_batched_tokens -= num_batched_tokens\n\n    def add_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            return\n\n        self._request_ids_num_curr_seqs.add(req_id)\n        self._num_curr_seqs += num_curr_seqs\n\n    def subtract_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            self._request_ids_num_curr_seqs.remove(req_id)\n            self._num_curr_seqs -= num_curr_seqs\n\n    @property\n    def num_batched_tokens(self):\n        return self._num_batched_tokens\n\n    @property\n    def num_curr_seqs(self):\n        return self._num_curr_seqs\n\n\n@dataclass\nclass ScheduledSequenceGroup:\n    # A sequence group that's scheduled.\n    seq_group: SequenceGroup\n    # The total chunk size (number of tokens) to process for next iteration.\n    # 1 for decoding. Same as prompt tokens for prefill, but if prefill is\n    # chunked, it can be smaller than that.\n    token_chunk_size: int\n\n\n@dataclass\nclass SchedulerOutputs:\n    \"\"\"The scheduling decision made from a scheduler.\"\"\"\n    # Scheduled sequence groups.\n    scheduled_seq_groups: Iterable[ScheduledSequenceGroup]\n    # Number of prefill groups scheduled.\n    num_prefill_groups: int\n    # Total number of batched tokens.\n    num_batched_tokens: int\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]]\n    # Sequence groups that are going to be ignored.\n    ignored_seq_groups: List[SequenceGroup]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # The number of requests in the running queue\n    running_queue_size: int\n    preempted: int\n\n    def __post_init__(self):\n        # Swap in and swap out should never happen at the same time.\n        assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)\n\n        self.num_loras: int = len(self.lora_requests)\n        if self.num_loras > 0:\n            self._sort_by_lora_ids()\n\n        self.num_prompt_adapters: int = len(self.prompt_adapter_requests)\n\n    def is_empty(self) -> bool:\n        # NOTE: We do not consider the ignored sequence groups.\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\n\n    def _sort_by_lora_ids(self):\n        self.scheduled_seq_groups = sorted(\n            self.scheduled_seq_groups,\n            key=lambda g: (g.seq_group.lora_int_id, g.seq_group.request_id))\n\n    @property\n    def lora_requests(self) -> Set[LoRARequest]:\n        return {\n            g.seq_group.lora_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.lora_request is not None\n        }\n\n    @property\n    def prompt_adapter_requests(self) -> Set[PromptAdapterRequest]:\n        return {\n            g.seq_group.prompt_adapter_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.prompt_adapter_request is not None\n        }\n\n\n@dataclass\nclass SchedulerRunningOutputs:\n    \"\"\"The requests that are scheduled from a running queue.\n\n    Could contain prefill (prefill that's chunked) or decodes. If there's not\n    enough memory, it can be preempted (for recompute) or swapped out.\n    \"\"\"\n    # Selected sequences that are running and in a decoding phase.\n    decode_seq_groups: List[ScheduledSequenceGroup]\n    # Selected sequences that are running and in a prefill phase.\n    # I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[ScheduledSequenceGroup]\n    # The preempted sequences.\n    preempted: List[SequenceGroup]\n    # Sequences that are swapped out.\n    swapped_out: List[SequenceGroup]\n    # The blocks to swap out.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    # Optimization for fast-access to seq_group lists\n    decode_seq_groups_list: List[SequenceGroup]\n    prefill_seq_groups_list: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerRunningOutputs\":\n        return SchedulerRunningOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            preempted=[],\n            swapped_out=[],\n            blocks_to_swap_out=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            decode_seq_groups_list=[],\n            prefill_seq_groups_list=[],\n        )\n\n\n@dataclass\nclass SchedulerSwappedInOutputs:\n    \"\"\"The requests that are scheduled from a swap queue.\n\n    Could contain prefill (prefill that's chunked) or decodes.\n    \"\"\"\n    # Selected sequences that are going to be swapped in and is in a\n    # decoding phase.\n    decode_seq_groups: List[SequenceGroup]\n    # Selected sequences that are going to be swapped in and in a prefill\n    # phase. I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[SequenceGroup]\n    # The blocks to swap in.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # Infeasible sequence groups.\n    infeasible_seq_groups: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerSwappedInOutputs\":\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            blocks_to_swap_in=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            infeasible_seq_groups=[],\n        )\n\n\n@dataclass\nclass SchedulerPrefillOutputs:\n    \"\"\"The requests that are scheduled from a waiting queue.\n\n    Could contain a fresh prefill requests or preempted requests that need\n    to be recomputed from scratch.\n    \"\"\"\n    # Selected sequences for prefill.\n    seq_groups: List[SequenceGroup]\n    # Ignored sequence groups.\n    ignored_seq_groups: List[SequenceGroup]\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerPrefillOutputs\":\n        return SchedulerPrefillOutputs(\n            seq_groups=[],\n            ignored_seq_groups=[],\n            num_lookahead_slots=0,\n        )\n\n\ndef seq_group_metadata_builder():\n    return SequenceGroupMetadata(request_id=\"\",\n                                 is_prompt=False,\n                                 seq_data={},\n                                 sampling_params=None,\n                                 block_tables={})\n\n\ndef scheduler_running_outputs_builder():\n    return SchedulerRunningOutputs(decode_seq_groups=[],\n                                   prefill_seq_groups=[],\n                                   preempted=[],\n                                   swapped_out=[],\n                                   blocks_to_swap_out=[],\n                                   blocks_to_copy=[],\n                                   num_lookahead_slots=0,\n                                   prefill_seq_groups_list=[],\n                                   decode_seq_groups_list=[])\n\n\ndef scheduled_seq_group_builder():\n    return ScheduledSequenceGroup(seq_group=None, token_chunk_size=0)\n\n\nclass Scheduler:\n\n    def __init__(\n        self,\n        scheduler_config: SchedulerConfig,\n        cache_config: CacheConfig,\n        lora_config: Optional[LoRAConfig],\n        pipeline_parallel_size: int = 1,\n    ) -> None:\n        self.scheduler_config = scheduler_config\n        self.cache_config = cache_config\n        # Note for LoRA scheduling: the current policy is extremely\n        # simple and NOT fair. It can lead to starvation of some\n        # LoRAs. This should be improved in the future.\n        self.lora_config = lora_config\n\n        version = \"v1\"\n        if self.scheduler_config.use_v2_block_manager:\n            version = \"v2\"\n        if self.scheduler_config.embedding_mode:\n            version = \"embedding\"\n\n        BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(\n            version)\n\n        num_gpu_blocks = cache_config.num_gpu_blocks\n        if num_gpu_blocks:\n            num_gpu_blocks //= pipeline_parallel_size\n\n        num_cpu_blocks = cache_config.num_cpu_blocks\n        if num_cpu_blocks:\n            num_cpu_blocks //= pipeline_parallel_size\n\n        # Create the block space manager.\n        self.block_manager = BlockSpaceManagerImpl(\n            block_size=self.cache_config.block_size,\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            sliding_window=self.cache_config.sliding_window,\n            enable_caching=self.cache_config.enable_prefix_caching)\n\n        # Sequence groups in the WAITING state.\n        # Contain new prefill or preempted requests.\n        self.waiting: Deque[SequenceGroup] = deque()\n        # Sequence groups in the RUNNING state.\n        # Contain decode requests.\n        self.running: Deque[SequenceGroup] = deque()\n        # Sequence groups in the SWAPPED state.\n        # Contain decode requests that are swapped out.\n        self.swapped: Deque[SequenceGroup] = deque()\n        # Sequence groups finished requests ids since last step iteration.\n        # It lets the model know that any state associated with these requests\n        # can and must be released after the current step.\n        # This is used to evict the finished requests from the Mamba cache.\n        self._finished_requests_ids: List[str] = list()\n        # Time at previous scheduling step\n        self.prev_time = 0.0\n        # Did we schedule a prompt at previous step?\n        self.prev_prompt = False\n        # Latency of the last prompt step\n        self.last_prompt_latency = 0.0\n        # preemption mode, RECOMPUTE or SWAP\n        self.user_specified_preemption_mode = scheduler_config.preemption_mode\n\n        # The following field is test-only. It is used to inject artificial\n        # preemption.\n        self.enable_artificial_preemption = ENABLE_ARTIFICIAL_PREEMPT\n        self.artificial_preempt_cnt = (ARTIFICIAL_PREEMPTION_MAX_CNT\n                                       if self.enable_artificial_preemption\n                                       else 0)\n        self.num_cumulative_preemption: int = 0\n\n        # Used to cache python objects\n        self._scheduler_running_outputs_cache: PyObjectCache = PyObjectCache(\n            scheduler_running_outputs_builder)\n        self._scheduled_seq_group_cache: PyObjectCache = PyObjectCache(\n            scheduled_seq_group_builder)\n\n    @property\n    def lora_enabled(self) -> bool:\n        return bool(self.lora_config)\n\n    @property\n    def num_decoding_tokens_per_seq(self) -> int:\n        \"\"\"The number of new tokens.\"\"\"\n        return 1\n\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the waiting queue.\n        self.waiting.append(seq_group)\n\n    def _add_seq_group_to_running(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the running queue.\n        # Only for testing purposes.\n        self.running.append(seq_group)\n\n    def _add_seq_group_to_swapped(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the swapped queue.\n        # Only for testing purposes.\n        self.swapped.append(seq_group)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _free_seq_group_cross_attn_blocks(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        \"\"\"\n        Free a sequence group from a cross-attention block table.\n        Has no effect on decoder-only models.\n        \"\"\"\n        if seq_group.is_encoder_decoder():\n            self.block_manager.free_cross(seq_group)\n\n    def has_unfinished_seqs(self) -> bool:\n        return len(self.waiting) != 0 or len(self.running) != 0 or len(\n            self.swapped) != 0\n\n    def get_num_unfinished_seq_groups(self) -> int:\n        return len(self.waiting) + len(self.running) + len(self.swapped)\n\n    def get_and_reset_finished_requests_ids(self) -> List[str]:\n        \"\"\"Flushes the list of request ids of previously finished seq_groups.\"\"\"\n        finished_requests_ids = self._finished_requests_ids\n        self._finished_requests_ids = list()\n        return finished_requests_ids\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_prompt_limit(self, seq_group: SequenceGroup) -> int:\n        if self.scheduler_config.chunked_prefill_enabled:\n            prompt_limit = self.scheduler_config.max_model_len\n        else:\n            prompt_limit = min(self.scheduler_config.max_model_len,\n                               self.scheduler_config.max_num_batched_tokens)\n\n        # Model is fine tuned with long context. Return the fine tuned max_len.\n        if (seq_group.lora_request\n                and seq_group.lora_request.long_lora_max_len):\n            assert prompt_limit <= seq_group.lora_request.long_lora_max_len\n            return seq_group.lora_request.long_lora_max_len\n        else:\n            return prompt_limit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _schedule_default(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        The current policy is designed to optimize the throughput. First,\n        it batches as many prefill requests as possible. And it schedules\n        decodes. If there's a pressure on GPU memory, decode requests can\n        be swapped or preempted.\n        \"\"\"\n        # Include running requests to the budget.\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        # Make sure we include num running seqs before scheduling prefill,\n        # so that we don't schedule beyond max_num_seqs for prefill.\n        for seq_group in self.running:\n            budget.add_num_seqs(seq_group.request_id,\n                                seq_group.get_max_num_running_seqs())\n        curr_loras = set(\n            seq_group.lora_int_id for seq_group in self.running\n            if seq_group.lora_int_id > 0) if self.lora_enabled else None\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        running_scheduled = SchedulerRunningOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # If any requests are swapped, prioritized swapped requests.\n        if not self.swapped:\n            prefills = self._schedule_prefills(budget,\n                                               curr_loras,\n                                               enable_chunking=False)\n\n        # Don't schedule decodes if prefills are scheduled.\n        # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running\n        # only contains decode requests, not chunked prefills.\n        if len(prefills.seq_groups) == 0:\n            running_scheduled = self._schedule_running(budget,\n                                                       curr_loras,\n                                                       enable_chunking=False)\n\n            # If any sequence group is preempted, do not swap in any sequence\n            # group. because it means there's no slot for new running requests.\n            if len(running_scheduled.preempted) + len(\n                    running_scheduled.swapped_out) == 0:\n                swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        if len(prefills.seq_groups) > 0:\n            self.running.extend([s.seq_group for s in prefills.seq_groups])\n\n        self.running.extend(running_scheduled.decode_seq_groups_list)\n\n        if len(swapped_in.decode_seq_groups) > 0:\n            self.running.extend(\n                [s.seq_group for s in swapped_in.decode_seq_groups])\n\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        preempted = (len(running_scheduled.preempted) +\n                     len(running_scheduled.swapped_out))\n\n        # There should be no prefill from running queue because this policy\n        # doesn't allow chunked prefills.\n        assert len(running_scheduled.prefill_seq_groups) == 0\n        assert len(swapped_in.prefill_seq_groups) == 0\n\n        # Merge lists\n        num_prefill_groups = len(prefills.seq_groups)\n        if num_prefill_groups > 0:\n            scheduled_seq_groups = prefills.seq_groups\n            scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n        else:\n            scheduled_seq_groups = running_scheduled.decode_seq_groups\n        scheduled_seq_groups.extend(swapped_in.decode_seq_groups)\n\n        blocks_to_copy = running_scheduled.blocks_to_copy\n        blocks_to_copy.extend(swapped_in.blocks_to_copy)\n\n        ignored_seq_groups = prefills.ignored_seq_groups\n        ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)\n\n        return SchedulerOutputs(\n            scheduled_seq_groups=scheduled_seq_groups,\n            num_prefill_groups=num_prefill_groups,\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=preempted,\n        )\n\n    def _schedule_chunked_prefill(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        Chunked prefill allows to chunk prefill requests, batch them together\n        with decode requests. This policy 1. schedule as many decoding requests\n        as possible. 2. schedule chunked prefill requests that are not\n        finished. 3. schedule swapped request. 4. schedule new prefill\n        requests.\n\n        The policy can sustain the high GPU utilization because it can put\n        prefill and decodes requests to the same batch, while it improves\n        inter token latency because decodes requests don't need to be blocked\n        by prefill requests.\n        \"\"\"\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        curr_loras: Set[int] = set()\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # Decoding should be always scheduled first by fcfs.\n        running_scheduled = self._schedule_running(budget,\n                                                   curr_loras,\n                                                   enable_chunking=True)\n\n        # Schedule swapped out requests.\n        # If preemption happens, it means we don't have space for swap-in.\n        if len(running_scheduled.preempted) + len(\n                running_scheduled.swapped_out) == 0:\n            swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        # Schedule new prefills.\n        prefills = self._schedule_prefills(budget,\n                                           curr_loras,\n                                           enable_chunking=True)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.prefill_seq_groups])\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled\n```\n# Function Signature: def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:\n# Natural Language Annotations: \"\"\"Aborts a sequence group with the given ID.\n\n        Check if the sequence group with the given ID\n            is present in any of the state queue.\n        If present, remove the sequence group from the state queue.\n            Also, if any of the sequences in the sequence group is not finished,\n                free the sequence with status `FINISHED_ABORTED`.\n        Otherwise, do nothing.\n\n        Args:\n            request_id: The ID(s) of the sequence group to abort.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef abort_seq_group(self, request_id:", "  ```\nimport copy\n\ndef abort_seq_group(self,", "  ```\ndef abort_seq_group(self, request_id:"]}, "vllm-vllm/core/scheduler.py-_schedule_running": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/scheduler.py:\n```\nimport enum\nimport os\nimport random\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom typing import Deque, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                           SequenceGroupMetadata, SequenceGroupMetadataDelta,\n                           SequenceStatus)\nfrom vllm.utils import PyObjectCache\n\nlogger = init_logger(__name__)\n\n# Test-only. If configured, decode is preempted with\n# ARTIFICIAL_PREEMPTION_PROB% probability.\nENABLE_ARTIFICIAL_PREEMPT = bool(\n    os.getenv(\"VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT\", False))  # noqa\nARTIFICIAL_PREEMPTION_PROB = 0.5\nARTIFICIAL_PREEMPTION_MAX_CNT = 500\n\n\nclass PreemptionMode(enum.Enum):\n    \"\"\"Preemption modes.\n\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\n    and swap them back in when the sequences are resumed.\n    2. Recomputation: Discard the blocks of the preempted sequences and\n    recompute them when the sequences are resumed, treating the sequences as\n    new prompts.\n    \"\"\"\n    SWAP = enum.auto()\n    RECOMPUTE = enum.auto()\n\n\n@dataclass\nclass SchedulingBudget:\n    \"\"\"The available slots for scheduling.\n\n    TODO(sang): Right now, the budget is request_id-aware meaning it can ignore\n    budget update from the same request_id. It is because in normal scheduling\n    path, we update RUNNING num_seqs ahead of time, meaning it could be\n    updated more than once when scheduling RUNNING requests. Since this won't\n    happen if we only have chunked prefill scheduling, we can remove this\n    feature from the API when chunked prefill is enabled by default.\n    \"\"\"\n    token_budget: int\n    max_num_seqs: int\n    _request_ids_num_batched_tokens: Set[str] = field(default_factory=set)\n    _request_ids_num_curr_seqs: Set[str] = field(default_factory=set)\n    _num_batched_tokens: int = 0\n    _num_curr_seqs: int = 0\n\n    def can_schedule(self, *, num_new_tokens: int, num_new_seqs: int):\n        assert num_new_tokens != 0\n        assert num_new_seqs != 0\n        return (self.num_batched_tokens + num_new_tokens <= self.token_budget\n                and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)\n\n    def remaining_token_budget(self):\n        return self.token_budget - self.num_batched_tokens\n\n    def add_num_batched_tokens(self, req_id: str, num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            return\n\n        self._request_ids_num_batched_tokens.add(req_id)\n        self._num_batched_tokens += num_batched_tokens\n\n    def subtract_num_batched_tokens(self, req_id: str,\n                                    num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            self._request_ids_num_batched_tokens.remove(req_id)\n            self._num_batched_tokens -= num_batched_tokens\n\n    def add_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            return\n\n        self._request_ids_num_curr_seqs.add(req_id)\n        self._num_curr_seqs += num_curr_seqs\n\n    def subtract_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            self._request_ids_num_curr_seqs.remove(req_id)\n            self._num_curr_seqs -= num_curr_seqs\n\n    @property\n    def num_batched_tokens(self):\n        return self._num_batched_tokens\n\n    @property\n    def num_curr_seqs(self):\n        return self._num_curr_seqs\n\n\n@dataclass\nclass ScheduledSequenceGroup:\n    # A sequence group that's scheduled.\n    seq_group: SequenceGroup\n    # The total chunk size (number of tokens) to process for next iteration.\n    # 1 for decoding. Same as prompt tokens for prefill, but if prefill is\n    # chunked, it can be smaller than that.\n    token_chunk_size: int\n\n\n@dataclass\nclass SchedulerOutputs:\n    \"\"\"The scheduling decision made from a scheduler.\"\"\"\n    # Scheduled sequence groups.\n    scheduled_seq_groups: Iterable[ScheduledSequenceGroup]\n    # Number of prefill groups scheduled.\n    num_prefill_groups: int\n    # Total number of batched tokens.\n    num_batched_tokens: int\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]]\n    # Sequence groups that are going to be ignored.\n    ignored_seq_groups: List[SequenceGroup]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # The number of requests in the running queue\n    running_queue_size: int\n    preempted: int\n\n    def __post_init__(self):\n        # Swap in and swap out should never happen at the same time.\n        assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)\n\n        self.num_loras: int = len(self.lora_requests)\n        if self.num_loras > 0:\n            self._sort_by_lora_ids()\n\n        self.num_prompt_adapters: int = len(self.prompt_adapter_requests)\n\n    def is_empty(self) -> bool:\n        # NOTE: We do not consider the ignored sequence groups.\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\n\n    def _sort_by_lora_ids(self):\n        self.scheduled_seq_groups = sorted(\n            self.scheduled_seq_groups,\n            key=lambda g: (g.seq_group.lora_int_id, g.seq_group.request_id))\n\n    @property\n    def lora_requests(self) -> Set[LoRARequest]:\n        return {\n            g.seq_group.lora_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.lora_request is not None\n        }\n\n    @property\n    def prompt_adapter_requests(self) -> Set[PromptAdapterRequest]:\n        return {\n            g.seq_group.prompt_adapter_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.prompt_adapter_request is not None\n        }\n\n\n@dataclass\nclass SchedulerRunningOutputs:\n    \"\"\"The requests that are scheduled from a running queue.\n\n    Could contain prefill (prefill that's chunked) or decodes. If there's not\n    enough memory, it can be preempted (for recompute) or swapped out.\n    \"\"\"\n    # Selected sequences that are running and in a decoding phase.\n    decode_seq_groups: List[ScheduledSequenceGroup]\n    # Selected sequences that are running and in a prefill phase.\n    # I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[ScheduledSequenceGroup]\n    # The preempted sequences.\n    preempted: List[SequenceGroup]\n    # Sequences that are swapped out.\n    swapped_out: List[SequenceGroup]\n    # The blocks to swap out.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    # Optimization for fast-access to seq_group lists\n    decode_seq_groups_list: List[SequenceGroup]\n    prefill_seq_groups_list: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerRunningOutputs\":\n        return SchedulerRunningOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            preempted=[],\n            swapped_out=[],\n            blocks_to_swap_out=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            decode_seq_groups_list=[],\n            prefill_seq_groups_list=[],\n        )\n\n\n@dataclass\nclass SchedulerSwappedInOutputs:\n    \"\"\"The requests that are scheduled from a swap queue.\n\n    Could contain prefill (prefill that's chunked) or decodes.\n    \"\"\"\n    # Selected sequences that are going to be swapped in and is in a\n    # decoding phase.\n    decode_seq_groups: List[SequenceGroup]\n    # Selected sequences that are going to be swapped in and in a prefill\n    # phase. I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[SequenceGroup]\n    # The blocks to swap in.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # Infeasible sequence groups.\n    infeasible_seq_groups: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerSwappedInOutputs\":\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            blocks_to_swap_in=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            infeasible_seq_groups=[],\n        )\n\n\n@dataclass\nclass SchedulerPrefillOutputs:\n    \"\"\"The requests that are scheduled from a waiting queue.\n\n    Could contain a fresh prefill requests or preempted requests that need\n    to be recomputed from scratch.\n    \"\"\"\n    # Selected sequences for prefill.\n    seq_groups: List[SequenceGroup]\n    # Ignored sequence groups.\n    ignored_seq_groups: List[SequenceGroup]\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerPrefillOutputs\":\n        return SchedulerPrefillOutputs(\n            seq_groups=[],\n            ignored_seq_groups=[],\n            num_lookahead_slots=0,\n        )\n\n\ndef seq_group_metadata_builder():\n    return SequenceGroupMetadata(request_id=\"\",\n                                 is_prompt=False,\n                                 seq_data={},\n                                 sampling_params=None,\n                                 block_tables={})\n\n\ndef scheduler_running_outputs_builder():\n    return SchedulerRunningOutputs(decode_seq_groups=[],\n                                   prefill_seq_groups=[],\n                                   preempted=[],\n                                   swapped_out=[],\n                                   blocks_to_swap_out=[],\n                                   blocks_to_copy=[],\n                                   num_lookahead_slots=0,\n                                   prefill_seq_groups_list=[],\n                                   decode_seq_groups_list=[])\n\n\ndef scheduled_seq_group_builder():\n    return ScheduledSequenceGroup(seq_group=None, token_chunk_size=0)\n\n\nclass Scheduler:\n\n    def __init__(\n        self,\n        scheduler_config: SchedulerConfig,\n        cache_config: CacheConfig,\n        lora_config: Optional[LoRAConfig],\n        pipeline_parallel_size: int = 1,\n    ) -> None:\n        self.scheduler_config = scheduler_config\n        self.cache_config = cache_config\n        # Note for LoRA scheduling: the current policy is extremely\n        # simple and NOT fair. It can lead to starvation of some\n        # LoRAs. This should be improved in the future.\n        self.lora_config = lora_config\n\n        version = \"v1\"\n        if self.scheduler_config.use_v2_block_manager:\n            version = \"v2\"\n        if self.scheduler_config.embedding_mode:\n            version = \"embedding\"\n\n        BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(\n            version)\n\n        num_gpu_blocks = cache_config.num_gpu_blocks\n        if num_gpu_blocks:\n            num_gpu_blocks //= pipeline_parallel_size\n\n        num_cpu_blocks = cache_config.num_cpu_blocks\n        if num_cpu_blocks:\n            num_cpu_blocks //= pipeline_parallel_size\n\n        # Create the block space manager.\n        self.block_manager = BlockSpaceManagerImpl(\n            block_size=self.cache_config.block_size,\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            sliding_window=self.cache_config.sliding_window,\n            enable_caching=self.cache_config.enable_prefix_caching)\n\n        # Sequence groups in the WAITING state.\n        # Contain new prefill or preempted requests.\n        self.waiting: Deque[SequenceGroup] = deque()\n        # Sequence groups in the RUNNING state.\n        # Contain decode requests.\n        self.running: Deque[SequenceGroup] = deque()\n        # Sequence groups in the SWAPPED state.\n        # Contain decode requests that are swapped out.\n        self.swapped: Deque[SequenceGroup] = deque()\n        # Sequence groups finished requests ids since last step iteration.\n        # It lets the model know that any state associated with these requests\n        # can and must be released after the current step.\n        # This is used to evict the finished requests from the Mamba cache.\n        self._finished_requests_ids: List[str] = list()\n        # Time at previous scheduling step\n        self.prev_time = 0.0\n        # Did we schedule a prompt at previous step?\n        self.prev_prompt = False\n        # Latency of the last prompt step\n        self.last_prompt_latency = 0.0\n        # preemption mode, RECOMPUTE or SWAP\n        self.user_specified_preemption_mode = scheduler_config.preemption_mode\n\n        # The following field is test-only. It is used to inject artificial\n        # preemption.\n        self.enable_artificial_preemption = ENABLE_ARTIFICIAL_PREEMPT\n        self.artificial_preempt_cnt = (ARTIFICIAL_PREEMPTION_MAX_CNT\n                                       if self.enable_artificial_preemption\n                                       else 0)\n        self.num_cumulative_preemption: int = 0\n\n        # Used to cache python objects\n        self._scheduler_running_outputs_cache: PyObjectCache = PyObjectCache(\n            scheduler_running_outputs_builder)\n        self._scheduled_seq_group_cache: PyObjectCache = PyObjectCache(\n            scheduled_seq_group_builder)\n\n    @property\n    def lora_enabled(self) -> bool:\n        return bool(self.lora_config)\n\n    @property\n    def num_decoding_tokens_per_seq(self) -> int:\n        \"\"\"The number of new tokens.\"\"\"\n        return 1\n\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the waiting queue.\n        self.waiting.append(seq_group)\n\n    def _add_seq_group_to_running(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the running queue.\n        # Only for testing purposes.\n        self.running.append(seq_group)\n\n    def _add_seq_group_to_swapped(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the swapped queue.\n        # Only for testing purposes.\n        self.swapped.append(seq_group)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _free_seq_group_cross_attn_blocks(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        \"\"\"\n        Free a sequence group from a cross-attention block table.\n        Has no effect on decoder-only models.\n        \"\"\"\n        if seq_group.is_encoder_decoder():\n            self.block_manager.free_cross(seq_group)\n\n    def has_unfinished_seqs(self) -> bool:\n        return len(self.waiting) != 0 or len(self.running) != 0 or len(\n            self.swapped) != 0\n\n    def get_num_unfinished_seq_groups(self) -> int:\n        return len(self.waiting) + len(self.running) + len(self.swapped)\n\n    def get_and_reset_finished_requests_ids(self) -> List[str]:\n        \"\"\"Flushes the list of request ids of previously finished seq_groups.\"\"\"\n        finished_requests_ids = self._finished_requests_ids\n        self._finished_requests_ids = list()\n        return finished_requests_ids\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_prompt_limit(self, seq_group: SequenceGroup) -> int:\n        if self.scheduler_config.chunked_prefill_enabled:\n            prompt_limit = self.scheduler_config.max_model_len\n        else:\n            prompt_limit = min(self.scheduler_config.max_model_len,\n                               self.scheduler_config.max_num_batched_tokens)\n\n        # Model is fine tuned with long context. Return the fine tuned max_len.\n        if (seq_group.lora_request\n                and seq_group.lora_request.long_lora_max_len):\n            assert prompt_limit <= seq_group.lora_request.long_lora_max_len\n            return seq_group.lora_request.long_lora_max_len\n        else:\n            return prompt_limit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _schedule_default(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        The current policy is designed to optimize the throughput. First,\n        it batches as many prefill requests as possible. And it schedules\n        decodes. If there's a pressure on GPU memory, decode requests can\n        be swapped or preempted.\n        \"\"\"\n        # Include running requests to the budget.\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        # Make sure we include num running seqs before scheduling prefill,\n        # so that we don't schedule beyond max_num_seqs for prefill.\n        for seq_group in self.running:\n            budget.add_num_seqs(seq_group.request_id,\n                                seq_group.get_max_num_running_seqs())\n        curr_loras = set(\n            seq_group.lora_int_id for seq_group in self.running\n            if seq_group.lora_int_id > 0) if self.lora_enabled else None\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        running_scheduled = SchedulerRunningOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # If any requests are swapped, prioritized swapped requests.\n        if not self.swapped:\n            prefills = self._schedule_prefills(budget,\n                                               curr_loras,\n                                               enable_chunking=False)\n\n        # Don't schedule decodes if prefills are scheduled.\n        # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running\n        # only contains decode requests, not chunked prefills.\n        if len(prefills.seq_groups) == 0:\n            running_scheduled = self._schedule_running(budget,\n                                                       curr_loras,\n                                                       enable_chunking=False)\n\n            # If any sequence group is preempted, do not swap in any sequence\n            # group. because it means there's no slot for new running requests.\n            if len(running_scheduled.preempted) + len(\n                    running_scheduled.swapped_out) == 0:\n                swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        if len(prefills.seq_groups) > 0:\n            self.running.extend([s.seq_group for s in prefills.seq_groups])\n\n        self.running.extend(running_scheduled.decode_seq_groups_list)\n\n        if len(swapped_in.decode_seq_groups) > 0:\n            self.running.extend(\n                [s.seq_group for s in swapped_in.decode_seq_groups])\n\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        preempted = (len(running_scheduled.preempted) +\n                     len(running_scheduled.swapped_out))\n\n        # There should be no prefill from running queue because this policy\n        # doesn't allow chunked prefills.\n        assert len(running_scheduled.prefill_seq_groups) == 0\n        assert len(swapped_in.prefill_seq_groups) == 0\n\n        # Merge lists\n        num_prefill_groups = len(prefills.seq_groups)\n        if num_prefill_groups > 0:\n            scheduled_seq_groups = prefills.seq_groups\n            scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n        else:\n            scheduled_seq_groups = running_scheduled.decode_seq_groups\n        scheduled_seq_groups.extend(swapped_in.decode_seq_groups)\n\n        blocks_to_copy = running_scheduled.blocks_to_copy\n        blocks_to_copy.extend(swapped_in.blocks_to_copy)\n\n        ignored_seq_groups = prefills.ignored_seq_groups\n        ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)\n\n        return SchedulerOutputs(\n            scheduled_seq_groups=scheduled_seq_groups,\n            num_prefill_groups=num_prefill_groups,\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=preempted,\n        )\n\n    def _schedule_chunked_prefill(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        Chunked prefill allows to chunk prefill requests, batch them together\n        with decode requests. This policy 1. schedule as many decoding requests\n        as possible. 2. schedule chunked prefill requests that are not\n        finished. 3. schedule swapped request. 4. schedule new prefill\n        requests.\n\n        The policy can sustain the high GPU utilization because it can put\n        prefill and decodes requests to the same batch, while it improves\n        inter token latency because decodes requests don't need to be blocked\n        by prefill requests.\n        \"\"\"\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        curr_loras: Set[int] = set()\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # Decoding should be always scheduled first by fcfs.\n        running_scheduled = self._schedule_running(budget,\n                                                   curr_loras,\n                                                   enable_chunking=True)\n\n        # Schedule swapped out requests.\n        # If preemption happens, it means we don't have space for swap-in.\n        if len(running_scheduled.preempted) + len(\n                running_scheduled.swapped_out) == 0:\n            swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        # Schedule new prefills.\n        prefills = self._schedule_prefills(budget,\n                                           curr_loras,\n                                           enable_chunking=True)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.prefill_seq_groups])\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled\n```\n# Function Signature: def _schedule_running(\n# Natural Language Annotations: \"\"\"Schedule sequence groups that are running.\n\n        Running queue should include decode and chunked prefill requests.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any decodes are preempted.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any decodes are preempted.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n    \n        Returns:\n            SchedulerRunningOutputs.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n    def _schedule_running(\n        self,\n", "  ```\ndef _schedule_running(\n        self,\n       ", "  ```python\ndef _schedule_running(\n    self,\n"]}, "vllm-vllm/core/scheduler.py-_schedule_swapped": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/scheduler.py:\n```\nimport enum\nimport os\nimport random\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom typing import Deque, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                           SequenceGroupMetadata, SequenceGroupMetadataDelta,\n                           SequenceStatus)\nfrom vllm.utils import PyObjectCache\n\nlogger = init_logger(__name__)\n\n# Test-only. If configured, decode is preempted with\n# ARTIFICIAL_PREEMPTION_PROB% probability.\nENABLE_ARTIFICIAL_PREEMPT = bool(\n    os.getenv(\"VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT\", False))  # noqa\nARTIFICIAL_PREEMPTION_PROB = 0.5\nARTIFICIAL_PREEMPTION_MAX_CNT = 500\n\n\nclass PreemptionMode(enum.Enum):\n    \"\"\"Preemption modes.\n\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\n    and swap them back in when the sequences are resumed.\n    2. Recomputation: Discard the blocks of the preempted sequences and\n    recompute them when the sequences are resumed, treating the sequences as\n    new prompts.\n    \"\"\"\n    SWAP = enum.auto()\n    RECOMPUTE = enum.auto()\n\n\n@dataclass\nclass SchedulingBudget:\n    \"\"\"The available slots for scheduling.\n\n    TODO(sang): Right now, the budget is request_id-aware meaning it can ignore\n    budget update from the same request_id. It is because in normal scheduling\n    path, we update RUNNING num_seqs ahead of time, meaning it could be\n    updated more than once when scheduling RUNNING requests. Since this won't\n    happen if we only have chunked prefill scheduling, we can remove this\n    feature from the API when chunked prefill is enabled by default.\n    \"\"\"\n    token_budget: int\n    max_num_seqs: int\n    _request_ids_num_batched_tokens: Set[str] = field(default_factory=set)\n    _request_ids_num_curr_seqs: Set[str] = field(default_factory=set)\n    _num_batched_tokens: int = 0\n    _num_curr_seqs: int = 0\n\n    def can_schedule(self, *, num_new_tokens: int, num_new_seqs: int):\n        assert num_new_tokens != 0\n        assert num_new_seqs != 0\n        return (self.num_batched_tokens + num_new_tokens <= self.token_budget\n                and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)\n\n    def remaining_token_budget(self):\n        return self.token_budget - self.num_batched_tokens\n\n    def add_num_batched_tokens(self, req_id: str, num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            return\n\n        self._request_ids_num_batched_tokens.add(req_id)\n        self._num_batched_tokens += num_batched_tokens\n\n    def subtract_num_batched_tokens(self, req_id: str,\n                                    num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            self._request_ids_num_batched_tokens.remove(req_id)\n            self._num_batched_tokens -= num_batched_tokens\n\n    def add_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            return\n\n        self._request_ids_num_curr_seqs.add(req_id)\n        self._num_curr_seqs += num_curr_seqs\n\n    def subtract_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            self._request_ids_num_curr_seqs.remove(req_id)\n            self._num_curr_seqs -= num_curr_seqs\n\n    @property\n    def num_batched_tokens(self):\n        return self._num_batched_tokens\n\n    @property\n    def num_curr_seqs(self):\n        return self._num_curr_seqs\n\n\n@dataclass\nclass ScheduledSequenceGroup:\n    # A sequence group that's scheduled.\n    seq_group: SequenceGroup\n    # The total chunk size (number of tokens) to process for next iteration.\n    # 1 for decoding. Same as prompt tokens for prefill, but if prefill is\n    # chunked, it can be smaller than that.\n    token_chunk_size: int\n\n\n@dataclass\nclass SchedulerOutputs:\n    \"\"\"The scheduling decision made from a scheduler.\"\"\"\n    # Scheduled sequence groups.\n    scheduled_seq_groups: Iterable[ScheduledSequenceGroup]\n    # Number of prefill groups scheduled.\n    num_prefill_groups: int\n    # Total number of batched tokens.\n    num_batched_tokens: int\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]]\n    # Sequence groups that are going to be ignored.\n    ignored_seq_groups: List[SequenceGroup]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # The number of requests in the running queue\n    running_queue_size: int\n    preempted: int\n\n    def __post_init__(self):\n        # Swap in and swap out should never happen at the same time.\n        assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)\n\n        self.num_loras: int = len(self.lora_requests)\n        if self.num_loras > 0:\n            self._sort_by_lora_ids()\n\n        self.num_prompt_adapters: int = len(self.prompt_adapter_requests)\n\n    def is_empty(self) -> bool:\n        # NOTE: We do not consider the ignored sequence groups.\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\n\n    def _sort_by_lora_ids(self):\n        self.scheduled_seq_groups = sorted(\n            self.scheduled_seq_groups,\n            key=lambda g: (g.seq_group.lora_int_id, g.seq_group.request_id))\n\n    @property\n    def lora_requests(self) -> Set[LoRARequest]:\n        return {\n            g.seq_group.lora_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.lora_request is not None\n        }\n\n    @property\n    def prompt_adapter_requests(self) -> Set[PromptAdapterRequest]:\n        return {\n            g.seq_group.prompt_adapter_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.prompt_adapter_request is not None\n        }\n\n\n@dataclass\nclass SchedulerRunningOutputs:\n    \"\"\"The requests that are scheduled from a running queue.\n\n    Could contain prefill (prefill that's chunked) or decodes. If there's not\n    enough memory, it can be preempted (for recompute) or swapped out.\n    \"\"\"\n    # Selected sequences that are running and in a decoding phase.\n    decode_seq_groups: List[ScheduledSequenceGroup]\n    # Selected sequences that are running and in a prefill phase.\n    # I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[ScheduledSequenceGroup]\n    # The preempted sequences.\n    preempted: List[SequenceGroup]\n    # Sequences that are swapped out.\n    swapped_out: List[SequenceGroup]\n    # The blocks to swap out.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    # Optimization for fast-access to seq_group lists\n    decode_seq_groups_list: List[SequenceGroup]\n    prefill_seq_groups_list: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerRunningOutputs\":\n        return SchedulerRunningOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            preempted=[],\n            swapped_out=[],\n            blocks_to_swap_out=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            decode_seq_groups_list=[],\n            prefill_seq_groups_list=[],\n        )\n\n\n@dataclass\nclass SchedulerSwappedInOutputs:\n    \"\"\"The requests that are scheduled from a swap queue.\n\n    Could contain prefill (prefill that's chunked) or decodes.\n    \"\"\"\n    # Selected sequences that are going to be swapped in and is in a\n    # decoding phase.\n    decode_seq_groups: List[SequenceGroup]\n    # Selected sequences that are going to be swapped in and in a prefill\n    # phase. I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[SequenceGroup]\n    # The blocks to swap in.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # Infeasible sequence groups.\n    infeasible_seq_groups: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerSwappedInOutputs\":\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            blocks_to_swap_in=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            infeasible_seq_groups=[],\n        )\n\n\n@dataclass\nclass SchedulerPrefillOutputs:\n    \"\"\"The requests that are scheduled from a waiting queue.\n\n    Could contain a fresh prefill requests or preempted requests that need\n    to be recomputed from scratch.\n    \"\"\"\n    # Selected sequences for prefill.\n    seq_groups: List[SequenceGroup]\n    # Ignored sequence groups.\n    ignored_seq_groups: List[SequenceGroup]\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerPrefillOutputs\":\n        return SchedulerPrefillOutputs(\n            seq_groups=[],\n            ignored_seq_groups=[],\n            num_lookahead_slots=0,\n        )\n\n\ndef seq_group_metadata_builder():\n    return SequenceGroupMetadata(request_id=\"\",\n                                 is_prompt=False,\n                                 seq_data={},\n                                 sampling_params=None,\n                                 block_tables={})\n\n\ndef scheduler_running_outputs_builder():\n    return SchedulerRunningOutputs(decode_seq_groups=[],\n                                   prefill_seq_groups=[],\n                                   preempted=[],\n                                   swapped_out=[],\n                                   blocks_to_swap_out=[],\n                                   blocks_to_copy=[],\n                                   num_lookahead_slots=0,\n                                   prefill_seq_groups_list=[],\n                                   decode_seq_groups_list=[])\n\n\ndef scheduled_seq_group_builder():\n    return ScheduledSequenceGroup(seq_group=None, token_chunk_size=0)\n\n\nclass Scheduler:\n\n    def __init__(\n        self,\n        scheduler_config: SchedulerConfig,\n        cache_config: CacheConfig,\n        lora_config: Optional[LoRAConfig],\n        pipeline_parallel_size: int = 1,\n    ) -> None:\n        self.scheduler_config = scheduler_config\n        self.cache_config = cache_config\n        # Note for LoRA scheduling: the current policy is extremely\n        # simple and NOT fair. It can lead to starvation of some\n        # LoRAs. This should be improved in the future.\n        self.lora_config = lora_config\n\n        version = \"v1\"\n        if self.scheduler_config.use_v2_block_manager:\n            version = \"v2\"\n        if self.scheduler_config.embedding_mode:\n            version = \"embedding\"\n\n        BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(\n            version)\n\n        num_gpu_blocks = cache_config.num_gpu_blocks\n        if num_gpu_blocks:\n            num_gpu_blocks //= pipeline_parallel_size\n\n        num_cpu_blocks = cache_config.num_cpu_blocks\n        if num_cpu_blocks:\n            num_cpu_blocks //= pipeline_parallel_size\n\n        # Create the block space manager.\n        self.block_manager = BlockSpaceManagerImpl(\n            block_size=self.cache_config.block_size,\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            sliding_window=self.cache_config.sliding_window,\n            enable_caching=self.cache_config.enable_prefix_caching)\n\n        # Sequence groups in the WAITING state.\n        # Contain new prefill or preempted requests.\n        self.waiting: Deque[SequenceGroup] = deque()\n        # Sequence groups in the RUNNING state.\n        # Contain decode requests.\n        self.running: Deque[SequenceGroup] = deque()\n        # Sequence groups in the SWAPPED state.\n        # Contain decode requests that are swapped out.\n        self.swapped: Deque[SequenceGroup] = deque()\n        # Sequence groups finished requests ids since last step iteration.\n        # It lets the model know that any state associated with these requests\n        # can and must be released after the current step.\n        # This is used to evict the finished requests from the Mamba cache.\n        self._finished_requests_ids: List[str] = list()\n        # Time at previous scheduling step\n        self.prev_time = 0.0\n        # Did we schedule a prompt at previous step?\n        self.prev_prompt = False\n        # Latency of the last prompt step\n        self.last_prompt_latency = 0.0\n        # preemption mode, RECOMPUTE or SWAP\n        self.user_specified_preemption_mode = scheduler_config.preemption_mode\n\n        # The following field is test-only. It is used to inject artificial\n        # preemption.\n        self.enable_artificial_preemption = ENABLE_ARTIFICIAL_PREEMPT\n        self.artificial_preempt_cnt = (ARTIFICIAL_PREEMPTION_MAX_CNT\n                                       if self.enable_artificial_preemption\n                                       else 0)\n        self.num_cumulative_preemption: int = 0\n\n        # Used to cache python objects\n        self._scheduler_running_outputs_cache: PyObjectCache = PyObjectCache(\n            scheduler_running_outputs_builder)\n        self._scheduled_seq_group_cache: PyObjectCache = PyObjectCache(\n            scheduled_seq_group_builder)\n\n    @property\n    def lora_enabled(self) -> bool:\n        return bool(self.lora_config)\n\n    @property\n    def num_decoding_tokens_per_seq(self) -> int:\n        \"\"\"The number of new tokens.\"\"\"\n        return 1\n\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the waiting queue.\n        self.waiting.append(seq_group)\n\n    def _add_seq_group_to_running(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the running queue.\n        # Only for testing purposes.\n        self.running.append(seq_group)\n\n    def _add_seq_group_to_swapped(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the swapped queue.\n        # Only for testing purposes.\n        self.swapped.append(seq_group)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _free_seq_group_cross_attn_blocks(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        \"\"\"\n        Free a sequence group from a cross-attention block table.\n        Has no effect on decoder-only models.\n        \"\"\"\n        if seq_group.is_encoder_decoder():\n            self.block_manager.free_cross(seq_group)\n\n    def has_unfinished_seqs(self) -> bool:\n        return len(self.waiting) != 0 or len(self.running) != 0 or len(\n            self.swapped) != 0\n\n    def get_num_unfinished_seq_groups(self) -> int:\n        return len(self.waiting) + len(self.running) + len(self.swapped)\n\n    def get_and_reset_finished_requests_ids(self) -> List[str]:\n        \"\"\"Flushes the list of request ids of previously finished seq_groups.\"\"\"\n        finished_requests_ids = self._finished_requests_ids\n        self._finished_requests_ids = list()\n        return finished_requests_ids\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_prompt_limit(self, seq_group: SequenceGroup) -> int:\n        if self.scheduler_config.chunked_prefill_enabled:\n            prompt_limit = self.scheduler_config.max_model_len\n        else:\n            prompt_limit = min(self.scheduler_config.max_model_len,\n                               self.scheduler_config.max_num_batched_tokens)\n\n        # Model is fine tuned with long context. Return the fine tuned max_len.\n        if (seq_group.lora_request\n                and seq_group.lora_request.long_lora_max_len):\n            assert prompt_limit <= seq_group.lora_request.long_lora_max_len\n            return seq_group.lora_request.long_lora_max_len\n        else:\n            return prompt_limit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _schedule_default(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        The current policy is designed to optimize the throughput. First,\n        it batches as many prefill requests as possible. And it schedules\n        decodes. If there's a pressure on GPU memory, decode requests can\n        be swapped or preempted.\n        \"\"\"\n        # Include running requests to the budget.\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        # Make sure we include num running seqs before scheduling prefill,\n        # so that we don't schedule beyond max_num_seqs for prefill.\n        for seq_group in self.running:\n            budget.add_num_seqs(seq_group.request_id,\n                                seq_group.get_max_num_running_seqs())\n        curr_loras = set(\n            seq_group.lora_int_id for seq_group in self.running\n            if seq_group.lora_int_id > 0) if self.lora_enabled else None\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        running_scheduled = SchedulerRunningOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # If any requests are swapped, prioritized swapped requests.\n        if not self.swapped:\n            prefills = self._schedule_prefills(budget,\n                                               curr_loras,\n                                               enable_chunking=False)\n\n        # Don't schedule decodes if prefills are scheduled.\n        # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running\n        # only contains decode requests, not chunked prefills.\n        if len(prefills.seq_groups) == 0:\n            running_scheduled = self._schedule_running(budget,\n                                                       curr_loras,\n                                                       enable_chunking=False)\n\n            # If any sequence group is preempted, do not swap in any sequence\n            # group. because it means there's no slot for new running requests.\n            if len(running_scheduled.preempted) + len(\n                    running_scheduled.swapped_out) == 0:\n                swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        if len(prefills.seq_groups) > 0:\n            self.running.extend([s.seq_group for s in prefills.seq_groups])\n\n        self.running.extend(running_scheduled.decode_seq_groups_list)\n\n        if len(swapped_in.decode_seq_groups) > 0:\n            self.running.extend(\n                [s.seq_group for s in swapped_in.decode_seq_groups])\n\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        preempted = (len(running_scheduled.preempted) +\n                     len(running_scheduled.swapped_out))\n\n        # There should be no prefill from running queue because this policy\n        # doesn't allow chunked prefills.\n        assert len(running_scheduled.prefill_seq_groups) == 0\n        assert len(swapped_in.prefill_seq_groups) == 0\n\n        # Merge lists\n        num_prefill_groups = len(prefills.seq_groups)\n        if num_prefill_groups > 0:\n            scheduled_seq_groups = prefills.seq_groups\n            scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n        else:\n            scheduled_seq_groups = running_scheduled.decode_seq_groups\n        scheduled_seq_groups.extend(swapped_in.decode_seq_groups)\n\n        blocks_to_copy = running_scheduled.blocks_to_copy\n        blocks_to_copy.extend(swapped_in.blocks_to_copy)\n\n        ignored_seq_groups = prefills.ignored_seq_groups\n        ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)\n\n        return SchedulerOutputs(\n            scheduled_seq_groups=scheduled_seq_groups,\n            num_prefill_groups=num_prefill_groups,\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=preempted,\n        )\n\n    def _schedule_chunked_prefill(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        Chunked prefill allows to chunk prefill requests, batch them together\n        with decode requests. This policy 1. schedule as many decoding requests\n        as possible. 2. schedule chunked prefill requests that are not\n        finished. 3. schedule swapped request. 4. schedule new prefill\n        requests.\n\n        The policy can sustain the high GPU utilization because it can put\n        prefill and decodes requests to the same batch, while it improves\n        inter token latency because decodes requests don't need to be blocked\n        by prefill requests.\n        \"\"\"\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        curr_loras: Set[int] = set()\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # Decoding should be always scheduled first by fcfs.\n        running_scheduled = self._schedule_running(budget,\n                                                   curr_loras,\n                                                   enable_chunking=True)\n\n        # Schedule swapped out requests.\n        # If preemption happens, it means we don't have space for swap-in.\n        if len(running_scheduled.preempted) + len(\n                running_scheduled.swapped_out) == 0:\n            swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        # Schedule new prefills.\n        prefills = self._schedule_prefills(budget,\n                                           curr_loras,\n                                           enable_chunking=True)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.prefill_seq_groups])\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled\n```\n# Function Signature: def _schedule_swapped(\n# Natural Language Annotations: \"\"\"Schedule sequence groups that are swapped out.\n\n        It schedules swapped requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are swapped in.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are swapped in.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            SchedulerSwappedInOutputs.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _schedule_swapped(self, budget: Sch", "  ```python\ndef _schedule_swapped(\n        self,", "  ```\ndef _schedule_swapped(budget, curr_"]}, "vllm-vllm/core/scheduler.py-_schedule_prefills": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/scheduler.py:\n```\nimport enum\nimport os\nimport random\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom typing import Deque, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                           SequenceGroupMetadata, SequenceGroupMetadataDelta,\n                           SequenceStatus)\nfrom vllm.utils import PyObjectCache\n\nlogger = init_logger(__name__)\n\n# Test-only. If configured, decode is preempted with\n# ARTIFICIAL_PREEMPTION_PROB% probability.\nENABLE_ARTIFICIAL_PREEMPT = bool(\n    os.getenv(\"VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT\", False))  # noqa\nARTIFICIAL_PREEMPTION_PROB = 0.5\nARTIFICIAL_PREEMPTION_MAX_CNT = 500\n\n\nclass PreemptionMode(enum.Enum):\n    \"\"\"Preemption modes.\n\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\n    and swap them back in when the sequences are resumed.\n    2. Recomputation: Discard the blocks of the preempted sequences and\n    recompute them when the sequences are resumed, treating the sequences as\n    new prompts.\n    \"\"\"\n    SWAP = enum.auto()\n    RECOMPUTE = enum.auto()\n\n\n@dataclass\nclass SchedulingBudget:\n    \"\"\"The available slots for scheduling.\n\n    TODO(sang): Right now, the budget is request_id-aware meaning it can ignore\n    budget update from the same request_id. It is because in normal scheduling\n    path, we update RUNNING num_seqs ahead of time, meaning it could be\n    updated more than once when scheduling RUNNING requests. Since this won't\n    happen if we only have chunked prefill scheduling, we can remove this\n    feature from the API when chunked prefill is enabled by default.\n    \"\"\"\n    token_budget: int\n    max_num_seqs: int\n    _request_ids_num_batched_tokens: Set[str] = field(default_factory=set)\n    _request_ids_num_curr_seqs: Set[str] = field(default_factory=set)\n    _num_batched_tokens: int = 0\n    _num_curr_seqs: int = 0\n\n    def can_schedule(self, *, num_new_tokens: int, num_new_seqs: int):\n        assert num_new_tokens != 0\n        assert num_new_seqs != 0\n        return (self.num_batched_tokens + num_new_tokens <= self.token_budget\n                and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)\n\n    def remaining_token_budget(self):\n        return self.token_budget - self.num_batched_tokens\n\n    def add_num_batched_tokens(self, req_id: str, num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            return\n\n        self._request_ids_num_batched_tokens.add(req_id)\n        self._num_batched_tokens += num_batched_tokens\n\n    def subtract_num_batched_tokens(self, req_id: str,\n                                    num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            self._request_ids_num_batched_tokens.remove(req_id)\n            self._num_batched_tokens -= num_batched_tokens\n\n    def add_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            return\n\n        self._request_ids_num_curr_seqs.add(req_id)\n        self._num_curr_seqs += num_curr_seqs\n\n    def subtract_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            self._request_ids_num_curr_seqs.remove(req_id)\n            self._num_curr_seqs -= num_curr_seqs\n\n    @property\n    def num_batched_tokens(self):\n        return self._num_batched_tokens\n\n    @property\n    def num_curr_seqs(self):\n        return self._num_curr_seqs\n\n\n@dataclass\nclass ScheduledSequenceGroup:\n    # A sequence group that's scheduled.\n    seq_group: SequenceGroup\n    # The total chunk size (number of tokens) to process for next iteration.\n    # 1 for decoding. Same as prompt tokens for prefill, but if prefill is\n    # chunked, it can be smaller than that.\n    token_chunk_size: int\n\n\n@dataclass\nclass SchedulerOutputs:\n    \"\"\"The scheduling decision made from a scheduler.\"\"\"\n    # Scheduled sequence groups.\n    scheduled_seq_groups: Iterable[ScheduledSequenceGroup]\n    # Number of prefill groups scheduled.\n    num_prefill_groups: int\n    # Total number of batched tokens.\n    num_batched_tokens: int\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]]\n    # Sequence groups that are going to be ignored.\n    ignored_seq_groups: List[SequenceGroup]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # The number of requests in the running queue\n    running_queue_size: int\n    preempted: int\n\n    def __post_init__(self):\n        # Swap in and swap out should never happen at the same time.\n        assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)\n\n        self.num_loras: int = len(self.lora_requests)\n        if self.num_loras > 0:\n            self._sort_by_lora_ids()\n\n        self.num_prompt_adapters: int = len(self.prompt_adapter_requests)\n\n    def is_empty(self) -> bool:\n        # NOTE: We do not consider the ignored sequence groups.\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\n\n    def _sort_by_lora_ids(self):\n        self.scheduled_seq_groups = sorted(\n            self.scheduled_seq_groups,\n            key=lambda g: (g.seq_group.lora_int_id, g.seq_group.request_id))\n\n    @property\n    def lora_requests(self) -> Set[LoRARequest]:\n        return {\n            g.seq_group.lora_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.lora_request is not None\n        }\n\n    @property\n    def prompt_adapter_requests(self) -> Set[PromptAdapterRequest]:\n        return {\n            g.seq_group.prompt_adapter_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.prompt_adapter_request is not None\n        }\n\n\n@dataclass\nclass SchedulerRunningOutputs:\n    \"\"\"The requests that are scheduled from a running queue.\n\n    Could contain prefill (prefill that's chunked) or decodes. If there's not\n    enough memory, it can be preempted (for recompute) or swapped out.\n    \"\"\"\n    # Selected sequences that are running and in a decoding phase.\n    decode_seq_groups: List[ScheduledSequenceGroup]\n    # Selected sequences that are running and in a prefill phase.\n    # I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[ScheduledSequenceGroup]\n    # The preempted sequences.\n    preempted: List[SequenceGroup]\n    # Sequences that are swapped out.\n    swapped_out: List[SequenceGroup]\n    # The blocks to swap out.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    # Optimization for fast-access to seq_group lists\n    decode_seq_groups_list: List[SequenceGroup]\n    prefill_seq_groups_list: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerRunningOutputs\":\n        return SchedulerRunningOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            preempted=[],\n            swapped_out=[],\n            blocks_to_swap_out=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            decode_seq_groups_list=[],\n            prefill_seq_groups_list=[],\n        )\n\n\n@dataclass\nclass SchedulerSwappedInOutputs:\n    \"\"\"The requests that are scheduled from a swap queue.\n\n    Could contain prefill (prefill that's chunked) or decodes.\n    \"\"\"\n    # Selected sequences that are going to be swapped in and is in a\n    # decoding phase.\n    decode_seq_groups: List[SequenceGroup]\n    # Selected sequences that are going to be swapped in and in a prefill\n    # phase. I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[SequenceGroup]\n    # The blocks to swap in.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # Infeasible sequence groups.\n    infeasible_seq_groups: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerSwappedInOutputs\":\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            blocks_to_swap_in=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            infeasible_seq_groups=[],\n        )\n\n\n@dataclass\nclass SchedulerPrefillOutputs:\n    \"\"\"The requests that are scheduled from a waiting queue.\n\n    Could contain a fresh prefill requests or preempted requests that need\n    to be recomputed from scratch.\n    \"\"\"\n    # Selected sequences for prefill.\n    seq_groups: List[SequenceGroup]\n    # Ignored sequence groups.\n    ignored_seq_groups: List[SequenceGroup]\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerPrefillOutputs\":\n        return SchedulerPrefillOutputs(\n            seq_groups=[],\n            ignored_seq_groups=[],\n            num_lookahead_slots=0,\n        )\n\n\ndef seq_group_metadata_builder():\n    return SequenceGroupMetadata(request_id=\"\",\n                                 is_prompt=False,\n                                 seq_data={},\n                                 sampling_params=None,\n                                 block_tables={})\n\n\ndef scheduler_running_outputs_builder():\n    return SchedulerRunningOutputs(decode_seq_groups=[],\n                                   prefill_seq_groups=[],\n                                   preempted=[],\n                                   swapped_out=[],\n                                   blocks_to_swap_out=[],\n                                   blocks_to_copy=[],\n                                   num_lookahead_slots=0,\n                                   prefill_seq_groups_list=[],\n                                   decode_seq_groups_list=[])\n\n\ndef scheduled_seq_group_builder():\n    return ScheduledSequenceGroup(seq_group=None, token_chunk_size=0)\n\n\nclass Scheduler:\n\n    def __init__(\n        self,\n        scheduler_config: SchedulerConfig,\n        cache_config: CacheConfig,\n        lora_config: Optional[LoRAConfig],\n        pipeline_parallel_size: int = 1,\n    ) -> None:\n        self.scheduler_config = scheduler_config\n        self.cache_config = cache_config\n        # Note for LoRA scheduling: the current policy is extremely\n        # simple and NOT fair. It can lead to starvation of some\n        # LoRAs. This should be improved in the future.\n        self.lora_config = lora_config\n\n        version = \"v1\"\n        if self.scheduler_config.use_v2_block_manager:\n            version = \"v2\"\n        if self.scheduler_config.embedding_mode:\n            version = \"embedding\"\n\n        BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(\n            version)\n\n        num_gpu_blocks = cache_config.num_gpu_blocks\n        if num_gpu_blocks:\n            num_gpu_blocks //= pipeline_parallel_size\n\n        num_cpu_blocks = cache_config.num_cpu_blocks\n        if num_cpu_blocks:\n            num_cpu_blocks //= pipeline_parallel_size\n\n        # Create the block space manager.\n        self.block_manager = BlockSpaceManagerImpl(\n            block_size=self.cache_config.block_size,\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            sliding_window=self.cache_config.sliding_window,\n            enable_caching=self.cache_config.enable_prefix_caching)\n\n        # Sequence groups in the WAITING state.\n        # Contain new prefill or preempted requests.\n        self.waiting: Deque[SequenceGroup] = deque()\n        # Sequence groups in the RUNNING state.\n        # Contain decode requests.\n        self.running: Deque[SequenceGroup] = deque()\n        # Sequence groups in the SWAPPED state.\n        # Contain decode requests that are swapped out.\n        self.swapped: Deque[SequenceGroup] = deque()\n        # Sequence groups finished requests ids since last step iteration.\n        # It lets the model know that any state associated with these requests\n        # can and must be released after the current step.\n        # This is used to evict the finished requests from the Mamba cache.\n        self._finished_requests_ids: List[str] = list()\n        # Time at previous scheduling step\n        self.prev_time = 0.0\n        # Did we schedule a prompt at previous step?\n        self.prev_prompt = False\n        # Latency of the last prompt step\n        self.last_prompt_latency = 0.0\n        # preemption mode, RECOMPUTE or SWAP\n        self.user_specified_preemption_mode = scheduler_config.preemption_mode\n\n        # The following field is test-only. It is used to inject artificial\n        # preemption.\n        self.enable_artificial_preemption = ENABLE_ARTIFICIAL_PREEMPT\n        self.artificial_preempt_cnt = (ARTIFICIAL_PREEMPTION_MAX_CNT\n                                       if self.enable_artificial_preemption\n                                       else 0)\n        self.num_cumulative_preemption: int = 0\n\n        # Used to cache python objects\n        self._scheduler_running_outputs_cache: PyObjectCache = PyObjectCache(\n            scheduler_running_outputs_builder)\n        self._scheduled_seq_group_cache: PyObjectCache = PyObjectCache(\n            scheduled_seq_group_builder)\n\n    @property\n    def lora_enabled(self) -> bool:\n        return bool(self.lora_config)\n\n    @property\n    def num_decoding_tokens_per_seq(self) -> int:\n        \"\"\"The number of new tokens.\"\"\"\n        return 1\n\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the waiting queue.\n        self.waiting.append(seq_group)\n\n    def _add_seq_group_to_running(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the running queue.\n        # Only for testing purposes.\n        self.running.append(seq_group)\n\n    def _add_seq_group_to_swapped(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the swapped queue.\n        # Only for testing purposes.\n        self.swapped.append(seq_group)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _free_seq_group_cross_attn_blocks(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        \"\"\"\n        Free a sequence group from a cross-attention block table.\n        Has no effect on decoder-only models.\n        \"\"\"\n        if seq_group.is_encoder_decoder():\n            self.block_manager.free_cross(seq_group)\n\n    def has_unfinished_seqs(self) -> bool:\n        return len(self.waiting) != 0 or len(self.running) != 0 or len(\n            self.swapped) != 0\n\n    def get_num_unfinished_seq_groups(self) -> int:\n        return len(self.waiting) + len(self.running) + len(self.swapped)\n\n    def get_and_reset_finished_requests_ids(self) -> List[str]:\n        \"\"\"Flushes the list of request ids of previously finished seq_groups.\"\"\"\n        finished_requests_ids = self._finished_requests_ids\n        self._finished_requests_ids = list()\n        return finished_requests_ids\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_prompt_limit(self, seq_group: SequenceGroup) -> int:\n        if self.scheduler_config.chunked_prefill_enabled:\n            prompt_limit = self.scheduler_config.max_model_len\n        else:\n            prompt_limit = min(self.scheduler_config.max_model_len,\n                               self.scheduler_config.max_num_batched_tokens)\n\n        # Model is fine tuned with long context. Return the fine tuned max_len.\n        if (seq_group.lora_request\n                and seq_group.lora_request.long_lora_max_len):\n            assert prompt_limit <= seq_group.lora_request.long_lora_max_len\n            return seq_group.lora_request.long_lora_max_len\n        else:\n            return prompt_limit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _schedule_default(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        The current policy is designed to optimize the throughput. First,\n        it batches as many prefill requests as possible. And it schedules\n        decodes. If there's a pressure on GPU memory, decode requests can\n        be swapped or preempted.\n        \"\"\"\n        # Include running requests to the budget.\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        # Make sure we include num running seqs before scheduling prefill,\n        # so that we don't schedule beyond max_num_seqs for prefill.\n        for seq_group in self.running:\n            budget.add_num_seqs(seq_group.request_id,\n                                seq_group.get_max_num_running_seqs())\n        curr_loras = set(\n            seq_group.lora_int_id for seq_group in self.running\n            if seq_group.lora_int_id > 0) if self.lora_enabled else None\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        running_scheduled = SchedulerRunningOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # If any requests are swapped, prioritized swapped requests.\n        if not self.swapped:\n            prefills = self._schedule_prefills(budget,\n                                               curr_loras,\n                                               enable_chunking=False)\n\n        # Don't schedule decodes if prefills are scheduled.\n        # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running\n        # only contains decode requests, not chunked prefills.\n        if len(prefills.seq_groups) == 0:\n            running_scheduled = self._schedule_running(budget,\n                                                       curr_loras,\n                                                       enable_chunking=False)\n\n            # If any sequence group is preempted, do not swap in any sequence\n            # group. because it means there's no slot for new running requests.\n            if len(running_scheduled.preempted) + len(\n                    running_scheduled.swapped_out) == 0:\n                swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        if len(prefills.seq_groups) > 0:\n            self.running.extend([s.seq_group for s in prefills.seq_groups])\n\n        self.running.extend(running_scheduled.decode_seq_groups_list)\n\n        if len(swapped_in.decode_seq_groups) > 0:\n            self.running.extend(\n                [s.seq_group for s in swapped_in.decode_seq_groups])\n\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        preempted = (len(running_scheduled.preempted) +\n                     len(running_scheduled.swapped_out))\n\n        # There should be no prefill from running queue because this policy\n        # doesn't allow chunked prefills.\n        assert len(running_scheduled.prefill_seq_groups) == 0\n        assert len(swapped_in.prefill_seq_groups) == 0\n\n        # Merge lists\n        num_prefill_groups = len(prefills.seq_groups)\n        if num_prefill_groups > 0:\n            scheduled_seq_groups = prefills.seq_groups\n            scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n        else:\n            scheduled_seq_groups = running_scheduled.decode_seq_groups\n        scheduled_seq_groups.extend(swapped_in.decode_seq_groups)\n\n        blocks_to_copy = running_scheduled.blocks_to_copy\n        blocks_to_copy.extend(swapped_in.blocks_to_copy)\n\n        ignored_seq_groups = prefills.ignored_seq_groups\n        ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)\n\n        return SchedulerOutputs(\n            scheduled_seq_groups=scheduled_seq_groups,\n            num_prefill_groups=num_prefill_groups,\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=preempted,\n        )\n\n    def _schedule_chunked_prefill(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        Chunked prefill allows to chunk prefill requests, batch them together\n        with decode requests. This policy 1. schedule as many decoding requests\n        as possible. 2. schedule chunked prefill requests that are not\n        finished. 3. schedule swapped request. 4. schedule new prefill\n        requests.\n\n        The policy can sustain the high GPU utilization because it can put\n        prefill and decodes requests to the same batch, while it improves\n        inter token latency because decodes requests don't need to be blocked\n        by prefill requests.\n        \"\"\"\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        curr_loras: Set[int] = set()\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # Decoding should be always scheduled first by fcfs.\n        running_scheduled = self._schedule_running(budget,\n                                                   curr_loras,\n                                                   enable_chunking=True)\n\n        # Schedule swapped out requests.\n        # If preemption happens, it means we don't have space for swap-in.\n        if len(running_scheduled.preempted) + len(\n                running_scheduled.swapped_out) == 0:\n            swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        # Schedule new prefills.\n        prefills = self._schedule_prefills(budget,\n                                           curr_loras,\n                                           enable_chunking=True)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.prefill_seq_groups])\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled\n```\n# Function Signature: def _schedule_prefills(\n# Natural Language Annotations: \"\"\"Schedule sequence groups that are in prefill stage.\n\n        Note that the current scheduler treats PREEMPTED_FOR_RECOMPUTE\n        as a new prefill (that starts from beginning -> most recently generated\n        tokens).\n\n        It schedules waiting requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are scheduled.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are scheduled.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            SchedulerPrefillOutputs.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n@dataclass\nclass SchedulerPrefillOutputs:", "  ```\ndef _schedule_prefills(budget, curr", "  ```\ndef _schedule_prefills(\n        self,"]}, "vllm-vllm/core/block/cpu_gpu_block_allocator.py-free": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/block/cpu_gpu_block_allocator.py:\n```\nfrom typing import Dict, FrozenSet, List, Optional, Tuple\n\nfrom vllm.core.block.interfaces import (Block, BlockAllocator, BlockId,\n                                        DeviceAwareBlockAllocator)\nfrom vllm.core.block.naive_block import NaiveBlock, NaiveBlockAllocator\nfrom vllm.core.block.prefix_caching_block import PrefixCachingBlockAllocator\nfrom vllm.utils import Device\n\n\nclass CpuGpuBlockAllocator(DeviceAwareBlockAllocator):\n    \"\"\"A block allocator that can allocate blocks on both CPU and GPU memory.\n\n    This class implements the `DeviceAwareBlockAllocator` interface and provides\n    functionality for allocating and managing blocks of memory on both CPU and\n    GPU devices.\n\n    The `CpuGpuBlockAllocator` maintains separate memory pools for CPU and GPU\n    blocks, and allows for allocation, deallocation, forking, and swapping of\n    blocks across these memory pools.\n    \"\"\"\n\n    @staticmethod\n    def create(\n        allocator_type: str,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        block_size: int,\n    ) -> DeviceAwareBlockAllocator:\n        \"\"\"Creates a CpuGpuBlockAllocator instance with the specified\n        configuration.\n\n        This static method creates and returns a CpuGpuBlockAllocator instance\n        based on the provided parameters. It initializes the CPU and GPU block\n        allocators with the specified number of blocks, block size, and\n        allocator type.\n\n        Args:\n            allocator_type (str): The type of block allocator to use for CPU\n                and GPU blocks. Currently supported values are \"naive\" and\n                \"prefix_caching\".\n            num_gpu_blocks (int): The number of blocks to allocate for GPU\n                memory.\n            num_cpu_blocks (int): The number of blocks to allocate for CPU\n                memory.\n            block_size (int): The size of each block in number of tokens.\n\n        Returns:\n            DeviceAwareBlockAllocator: A CpuGpuBlockAllocator instance with the\n                specified configuration.\n\n        Notes:\n            - The block IDs are assigned contiguously, with GPU block IDs coming\n                before CPU block IDs.\n        \"\"\"\n        block_ids = list(range(num_gpu_blocks + num_cpu_blocks))\n        gpu_block_ids = block_ids[:num_gpu_blocks]\n        cpu_block_ids = block_ids[num_gpu_blocks:]\n\n        if allocator_type == \"naive\":\n            gpu_allocator: BlockAllocator = NaiveBlockAllocator(\n                create_block=NaiveBlock,  # type: ignore\n                num_blocks=num_gpu_blocks,\n                block_size=block_size,\n                block_ids=gpu_block_ids,\n            )\n\n            cpu_allocator: BlockAllocator = NaiveBlockAllocator(\n                create_block=NaiveBlock,  # type: ignore\n                num_blocks=num_cpu_blocks,\n                block_size=block_size,\n                block_ids=cpu_block_ids,\n            )\n        elif allocator_type == \"prefix_caching\":\n            gpu_allocator = PrefixCachingBlockAllocator(\n                num_blocks=num_gpu_blocks,\n                block_size=block_size,\n                block_ids=gpu_block_ids,\n            )\n\n            cpu_allocator = PrefixCachingBlockAllocator(\n                num_blocks=num_cpu_blocks,\n                block_size=block_size,\n                block_ids=cpu_block_ids,\n            )\n        else:\n            raise ValueError(f\"Unknown allocator type {allocator_type=}\")\n\n        return CpuGpuBlockAllocator(\n            cpu_block_allocator=cpu_allocator,\n            gpu_block_allocator=gpu_allocator,\n        )\n\n    def __init__(self, cpu_block_allocator: BlockAllocator,\n                 gpu_block_allocator: BlockAllocator):\n        assert not (\n            cpu_block_allocator.all_block_ids\n            & gpu_block_allocator.all_block_ids\n        ), \"cpu and gpu block allocators can't have intersection of block ids\"\n\n        self._allocators = {\n            Device.CPU: cpu_block_allocator,\n            Device.GPU: gpu_block_allocator,\n        }\n\n        self._swap_mapping: Dict[int, int] = {}\n        self._null_block: Optional[Block] = None\n\n        self._block_ids_to_allocator: Dict[int, BlockAllocator] = {}\n        for _, allocator in self._allocators.items():\n            for block_id in allocator.all_block_ids:\n                self._block_ids_to_allocator[block_id] = allocator\n\n    def allocate_or_get_null_block(self) -> Block:\n        if self._null_block is None:\n            self._null_block = NullBlock(\n                self.allocate_mutable_block(None, Device.GPU))\n        return self._null_block\n\n    def allocate_mutable_block(self, prev_block: Optional[Block],\n                               device: Device) -> Block:\n        \"\"\"Allocates a new mutable block on the specified device.\n\n        Args:\n            prev_block (Optional[Block]): The previous block to in the sequence.\n                Used for prefix hashing.\n            device (Device): The device on which to allocate the new block.\n\n        Returns:\n            Block: The newly allocated mutable block.\n        \"\"\"\n        return self._allocators[device].allocate_mutable_block(prev_block)\n\n    def allocate_immutable_blocks(self, prev_block: Optional[Block],\n                                  block_token_ids: List[List[int]],\n                                  device: Optional[Device]) -> List[Block]:\n        \"\"\"Allocates a new group of immutable blocks with the provided block \n        token IDs on the specified device.\n\n        Args:\n            prev_block (Optional[Block]): The previous block in the sequence.\n                Used for prefix hashing.\n            block_token_ids (List[int]): The list of block token IDs to be \n                stored in the new blocks.\n            device (Device): The device on which to allocate the new block.\n\n        Returns:\n            List[Block]: The newly allocated list of immutable blocks \n                containing the provided block token IDs.\n        \"\"\"\n        return self._allocators[device].allocate_immutable_blocks(\n            prev_block, block_token_ids)\n\n    def allocate_immutable_block(self, prev_block: Optional[Block],\n                                 token_ids: List[int],\n                                 device: Device) -> Block:\n        \"\"\"Allocates a new immutable block with the provided token IDs on the\n        specified device.\n\n        Args:\n            prev_block (Optional[Block]): The previous block in the sequence.\n                Used for prefix hashing.\n            token_ids (List[int]): The list of token IDs to be stored in the new\n                block.\n            device (Device): The device on which to allocate the new block.\n\n        Returns:\n            Block: The newly allocated immutable block containing the provided\n                token IDs.\n        \"\"\"\n        return self._allocators[device].allocate_immutable_block(\n            prev_block, token_ids)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_num_free_blocks(self, device: Device) -> int:\n        \"\"\"Returns the number of free blocks available on the specified device.\n\n        Args:\n            device (Device): The device for which to query the number of free\n                blocks. AssertionError is raised if None is passed.\n\n        Returns:\n            int: The number of free blocks available on the specified device.\n        \"\"\"\n        return self._allocators[device].get_num_free_blocks()\n\n    def get_num_total_blocks(self, device: Device) -> int:\n        return self._allocators[device].get_num_total_blocks()\n\n    def get_physical_block_id(self, device: Device, absolute_id: int) -> int:\n        \"\"\"Returns the zero-offset block id on certain device given the \n        absolute block id.\n\n        Args:\n            device (Device): The device for which to query relative block id.\n                absolute_id (int): The absolute block id for the block in \n                whole allocator.\n\n        Returns:\n            int: The zero-offset block id on certain device.\n        \"\"\"\n        return self._allocators[device].get_physical_block_id(absolute_id)\n\n    def swap(self, blocks: List[Block], src_device: Device,\n             dst_device: Device) -> Dict[int, int]:\n        \"\"\"Execute the swap for the given blocks from source_device\n        on to dest_device, save the current swap mapping and append \n        them to the accumulated `self._swap_mapping` for each \n        scheduling move.\n\n        Args:\n            blocks: List of blocks to be swapped.\n            src_device (Device): Device to swap the 'blocks' from.\n            dst_device (Device): Device to swap the 'blocks' to.\n        \n        Returns:\n            Dict[int, int]: Swap mapping from source_device\n                on to dest_device.\n        \"\"\"\n        src_block_ids = [block.block_id for block in blocks]\n        self._allocators[src_device].swap_out(blocks)\n        self._allocators[dst_device].swap_in(blocks)\n        dst_block_ids = [block.block_id for block in blocks]\n\n        current_swap_mapping: Dict[int, int] = {}\n        for src_block_id, dst_block_id in zip(src_block_ids, dst_block_ids):\n            if src_block_id is not None and dst_block_id is not None:\n                self._swap_mapping[src_block_id] = dst_block_id\n                current_swap_mapping[src_block_id] = dst_block_id\n        return current_swap_mapping\n\n    def get_num_blocks_touched(self,\n                               blocks: List[Block],\n                               device: Device,\n                               num_lookahead_slots: int = 0) -> int:\n        \"\"\"Returns the number of blocks that will be touched by\n        swapping in/out the given blocks on to the 'device'.\n\n        Args:\n            blocks: List of blocks to be swapped.\n            device (Device): Device to swap the 'blocks' on.\n            num_lookahead_slots (int): Number of lookahead slots used in \n                speculative decoding, default to 0.\n\n        Returns:\n            int: the number of blocks that will be touched by\n                swapping in/out the given blocks on to the 'device'.\n        \"\"\"\n        return self._allocators[device].get_num_blocks_touched(\n            blocks, num_lookahead_slots)\n\n    def clear_copy_on_writes(self) -> List[Tuple[int, int]]:\n        \"\"\"Clears the copy-on-write (CoW) state and returns the mapping of\n            source to destination block IDs.\n\n        Returns:\n            List[Tuple[int, int]]: A list mapping source block IDs to \n                destination block IDs.\n        \"\"\"\n        # CoW only supported on GPU\n        device = Device.GPU\n        return self._allocators[device].clear_copy_on_writes()\n\n    def mark_blocks_as_accessed(self, block_ids: List[int],\n                                now: float) -> None:\n        \"\"\"Mark blocks as accessed, only use for prefix caching.\"\"\"\n        # Prefix caching only supported on GPU.\n        device = Device.GPU\n        return self._allocators[device].mark_blocks_as_accessed(block_ids, now)\n\n    def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n        \"\"\"Mark blocks as accessed, only use for prefix caching.\"\"\"\n        # Prefix caching only supported on GPU.\n        device = Device.GPU\n        return self._allocators[device].mark_blocks_as_computed(block_ids)\n\n    def get_computed_block_ids(self, prev_computed_block_ids: List[int],\n                               block_ids: List[int],\n                               skip_last_block_id: bool) -> List[int]:\n        # Prefix caching only supported on GPU.\n        device = Device.GPU\n        return self._allocators[device].get_computed_block_ids(\n            prev_computed_block_ids, block_ids, skip_last_block_id)\n\n    def get_common_computed_block_ids(\n            self, computed_seq_block_ids: List[List[int]]) -> List[int]:\n        # Prefix caching only supported on GPU.\n        device = Device.GPU\n        return self._allocators[device].get_common_computed_block_ids(\n            computed_seq_block_ids)\n\n    @property\n    def all_block_ids(self) -> FrozenSet[int]:\n        return frozenset(self._block_ids_to_allocator.keys())\n\n    def get_and_reset_swaps(self) -> List[Tuple[int, int]]:\n        \"\"\"Returns and clears the mapping of source to destination block IDs.\n        Will be called after every swapping operations for now, and after every\n        schedule when BlockManagerV2 become default. Currently not useful.\n\n        Returns:\n            List[Tuple[int, int]]: A mapping of source to destination block IDs.\n        \"\"\"\n        mapping = self._swap_mapping.copy()\n        self._swap_mapping.clear()\n        return list(mapping.items())\n\n\nclass NullBlock(Block):\n    \"\"\"\n    Null blocks are used as a placeholders for KV cache blocks that have\n    been dropped due to sliding window.\n    This implementation just wraps an ordinary block and prevents it from\n    being modified. It also allows for testing if a block is NullBlock\n    via isinstance().\n    \"\"\"\n\n    def __init__(self, proxy: Block):\n        super().__init__()\n        self._proxy = proxy\n\n    def append_token_ids(self, token_ids: List[BlockId]):\n        raise ValueError(\"null block should not be modified\")\n\n    @property\n    def block_id(self):\n        return self._proxy.block_id\n\n    @block_id.setter\n    def block_id(self, value: Optional[BlockId]):\n        raise ValueError(\"null block should not be modified\")\n\n    @property\n    def token_ids(self) -> List[BlockId]:\n        return self._proxy.token_ids\n\n    @property\n    def num_tokens_total(self) -> int:\n        raise NotImplementedError(\n            \"num_tokens_total is not used for null block\")\n\n    @property\n    def num_empty_slots(self) -> BlockId:\n        return self._proxy.num_empty_slots\n\n    @property\n    def is_full(self):\n        return self._proxy.is_full\n\n    @property\n    def prev_block(self):\n        return self._proxy.prev_block\n\n    @property\n    def computed(self):\n        return self._proxy.computed\n\n    @computed.setter\n    def computed(self, value):\n        self._proxy.computed = value\n\n    @property\n    def last_accessed(self) -> float:\n        return self._proxy.last_accessed\n\n    @last_accessed.setter\n    def last_accessed(self, last_accessed_ts: float):\n        self._proxy.last_accessed = last_accessed_ts\n\n    @property\n    def content_hash(self):\n        return self._proxy.content_hash\n\n```\n# Function Signature: def free(self, block: Block) -> None:\n# Natural Language Annotations: \"\"\"Frees the memory occupied by the given block.\n\n        Args:\n            block (Block): The block to be freed.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef free(self, block: Block) -> None:\n", "  ```\ndef free(self, block: Block) -> None:\n", "  ```\ndef free(self, block: Block) -> None:\n"]}, "vllm-vllm/core/block/cpu_gpu_block_allocator.py-fork": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/block/cpu_gpu_block_allocator.py:\n```\nfrom typing import Dict, FrozenSet, List, Optional, Tuple\n\nfrom vllm.core.block.interfaces import (Block, BlockAllocator, BlockId,\n                                        DeviceAwareBlockAllocator)\nfrom vllm.core.block.naive_block import NaiveBlock, NaiveBlockAllocator\nfrom vllm.core.block.prefix_caching_block import PrefixCachingBlockAllocator\nfrom vllm.utils import Device\n\n\nclass CpuGpuBlockAllocator(DeviceAwareBlockAllocator):\n    \"\"\"A block allocator that can allocate blocks on both CPU and GPU memory.\n\n    This class implements the `DeviceAwareBlockAllocator` interface and provides\n    functionality for allocating and managing blocks of memory on both CPU and\n    GPU devices.\n\n    The `CpuGpuBlockAllocator` maintains separate memory pools for CPU and GPU\n    blocks, and allows for allocation, deallocation, forking, and swapping of\n    blocks across these memory pools.\n    \"\"\"\n\n    @staticmethod\n    def create(\n        allocator_type: str,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        block_size: int,\n    ) -> DeviceAwareBlockAllocator:\n        \"\"\"Creates a CpuGpuBlockAllocator instance with the specified\n        configuration.\n\n        This static method creates and returns a CpuGpuBlockAllocator instance\n        based on the provided parameters. It initializes the CPU and GPU block\n        allocators with the specified number of blocks, block size, and\n        allocator type.\n\n        Args:\n            allocator_type (str): The type of block allocator to use for CPU\n                and GPU blocks. Currently supported values are \"naive\" and\n                \"prefix_caching\".\n            num_gpu_blocks (int): The number of blocks to allocate for GPU\n                memory.\n            num_cpu_blocks (int): The number of blocks to allocate for CPU\n                memory.\n            block_size (int): The size of each block in number of tokens.\n\n        Returns:\n            DeviceAwareBlockAllocator: A CpuGpuBlockAllocator instance with the\n                specified configuration.\n\n        Notes:\n            - The block IDs are assigned contiguously, with GPU block IDs coming\n                before CPU block IDs.\n        \"\"\"\n        block_ids = list(range(num_gpu_blocks + num_cpu_blocks))\n        gpu_block_ids = block_ids[:num_gpu_blocks]\n        cpu_block_ids = block_ids[num_gpu_blocks:]\n\n        if allocator_type == \"naive\":\n            gpu_allocator: BlockAllocator = NaiveBlockAllocator(\n                create_block=NaiveBlock,  # type: ignore\n                num_blocks=num_gpu_blocks,\n                block_size=block_size,\n                block_ids=gpu_block_ids,\n            )\n\n            cpu_allocator: BlockAllocator = NaiveBlockAllocator(\n                create_block=NaiveBlock,  # type: ignore\n                num_blocks=num_cpu_blocks,\n                block_size=block_size,\n                block_ids=cpu_block_ids,\n            )\n        elif allocator_type == \"prefix_caching\":\n            gpu_allocator = PrefixCachingBlockAllocator(\n                num_blocks=num_gpu_blocks,\n                block_size=block_size,\n                block_ids=gpu_block_ids,\n            )\n\n            cpu_allocator = PrefixCachingBlockAllocator(\n                num_blocks=num_cpu_blocks,\n                block_size=block_size,\n                block_ids=cpu_block_ids,\n            )\n        else:\n            raise ValueError(f\"Unknown allocator type {allocator_type=}\")\n\n        return CpuGpuBlockAllocator(\n            cpu_block_allocator=cpu_allocator,\n            gpu_block_allocator=gpu_allocator,\n        )\n\n    def __init__(self, cpu_block_allocator: BlockAllocator,\n                 gpu_block_allocator: BlockAllocator):\n        assert not (\n            cpu_block_allocator.all_block_ids\n            & gpu_block_allocator.all_block_ids\n        ), \"cpu and gpu block allocators can't have intersection of block ids\"\n\n        self._allocators = {\n            Device.CPU: cpu_block_allocator,\n            Device.GPU: gpu_block_allocator,\n        }\n\n        self._swap_mapping: Dict[int, int] = {}\n        self._null_block: Optional[Block] = None\n\n        self._block_ids_to_allocator: Dict[int, BlockAllocator] = {}\n        for _, allocator in self._allocators.items():\n            for block_id in allocator.all_block_ids:\n                self._block_ids_to_allocator[block_id] = allocator\n\n    def allocate_or_get_null_block(self) -> Block:\n        if self._null_block is None:\n            self._null_block = NullBlock(\n                self.allocate_mutable_block(None, Device.GPU))\n        return self._null_block\n\n    def allocate_mutable_block(self, prev_block: Optional[Block],\n                               device: Device) -> Block:\n        \"\"\"Allocates a new mutable block on the specified device.\n\n        Args:\n            prev_block (Optional[Block]): The previous block to in the sequence.\n                Used for prefix hashing.\n            device (Device): The device on which to allocate the new block.\n\n        Returns:\n            Block: The newly allocated mutable block.\n        \"\"\"\n        return self._allocators[device].allocate_mutable_block(prev_block)\n\n    def allocate_immutable_blocks(self, prev_block: Optional[Block],\n                                  block_token_ids: List[List[int]],\n                                  device: Optional[Device]) -> List[Block]:\n        \"\"\"Allocates a new group of immutable blocks with the provided block \n        token IDs on the specified device.\n\n        Args:\n            prev_block (Optional[Block]): The previous block in the sequence.\n                Used for prefix hashing.\n            block_token_ids (List[int]): The list of block token IDs to be \n                stored in the new blocks.\n            device (Device): The device on which to allocate the new block.\n\n        Returns:\n            List[Block]: The newly allocated list of immutable blocks \n                containing the provided block token IDs.\n        \"\"\"\n        return self._allocators[device].allocate_immutable_blocks(\n            prev_block, block_token_ids)\n\n    def allocate_immutable_block(self, prev_block: Optional[Block],\n                                 token_ids: List[int],\n                                 device: Device) -> Block:\n        \"\"\"Allocates a new immutable block with the provided token IDs on the\n        specified device.\n\n        Args:\n            prev_block (Optional[Block]): The previous block in the sequence.\n                Used for prefix hashing.\n            token_ids (List[int]): The list of token IDs to be stored in the new\n                block.\n            device (Device): The device on which to allocate the new block.\n\n        Returns:\n            Block: The newly allocated immutable block containing the provided\n                token IDs.\n        \"\"\"\n        return self._allocators[device].allocate_immutable_block(\n            prev_block, token_ids)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_num_free_blocks(self, device: Device) -> int:\n        \"\"\"Returns the number of free blocks available on the specified device.\n\n        Args:\n            device (Device): The device for which to query the number of free\n                blocks. AssertionError is raised if None is passed.\n\n        Returns:\n            int: The number of free blocks available on the specified device.\n        \"\"\"\n        return self._allocators[device].get_num_free_blocks()\n\n    def get_num_total_blocks(self, device: Device) -> int:\n        return self._allocators[device].get_num_total_blocks()\n\n    def get_physical_block_id(self, device: Device, absolute_id: int) -> int:\n        \"\"\"Returns the zero-offset block id on certain device given the \n        absolute block id.\n\n        Args:\n            device (Device): The device for which to query relative block id.\n                absolute_id (int): The absolute block id for the block in \n                whole allocator.\n\n        Returns:\n            int: The zero-offset block id on certain device.\n        \"\"\"\n        return self._allocators[device].get_physical_block_id(absolute_id)\n\n    def swap(self, blocks: List[Block], src_device: Device,\n             dst_device: Device) -> Dict[int, int]:\n        \"\"\"Execute the swap for the given blocks from source_device\n        on to dest_device, save the current swap mapping and append \n        them to the accumulated `self._swap_mapping` for each \n        scheduling move.\n\n        Args:\n            blocks: List of blocks to be swapped.\n            src_device (Device): Device to swap the 'blocks' from.\n            dst_device (Device): Device to swap the 'blocks' to.\n        \n        Returns:\n            Dict[int, int]: Swap mapping from source_device\n                on to dest_device.\n        \"\"\"\n        src_block_ids = [block.block_id for block in blocks]\n        self._allocators[src_device].swap_out(blocks)\n        self._allocators[dst_device].swap_in(blocks)\n        dst_block_ids = [block.block_id for block in blocks]\n\n        current_swap_mapping: Dict[int, int] = {}\n        for src_block_id, dst_block_id in zip(src_block_ids, dst_block_ids):\n            if src_block_id is not None and dst_block_id is not None:\n                self._swap_mapping[src_block_id] = dst_block_id\n                current_swap_mapping[src_block_id] = dst_block_id\n        return current_swap_mapping\n\n    def get_num_blocks_touched(self,\n                               blocks: List[Block],\n                               device: Device,\n                               num_lookahead_slots: int = 0) -> int:\n        \"\"\"Returns the number of blocks that will be touched by\n        swapping in/out the given blocks on to the 'device'.\n\n        Args:\n            blocks: List of blocks to be swapped.\n            device (Device): Device to swap the 'blocks' on.\n            num_lookahead_slots (int): Number of lookahead slots used in \n                speculative decoding, default to 0.\n\n        Returns:\n            int: the number of blocks that will be touched by\n                swapping in/out the given blocks on to the 'device'.\n        \"\"\"\n        return self._allocators[device].get_num_blocks_touched(\n            blocks, num_lookahead_slots)\n\n    def clear_copy_on_writes(self) -> List[Tuple[int, int]]:\n        \"\"\"Clears the copy-on-write (CoW) state and returns the mapping of\n            source to destination block IDs.\n\n        Returns:\n            List[Tuple[int, int]]: A list mapping source block IDs to \n                destination block IDs.\n        \"\"\"\n        # CoW only supported on GPU\n        device = Device.GPU\n        return self._allocators[device].clear_copy_on_writes()\n\n    def mark_blocks_as_accessed(self, block_ids: List[int],\n                                now: float) -> None:\n        \"\"\"Mark blocks as accessed, only use for prefix caching.\"\"\"\n        # Prefix caching only supported on GPU.\n        device = Device.GPU\n        return self._allocators[device].mark_blocks_as_accessed(block_ids, now)\n\n    def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n        \"\"\"Mark blocks as accessed, only use for prefix caching.\"\"\"\n        # Prefix caching only supported on GPU.\n        device = Device.GPU\n        return self._allocators[device].mark_blocks_as_computed(block_ids)\n\n    def get_computed_block_ids(self, prev_computed_block_ids: List[int],\n                               block_ids: List[int],\n                               skip_last_block_id: bool) -> List[int]:\n        # Prefix caching only supported on GPU.\n        device = Device.GPU\n        return self._allocators[device].get_computed_block_ids(\n            prev_computed_block_ids, block_ids, skip_last_block_id)\n\n    def get_common_computed_block_ids(\n            self, computed_seq_block_ids: List[List[int]]) -> List[int]:\n        # Prefix caching only supported on GPU.\n        device = Device.GPU\n        return self._allocators[device].get_common_computed_block_ids(\n            computed_seq_block_ids)\n\n    @property\n    def all_block_ids(self) -> FrozenSet[int]:\n        return frozenset(self._block_ids_to_allocator.keys())\n\n    def get_and_reset_swaps(self) -> List[Tuple[int, int]]:\n        \"\"\"Returns and clears the mapping of source to destination block IDs.\n        Will be called after every swapping operations for now, and after every\n        schedule when BlockManagerV2 become default. Currently not useful.\n\n        Returns:\n            List[Tuple[int, int]]: A mapping of source to destination block IDs.\n        \"\"\"\n        mapping = self._swap_mapping.copy()\n        self._swap_mapping.clear()\n        return list(mapping.items())\n\n\nclass NullBlock(Block):\n    \"\"\"\n    Null blocks are used as a placeholders for KV cache blocks that have\n    been dropped due to sliding window.\n    This implementation just wraps an ordinary block and prevents it from\n    being modified. It also allows for testing if a block is NullBlock\n    via isinstance().\n    \"\"\"\n\n    def __init__(self, proxy: Block):\n        super().__init__()\n        self._proxy = proxy\n\n    def append_token_ids(self, token_ids: List[BlockId]):\n        raise ValueError(\"null block should not be modified\")\n\n    @property\n    def block_id(self):\n        return self._proxy.block_id\n\n    @block_id.setter\n    def block_id(self, value: Optional[BlockId]):\n        raise ValueError(\"null block should not be modified\")\n\n    @property\n    def token_ids(self) -> List[BlockId]:\n        return self._proxy.token_ids\n\n    @property\n    def num_tokens_total(self) -> int:\n        raise NotImplementedError(\n            \"num_tokens_total is not used for null block\")\n\n    @property\n    def num_empty_slots(self) -> BlockId:\n        return self._proxy.num_empty_slots\n\n    @property\n    def is_full(self):\n        return self._proxy.is_full\n\n    @property\n    def prev_block(self):\n        return self._proxy.prev_block\n\n    @property\n    def computed(self):\n        return self._proxy.computed\n\n    @computed.setter\n    def computed(self, value):\n        self._proxy.computed = value\n\n    @property\n    def last_accessed(self) -> float:\n        return self._proxy.last_accessed\n\n    @last_accessed.setter\n    def last_accessed(self, last_accessed_ts: float):\n        self._proxy.last_accessed = last_accessed_ts\n\n    @property\n    def content_hash(self):\n        return self._proxy.content_hash\n\n```\n# Function Signature: def fork(self, last_block: Block) -> List[Block]:\n# Natural Language Annotations: \"\"\"Creates a new sequence of blocks that shares the same underlying\n            memory as the original sequence.\n\n        Args:\n            last_block (Block): The last block in the original sequence.\n\n        Returns:\n            List[Block]: A new list of blocks that shares the same memory as the\n                original sequence.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef fork(self, last_block: Block) -> List", "  ```\ndef fork(self, last_block: Block) -> List", "  ```\nclass CpuGpuBlockAllocator(DeviceAware"]}, "vllm-vllm/core/block/prefix_caching_block.py-append_token_ids": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/block/prefix_caching_block.py:\n```\n\"\"\"Token blocks.\"\"\"\n\nfrom os.path import commonprefix\nfrom typing import Dict, FrozenSet, Iterable, List, Optional, Tuple\n\nfrom vllm.core.block.common import (CopyOnWriteTracker,\n                                    get_all_blocks_recursively)\nfrom vllm.core.block.interfaces import Block, BlockAllocator, BlockId, Device\nfrom vllm.core.block.naive_block import (BlockPool, NaiveBlock,\n                                         NaiveBlockAllocator)\nfrom vllm.core.evictor_v2 import EvictionPolicy, Evictor, make_evictor\nfrom vllm.utils import cdiv\n\nPrefixHash = int\n\n# By default, we init our block access time as _DEFAULT_LAST_ACCESSED_TIME\n# so that if we find one block is still hold _DEFAULT_LAST_ACCESSED_TIME,\n# then we know this block hasn't been accessed yet.\n_DEFAULT_LAST_ACCESSED_TIME = -1\n\n\nclass BlockTracker:\n    \"\"\"Used to track the status of a block inside the prefix caching allocator\n    \"\"\"\n    __slots__ = (\"active\", \"last_accessed\", \"computed\")\n\n    def reset(self):\n        self.last_accessed: float = _DEFAULT_LAST_ACCESSED_TIME\n        self.computed: bool = False\n\n    def __init__(self):\n        self.active: bool = False\n        self.reset()\n\n    def enable(self):\n        assert not self.active\n        self.active = True\n        self.reset()\n\n    def disable(self):\n        assert self.active\n        self.active = False\n        self.reset()\n\n\nclass PrefixCachingBlockAllocator(BlockAllocator):\n    \"\"\"A block allocator that implements prefix caching.\n\n    The PrefixCachingBlockAllocator maintains a cache of blocks based on their\n    content hash. It reuses blocks with the same content hash to avoid redundant\n    memory allocation. The allocator also supports copy-on-write operations.\n\n    Args:\n        num_blocks (int): The total number of blocks to manage.\n        block_size (int): The size of each block in tokens.\n        block_ids(Optional[Iterable[int]], optional): An optional iterable of\n            block IDs. If not provided, block IDs will be assigned sequentially\n            from 0 to num_blocks - 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_blocks: int,\n        block_size: int,\n        block_ids: Optional[Iterable[int]] = None,\n        eviction_policy: EvictionPolicy = EvictionPolicy.LRU,\n    ):\n        if block_ids is None:\n            block_ids = range(num_blocks)\n\n        self._block_size = block_size\n\n        # A mapping of prefix hash to block index. All blocks which have a\n        # prefix hash will be in this dict, even if they have refcount 0.\n        self._cached_blocks: Dict[PrefixHash, BlockId] = {}\n\n        # Used to track status of each physical block id\n        self._block_tracker: Dict[BlockId, BlockTracker] = {}\n        for block_id in block_ids:\n            self._block_tracker[block_id] = BlockTracker()\n\n        # Pre-allocate \"num_blocks * extra_factor\" block objects.\n        # The \"* extra_factor\" is a buffer to allow more block objects\n        # than physical blocks\n        extra_factor = 4\n        self._block_pool = BlockPool(self._block_size, self._create_block,\n                                     self, num_blocks * extra_factor)\n\n        # An allocator for blocks that do not have prefix hashes.\n        self._hashless_allocator = NaiveBlockAllocator(\n            create_block=self._create_block,  # type: ignore\n            num_blocks=num_blocks,\n            block_size=block_size,\n            block_ids=block_ids,\n            block_pool=self._block_pool,  # Share block pool here\n        )\n\n        # Evitor used to maintain how we want to handle those computed blocks\n        # if we find memory pressure is high.\n        self.evictor: Evictor = make_evictor(eviction_policy)\n\n        # We share the refcounter between allocators. This allows us to promote\n        # blocks originally allocated in the hashless allocator to immutable\n        # blocks.\n        self._refcounter = self._hashless_allocator.refcounter\n\n        self._cow_tracker = CopyOnWriteTracker(\n            refcounter=self._refcounter.as_readonly())\n\n    # Implements Block.Factory.\n    def _create_block(\n        self,\n        prev_block: Optional[Block],\n        token_ids: List[int],\n        block_size: int,\n        allocator: BlockAllocator,\n        block_id: Optional[int] = None,\n        computed: bool = False,\n    ) -> Block:\n        # Bind block to self.\n        allocator = self\n\n        return PrefixCachingBlock(\n            prev_block=prev_block,\n            token_ids=token_ids,\n            block_size=block_size,\n            block_id=block_id,\n            allocator=allocator,\n            computed=computed,\n        )\n\n    def allocate_immutable_block(self,\n                                 prev_block: Optional[Block],\n                                 token_ids: List[int],\n                                 device: Optional[Device] = None) -> Block:\n        \"\"\"Allocates an immutable block with the given token IDs, reusing cached\n        blocks if possible.\n\n        Args:\n            prev_block (Optional[Block]): The previous block in the sequence.\n            token_ids (List[int]): The token IDs to be stored in the block.\n\n        Returns:\n            Block: The allocated immutable block.\n        \"\"\"\n        assert device is None\n        assert_prefix_caching_block_or_none(prev_block)\n\n        # First, try to create a block that points to cached data\n        block = self._block_pool.init_block(prev_block=prev_block,\n                                            token_ids=token_ids,\n                                            block_size=self._block_size,\n                                            physical_block_id=None)\n        assert block.content_hash is not None\n\n        cached_block_id = self._cached_blocks.get(block.content_hash, None)\n        if cached_block_id is not None:\n            block.block_id = cached_block_id\n            self._incr_refcount_cached_block(block)\n            return block\n        self._block_pool.free_block(block)\n\n        # No cached block => Allocate a new block\n        block = self.allocate_mutable_block(prev_block)\n        block.append_token_ids(token_ids)\n        return block\n\n    def allocate_immutable_blocks(\n            self,\n            prev_block: Optional[Block],\n            block_token_ids: List[List[int]],\n            device: Optional[Device] = None) -> List[Block]:\n        blocks = []\n        for token_ids in block_token_ids:\n            prev_block = self.allocate_immutable_block(prev_block=prev_block,\n                                                       token_ids=token_ids,\n                                                       device=device)\n            blocks.append(prev_block)\n        return blocks\n\n    def allocate_mutable_block(self,\n                               prev_block: Optional[Block],\n                               device: Optional[Device] = None) -> Block:\n        \"\"\"Allocates a mutable block. If there are no free blocks, this will\n        evict unused cached blocks.\n\n        Args:\n            prev_block (Block): The previous block in the sequence.\n                None is not allowed unlike it is super class.\n\n        Returns:\n            Block: The allocated mutable block.\n        \"\"\"\n        assert device is None\n        assert_prefix_caching_block_or_none(prev_block)\n\n        block_id = self._allocate_block_id()\n        block = self._block_pool.init_block(prev_block=prev_block,\n                                            token_ids=[],\n                                            block_size=self._block_size,\n                                            physical_block_id=block_id)\n        assert not block.computed\n        assert block.content_hash is None\n        return block\n\n    def _incr_refcount_cached_block(self, block: Block) -> None:\n        # Set this block to be \"computed\" since it is pointing to a\n        # cached block id (which was already computed)\n        block.computed = True\n\n        block_id = block.block_id\n        assert block_id is not None\n\n        refcount = self._refcounter.incr(block_id)\n        if refcount == 1:\n            # In case a cached block was evicted, restore its tracking\n            if block_id in self.evictor:\n                self.evictor.remove(block_id)\n\n            self._track_block_id(block_id, computed=True)\n\n    def _decr_refcount_cached_block(self, block: Block) -> None:\n        # Ensure this is immutable/cached block\n        assert block.content_hash is not None\n\n        block_id = block.block_id\n        assert block_id is not None\n\n        refcount = self._refcounter.decr(block_id)\n        if refcount > 0:\n            block.block_id = None\n            return\n        else:\n            assert refcount == 0\n\n        # No longer used\n        assert block.content_hash in self._cached_blocks\n\n        # Add the cached block to the evictor\n        # (This keeps the cached block around so it can be reused)\n        self.evictor.add(block_id, block.content_hash, block.num_tokens_total,\n                         self._block_tracker[block_id].last_accessed)\n\n        # Stop tracking the block\n        self._untrack_block_id(block_id)\n\n        block.block_id = None\n\n    def _decr_refcount_hashless_block(self, block: Block) -> None:\n        block_id = block.block_id\n        assert block_id is not None\n\n        # We may have a fork case where block is shared,\n        # in which case, we cannot remove it from tracking\n        refcount = self._refcounter.get(block_id)\n        if refcount == 1:\n            self._untrack_block_id(block_id)\n\n        # Decrement refcount of the block_id, but do not free the block object\n        # itself (will be handled by the caller)\n        self._hashless_allocator.free(block, keep_block_object=True)\n\n    def _allocate_block_id(self) -> BlockId:\n        \"\"\"First tries to allocate a block id from the hashless allocator,\n        and if there are no blocks, then tries to evict an unused cached block.\n        \"\"\"\n        hashless_block_id = self._maybe_allocate_hashless_block_id()\n        if hashless_block_id is not None:\n            return hashless_block_id\n\n        evicted_block_id = self._maybe_allocate_evicted_block_id()\n        if evicted_block_id is not None:\n            return evicted_block_id\n\n        # No block available in hashless allocator, nor in unused cache blocks.\n        raise BlockAllocator.NoFreeBlocksError()\n\n    def _maybe_allocate_hashless_block_id(self) -> Optional[BlockId]:\n        try:\n            # Allocate mutable block and extract its block_id\n            block = self._hashless_allocator.allocate_mutable_block(\n                prev_block=None)\n            block_id = block.block_id\n            self._block_pool.free_block(block)\n\n            self._track_block_id(block_id, computed=False)\n            return block_id\n        except BlockAllocator.NoFreeBlocksError:\n            return None\n\n    def _maybe_allocate_evicted_block_id(self) -> Optional[BlockId]:\n        if self.evictor.num_blocks == 0:\n            return None\n\n        # Here we get an evicted block, which is only added\n        # into evictor if its ref counter is 0\n        # and since its content would be changed, we need\n        # to remove it from _cached_blocks's tracking list\n        block_id, content_hash_to_evict = self.evictor.evict()\n\n        # Sanity checks\n        assert content_hash_to_evict in self._cached_blocks\n        _block_id = self._cached_blocks[content_hash_to_evict]\n        assert self._refcounter.get(_block_id) == 0\n        assert _block_id == block_id\n\n        self._cached_blocks.pop(content_hash_to_evict)\n\n        self._refcounter.incr(block_id)\n        self._track_block_id(block_id, computed=False)\n\n        return block_id\n\n    def _free_block_id(self, block: Block) -> None:\n        \"\"\"Decrements the refcount of the block. The block may be in two \n        possible states: (1) immutable/cached or (2) mutable/hashless. \n        In the first case, the refcount is decremented directly and the block\n        may be possibly added to the evictor. In other case, hashless \n        allocator free(..) with keep_block_object=True is called to only free\n        the block id (since the block object may be reused by the caller)\n        \"\"\"\n        block_id = block.block_id\n        assert block_id is not None, \"Freeing unallocated block is undefined\"\n\n        if block.content_hash is not None:\n            # Immutable: This type of block is always cached, and we want to\n            # keep it in the evictor for future reuse\n            self._decr_refcount_cached_block(block)\n        else:\n            # Mutable: This type of block is not cached, so we release it\n            # directly to the hashless allocator\n            self._decr_refcount_hashless_block(block)\n\n        assert block.block_id is None\n\n    def free(self, block: Block, keep_block_object: bool = False) -> None:\n        \"\"\"Release the block (look at free_block_id(..) docs)\n        \"\"\"\n        # Release the physical block index\n        self._free_block_id(block)\n\n        # Release the block object to the pool\n        if not keep_block_object:\n            self._block_pool.free_block(block)\n\n    def fork(self, last_block: Block) -> List[Block]:\n        \"\"\"Creates a new sequence of blocks that shares the same underlying\n        memory as the original sequence.\n\n        Args:\n            last_block (Block): The last block in the original sequence.\n\n        Returns:\n            List[Block]: The new sequence of blocks that shares the same memory\n                as the original sequence.\n        \"\"\"\n        source_blocks = get_all_blocks_recursively(last_block)\n\n        forked_blocks: List[Block] = []\n        prev_block = None\n        for block in source_blocks:\n            block_id = block.block_id\n            assert block_id is not None\n\n            refcount = self._refcounter.incr(block_id)\n            assert refcount != 1, \"can't fork free'd block_id = {}\".format(\n                block_id)\n\n            forked_block = self._block_pool.init_block(\n                prev_block=prev_block,\n                token_ids=block.token_ids,\n                block_size=self._block_size,\n                physical_block_id=block_id)\n\n            forked_blocks.append(forked_block)\n            prev_block = forked_blocks[-1]\n\n        return forked_blocks\n\n    def get_num_free_blocks(self, device: Optional[Device] = None) -> int:\n        assert device is None\n        # The number of free blocks is the number of hashless free blocks\n        # plus the number of blocks evictor could free from its list.\n        return self._hashless_allocator.get_num_free_blocks(\n        ) + self.evictor.num_blocks\n\n    def get_num_total_blocks(self) -> int:\n        return self._hashless_allocator.get_num_total_blocks()\n\n    def get_physical_block_id(self, absolute_id: int) -> int:\n        \"\"\"Returns the zero-offset block id on certain block allocator\n        given the absolute block id.\n\n        Args:\n            absolute_id (int): The absolute block id for the block \n                in whole allocator.\n\n        Returns:\n            int: The rzero-offset block id on certain device.\n        \"\"\"\n        return sorted(self.all_block_ids).index(absolute_id)\n\n    @property\n    def all_block_ids(self) -> FrozenSet[int]:\n        return self._hashless_allocator.all_block_ids\n\n    def is_block_cached(self, block: Block) -> bool:\n        assert block.content_hash is not None\n        if block.content_hash in self._cached_blocks:\n            return True\n        return False\n\n    def promote_to_immutable_block(self, block: Block) -> BlockId:\n        \"\"\"Once a mutable block is full, it can be promoted to an immutable\n        block. This means that its content can be referenced by future blocks\n        having the same prefix.\n\n        Note that if we already have a cached block with the same content, we\n        will replace the newly-promoted block's mapping with the existing cached\n        block id.\n\n        Args:\n            block: The mutable block to be promoted.\n\n        Returns:\n            BlockId: Either the original block index, or the block index of\n                the previously cached block matching the same content.\n        \"\"\"\n        # Ensure block can be promoted\n        assert block.content_hash is not None\n        assert block.block_id is not None\n        assert self._refcounter.get(block.block_id) > 0\n\n        if block.content_hash not in self._cached_blocks:\n            # No cached content hash => Set this block as cached\n            # (Note that this block is not computed yet =>\n            #  Will be computed after free())\n            self._cached_blocks[block.content_hash] = block.block_id\n            return block.block_id\n\n        # Reuse the cached content hash\n        self._decr_refcount_hashless_block(block)\n        block.block_id = self._cached_blocks[block.content_hash]\n\n        # Increment refcount of the cached block and (possibly) restore\n        # it from the evictor.\n        # Note that in this case, the block is marked as computed\n        self._incr_refcount_cached_block(block)\n\n        return block.block_id\n\n    def cow_block_if_not_appendable(self, block: Block) -> BlockId:\n        \"\"\"Performs a copy-on-write operation on the given block if it is not\n        appendable.\n\n        Args:\n            block (Block): The block to check for copy-on-write.\n\n        Returns:\n            BlockId: The block index of the new block if a copy-on-write \n                operation was performed, or the original block index if\n                no copy-on-write was necessary.\n        \"\"\"\n        src_block_id = block.block_id\n        assert src_block_id is not None\n\n        if self._cow_tracker.is_appendable(block):\n            return src_block_id\n\n        self._free_block_id(block)\n        trg_block_id = self._allocate_block_id()\n\n        self._cow_tracker.record_cow(src_block_id, trg_block_id)\n\n        return trg_block_id\n\n    def clear_copy_on_writes(self) -> List[Tuple[BlockId, BlockId]]:\n        \"\"\"Returns the copy-on-write source->destination mapping and clears it.\n\n        Returns:\n            List[Tuple[BlockId, BlockId]]: A list mapping source\n                block indices to destination block indices.\n        \"\"\"\n        return self._cow_tracker.clear_cows()\n\n    def mark_blocks_as_accessed(self, block_ids: List[int],\n                                now: float) -> None:\n        \"\"\"Mark blocks as accessed, used in prefix caching.\n\n        If the block is added into evictor, we need to update corresponding\n        info in evictor's metadata.\n        \"\"\"\n\n        for block_id in block_ids:\n            if self._block_tracker[block_id].active:\n                self._block_tracker[block_id].last_accessed = now\n            elif block_id in self.evictor:\n                self.evictor.update(block_id, now)\n            else:\n                raise ValueError(\n                    \"Mark block as accessed which is not belonged to GPU\")\n\n    def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n        raise NotImplementedError(\"Marking as computed is incremental\")\n\n    def _track_block_id(self, block_id: Optional[BlockId],\n                        computed: bool) -> None:\n        assert block_id is not None\n        self._block_tracker[block_id].enable()\n        self._block_tracker[block_id].computed = computed\n\n    def _untrack_block_id(self, block_id: Optional[BlockId]) -> None:\n        assert block_id is not None\n        self._block_tracker[block_id].disable()\n\n    def block_is_computed(self, block_id: int) -> bool:\n        if self._block_tracker[block_id].active:\n            return self._block_tracker[block_id].computed\n        else:\n            return block_id in self.evictor\n\n    def get_computed_block_ids(self,\n                               prev_computed_block_ids: List[int],\n                               block_ids: List[int],\n                               skip_last_block_id: bool = True) -> List[int]:\n        prev_prefix_size = len(prev_computed_block_ids)\n        cur_size = len(block_ids)\n        if skip_last_block_id:\n            cur_size -= 1\n\n        # Sanity checks\n        assert cur_size >= 0\n        assert prev_prefix_size <= cur_size\n\n        ret = prev_computed_block_ids\n        for i in range(prev_prefix_size, cur_size):\n            block_id = block_ids[i]\n            if self.block_is_computed(block_id):\n                ret.append(block_id)\n        return ret\n\n    def get_common_computed_block_ids(\n            self, computed_seq_block_ids: List[List[int]]) -> List[int]:\n        \"\"\"Return the block ids that are common for a given sequence group.\n\n        Only those blocks that are immutable and already be marked\n        compyted would be taken consideration.\n        \"\"\"\n\n        # NOTE We exclude the last block to avoid the case where the entire\n        # prompt is cached. This would cause erroneous behavior in model\n        # runner.\n\n        # It returns a list of int although type annotation says list of string.\n        if len(computed_seq_block_ids) == 1:\n            return computed_seq_block_ids[0]\n\n        return commonprefix([\n            ids for ids in computed_seq_block_ids  # type: ignore\n            if ids\n        ])\n\n    def get_num_blocks_touched(self,\n                               blocks: List[Block],\n                               num_lookahead_slots: int = 0) -> int:\n        \"\"\"Determine the number of blocks that will be touched by\n        swapping in/out the given blocks from certain sequence\n        group with the provided num_lookahead_slots.\n\n        Args:\n            blocks (List[Block]): The potential blocks to swap.\n            num_lookahead_slots (int): number of lookahead slots (0 for \n                swap out).\n        \n        Returns:\n            int: the number of blocks that will be touched by\n                swapping in/out the given blocks and num_lookahead_slots.\n        \"\"\"\n        num_touched_blocks = 0\n        for block in blocks:\n            if not block.is_full:\n                num_touched_blocks += 1\n                if num_lookahead_slots > block.num_empty_slots:\n                    num_touched_blocks += cdiv(\n                        num_lookahead_slots - block.num_empty_slots,\n                        self._block_size)\n            else:\n                # If the block has a match in the cache and the cached block\n                # is not referenced, then we still count it as a touched block\n                if not self.is_block_cached(block) or \\\n                    (block.content_hash is not None and \\\n                     self._cached_blocks[block.content_hash] in self.evictor):\n                    num_touched_blocks += 1\n        return num_touched_blocks\n\n    def swap_out(self, blocks: List[Block]) -> None:\n        \"\"\"Execute the swap out actions. Basically just free the \n        given blocks.\n\n        Args:\n            blocks: List of blocks to be swapped out.\n        \"\"\"\n        for block in blocks:\n            self._free_block_id(block)\n\n    def swap_in(self, blocks: List[Block]) -> None:\n        \"\"\"Execute the swap in actions. Change the block id from \n        old allocator to current allocator for each block to finish \n        the block table update. \n\n        Args:\n            blocks: List of blocks to be swapped in.\n        \"\"\"\n        for block in blocks:\n            # Here we allocate either immutable or mutable block and then\n            # extract its block_id. Note that the block object is released\n            # and the block_id is assigned to \"block\" to allow reusing the\n            # existing \"block\" object\n            if block.is_full:\n                tmp_block = self.allocate_immutable_block(\n                    prev_block=block.prev_block, token_ids=block.token_ids)\n            else:\n                tmp_block = self.allocate_mutable_block(\n                    prev_block=block.prev_block)\n                tmp_block.append\n```\n# Function Signature: def append_token_ids(self, token_ids: List[int]) -> None:\n# Natural Language Annotations: \"\"\"Appends the given token IDs to the block and registers the block as\n        immutable if the block becomes full.\n\n        Args:\n            token_ids (List[int]): The token IDs to be appended to the block.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef append_token_ids(self, token_ids:", "  ```\ndef append_token_ids(self, token_ids:", "  ```\ndef append_token_ids(self, token_ids:"]}, "vllm-vllm/core/block/block_table.py-allocate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/block/block_table.py:\n```\nimport math\nfrom typing import List, Optional\n\nfrom vllm.core.block.common import BlockList\nfrom vllm.core.block.interfaces import Block, DeviceAwareBlockAllocator\nfrom vllm.utils import Device, cdiv, chunk_list\n\n\nclass BlockTable:\n    \"\"\"A class to manage blocks for a specific sequence.\n\n    The BlockTable maps a sequence of tokens to a list of blocks, where each\n    block represents a contiguous memory allocation for a portion of the \n    sequence. The blocks are managed by a DeviceAwareBlockAllocator, which is\n    responsible for allocating and freeing memory for the blocks.\n\n    Args:\n        block_size (int): The maximum number of tokens that can be stored in a\n            single block.\n        block_allocator (DeviceAwareBlockAllocator): The block allocator used to\n            manage memory for the blocks.\n        _blocks (Optional[List[Block]], optional): An optional list of existing\n            blocks to initialize the BlockTable with. If not provided, an empty\n            BlockTable is created.\n        max_block_sliding_window (Optional[int], optional): The number of\n            blocks to keep around for each sequance. If None, all blocks\n            are kept (eg., when sliding window is not used).\n            It should at least fit the sliding window size of the model.\n\n    Attributes:\n        _block_size (int): The maximum number of tokens that can be stored in a\n            single block.\n        _allocator (DeviceAwareBlockAllocator): The block allocator used to\n            manage memory for the blocks.\n        _blocks (Optional[List[Block]]): The list of blocks managed by this\n            BlockTable.\n        _num_full_slots (int): The number of tokens currently stored in the\n            blocks.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        block_allocator: DeviceAwareBlockAllocator,\n        _blocks: Optional[List[Block]] = None,\n        max_block_sliding_window: Optional[int] = None,\n    ):\n        self._block_size = block_size\n        self._allocator = block_allocator\n        if _blocks is None:\n            _blocks = []\n        self._blocks: BlockList = BlockList(_blocks)\n\n        self._max_block_sliding_window = max_block_sliding_window\n        self._num_full_slots = self._get_num_token_ids()\n\n    @staticmethod\n    def get_num_required_blocks(token_ids: List[int], block_size: int) -> int:\n        \"\"\"Calculates the minimum number of blocks required to store a given\n        sequence of token IDs.\n\n        This assumes worst-case scenario, where every block requires a new\n        allocation (e.g. ignoring prefix caching).\n\n        Args:\n            token_ids (List[int]): The sequence of token IDs to be stored.\n            block_size (int): The maximum number of tokens that can be stored in\n                a single block.\n\n        Returns:\n            int: The minimum number of blocks required to store the given\n                sequence of token IDs.\n        \"\"\"\n        return cdiv(len(token_ids), block_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def update(self, blocks: List[Block]) -> None:\n        \"\"\"Resets the table to the newly provided blocks \n        (with their corresponding block ids)\n        \"\"\"\n        self._blocks.update(blocks)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def ensure_num_empty_slots(self, num_empty_slots: int) -> None:\n        \"\"\"Ensures that the BlockTable has at least the specified number of\n        empty slots available.\n\n        This method checks if the BlockTable has enough empty slots (i.e.,\n        available space) to accommodate the requested number of tokens. If not,\n        it allocates additional blocks on the GPU to ensure that the required\n        number of empty slots is available.\n\n        Args:\n            num_empty_slots (int): The minimum number of empty slots required.\n        \"\"\"\n        # Currently the block table only supports\n        # appending tokens to GPU blocks.\n        device = Device.GPU\n        assert self._is_allocated\n\n        if self._num_empty_slots >= num_empty_slots:\n            return\n\n        slots_to_allocate = num_empty_slots - self._num_empty_slots\n        blocks_to_allocate = cdiv(slots_to_allocate, self._block_size)\n\n        for _ in range(blocks_to_allocate):\n            assert len(self._blocks) > 0\n            self._blocks.append(\n                self._allocator.allocate_mutable_block(\n                    prev_block=self._blocks[-1], device=device))\n\n    def fork(self) -> \"BlockTable\":\n        \"\"\"Creates a new BlockTable instance with a copy of the blocks from the\n        current instance.\n\n        This method creates a new BlockTable instance with the same block size,\n        block allocator, and a copy of the blocks from the current instance. The\n        new BlockTable has its own independent set of blocks, but shares the\n        same underlying memory allocation with the original BlockTable.\n\n        Returns:\n            BlockTable: A new BlockTable instance with a copy of the blocks from\n                the current instance.\n        \"\"\"\n        assert self._is_allocated\n        assert len(self._blocks) > 0\n        forked_blocks = self._allocator.fork(self._blocks[-1])\n        return BlockTable(\n            block_size=self._block_size,\n            block_allocator=self._allocator,\n            _blocks=forked_blocks,\n            max_block_sliding_window=self._max_block_sliding_window,\n        )\n\n    def free(self) -> None:\n        \"\"\"Frees the memory occupied by the blocks in the BlockTable.\n\n        This method iterates over all the blocks in the `_blocks` list and calls\n        the `free` method of the `_allocator` object to release the memory\n        occupied by each block. After freeing all the blocks, the `_blocks` list\n        is set to `None`.\n        \"\"\"\n        assert self._is_allocated\n        for block in self.blocks:\n            self._allocator.free(block)\n        self._blocks.reset()\n\n    @property\n    def physical_block_ids(self) -> List[int]:\n        \"\"\"Returns a list of physical block indices for the blocks in the\n        BlockTable.\n\n        This property returns a list of integers, where each integer represents\n        the physical block index of a corresponding block in the `_blocks` list.\n        The physical block index is a unique identifier for the memory location\n        occupied by the block.\n\n        Returns:\n            List[int]: A list of physical block indices for the blocks in the\n                BlockTable.\n        \"\"\"\n        assert self._is_allocated\n        return self._blocks.ids()\n\n    def get_unseen_token_ids(self, sequence_token_ids: List[int]) -> List[int]:\n        \"\"\"Get the number of \"unseen\" tokens in the sequence.\n\n        Unseen tokens are tokens in the sequence corresponding to this block\n        table, but are not yet appended to this block table.\n\n        Args:\n            sequence_token_ids (List[int]): The list of token ids in the\n                sequence.\n\n        Returns:\n            List[int]: The postfix of sequence_token_ids that has not yet been\n                appended to the block table.\n        \"\"\"\n\n        # Since the block table is append-only, the unseen token ids are the\n        # ones after the appended ones.\n        return sequence_token_ids[self.num_full_slots:]\n\n    def _allocate_blocks_for_token_ids(self, prev_block: Optional[Block],\n                                       token_ids: List[int],\n                                       device: Device) -> List[Block]:\n        blocks: List[Block] = []\n\n        block_token_ids = []\n        tail_token_ids = []\n        for cur_token_ids in chunk_list(token_ids, self._block_size):\n            if len(cur_token_ids) == self._block_size:\n                block_token_ids.append(cur_token_ids)\n            else:\n                tail_token_ids.append(cur_token_ids)\n\n        if block_token_ids:\n            blocks.extend(\n                self._allocator.allocate_immutable_blocks(\n                    prev_block, block_token_ids=block_token_ids,\n                    device=device))\n            prev_block = blocks[-1]\n\n        if tail_token_ids:\n            assert len(tail_token_ids) == 1\n            cur_token_ids = tail_token_ids[0]\n\n            block = self._allocator.allocate_mutable_block(\n                prev_block=prev_block, device=device)\n            block.append_token_ids(cur_token_ids)\n\n            blocks.append(block)\n\n        return blocks\n\n    def _get_all_token_ids(self) -> List[int]:\n        # NOTE: This function is O(seq_len); use sparingly.\n        token_ids: List[int] = []\n\n        if not self._is_allocated:\n            return token_ids\n\n        for block in self.blocks:\n            token_ids.extend(block.token_ids)\n\n        return token_ids\n\n    def _get_num_token_ids(self) -> int:\n        res = 0\n        for block in self.blocks:\n            res += len(block.token_ids)\n\n        return res\n\n    @property\n    def _is_allocated(self) -> bool:\n        return len(self._blocks) > 0\n\n    @property\n    def blocks(self) -> List[Block]:\n        return self._blocks.list()\n\n    @property\n    def _num_empty_slots(self) -> int:\n        assert self._is_allocated\n        return len(self._blocks) * self._block_size - self._num_full_slots\n\n    @property\n    def num_full_slots(self) -> int:\n        \"\"\"Returns the total number of tokens currently stored in the\n        BlockTable.\n\n        Returns:\n            int: The total number of tokens currently stored in the BlockTable.\n        \"\"\"\n        return self._num_full_slots\n\n    def get_num_blocks_touched_by_append_slots(\n            self, token_ids: List[int], num_lookahead_slots: int) -> int:\n        \"\"\"Determine how many blocks will be \"touched\" by appending the token\n        ids.\n\n        This is required for the scheduler to determine whether a sequence can\n        continue generation, or if it must be preempted.\n        \"\"\"\n        # Math below is equivalent to:\n        # all_token_ids = token_ids + [-1] * num_lookahead_slots\n        # token_blocks = self._chunk_token_blocks_for_append(all_token_ids)\n        # return len(token_blocks)\n\n        num_token_ids = len(token_ids) + num_lookahead_slots\n        first_chunk_size = self._block_size - (self._num_full_slots %\n                                               self._block_size)\n        num_token_blocks = (1 + math.ceil(\n            (num_token_ids - first_chunk_size) / self._block_size))\n        return num_token_blocks\n\n    def _chunk_token_blocks_for_append(\n            self, token_ids: List[int]) -> List[List[int]]:\n        \"\"\"Split the token ids into block-sized chunks so they can be easily\n        appended to blocks. The first such \"token block\" may have less token ids\n        than the block size, since the last allocated block may be partially\n        full.\n\n        If no token ids are provided, then no chunks are returned.\n        \"\"\"\n\n        if not token_ids:\n            return []\n\n        first_chunk_size = self._block_size - (self._num_full_slots %\n                                               self._block_size)\n        token_blocks = [token_ids[:first_chunk_size]]\n        token_blocks.extend(\n            chunk_list(token_ids[first_chunk_size:], self._block_size))\n        return token_blocks\n\n```\n# Function Signature: def allocate(self,\n# Natural Language Annotations: \"\"\"Allocates memory blocks for storing the given sequence of token IDs.\n\n        This method allocates the required number of blocks to store the given\n        sequence of token IDs.\n\n        Args:\n            token_ids (List[int]): The sequence of token IDs to be stored.\n            device (Device, optional): The device on which the blocks should be\n                allocated. Defaults to Device.GPU.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef allocate(self,\n             token_ids: List[", "  ```\ndef allocate(self, token_ids: List[int],", "  ```\ndef allocate(self, token_ids: List[int],"]}, "vllm-vllm/core/block/block_table.py-append_token_ids": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/core/block/block_table.py:\n```\nimport math\nfrom typing import List, Optional\n\nfrom vllm.core.block.common import BlockList\nfrom vllm.core.block.interfaces import Block, DeviceAwareBlockAllocator\nfrom vllm.utils import Device, cdiv, chunk_list\n\n\nclass BlockTable:\n    \"\"\"A class to manage blocks for a specific sequence.\n\n    The BlockTable maps a sequence of tokens to a list of blocks, where each\n    block represents a contiguous memory allocation for a portion of the \n    sequence. The blocks are managed by a DeviceAwareBlockAllocator, which is\n    responsible for allocating and freeing memory for the blocks.\n\n    Args:\n        block_size (int): The maximum number of tokens that can be stored in a\n            single block.\n        block_allocator (DeviceAwareBlockAllocator): The block allocator used to\n            manage memory for the blocks.\n        _blocks (Optional[List[Block]], optional): An optional list of existing\n            blocks to initialize the BlockTable with. If not provided, an empty\n            BlockTable is created.\n        max_block_sliding_window (Optional[int], optional): The number of\n            blocks to keep around for each sequance. If None, all blocks\n            are kept (eg., when sliding window is not used).\n            It should at least fit the sliding window size of the model.\n\n    Attributes:\n        _block_size (int): The maximum number of tokens that can be stored in a\n            single block.\n        _allocator (DeviceAwareBlockAllocator): The block allocator used to\n            manage memory for the blocks.\n        _blocks (Optional[List[Block]]): The list of blocks managed by this\n            BlockTable.\n        _num_full_slots (int): The number of tokens currently stored in the\n            blocks.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        block_allocator: DeviceAwareBlockAllocator,\n        _blocks: Optional[List[Block]] = None,\n        max_block_sliding_window: Optional[int] = None,\n    ):\n        self._block_size = block_size\n        self._allocator = block_allocator\n        if _blocks is None:\n            _blocks = []\n        self._blocks: BlockList = BlockList(_blocks)\n\n        self._max_block_sliding_window = max_block_sliding_window\n        self._num_full_slots = self._get_num_token_ids()\n\n    @staticmethod\n    def get_num_required_blocks(token_ids: List[int], block_size: int) -> int:\n        \"\"\"Calculates the minimum number of blocks required to store a given\n        sequence of token IDs.\n\n        This assumes worst-case scenario, where every block requires a new\n        allocation (e.g. ignoring prefix caching).\n\n        Args:\n            token_ids (List[int]): The sequence of token IDs to be stored.\n            block_size (int): The maximum number of tokens that can be stored in\n                a single block.\n\n        Returns:\n            int: The minimum number of blocks required to store the given\n                sequence of token IDs.\n        \"\"\"\n        return cdiv(len(token_ids), block_size)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def update(self, blocks: List[Block]) -> None:\n        \"\"\"Resets the table to the newly provided blocks \n        (with their corresponding block ids)\n        \"\"\"\n        self._blocks.update(blocks)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def ensure_num_empty_slots(self, num_empty_slots: int) -> None:\n        \"\"\"Ensures that the BlockTable has at least the specified number of\n        empty slots available.\n\n        This method checks if the BlockTable has enough empty slots (i.e.,\n        available space) to accommodate the requested number of tokens. If not,\n        it allocates additional blocks on the GPU to ensure that the required\n        number of empty slots is available.\n\n        Args:\n            num_empty_slots (int): The minimum number of empty slots required.\n        \"\"\"\n        # Currently the block table only supports\n        # appending tokens to GPU blocks.\n        device = Device.GPU\n        assert self._is_allocated\n\n        if self._num_empty_slots >= num_empty_slots:\n            return\n\n        slots_to_allocate = num_empty_slots - self._num_empty_slots\n        blocks_to_allocate = cdiv(slots_to_allocate, self._block_size)\n\n        for _ in range(blocks_to_allocate):\n            assert len(self._blocks) > 0\n            self._blocks.append(\n                self._allocator.allocate_mutable_block(\n                    prev_block=self._blocks[-1], device=device))\n\n    def fork(self) -> \"BlockTable\":\n        \"\"\"Creates a new BlockTable instance with a copy of the blocks from the\n        current instance.\n\n        This method creates a new BlockTable instance with the same block size,\n        block allocator, and a copy of the blocks from the current instance. The\n        new BlockTable has its own independent set of blocks, but shares the\n        same underlying memory allocation with the original BlockTable.\n\n        Returns:\n            BlockTable: A new BlockTable instance with a copy of the blocks from\n                the current instance.\n        \"\"\"\n        assert self._is_allocated\n        assert len(self._blocks) > 0\n        forked_blocks = self._allocator.fork(self._blocks[-1])\n        return BlockTable(\n            block_size=self._block_size,\n            block_allocator=self._allocator,\n            _blocks=forked_blocks,\n            max_block_sliding_window=self._max_block_sliding_window,\n        )\n\n    def free(self) -> None:\n        \"\"\"Frees the memory occupied by the blocks in the BlockTable.\n\n        This method iterates over all the blocks in the `_blocks` list and calls\n        the `free` method of the `_allocator` object to release the memory\n        occupied by each block. After freeing all the blocks, the `_blocks` list\n        is set to `None`.\n        \"\"\"\n        assert self._is_allocated\n        for block in self.blocks:\n            self._allocator.free(block)\n        self._blocks.reset()\n\n    @property\n    def physical_block_ids(self) -> List[int]:\n        \"\"\"Returns a list of physical block indices for the blocks in the\n        BlockTable.\n\n        This property returns a list of integers, where each integer represents\n        the physical block index of a corresponding block in the `_blocks` list.\n        The physical block index is a unique identifier for the memory location\n        occupied by the block.\n\n        Returns:\n            List[int]: A list of physical block indices for the blocks in the\n                BlockTable.\n        \"\"\"\n        assert self._is_allocated\n        return self._blocks.ids()\n\n    def get_unseen_token_ids(self, sequence_token_ids: List[int]) -> List[int]:\n        \"\"\"Get the number of \"unseen\" tokens in the sequence.\n\n        Unseen tokens are tokens in the sequence corresponding to this block\n        table, but are not yet appended to this block table.\n\n        Args:\n            sequence_token_ids (List[int]): The list of token ids in the\n                sequence.\n\n        Returns:\n            List[int]: The postfix of sequence_token_ids that has not yet been\n                appended to the block table.\n        \"\"\"\n\n        # Since the block table is append-only, the unseen token ids are the\n        # ones after the appended ones.\n        return sequence_token_ids[self.num_full_slots:]\n\n    def _allocate_blocks_for_token_ids(self, prev_block: Optional[Block],\n                                       token_ids: List[int],\n                                       device: Device) -> List[Block]:\n        blocks: List[Block] = []\n\n        block_token_ids = []\n        tail_token_ids = []\n        for cur_token_ids in chunk_list(token_ids, self._block_size):\n            if len(cur_token_ids) == self._block_size:\n                block_token_ids.append(cur_token_ids)\n            else:\n                tail_token_ids.append(cur_token_ids)\n\n        if block_token_ids:\n            blocks.extend(\n                self._allocator.allocate_immutable_blocks(\n                    prev_block, block_token_ids=block_token_ids,\n                    device=device))\n            prev_block = blocks[-1]\n\n        if tail_token_ids:\n            assert len(tail_token_ids) == 1\n            cur_token_ids = tail_token_ids[0]\n\n            block = self._allocator.allocate_mutable_block(\n                prev_block=prev_block, device=device)\n            block.append_token_ids(cur_token_ids)\n\n            blocks.append(block)\n\n        return blocks\n\n    def _get_all_token_ids(self) -> List[int]:\n        # NOTE: This function is O(seq_len); use sparingly.\n        token_ids: List[int] = []\n\n        if not self._is_allocated:\n            return token_ids\n\n        for block in self.blocks:\n            token_ids.extend(block.token_ids)\n\n        return token_ids\n\n    def _get_num_token_ids(self) -> int:\n        res = 0\n        for block in self.blocks:\n            res += len(block.token_ids)\n\n        return res\n\n    @property\n    def _is_allocated(self) -> bool:\n        return len(self._blocks) > 0\n\n    @property\n    def blocks(self) -> List[Block]:\n        return self._blocks.list()\n\n    @property\n    def _num_empty_slots(self) -> int:\n        assert self._is_allocated\n        return len(self._blocks) * self._block_size - self._num_full_slots\n\n    @property\n    def num_full_slots(self) -> int:\n        \"\"\"Returns the total number of tokens currently stored in the\n        BlockTable.\n\n        Returns:\n            int: The total number of tokens currently stored in the BlockTable.\n        \"\"\"\n        return self._num_full_slots\n\n    def get_num_blocks_touched_by_append_slots(\n            self, token_ids: List[int], num_lookahead_slots: int) -> int:\n        \"\"\"Determine how many blocks will be \"touched\" by appending the token\n        ids.\n\n        This is required for the scheduler to determine whether a sequence can\n        continue generation, or if it must be preempted.\n        \"\"\"\n        # Math below is equivalent to:\n        # all_token_ids = token_ids + [-1] * num_lookahead_slots\n        # token_blocks = self._chunk_token_blocks_for_append(all_token_ids)\n        # return len(token_blocks)\n\n        num_token_ids = len(token_ids) + num_lookahead_slots\n        first_chunk_size = self._block_size - (self._num_full_slots %\n                                               self._block_size)\n        num_token_blocks = (1 + math.ceil(\n            (num_token_ids - first_chunk_size) / self._block_size))\n        return num_token_blocks\n\n    def _chunk_token_blocks_for_append(\n            self, token_ids: List[int]) -> List[List[int]]:\n        \"\"\"Split the token ids into block-sized chunks so they can be easily\n        appended to blocks. The first such \"token block\" may have less token ids\n        than the block size, since the last allocated block may be partially\n        full.\n\n        If no token ids are provided, then no chunks are returned.\n        \"\"\"\n\n        if not token_ids:\n            return []\n\n        first_chunk_size = self._block_size - (self._num_full_slots %\n                                               self._block_size)\n        token_blocks = [token_ids[:first_chunk_size]]\n        token_blocks.extend(\n            chunk_list(token_ids[first_chunk_size:], self._block_size))\n        return token_blocks\n\n```\n# Function Signature: def append_token_ids(self,\n# Natural Language Annotations: \"\"\"Appends a sequence of token IDs to the existing blocks in the\n        BlockTable.\n\n        This method appends the given sequence of token IDs to the existing\n        blocks in the BlockTable. If there is not enough space in the existing\n        blocks, new blocks are allocated using the `ensure_num_empty_slots`\n        method to accommodate the additional tokens.\n\n        The token IDs are divided into chunks of size `block_size` (except for\n        the first chunk, which may be smaller), and each chunk is appended to a\n        separate block.\n\n        Args:\n            token_ids (List[int]): The sequence of token IDs to be appended.\n            num_computed_slots (Optional[int]): The number of KV cache slots\n                that are already filled (computed).\n                When sliding window is enabled, this is used to compute how many\n                blocks to drop at the front of the sequence.\n                Without sliding window, None can be passed.\n                Without chunked prefill, it should be the same as\n                _num_full_slots.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef append_token_ids(token_ids: List[", "  ```\ndef append_token_ids(self, token_ids:", "  ```\ndef append_token_ids(self, token_ids:"]}, "vllm-vllm/engine/llm_engine.py-from_engine_args": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/llm_engine.py:\n```\nimport time\nfrom contextlib import contextmanager\nfrom typing import (TYPE_CHECKING, Any, ClassVar, Dict, Iterable, List,\n                    Mapping, Optional)\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Tuple, Type, TypeVar, Union\n\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (CacheConfig, DecodingConfig, DeviceConfig,\n                         EngineConfig, LoadConfig, LoRAConfig, ModelConfig,\n                         ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig,\n                         SpeculativeConfig)\nfrom vllm.core.scheduler import (ScheduledSequenceGroup, Scheduler,\n                                 SchedulerOutputs)\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.metrics_types import StatLoggerBase, Stats\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.engine.output_processor.util import create_output_by_sequence_group\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster\nfrom vllm.inputs import (INPUT_REGISTRY, EncoderDecoderLLMInputs,\n                         InputRegistry, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import MultiModalDataDict\nfrom vllm.outputs import (EmbeddingRequestOutput, RequestOutput,\n                          RequestOutputFactory)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (EmbeddingSequenceGroupOutput, ExecuteModelRequest,\n                           PoolerOutput, SamplerOutput, Sequence,\n                           SequenceGroup, SequenceGroupMetadata,\n                           SequenceStatus)\nfrom vllm.tracing import (SpanAttributes, SpanKind, extract_trace_context,\n                          init_tracer)\nfrom vllm.transformers_utils.config import try_get_generation_config\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer_group import (\n    AnyTokenizer, BaseTokenizerGroup, init_tokenizer_from_configs)\nfrom vllm.usage.usage_lib import (UsageContext, is_usage_stats_enabled,\n                                  usage_message)\nfrom vllm.utils import Counter\nfrom vllm.version import __version__ as VLLM_VERSION\n\nlogger = init_logger(__name__)\n_LOCAL_LOGGING_INTERVAL_SEC = 5\n\n\ndef _load_generation_config_dict(model_config: ModelConfig) -> Dict[str, Any]:\n    config = try_get_generation_config(\n        model_config.model,\n        trust_remote_code=model_config.trust_remote_code,\n        revision=model_config.revision,\n    )\n\n    if config is None:\n        return {}\n\n    return config.to_diff_dict()\n\n\n_O = TypeVar(\"_O\", RequestOutput, EmbeddingRequestOutput)\n\nPromptComponents = Tuple[Optional[str], List[int],\n                         Optional[MultiModalDataDict]]\nDecoderPromptComponents = Tuple[Optional[str], Optional[List[int]],\n                                Optional[MultiModalDataDict]]\n\n\nclass LLMEngine:\n    \"\"\"An LLM engine that receives requests and generates texts.\n\n    This is the main class for the vLLM engine. It receives requests\n    from clients and generates texts from the LLM. It includes a tokenizer, a\n    language model (possibly distributed across multiple GPUs), and GPU memory\n    space allocated for intermediate states (aka KV cache). This class utilizes\n    iteration-level scheduling and efficient memory management to maximize the\n    serving throughput.\n\n    The :class:`~vllm.LLM` class wraps this class for offline batched inference\n    and the :class:`AsyncLLMEngine` class wraps this class for online serving.\n\n    The config arguments are derived from :class:`~vllm.EngineArgs`. (See\n    :ref:`engine_args`)\n\n    Args:\n        model_config: The configuration related to the LLM model.\n        cache_config: The configuration related to the KV cache memory\n            management.\n        parallel_config: The configuration related to distributed execution.\n        scheduler_config: The configuration related to the request scheduler.\n        device_config: The configuration related to the device.\n        lora_config (Optional): The configuration related to serving multi-LoRA.\n        speculative_config (Optional): The configuration related to speculative\n            decoding.\n        executor_class: The model executor class for managing distributed\n            execution.\n        prompt_adapter_config (Optional): The configuration related to serving \n            prompt adapters.\n        log_stats: Whether to log statistics.\n        usage_context: Specified entry point, used for usage info collection.\n    \"\"\"\n\n    DO_VALIDATE_OUTPUT: ClassVar[bool] = False\n    \"\"\"A flag to toggle whether to validate the type of request output.\"\"\"\n\n    @classmethod\n    @contextmanager\n    def enable_output_validation(cls):\n        cls.DO_VALIDATE_OUTPUT = True\n\n        yield\n\n        cls.DO_VALIDATE_OUTPUT = False\n\n    @classmethod\n    def validate_output(\n        cls,\n        output: object,\n        output_type: Type[_O],\n    ) -> _O:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        if ((TYPE_CHECKING or do_validate)\n                and not isinstance(output, output_type)):\n            raise TypeError(f\"Expected output of type {output_type}, \"\n                            f\"but found type {type(output)}\")\n\n        return output\n\n    @classmethod\n    def validate_outputs(\n        cls,\n        outputs: GenericSequence[object],\n        output_type: Type[_O],\n    ) -> List[_O]:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        outputs_: List[_O]\n        if TYPE_CHECKING or do_validate:\n            outputs_ = []\n            for output in outputs:\n                if not isinstance(output, output_type):\n                    raise TypeError(f\"Expected output of type {output_type}, \"\n                                    f\"but found type {type(output)}\")\n\n                outputs_.append(output)\n        else:\n            outputs_ = outputs\n\n        return outputs_\n\n    tokenizer: Optional[BaseTokenizerGroup]\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        cache_config: CacheConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        speculative_config: Optional[SpeculativeConfig],\n        decoding_config: Optional[DecodingConfig],\n        observability_config: Optional[ObservabilityConfig],\n        prompt_adapter_config: Optional[PromptAdapterConfig],\n        executor_class: Type[ExecutorBase],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n    ) -> None:\n        logger.info(\n            \"Initializing an LLM engine (v%s) with config: \"\n            \"model=%r, speculative_config=%r, tokenizer=%r, \"\n            \"skip_tokenizer_init=%s, tokenizer_mode=%s, revision=%s, \"\n            \"rope_scaling=%r, rope_theta=%r, tokenizer_revision=%s, \"\n            \"trust_remote_code=%s, dtype=%s, max_seq_len=%d, \"\n            \"download_dir=%r, load_format=%s, tensor_parallel_size=%d, \"\n            \"pipeline_parallel_size=%d, \"\n            \"disable_custom_all_reduce=%s, quantization=%s, \"\n            \"enforce_eager=%s, kv_cache_dtype=%s, \"\n            \"quantization_param_path=%s, device_config=%s, \"\n            \"decoding_config=%r, observability_config=%r, \"\n            \"seed=%d, served_model_name=%s, use_v2_block_manager=%s, \"\n            \"enable_prefix_caching=%s)\",\n            VLLM_VERSION,\n            model_config.model,\n            speculative_config,\n            model_config.tokenizer,\n            model_config.skip_tokenizer_init,\n            model_config.tokenizer_mode,\n            model_config.revision,\n            model_config.rope_scaling,\n            model_config.rope_theta,\n            model_config.tokenizer_revision,\n            model_config.trust_remote_code,\n            model_config.dtype,\n            model_config.max_model_len,\n            load_config.download_dir,\n            load_config.load_format,\n            parallel_config.tensor_parallel_size,\n            parallel_config.pipeline_parallel_size,\n            parallel_config.disable_custom_all_reduce,\n            model_config.quantization,\n            model_config.enforce_eager,\n            cache_config.cache_dtype,\n            model_config.quantization_param_path,\n            device_config.device,\n            decoding_config,\n            observability_config,\n            model_config.seed,\n            model_config.served_model_name,\n            scheduler_config.use_v2_block_manager,\n            cache_config.enable_prefix_caching,\n        )\n        # TODO(woosuk): Print more configs in debug mode.\n        from vllm.plugins import load_general_plugins\n        load_general_plugins()\n\n        self.model_config = model_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.speculative_config = speculative_config\n        self.load_config = load_config\n        self.decoding_config = decoding_config or DecodingConfig()\n        self.prompt_adapter_config = prompt_adapter_config\n        self.observability_config = observability_config or ObservabilityConfig(\n        )\n        self.log_stats = log_stats\n\n        if not self.model_config.skip_tokenizer_init:\n            self.tokenizer = self._init_tokenizer()\n            self.detokenizer = Detokenizer(self.tokenizer)\n            tokenizer_group = self.get_tokenizer_group()\n        else:\n            self.tokenizer = None\n            self.detokenizer = None\n            tokenizer_group = None\n\n        # Ensure that the function doesn't contain a reference to self,\n        # to avoid engine GC issues\n        def get_tokenizer_for_seq(sequence: Sequence) -> AnyTokenizer:\n            assert tokenizer_group, (\"tokenizer_group cannot be None, \"\n                                     \"make sure skip_tokenizer_init is False\")\n            return tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n\n        self.seq_counter = Counter()\n        self.generation_config_fields = _load_generation_config_dict(\n            model_config)\n\n        self.input_registry = input_registry\n        self.input_processor = input_registry.create_input_processor(\n            model_config)\n\n        self.model_executor = executor_class(\n            model_config=model_config,\n            cache_config=cache_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            lora_config=lora_config,\n            speculative_config=speculative_config,\n            load_config=load_config,\n            prompt_adapter_config=prompt_adapter_config,\n            observability_config=self.observability_config,\n        )\n\n        if not self.model_config.embedding_mode:\n            self._initialize_kv_caches()\n\n        # If usage stat is enabled, collect relevant info.\n        if is_usage_stats_enabled():\n            from vllm.model_executor.model_loader import (\n                get_architecture_class_name)\n            usage_message.report_usage(\n                get_architecture_class_name(model_config),\n                usage_context,\n                extra_kvs={\n                    # Common configuration\n                    \"dtype\":\n                    str(model_config.dtype),\n                    \"tensor_parallel_size\":\n                    parallel_config.tensor_parallel_size,\n                    \"block_size\":\n                    cache_config.block_size,\n                    \"gpu_memory_utilization\":\n                    cache_config.gpu_memory_utilization,\n\n                    # Quantization\n                    \"quantization\":\n                    model_config.quantization,\n                    \"kv_cache_dtype\":\n                    str(cache_config.cache_dtype),\n\n                    # Feature flags\n                    \"enable_lora\":\n                    bool(lora_config),\n                    \"enable_prompt_adapter\":\n                    bool(prompt_adapter_config),\n                    \"enable_prefix_caching\":\n                    cache_config.enable_prefix_caching,\n                    \"enforce_eager\":\n                    model_config.enforce_eager,\n                    \"disable_custom_all_reduce\":\n                    parallel_config.disable_custom_all_reduce,\n                })\n\n        if self.tokenizer:\n            # Ping the tokenizer to ensure liveness if it runs in a\n            # different process.\n            self.tokenizer.ping()\n\n        # Create the scheduler.\n        # NOTE: the cache_config here have been updated with the numbers of\n        # GPU and CPU blocks, which are profiled in the distributed executor.\n        self.scheduler = [\n            Scheduler(scheduler_config, cache_config, lora_config,\n                      parallel_config.pipeline_parallel_size)\n            for _ in range(parallel_config.pipeline_parallel_size)\n        ]\n\n        # Metric Logging.\n        if self.log_stats:\n            if stat_loggers is not None:\n                self.stat_loggers = stat_loggers\n            else:\n                # Lazy import for prometheus multiprocessing.\n                # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n                # before prometheus_client is imported.\n                # See https://prometheus.github.io/client_python/multiprocess/\n                from vllm.engine.metrics import (LoggingStatLogger,\n                                                 PrometheusStatLogger)\n\n                self.stat_loggers = {\n                    \"logging\":\n                    LoggingStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC),\n                    \"prometheus\":\n                    PrometheusStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC,\n                        labels=dict(model_name=model_config.served_model_name),\n                        max_model_len=self.model_config.max_model_len),\n                }\n                self.stat_loggers[\"prometheus\"].info(\"cache_config\",\n                                                     self.cache_config)\n\n        self.tracer = None\n        if self.observability_config.otlp_traces_endpoint:\n            self.tracer = init_tracer(\n                \"vllm.llm_engine\",\n                self.observability_config.otlp_traces_endpoint)\n\n        # Create sequence output processor, e.g. for beam search or\n        # speculative decoding.\n        self.output_processor = (\n            SequenceGroupOutputProcessor.create_output_processor(\n                self.scheduler_config,\n                self.detokenizer,\n                self.scheduler,\n                self.seq_counter,\n                get_tokenizer_for_seq,\n                stop_checker=StopChecker(\n                    self.scheduler_config.max_model_len,\n                    get_tokenizer_for_seq,\n                ),\n            ))\n\n    def _initialize_kv_caches(self) -> None:\n        \"\"\"Initialize the KV cache in the worker(s).\n\n        The workers will determine the number of blocks in both the GPU cache\n        and the swap CPU cache.\n        \"\"\"\n        num_gpu_blocks, num_cpu_blocks = (\n            self.model_executor.determine_num_available_blocks())\n\n        if self.cache_config.num_gpu_blocks_override is not None:\n            num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\n            logger.info(\n                \"Overriding num_gpu_blocks=%d with \"\n                \"num_gpu_blocks_override=%d\", num_gpu_blocks,\n                num_gpu_blocks_override)\n            num_gpu_blocks = num_gpu_blocks_override\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n\n    @classmethod\n    def _get_executor_cls(cls,\n                          engine_config: EngineConfig) -> Type[ExecutorBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        # Initialize the cluster and specify the executor class.\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutor\n            executor_class = NeuronExecutor\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutor\n                executor_class = RayTPUExecutor\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutor\n                executor_class = TPUExecutor\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutor\n            executor_class = CPUExecutor\n        elif engine_config.device_config.device_type == \"openvino\":\n            from vllm.executor.openvino_executor import OpenVINOExecutor\n            executor_class = OpenVINOExecutor\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutor\n                executor_class = RayXPUExecutor\n            else:\n                from vllm.executor.xpu_executor import XPUExecutor\n                executor_class = XPUExecutor\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutor\n            executor_class = RayGPUExecutor\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutor)\n            assert not envs.VLLM_USE_RAY_SPMD_WORKER, (\n                \"multiprocessing distributed executor backend does not \"\n                \"support VLLM_USE_RAY_SPMD_WORKER=1\")\n            executor_class = MultiprocessingGPUExecutor\n        else:\n            from vllm.executor.gpu_executor import GPUExecutor\n            executor_class = GPUExecutor\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __reduce__(self):\n        # This is to ensure that the LLMEngine is not referenced in\n        # the closure used to initialize Ray worker actors\n        raise RuntimeError(\"LLMEngine should not be pickled!\")\n\n    def __del__(self):\n        # Shutdown model executor when engine is garbage collected\n        # Use getattr since __init__ can fail before the field is set\n        if model_executor := getattr(self, \"model_executor\", None):\n            model_executor.shutdown()\n\n    MISSING_TOKENIZER_GROUP_MSG = (\"Unable to get tokenizer because \"\n                                   \"skip_tokenizer_init is True\")\n\n    def get_tokenizer_group(\n            self,\n            fail_msg: str = MISSING_TOKENIZER_GROUP_MSG) -> BaseTokenizerGroup:\n        if self.tokenizer is None:\n            raise ValueError(fail_msg)\n\n        return self.tokenizer\n\n    def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.get_tokenizer_group().get_lora_tokenizer(lora_request)\n\n    def _init_tokenizer(self) -> BaseTokenizerGroup:\n        return init_tokenizer_from_configs(\n            model_config=self.model_config,\n            scheduler_config=self.scheduler_config,\n            parallel_config=self.parallel_config,\n            enable_lora=bool(self.lora_config))\n\n    def _verify_args(self) -> None:\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n    def _get_bos_token_id(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for BOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).bos_token_id\n\n    def _get_eos_token_id(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for EOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).eos_token_id\n\n    def _get_decoder_start_token_id(self) -> Optional[int]:\n        '''\n        Obtain the decoder start token id employed by an encoder/decoder\n        model. Returns None for non-encoder/decoder models or if the\n        model config is unavailable.\n        '''\n\n        if not self.is_encoder_decoder_model():\n            logger.warning(\"Using None for decoder start token id because \"\n                           \"this is not an encoder/decoder model.\")\n            return None\n\n        if (self.model_config is None or self.model_config.hf_config is None):\n            logger.warning(\"Using None for decoder start token id because \"\n                           \"model config is not available.\")\n            return None\n\n        dec_start_token_id = getattr(self.model_config.hf_config,\n                                     'decoder_start_token_id', None)\n        if dec_start_token_id is None:\n            logger.warning(\"Falling back on <BOS> for decoder start token id \"\n                           \"because decoder start token id is not available.\")\n            dec_start_token_id = self._get_bos_token_id()\n\n        return dec_start_token_id\n\n    def _add_processed_request(\n        self,\n        request_id: str,\n        processed_inputs: Union[LLMInputs, EncoderDecoderLLMInputs],\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n    ) -> None:\n        # Create the sequences.\n        block_size = self.cache_config.block_size\n        seq_id = next(self.seq_counter)\n        eos_token_id = self._get_eos_token_id(lora_request)\n\n        seq = Sequence(seq_id, processed_inputs, block_size, eos_token_id,\n                       lora_request, prompt_adapter_request)\n\n        encoder_seq = None\n        if 'encoder_prompt_token_ids' in processed_inputs:\n            encoder_seq = Sequence(seq_id,\n                                   processed_inputs,\n                                   block_size,\n                                   eos_token_id,\n                                   lora_request,\n                                   prompt_adapter_request,\n                                   from_decoder_prompt=False)\n\n        # Create a SequenceGroup based on SamplingParams or PoolingParams\n        if isinstance(params, SamplingParams):\n            seq_group = self._create_sequence_group_with_sampling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq)\n        elif isinstance(params, PoolingParams):\n            seq_group = self._create_sequence_group_with_pooling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq)\n        else:\n            raise ValueError(\n                \"Either SamplingParams or PoolingParams must be provided.\")\n\n        # Add the sequence group to the scheduler with least unfinished seqs.\n        costs = [\n            scheduler.get_num_unfinished_seq_groups()\n            for scheduler in self.scheduler\n        ]\n        min_cost_scheduler = self.scheduler[costs.index(min(costs))]\n        min_cost_scheduler.add_seq_group(seq_group)\n\n    def stop_remote_worker_execution_loop(self) -> None:\n        self.model_executor.stop_remote_worker_execution_loop()\n\n    _LLMInputComponentsType = Tuple[str, List[int]]\n\n    def _prepare_decoder_input_ids_for_generation(\n        self,\n        decoder_input_ids: Optional[List[int]],\n    ) -> List[int]:\n        \"\"\"\n        Prepares `decoder_input_ids` for generation with encoder-decoder models.\n\n        Based on\n\n        https://github.com/huggingface/transformers/blob/\n        4037a2b5b1278736e566aec12e169100275545ea/\n        src/transformers/generation/utils.py\n\n        specifically GenerationMixin._prepare_decoder_input_ids_for_generation()\n\n\n```\n# Function Signature: def from_engine_args(\n# Natural Language Annotations: \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfrom vllm.engine import LLMEngine\n\n", "  ```\ndef from_engine_args(\n    engine_args:", "  ```\nfrom vllm.engine import LLMEngine, Engine"]}, "vllm-vllm/engine/llm_engine.py-add_request": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/llm_engine.py:\n```\nimport time\nfrom contextlib import contextmanager\nfrom typing import (TYPE_CHECKING, Any, ClassVar, Dict, Iterable, List,\n                    Mapping, Optional)\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Tuple, Type, TypeVar, Union\n\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (CacheConfig, DecodingConfig, DeviceConfig,\n                         EngineConfig, LoadConfig, LoRAConfig, ModelConfig,\n                         ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig,\n                         SpeculativeConfig)\nfrom vllm.core.scheduler import (ScheduledSequenceGroup, Scheduler,\n                                 SchedulerOutputs)\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.metrics_types import StatLoggerBase, Stats\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.engine.output_processor.util import create_output_by_sequence_group\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster\nfrom vllm.inputs import (INPUT_REGISTRY, EncoderDecoderLLMInputs,\n                         InputRegistry, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import MultiModalDataDict\nfrom vllm.outputs import (EmbeddingRequestOutput, RequestOutput,\n                          RequestOutputFactory)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (EmbeddingSequenceGroupOutput, ExecuteModelRequest,\n                           PoolerOutput, SamplerOutput, Sequence,\n                           SequenceGroup, SequenceGroupMetadata,\n                           SequenceStatus)\nfrom vllm.tracing import (SpanAttributes, SpanKind, extract_trace_context,\n                          init_tracer)\nfrom vllm.transformers_utils.config import try_get_generation_config\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer_group import (\n    AnyTokenizer, BaseTokenizerGroup, init_tokenizer_from_configs)\nfrom vllm.usage.usage_lib import (UsageContext, is_usage_stats_enabled,\n                                  usage_message)\nfrom vllm.utils import Counter\nfrom vllm.version import __version__ as VLLM_VERSION\n\nlogger = init_logger(__name__)\n_LOCAL_LOGGING_INTERVAL_SEC = 5\n\n\ndef _load_generation_config_dict(model_config: ModelConfig) -> Dict[str, Any]:\n    config = try_get_generation_config(\n        model_config.model,\n        trust_remote_code=model_config.trust_remote_code,\n        revision=model_config.revision,\n    )\n\n    if config is None:\n        return {}\n\n    return config.to_diff_dict()\n\n\n_O = TypeVar(\"_O\", RequestOutput, EmbeddingRequestOutput)\n\nPromptComponents = Tuple[Optional[str], List[int],\n                         Optional[MultiModalDataDict]]\nDecoderPromptComponents = Tuple[Optional[str], Optional[List[int]],\n                                Optional[MultiModalDataDict]]\n\n\nclass LLMEngine:\n    \"\"\"An LLM engine that receives requests and generates texts.\n\n    This is the main class for the vLLM engine. It receives requests\n    from clients and generates texts from the LLM. It includes a tokenizer, a\n    language model (possibly distributed across multiple GPUs), and GPU memory\n    space allocated for intermediate states (aka KV cache). This class utilizes\n    iteration-level scheduling and efficient memory management to maximize the\n    serving throughput.\n\n    The :class:`~vllm.LLM` class wraps this class for offline batched inference\n    and the :class:`AsyncLLMEngine` class wraps this class for online serving.\n\n    The config arguments are derived from :class:`~vllm.EngineArgs`. (See\n    :ref:`engine_args`)\n\n    Args:\n        model_config: The configuration related to the LLM model.\n        cache_config: The configuration related to the KV cache memory\n            management.\n        parallel_config: The configuration related to distributed execution.\n        scheduler_config: The configuration related to the request scheduler.\n        device_config: The configuration related to the device.\n        lora_config (Optional): The configuration related to serving multi-LoRA.\n        speculative_config (Optional): The configuration related to speculative\n            decoding.\n        executor_class: The model executor class for managing distributed\n            execution.\n        prompt_adapter_config (Optional): The configuration related to serving \n            prompt adapters.\n        log_stats: Whether to log statistics.\n        usage_context: Specified entry point, used for usage info collection.\n    \"\"\"\n\n    DO_VALIDATE_OUTPUT: ClassVar[bool] = False\n    \"\"\"A flag to toggle whether to validate the type of request output.\"\"\"\n\n    @classmethod\n    @contextmanager\n    def enable_output_validation(cls):\n        cls.DO_VALIDATE_OUTPUT = True\n\n        yield\n\n        cls.DO_VALIDATE_OUTPUT = False\n\n    @classmethod\n    def validate_output(\n        cls,\n        output: object,\n        output_type: Type[_O],\n    ) -> _O:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        if ((TYPE_CHECKING or do_validate)\n                and not isinstance(output, output_type)):\n            raise TypeError(f\"Expected output of type {output_type}, \"\n                            f\"but found type {type(output)}\")\n\n        return output\n\n    @classmethod\n    def validate_outputs(\n        cls,\n        outputs: GenericSequence[object],\n        output_type: Type[_O],\n    ) -> List[_O]:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        outputs_: List[_O]\n        if TYPE_CHECKING or do_validate:\n            outputs_ = []\n            for output in outputs:\n                if not isinstance(output, output_type):\n                    raise TypeError(f\"Expected output of type {output_type}, \"\n                                    f\"but found type {type(output)}\")\n\n                outputs_.append(output)\n        else:\n            outputs_ = outputs\n\n        return outputs_\n\n    tokenizer: Optional[BaseTokenizerGroup]\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        cache_config: CacheConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        speculative_config: Optional[SpeculativeConfig],\n        decoding_config: Optional[DecodingConfig],\n        observability_config: Optional[ObservabilityConfig],\n        prompt_adapter_config: Optional[PromptAdapterConfig],\n        executor_class: Type[ExecutorBase],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n    ) -> None:\n        logger.info(\n            \"Initializing an LLM engine (v%s) with config: \"\n            \"model=%r, speculative_config=%r, tokenizer=%r, \"\n            \"skip_tokenizer_init=%s, tokenizer_mode=%s, revision=%s, \"\n            \"rope_scaling=%r, rope_theta=%r, tokenizer_revision=%s, \"\n            \"trust_remote_code=%s, dtype=%s, max_seq_len=%d, \"\n            \"download_dir=%r, load_format=%s, tensor_parallel_size=%d, \"\n            \"pipeline_parallel_size=%d, \"\n            \"disable_custom_all_reduce=%s, quantization=%s, \"\n            \"enforce_eager=%s, kv_cache_dtype=%s, \"\n            \"quantization_param_path=%s, device_config=%s, \"\n            \"decoding_config=%r, observability_config=%r, \"\n            \"seed=%d, served_model_name=%s, use_v2_block_manager=%s, \"\n            \"enable_prefix_caching=%s)\",\n            VLLM_VERSION,\n            model_config.model,\n            speculative_config,\n            model_config.tokenizer,\n            model_config.skip_tokenizer_init,\n            model_config.tokenizer_mode,\n            model_config.revision,\n            model_config.rope_scaling,\n            model_config.rope_theta,\n            model_config.tokenizer_revision,\n            model_config.trust_remote_code,\n            model_config.dtype,\n            model_config.max_model_len,\n            load_config.download_dir,\n            load_config.load_format,\n            parallel_config.tensor_parallel_size,\n            parallel_config.pipeline_parallel_size,\n            parallel_config.disable_custom_all_reduce,\n            model_config.quantization,\n            model_config.enforce_eager,\n            cache_config.cache_dtype,\n            model_config.quantization_param_path,\n            device_config.device,\n            decoding_config,\n            observability_config,\n            model_config.seed,\n            model_config.served_model_name,\n            scheduler_config.use_v2_block_manager,\n            cache_config.enable_prefix_caching,\n        )\n        # TODO(woosuk): Print more configs in debug mode.\n        from vllm.plugins import load_general_plugins\n        load_general_plugins()\n\n        self.model_config = model_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.speculative_config = speculative_config\n        self.load_config = load_config\n        self.decoding_config = decoding_config or DecodingConfig()\n        self.prompt_adapter_config = prompt_adapter_config\n        self.observability_config = observability_config or ObservabilityConfig(\n        )\n        self.log_stats = log_stats\n\n        if not self.model_config.skip_tokenizer_init:\n            self.tokenizer = self._init_tokenizer()\n            self.detokenizer = Detokenizer(self.tokenizer)\n            tokenizer_group = self.get_tokenizer_group()\n        else:\n            self.tokenizer = None\n            self.detokenizer = None\n            tokenizer_group = None\n\n        # Ensure that the function doesn't contain a reference to self,\n        # to avoid engine GC issues\n        def get_tokenizer_for_seq(sequence: Sequence) -> AnyTokenizer:\n            assert tokenizer_group, (\"tokenizer_group cannot be None, \"\n                                     \"make sure skip_tokenizer_init is False\")\n            return tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n\n        self.seq_counter = Counter()\n        self.generation_config_fields = _load_generation_config_dict(\n            model_config)\n\n        self.input_registry = input_registry\n        self.input_processor = input_registry.create_input_processor(\n            model_config)\n\n        self.model_executor = executor_class(\n            model_config=model_config,\n            cache_config=cache_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            lora_config=lora_config,\n            speculative_config=speculative_config,\n            load_config=load_config,\n            prompt_adapter_config=prompt_adapter_config,\n            observability_config=self.observability_config,\n        )\n\n        if not self.model_config.embedding_mode:\n            self._initialize_kv_caches()\n\n        # If usage stat is enabled, collect relevant info.\n        if is_usage_stats_enabled():\n            from vllm.model_executor.model_loader import (\n                get_architecture_class_name)\n            usage_message.report_usage(\n                get_architecture_class_name(model_config),\n                usage_context,\n                extra_kvs={\n                    # Common configuration\n                    \"dtype\":\n                    str(model_config.dtype),\n                    \"tensor_parallel_size\":\n                    parallel_config.tensor_parallel_size,\n                    \"block_size\":\n                    cache_config.block_size,\n                    \"gpu_memory_utilization\":\n                    cache_config.gpu_memory_utilization,\n\n                    # Quantization\n                    \"quantization\":\n                    model_config.quantization,\n                    \"kv_cache_dtype\":\n                    str(cache_config.cache_dtype),\n\n                    # Feature flags\n                    \"enable_lora\":\n                    bool(lora_config),\n                    \"enable_prompt_adapter\":\n                    bool(prompt_adapter_config),\n                    \"enable_prefix_caching\":\n                    cache_config.enable_prefix_caching,\n                    \"enforce_eager\":\n                    model_config.enforce_eager,\n                    \"disable_custom_all_reduce\":\n                    parallel_config.disable_custom_all_reduce,\n                })\n\n        if self.tokenizer:\n            # Ping the tokenizer to ensure liveness if it runs in a\n            # different process.\n            self.tokenizer.ping()\n\n        # Create the scheduler.\n        # NOTE: the cache_config here have been updated with the numbers of\n        # GPU and CPU blocks, which are profiled in the distributed executor.\n        self.scheduler = [\n            Scheduler(scheduler_config, cache_config, lora_config,\n                      parallel_config.pipeline_parallel_size)\n            for _ in range(parallel_config.pipeline_parallel_size)\n        ]\n\n        # Metric Logging.\n        if self.log_stats:\n            if stat_loggers is not None:\n                self.stat_loggers = stat_loggers\n            else:\n                # Lazy import for prometheus multiprocessing.\n                # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n                # before prometheus_client is imported.\n                # See https://prometheus.github.io/client_python/multiprocess/\n                from vllm.engine.metrics import (LoggingStatLogger,\n                                                 PrometheusStatLogger)\n\n                self.stat_loggers = {\n                    \"logging\":\n                    LoggingStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC),\n                    \"prometheus\":\n                    PrometheusStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC,\n                        labels=dict(model_name=model_config.served_model_name),\n                        max_model_len=self.model_config.max_model_len),\n                }\n                self.stat_loggers[\"prometheus\"].info(\"cache_config\",\n                                                     self.cache_config)\n\n        self.tracer = None\n        if self.observability_config.otlp_traces_endpoint:\n            self.tracer = init_tracer(\n                \"vllm.llm_engine\",\n                self.observability_config.otlp_traces_endpoint)\n\n        # Create sequence output processor, e.g. for beam search or\n        # speculative decoding.\n        self.output_processor = (\n            SequenceGroupOutputProcessor.create_output_processor(\n                self.scheduler_config,\n                self.detokenizer,\n                self.scheduler,\n                self.seq_counter,\n                get_tokenizer_for_seq,\n                stop_checker=StopChecker(\n                    self.scheduler_config.max_model_len,\n                    get_tokenizer_for_seq,\n                ),\n            ))\n\n    def _initialize_kv_caches(self) -> None:\n        \"\"\"Initialize the KV cache in the worker(s).\n\n        The workers will determine the number of blocks in both the GPU cache\n        and the swap CPU cache.\n        \"\"\"\n        num_gpu_blocks, num_cpu_blocks = (\n            self.model_executor.determine_num_available_blocks())\n\n        if self.cache_config.num_gpu_blocks_override is not None:\n            num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\n            logger.info(\n                \"Overriding num_gpu_blocks=%d with \"\n                \"num_gpu_blocks_override=%d\", num_gpu_blocks,\n                num_gpu_blocks_override)\n            num_gpu_blocks = num_gpu_blocks_override\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n\n    @classmethod\n    def _get_executor_cls(cls,\n                          engine_config: EngineConfig) -> Type[ExecutorBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        # Initialize the cluster and specify the executor class.\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutor\n            executor_class = NeuronExecutor\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutor\n                executor_class = RayTPUExecutor\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutor\n                executor_class = TPUExecutor\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutor\n            executor_class = CPUExecutor\n        elif engine_config.device_config.device_type == \"openvino\":\n            from vllm.executor.openvino_executor import OpenVINOExecutor\n            executor_class = OpenVINOExecutor\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutor\n                executor_class = RayXPUExecutor\n            else:\n                from vllm.executor.xpu_executor import XPUExecutor\n                executor_class = XPUExecutor\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutor\n            executor_class = RayGPUExecutor\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutor)\n            assert not envs.VLLM_USE_RAY_SPMD_WORKER, (\n                \"multiprocessing distributed executor backend does not \"\n                \"support VLLM_USE_RAY_SPMD_WORKER=1\")\n            executor_class = MultiprocessingGPUExecutor\n        else:\n            from vllm.executor.gpu_executor import GPUExecutor\n            executor_class = GPUExecutor\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __reduce__(self):\n        # This is to ensure that the LLMEngine is not referenced in\n        # the closure used to initialize Ray worker actors\n        raise RuntimeError(\"LLMEngine should not be pickled!\")\n\n    def __del__(self):\n        # Shutdown model executor when engine is garbage collected\n        # Use getattr since __init__ can fail before the field is set\n        if model_executor := getattr(self, \"model_executor\", None):\n            model_executor.shutdown()\n\n    MISSING_TOKENIZER_GROUP_MSG = (\"Unable to get tokenizer because \"\n                                   \"skip_tokenizer_init is True\")\n\n    def get_tokenizer_group(\n            self,\n            fail_msg: str = MISSING_TOKENIZER_GROUP_MSG) -> BaseTokenizerGroup:\n        if self.tokenizer is None:\n            raise ValueError(fail_msg)\n\n        return self.tokenizer\n\n    def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.get_tokenizer_group().get_lora_tokenizer(lora_request)\n\n    def _init_tokenizer(self) -> BaseTokenizerGroup:\n        return init_tokenizer_from_configs(\n            model_config=self.model_config,\n            scheduler_config=self.scheduler_config,\n            parallel_config=self.parallel_config,\n            enable_lora=bool(self.lora_config))\n\n    def _verify_args(self) -> None:\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n    def _get_bos_token_id(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for BOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).bos_token_id\n\n    def _get_eos_token_id(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for EOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).eos_token_id\n\n    def _get_decoder_start_token_id(self) -> Optional[int]:\n        '''\n        Obtain the decoder start token id employed by an encoder/decoder\n        model. Returns None for non-encoder/decoder models or if the\n        model config is unavailable.\n        '''\n\n        if not self.is_encoder_decoder_model():\n            logger.warning(\"Using None for decoder start token id because \"\n                           \"this is not an encoder/decoder model.\")\n            return None\n\n        if (self.model_config is None or self.model_config.hf_config is None):\n            logger.warning(\"Using None for decoder start token id because \"\n                           \"model config is not available.\")\n            return None\n\n        dec_start_token_id = getattr(self.model_config.hf_config,\n                                     'decoder_start_token_id', None)\n        if dec_start_token_id is None:\n            logger.warning(\"Falling back on <BOS> for decoder start token id \"\n                           \"because decoder start token id is not available.\")\n            dec_start_token_id = self._get_bos_token_id()\n\n        return dec_start_token_id\n\n    def _add_processed_request(\n        self,\n        request_id: str,\n        processed_inputs: Union[LLMInputs, EncoderDecoderLLMInputs],\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n    ) -> None:\n        # Create the sequences.\n        block_size = self.cache_config.block_size\n        seq_id = next(self.seq_counter)\n        eos_token_id = self._get_eos_token_id(lora_request)\n\n        seq = Sequence(seq_id, processed_inputs, block_size, eos_token_id,\n                       lora_request, prompt_adapter_request)\n\n        encoder_seq = None\n        if 'encoder_prompt_token_ids' in processed_inputs:\n            encoder_seq = Sequence(seq_id,\n                                   processed_inputs,\n                                   block_size,\n                                   eos_token_id,\n                                   lora_request,\n                                   prompt_adapter_request,\n                                   from_decoder_prompt=False)\n\n        # Create a SequenceGroup based on SamplingParams or PoolingParams\n        if isinstance(params, SamplingParams):\n            seq_group = self._create_sequence_group_with_sampling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq)\n        elif isinstance(params, PoolingParams):\n            seq_group = self._create_sequence_group_with_pooling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq)\n        else:\n            raise ValueError(\n                \"Either SamplingParams or PoolingParams must be provided.\")\n\n        # Add the sequence group to the scheduler with least unfinished seqs.\n        costs = [\n            scheduler.get_num_unfinished_seq_groups()\n            for scheduler in self.scheduler\n        ]\n        min_cost_scheduler = self.scheduler[costs.index(min(costs))]\n        min_cost_scheduler.add_seq_group(seq_group)\n\n    def stop_remote_worker_execution_loop(self) -> None:\n        self.model_executor.stop_remote_worker_execution_loop()\n\n    _LLMInputComponentsType = Tuple[str, List[int]]\n\n    def _prepare_decoder_input_ids_for_generation(\n        self,\n        decoder_input_ids: Optional[List[int]],\n    ) -> List[int]:\n        \"\"\"\n        Prepares `decoder_input_ids` for generation with encoder-decoder models.\n\n        Based on\n\n        https://github.com/huggingface/transformers/blob/\n        4037a2b5b1278736e566aec12e169100275545ea/\n        src/transformers/generation/utils.py\n\n        specifically GenerationMixin._prepare_decoder_input_ids_for_generation()\n\n\n```\n# Function Signature: def add_request(\n# Natural Language Annotations: \"\"\"Add a request to the engine's request pool.\n\n        The request is added to the request pool and will be processed by the\n        scheduler as `engine.step()` is called. The exact scheduling policy is\n        determined by the scheduler.\n\n        Args:\n            request_id: The unique ID of the request.\n            inputs: The inputs to the LLM. See\n                :class:`~vllm.inputs.PromptInputs`\n                for more details about the format of each input.\n            params: Parameters for sampling or pooling.\n                :class:`~vllm.SamplingParams` for text generation.\n                :class:`~vllm.PoolingParams` for pooling.\n            arrival_time: The arrival time of the request. If None, we use\n                the current monotonic time.\n            trace_headers: OpenTelemetry trace headers.\n\n        Details:\n            - Set arrival_time to the current time if it is None.\n            - Set prompt_token_ids to the encoded prompt if it is None.\n            - Create `best_of` number of :class:`~vllm.Sequence` objects.\n            - Create a :class:`~vllm.SequenceGroup` object\n              from the list of :class:`~vllm.Sequence`.\n            - Add the :class:`~vllm.SequenceGroup` object to the scheduler.\n\n        Example:\n            >>> # initialize engine\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> # set request arguments\n            >>> example_prompt = \"Who is the president of the United States?\"\n            >>> sampling_params = SamplingParams(temperature=0.0)\n            >>> request_id = 0\n            >>>\n            >>> # add the request to the engine\n            >>> engine.add_request(\n            >>>    str(request_id),\n            >>>    example_prompt,\n            >>>    SamplingParams(temperature=0.0))\n            >>> # continue the request processing\n            >>> ...\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef add_request(\n        self,\n        request_", "  ```python\ndef add_request(\n    self,\n    request", "  ```\nclas LLMEngine(object):\n    def add"]}, "vllm-vllm/engine/llm_engine.py-step": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/llm_engine.py:\n```\nimport time\nfrom contextlib import contextmanager\nfrom typing import (TYPE_CHECKING, Any, ClassVar, Dict, Iterable, List,\n                    Mapping, Optional)\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Tuple, Type, TypeVar, Union\n\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (CacheConfig, DecodingConfig, DeviceConfig,\n                         EngineConfig, LoadConfig, LoRAConfig, ModelConfig,\n                         ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig,\n                         SpeculativeConfig)\nfrom vllm.core.scheduler import (ScheduledSequenceGroup, Scheduler,\n                                 SchedulerOutputs)\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.metrics_types import StatLoggerBase, Stats\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.engine.output_processor.util import create_output_by_sequence_group\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster\nfrom vllm.inputs import (INPUT_REGISTRY, EncoderDecoderLLMInputs,\n                         InputRegistry, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import MultiModalDataDict\nfrom vllm.outputs import (EmbeddingRequestOutput, RequestOutput,\n                          RequestOutputFactory)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (EmbeddingSequenceGroupOutput, ExecuteModelRequest,\n                           PoolerOutput, SamplerOutput, Sequence,\n                           SequenceGroup, SequenceGroupMetadata,\n                           SequenceStatus)\nfrom vllm.tracing import (SpanAttributes, SpanKind, extract_trace_context,\n                          init_tracer)\nfrom vllm.transformers_utils.config import try_get_generation_config\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer_group import (\n    AnyTokenizer, BaseTokenizerGroup, init_tokenizer_from_configs)\nfrom vllm.usage.usage_lib import (UsageContext, is_usage_stats_enabled,\n                                  usage_message)\nfrom vllm.utils import Counter\nfrom vllm.version import __version__ as VLLM_VERSION\n\nlogger = init_logger(__name__)\n_LOCAL_LOGGING_INTERVAL_SEC = 5\n\n\ndef _load_generation_config_dict(model_config: ModelConfig) -> Dict[str, Any]:\n    config = try_get_generation_config(\n        model_config.model,\n        trust_remote_code=model_config.trust_remote_code,\n        revision=model_config.revision,\n    )\n\n    if config is None:\n        return {}\n\n    return config.to_diff_dict()\n\n\n_O = TypeVar(\"_O\", RequestOutput, EmbeddingRequestOutput)\n\nPromptComponents = Tuple[Optional[str], List[int],\n                         Optional[MultiModalDataDict]]\nDecoderPromptComponents = Tuple[Optional[str], Optional[List[int]],\n                                Optional[MultiModalDataDict]]\n\n\nclass LLMEngine:\n    \"\"\"An LLM engine that receives requests and generates texts.\n\n    This is the main class for the vLLM engine. It receives requests\n    from clients and generates texts from the LLM. It includes a tokenizer, a\n    language model (possibly distributed across multiple GPUs), and GPU memory\n    space allocated for intermediate states (aka KV cache). This class utilizes\n    iteration-level scheduling and efficient memory management to maximize the\n    serving throughput.\n\n    The :class:`~vllm.LLM` class wraps this class for offline batched inference\n    and the :class:`AsyncLLMEngine` class wraps this class for online serving.\n\n    The config arguments are derived from :class:`~vllm.EngineArgs`. (See\n    :ref:`engine_args`)\n\n    Args:\n        model_config: The configuration related to the LLM model.\n        cache_config: The configuration related to the KV cache memory\n            management.\n        parallel_config: The configuration related to distributed execution.\n        scheduler_config: The configuration related to the request scheduler.\n        device_config: The configuration related to the device.\n        lora_config (Optional): The configuration related to serving multi-LoRA.\n        speculative_config (Optional): The configuration related to speculative\n            decoding.\n        executor_class: The model executor class for managing distributed\n            execution.\n        prompt_adapter_config (Optional): The configuration related to serving \n            prompt adapters.\n        log_stats: Whether to log statistics.\n        usage_context: Specified entry point, used for usage info collection.\n    \"\"\"\n\n    DO_VALIDATE_OUTPUT: ClassVar[bool] = False\n    \"\"\"A flag to toggle whether to validate the type of request output.\"\"\"\n\n    @classmethod\n    @contextmanager\n    def enable_output_validation(cls):\n        cls.DO_VALIDATE_OUTPUT = True\n\n        yield\n\n        cls.DO_VALIDATE_OUTPUT = False\n\n    @classmethod\n    def validate_output(\n        cls,\n        output: object,\n        output_type: Type[_O],\n    ) -> _O:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        if ((TYPE_CHECKING or do_validate)\n                and not isinstance(output, output_type)):\n            raise TypeError(f\"Expected output of type {output_type}, \"\n                            f\"but found type {type(output)}\")\n\n        return output\n\n    @classmethod\n    def validate_outputs(\n        cls,\n        outputs: GenericSequence[object],\n        output_type: Type[_O],\n    ) -> List[_O]:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        outputs_: List[_O]\n        if TYPE_CHECKING or do_validate:\n            outputs_ = []\n            for output in outputs:\n                if not isinstance(output, output_type):\n                    raise TypeError(f\"Expected output of type {output_type}, \"\n                                    f\"but found type {type(output)}\")\n\n                outputs_.append(output)\n        else:\n            outputs_ = outputs\n\n        return outputs_\n\n    tokenizer: Optional[BaseTokenizerGroup]\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        cache_config: CacheConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        speculative_config: Optional[SpeculativeConfig],\n        decoding_config: Optional[DecodingConfig],\n        observability_config: Optional[ObservabilityConfig],\n        prompt_adapter_config: Optional[PromptAdapterConfig],\n        executor_class: Type[ExecutorBase],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n    ) -> None:\n        logger.info(\n            \"Initializing an LLM engine (v%s) with config: \"\n            \"model=%r, speculative_config=%r, tokenizer=%r, \"\n            \"skip_tokenizer_init=%s, tokenizer_mode=%s, revision=%s, \"\n            \"rope_scaling=%r, rope_theta=%r, tokenizer_revision=%s, \"\n            \"trust_remote_code=%s, dtype=%s, max_seq_len=%d, \"\n            \"download_dir=%r, load_format=%s, tensor_parallel_size=%d, \"\n            \"pipeline_parallel_size=%d, \"\n            \"disable_custom_all_reduce=%s, quantization=%s, \"\n            \"enforce_eager=%s, kv_cache_dtype=%s, \"\n            \"quantization_param_path=%s, device_config=%s, \"\n            \"decoding_config=%r, observability_config=%r, \"\n            \"seed=%d, served_model_name=%s, use_v2_block_manager=%s, \"\n            \"enable_prefix_caching=%s)\",\n            VLLM_VERSION,\n            model_config.model,\n            speculative_config,\n            model_config.tokenizer,\n            model_config.skip_tokenizer_init,\n            model_config.tokenizer_mode,\n            model_config.revision,\n            model_config.rope_scaling,\n            model_config.rope_theta,\n            model_config.tokenizer_revision,\n            model_config.trust_remote_code,\n            model_config.dtype,\n            model_config.max_model_len,\n            load_config.download_dir,\n            load_config.load_format,\n            parallel_config.tensor_parallel_size,\n            parallel_config.pipeline_parallel_size,\n            parallel_config.disable_custom_all_reduce,\n            model_config.quantization,\n            model_config.enforce_eager,\n            cache_config.cache_dtype,\n            model_config.quantization_param_path,\n            device_config.device,\n            decoding_config,\n            observability_config,\n            model_config.seed,\n            model_config.served_model_name,\n            scheduler_config.use_v2_block_manager,\n            cache_config.enable_prefix_caching,\n        )\n        # TODO(woosuk): Print more configs in debug mode.\n        from vllm.plugins import load_general_plugins\n        load_general_plugins()\n\n        self.model_config = model_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.speculative_config = speculative_config\n        self.load_config = load_config\n        self.decoding_config = decoding_config or DecodingConfig()\n        self.prompt_adapter_config = prompt_adapter_config\n        self.observability_config = observability_config or ObservabilityConfig(\n        )\n        self.log_stats = log_stats\n\n        if not self.model_config.skip_tokenizer_init:\n            self.tokenizer = self._init_tokenizer()\n            self.detokenizer = Detokenizer(self.tokenizer)\n            tokenizer_group = self.get_tokenizer_group()\n        else:\n            self.tokenizer = None\n            self.detokenizer = None\n            tokenizer_group = None\n\n        # Ensure that the function doesn't contain a reference to self,\n        # to avoid engine GC issues\n        def get_tokenizer_for_seq(sequence: Sequence) -> AnyTokenizer:\n            assert tokenizer_group, (\"tokenizer_group cannot be None, \"\n                                     \"make sure skip_tokenizer_init is False\")\n            return tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n\n        self.seq_counter = Counter()\n        self.generation_config_fields = _load_generation_config_dict(\n            model_config)\n\n        self.input_registry = input_registry\n        self.input_processor = input_registry.create_input_processor(\n            model_config)\n\n        self.model_executor = executor_class(\n            model_config=model_config,\n            cache_config=cache_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            lora_config=lora_config,\n            speculative_config=speculative_config,\n            load_config=load_config,\n            prompt_adapter_config=prompt_adapter_config,\n            observability_config=self.observability_config,\n        )\n\n        if not self.model_config.embedding_mode:\n            self._initialize_kv_caches()\n\n        # If usage stat is enabled, collect relevant info.\n        if is_usage_stats_enabled():\n            from vllm.model_executor.model_loader import (\n                get_architecture_class_name)\n            usage_message.report_usage(\n                get_architecture_class_name(model_config),\n                usage_context,\n                extra_kvs={\n                    # Common configuration\n                    \"dtype\":\n                    str(model_config.dtype),\n                    \"tensor_parallel_size\":\n                    parallel_config.tensor_parallel_size,\n                    \"block_size\":\n                    cache_config.block_size,\n                    \"gpu_memory_utilization\":\n                    cache_config.gpu_memory_utilization,\n\n                    # Quantization\n                    \"quantization\":\n                    model_config.quantization,\n                    \"kv_cache_dtype\":\n                    str(cache_config.cache_dtype),\n\n                    # Feature flags\n                    \"enable_lora\":\n                    bool(lora_config),\n                    \"enable_prompt_adapter\":\n                    bool(prompt_adapter_config),\n                    \"enable_prefix_caching\":\n                    cache_config.enable_prefix_caching,\n                    \"enforce_eager\":\n                    model_config.enforce_eager,\n                    \"disable_custom_all_reduce\":\n                    parallel_config.disable_custom_all_reduce,\n                })\n\n        if self.tokenizer:\n            # Ping the tokenizer to ensure liveness if it runs in a\n            # different process.\n            self.tokenizer.ping()\n\n        # Create the scheduler.\n        # NOTE: the cache_config here have been updated with the numbers of\n        # GPU and CPU blocks, which are profiled in the distributed executor.\n        self.scheduler = [\n            Scheduler(scheduler_config, cache_config, lora_config,\n                      parallel_config.pipeline_parallel_size)\n            for _ in range(parallel_config.pipeline_parallel_size)\n        ]\n\n        # Metric Logging.\n        if self.log_stats:\n            if stat_loggers is not None:\n                self.stat_loggers = stat_loggers\n            else:\n                # Lazy import for prometheus multiprocessing.\n                # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n                # before prometheus_client is imported.\n                # See https://prometheus.github.io/client_python/multiprocess/\n                from vllm.engine.metrics import (LoggingStatLogger,\n                                                 PrometheusStatLogger)\n\n                self.stat_loggers = {\n                    \"logging\":\n                    LoggingStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC),\n                    \"prometheus\":\n                    PrometheusStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC,\n                        labels=dict(model_name=model_config.served_model_name),\n                        max_model_len=self.model_config.max_model_len),\n                }\n                self.stat_loggers[\"prometheus\"].info(\"cache_config\",\n                                                     self.cache_config)\n\n        self.tracer = None\n        if self.observability_config.otlp_traces_endpoint:\n            self.tracer = init_tracer(\n                \"vllm.llm_engine\",\n                self.observability_config.otlp_traces_endpoint)\n\n        # Create sequence output processor, e.g. for beam search or\n        # speculative decoding.\n        self.output_processor = (\n            SequenceGroupOutputProcessor.create_output_processor(\n                self.scheduler_config,\n                self.detokenizer,\n                self.scheduler,\n                self.seq_counter,\n                get_tokenizer_for_seq,\n                stop_checker=StopChecker(\n                    self.scheduler_config.max_model_len,\n                    get_tokenizer_for_seq,\n                ),\n            ))\n\n    def _initialize_kv_caches(self) -> None:\n        \"\"\"Initialize the KV cache in the worker(s).\n\n        The workers will determine the number of blocks in both the GPU cache\n        and the swap CPU cache.\n        \"\"\"\n        num_gpu_blocks, num_cpu_blocks = (\n            self.model_executor.determine_num_available_blocks())\n\n        if self.cache_config.num_gpu_blocks_override is not None:\n            num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\n            logger.info(\n                \"Overriding num_gpu_blocks=%d with \"\n                \"num_gpu_blocks_override=%d\", num_gpu_blocks,\n                num_gpu_blocks_override)\n            num_gpu_blocks = num_gpu_blocks_override\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n\n    @classmethod\n    def _get_executor_cls(cls,\n                          engine_config: EngineConfig) -> Type[ExecutorBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        # Initialize the cluster and specify the executor class.\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutor\n            executor_class = NeuronExecutor\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutor\n                executor_class = RayTPUExecutor\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutor\n                executor_class = TPUExecutor\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutor\n            executor_class = CPUExecutor\n        elif engine_config.device_config.device_type == \"openvino\":\n            from vllm.executor.openvino_executor import OpenVINOExecutor\n            executor_class = OpenVINOExecutor\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutor\n                executor_class = RayXPUExecutor\n            else:\n                from vllm.executor.xpu_executor import XPUExecutor\n                executor_class = XPUExecutor\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutor\n            executor_class = RayGPUExecutor\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutor)\n            assert not envs.VLLM_USE_RAY_SPMD_WORKER, (\n                \"multiprocessing distributed executor backend does not \"\n                \"support VLLM_USE_RAY_SPMD_WORKER=1\")\n            executor_class = MultiprocessingGPUExecutor\n        else:\n            from vllm.executor.gpu_executor import GPUExecutor\n            executor_class = GPUExecutor\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __reduce__(self):\n        # This is to ensure that the LLMEngine is not referenced in\n        # the closure used to initialize Ray worker actors\n        raise RuntimeError(\"LLMEngine should not be pickled!\")\n\n    def __del__(self):\n        # Shutdown model executor when engine is garbage collected\n        # Use getattr since __init__ can fail before the field is set\n        if model_executor := getattr(self, \"model_executor\", None):\n            model_executor.shutdown()\n\n    MISSING_TOKENIZER_GROUP_MSG = (\"Unable to get tokenizer because \"\n                                   \"skip_tokenizer_init is True\")\n\n    def get_tokenizer_group(\n            self,\n            fail_msg: str = MISSING_TOKENIZER_GROUP_MSG) -> BaseTokenizerGroup:\n        if self.tokenizer is None:\n            raise ValueError(fail_msg)\n\n        return self.tokenizer\n\n    def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.get_tokenizer_group().get_lora_tokenizer(lora_request)\n\n    def _init_tokenizer(self) -> BaseTokenizerGroup:\n        return init_tokenizer_from_configs(\n            model_config=self.model_config,\n            scheduler_config=self.scheduler_config,\n            parallel_config=self.parallel_config,\n            enable_lora=bool(self.lora_config))\n\n    def _verify_args(self) -> None:\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n    def _get_bos_token_id(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for BOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).bos_token_id\n\n    def _get_eos_token_id(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for EOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).eos_token_id\n\n    def _get_decoder_start_token_id(self) -> Optional[int]:\n        '''\n        Obtain the decoder start token id employed by an encoder/decoder\n        model. Returns None for non-encoder/decoder models or if the\n        model config is unavailable.\n        '''\n\n        if not self.is_encoder_decoder_model():\n            logger.warning(\"Using None for decoder start token id because \"\n                           \"this is not an encoder/decoder model.\")\n            return None\n\n        if (self.model_config is None or self.model_config.hf_config is None):\n            logger.warning(\"Using None for decoder start token id because \"\n                           \"model config is not available.\")\n            return None\n\n        dec_start_token_id = getattr(self.model_config.hf_config,\n                                     'decoder_start_token_id', None)\n        if dec_start_token_id is None:\n            logger.warning(\"Falling back on <BOS> for decoder start token id \"\n                           \"because decoder start token id is not available.\")\n            dec_start_token_id = self._get_bos_token_id()\n\n        return dec_start_token_id\n\n    def _add_processed_request(\n        self,\n        request_id: str,\n        processed_inputs: Union[LLMInputs, EncoderDecoderLLMInputs],\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n    ) -> None:\n        # Create the sequences.\n        block_size = self.cache_config.block_size\n        seq_id = next(self.seq_counter)\n        eos_token_id = self._get_eos_token_id(lora_request)\n\n        seq = Sequence(seq_id, processed_inputs, block_size, eos_token_id,\n                       lora_request, prompt_adapter_request)\n\n        encoder_seq = None\n        if 'encoder_prompt_token_ids' in processed_inputs:\n            encoder_seq = Sequence(seq_id,\n                                   processed_inputs,\n                                   block_size,\n                                   eos_token_id,\n                                   lora_request,\n                                   prompt_adapter_request,\n                                   from_decoder_prompt=False)\n\n        # Create a SequenceGroup based on SamplingParams or PoolingParams\n        if isinstance(params, SamplingParams):\n            seq_group = self._create_sequence_group_with_sampling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq)\n        elif isinstance(params, PoolingParams):\n            seq_group = self._create_sequence_group_with_pooling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq)\n        else:\n            raise ValueError(\n                \"Either SamplingParams or PoolingParams must be provided.\")\n\n        # Add the sequence group to the scheduler with least unfinished seqs.\n        costs = [\n            scheduler.get_num_unfinished_seq_groups()\n            for scheduler in self.scheduler\n        ]\n        min_cost_scheduler = self.scheduler[costs.index(min(costs))]\n        min_cost_scheduler.add_seq_group(seq_group)\n\n    def stop_remote_worker_execution_loop(self) -> None:\n        self.model_executor.stop_remote_worker_execution_loop()\n\n    _LLMInputComponentsType = Tuple[str, List[int]]\n\n    def _prepare_decoder_input_ids_for_generation(\n        self,\n        decoder_input_ids: Optional[List[int]],\n    ) -> List[int]:\n        \"\"\"\n        Prepares `decoder_input_ids` for generation with encoder-decoder models.\n\n        Based on\n\n        https://github.com/huggingface/transformers/blob/\n        4037a2b5b1278736e566aec12e169100275545ea/\n        src/transformers/generation/utils.py\n\n        specifically GenerationMixin._prepare_decoder_input_ids_for_generation()\n\n\n```\n# Function Signature: def step(self) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n# Natural Language Annotations: \"\"\"Performs one decoding iteration and returns newly generated results.\n\n        .. figure:: https://i.imgur.com/sv2HssD.png\n            :alt: Overview of the step function\n            :align: center\n\n            Overview of the step function.\n\n        Details:\n            - Step 1: Schedules the sequences to be executed in the next\n              iteration and the token blocks to be swapped in/out/copy.\n\n                - Depending on the scheduling policy,\n                  sequences may be `preempted/reordered`.\n                - A Sequence Group (SG) refer to a group of sequences\n                  that are generated from the same prompt.\n\n            - Step 2: Calls the distributed executor to execute the model.\n            - Step 3: Processes the model output. This mainly includes:\n\n                - Decodes the relevant outputs.\n                - Updates the scheduled sequence groups with model outputs\n                  based on its `sampling parameters` (`use_beam_search` or not).\n                - Frees the finished sequence groups.\n\n            - Finally, it creates and returns the newly generated results.\n\n        Example:\n            >>> # Please see the example/ folder for more detailed examples.\n            >>>\n            >>> # initialize engine and request arguments\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> example_inputs = [(0, \"What is LLM?\",\n            >>>    SamplingParams(temperature=0.0))]\n            >>>\n            >>> # Start the engine with an event loop\n            >>> while True:\n            >>>     if example_inputs:\n            >>>         req_id, prompt, sampling_params = example_inputs.pop(0)\n            >>>         engine.add_request(str(req_id),prompt,sampling_params)\n            >>>\n            >>>     # continue the request processing\n            >>>     request_outputs = engine.step()\n            >>>     for request_output in request_outputs:\n            >>>         if request_output.finished:\n            >>>             # return or show the request output\n            >>>\n            >>>     if not (engine.has_unfinished_requests() or example_inputs):\n            >>>         break\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport torch\n\nclass LLMEngine:\n   ", "  ```\ndef step(self) -> List[Union[RequestOutput,", "  ```\ndef step(self) -> List[Union[RequestOutput,"]}, "vllm-vllm/engine/async_llm_engine.py-process_request_output": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/async_llm_engine.py:\n```\nimport asyncio\nimport time\nfrom functools import partial\nfrom typing import (AsyncGenerator, Callable, Dict, Iterable, List, Mapping,\n                    Optional, Set, Tuple, Type, Union)\n\nfrom transformers import PreTrainedTokenizer\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, EngineConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_timeout import asyncio_timeout\nfrom vllm.engine.llm_engine import (DecoderPromptComponents, LLMEngine,\n                                    PromptComponents)\nfrom vllm.engine.metrics_types import StatLoggerBase\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster, ray\nfrom vllm.inputs import (EncoderDecoderLLMInputs, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import EmbeddingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import print_warning_once\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _log_task_completion(task: asyncio.Task,\n                         error_callback: Callable[[Exception], None]) -> None:\n    \"\"\"This function is only intended for the `engine.run_engine_loop()` task.\n\n    In particular, that task runs a `while True` loop that can only exit if\n    there is an exception.\n    \"\"\"\n\n    exception = None\n    try:\n        return_value = task.result()\n        raise AssertionError(\n            f\"The engine background task should never finish without an \"\n            f\"exception. {return_value}\")\n    except asyncio.exceptions.CancelledError:\n        # We assume that if the task is cancelled, we are gracefully shutting\n        # down. This should only happen on program exit.\n        logger.info(\"Engine is gracefully shutting down.\")\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            \"Task finished unexpectedly. This should never happen! \"\n            \"Please open an issue on Github. See stack trace above for the \"\n            \"actual cause.\") from e\n\n\nSTOP_ITERATION = Exception()  # Sentinel\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or EmbeddingRequestOutputs for a request\n    that can be iterated over asynchronously via an async generator.\"\"\"\n\n    def __init__(self, request_id: str, cancel: Callable[[str], None]) -> None:\n        self.request_id = request_id\n        self._cancel = cancel\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, EmbeddingRequestOutput,\n                              Exception]) -> None:\n        if self._finished:\n            return\n        self._queue.put_nowait(item)\n\n    def finish(\n        self,\n        exception: Optional[Union[BaseException, Type[BaseException]]] = None,\n    ) -> None:\n        if not self._finished:\n            self._finished = True\n            self._queue.put_nowait(\n                exception if exception is not None else STOP_ITERATION)\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    async def generator(\n        self\n    ) -> AsyncGenerator[Union[RequestOutput, EmbeddingRequestOutput], None]:\n        try:\n            while not self._finished:\n                result = await self._queue.get()\n                if isinstance(result, Exception):\n                    if result == STOP_ITERATION:\n                        return\n                    raise result\n                yield result\n        except GeneratorExit:\n            self._cancel(self.request_id)\n            raise asyncio.CancelledError from None\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._aborted_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self.abort_request(request_id, exception=exc)\n        else:\n            # NB: tuple() used here because self.abort_request pops the stream\n            # out of self._request_streams, so we can't iterate on it directly\n            for rid in tuple(self._request_streams.keys()):\n                self.abort_request(rid, exception=exc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: BaseException,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id, exception=exception)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    async def step_async(\n        self, virtual_engine: int\n    ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler[\n            virtual_engine].schedule()\n\n        if not scheduler_outputs.is_empty():\n            # Execute the model.\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                virtual_engine=virtual_engine,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids)\n            output = await self.model_executor.execute_model_async(\n                execute_model_req)\n        else:\n            output = []\n\n        request_outputs = self._process_model_outputs(\n            output, scheduler_outputs.scheduled_seq_groups,\n            scheduler_outputs.ignored_seq_groups, seq_group_metadata_list)\n\n        # Log stats.\n        self.do_log_stats(scheduler_outputs, output)\n\n        # Tracing\n        self.do_tracing(scheduler_outputs)\n\n        return request_outputs\n\n    async def stop_remote_worker_execution_loop_async(self) -> None:\n        \"\"\"Stop the remote worker execution loop.\"\"\"\n        await self.model_executor.stop_remote_worker_execution_loop_async()\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        request_id: str,\n        lora_request: Optional[LoRARequest],\n    ) -> List[int]:\n        \"\"\"Async version of :meth:`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group(\"prompts must be None if \"\n                                             \"skip_tokenizer_init is True\")\n\n        return await tokenizer.encode_async(request_id=request_id,\n                                            prompt=prompt,\n                                            lora_request=lora_request)\n\n    async def _extract_prompt_components_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> PromptComponents:\n        \"\"\"Async version of :meth:`_extract_prompt_components`.\"\"\"\n        if isinstance(inputs, str):\n            prompt = inputs\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt,\n                request_id=request_id,\n                lora_request=lora_request,\n            )\n            multi_modal_data = None\n        elif isinstance(inputs, dict):\n            if \"prompt_token_ids\" in inputs:\n                prompt = None\n                prompt_token_ids = inputs[\"prompt_token_ids\"]\n            else:\n                # NOTE: This extra assignment is required to pass mypy\n                prompt = parsed_prompt = inputs[\"prompt\"]\n                prompt_token_ids = await self._tokenize_prompt_async(\n                    parsed_prompt,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                )\n\n            multi_modal_data = inputs.get(\"multi_modal_data\")\n        else:\n            assert_never(inputs)\n\n        return prompt, prompt_token_ids, multi_modal_data\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n    ) -> EncoderDecoderLLMInputs:\n        \"\"\"Async version of :meth:`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_comps: PromptComponents\n        decoder_comps: DecoderPromptComponents\n\n        if is_explicit_encoder_decoder_prompt(inputs):\n            encoder_task = self._extract_prompt_components_async(\n                inputs[\"encoder_prompt\"],\n                request_id=request_id,\n            )\n\n            if (decoder_input := inputs[\"decoder_prompt\"]) is None:\n                encoder_comps = await encoder_task\n                decoder_comps = None, None, None\n            else:\n                decoder_task = self._extract_prompt_components_async(\n                    decoder_input,\n                    request_id=request_id,\n                )\n\n                encoder_comps, decoder_comps = await asyncio.gather(\n                    encoder_task, decoder_task)\n        else:\n            encoder_comps = await self._extract_prompt_components_async(\n                inputs,\n                request_id=request_id,\n            )\n\n            decoder_comps = None, None, None\n\n        return self._build_enc_dec_llm_inputs(encoder_comps, decoder_comps)\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> LLMInputs:\n        \"\"\"Async version of :meth:`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._extract_prompt_components_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def process_model_inputs_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> Union[LLMInputs, EncoderDecoderLLMInputs]:\n        \"\"\"Async version of :meth:`process_model_inputs`.\"\"\"\n        if self.is_encoder_decoder_model():\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            model_inputs = await self._process_encoder_decoder_prompt_async(\n                inputs,\n                request_id=request_id,\n            )\n        else:\n            if is_explicit_encoder_decoder_prompt(inputs):\n                raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                                 \"to decoder-only models\")\n\n            # Decoder-only operation\n            model_inputs = await self._process_decoder_only_prompt_async(\n                inputs,\n                request_id=request_id,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return self.input_processor(model_inputs)\n\n    async def add_request_async(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        \"\"\"Async version of :meth:`add_request`.\"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        processed_inputs = await self.process_model_inputs_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n        )\n\n    async def check_health_async(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for :class:`LLMEngine`.\n\n    This class is used to wrap the :class:`LLMEngine` class to make it\n    asynchronous. It uses asyncio to create a background loop that keeps\n    processing incoming requests. The :class:`LLMEngine` is kicked by the\n    generate method when there are requests in the waiting queue. The generate\n    method yields the outputs from the :class:`LLMEngine` to the caller.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for :class:`LLMEngine`.\n        **kwargs: Arguments for :class:`LLMEngine`.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.engine = self._init_engine(*args, **kwargs)\n\n        if self.engine_use_ray:\n            print_warning_once(\n                \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n                \"be removed in a future update. \"\n                \"See https://github.com/vllm-project/vllm/issues/7045.\")\n\n            if envs.VLLM_ALLOW_ENGINE_USE_RAY:\n                print_warning_once(\n                    \"VLLM_ALLOW_ENGINE_USE_RAY is set, force engine use Ray\")\n            else:\n                raise ValueError(\"`--engine-use-ray` is deprecated. \"\n                                 \"Set `VLLM_ALLOW_ENGINE_USE_RAY=1` to \"\n                                 \"force use it\")\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    @classmethod\n    def _get_executor_cls(\n            cls, engine_config: EngineConfig) -> Type[ExecutorAsyncBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorAsyncBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorAsyncBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutorAsync\n                executor_class = RayTPUExecutorAsync\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutorAsync\n                executor_class = TPUExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif engine_config.device_config.device_type == \"openvino\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with \"\n                \"the OpenVINO backend.\")\n            from vllm.executor.openvino_executor import OpenVINOExecutorAsync\n            executor_class = OpenVINOExecutorAsync\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend is None:\n                from vllm.executor.xpu_executor import XPUExecutorAsync\n                executor_class = XPUExecutorAsync\n            elif distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutorAsync\n                executor_class = RayXPUExecutorAsync\n            else:\n                raise RuntimeError(\n                    \"Not supported distributed execution model on XPU device.\")\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> \"PreTrainedTokenizer\":\n        if self.engine_use_ray:\n            return await self.engine.get_tokenizer.remote(  # type: ignore\n                lora_request)\n\n        return await (self.engine.get_tokenizer_group().\n                      get_lora_tokenizer_async(lora_request))\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_log_task_completion, error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def shutdown_background_loop(self) -> None:\n        \"\"\"\n        Shut down the background loop.\n\n        This method needs to be called during cleanup to remove\n        references to `self` and properly GC the resources held\n        by the async LLM engine (e.g., the executors as well as\n        their resources).\n        \"\"\"\n        if self._background_loop_unshielded is not None:\n            self._background_loop_unshielded.cancel()\n            self._background_loop_unshielded = None\n        self.background_loop = None\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = kwargs[\"cache_config\"]\n            parallel_config = kwargs[\"parallel_config\"]\n            if (parallel_config.tensor_parallel_size == 1\n                    and parallel_config.pipeline_parallel_size == 1):\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self, virtual_engine: int) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, aborted_requests = (\n            self._request_tracker.get_new_and_aborted_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            try:\n                if self.engine_use_ray:\n                    await self.engine.add_request.remote(  # type: ignore\n                        **new_request)\n                else:\n                    await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if aborted_requests:\n            await self._engine_abort(aborted_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await\n```\n# Function Signature: def process_request_output(self,\n# Natural Language Annotations: \"\"\"Process a request output from the engine.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef process_request_output(self, request_output:", "  ```\ndef process_request_output(self, request_output:", "  ```\ndef process_request_output(self,\n                           request"]}, "vllm-vllm/engine/async_llm_engine.py-add_request": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/async_llm_engine.py:\n```\nimport asyncio\nimport time\nfrom functools import partial\nfrom typing import (AsyncGenerator, Callable, Dict, Iterable, List, Mapping,\n                    Optional, Set, Tuple, Type, Union)\n\nfrom transformers import PreTrainedTokenizer\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, EngineConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_timeout import asyncio_timeout\nfrom vllm.engine.llm_engine import (DecoderPromptComponents, LLMEngine,\n                                    PromptComponents)\nfrom vllm.engine.metrics_types import StatLoggerBase\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster, ray\nfrom vllm.inputs import (EncoderDecoderLLMInputs, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import EmbeddingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import print_warning_once\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _log_task_completion(task: asyncio.Task,\n                         error_callback: Callable[[Exception], None]) -> None:\n    \"\"\"This function is only intended for the `engine.run_engine_loop()` task.\n\n    In particular, that task runs a `while True` loop that can only exit if\n    there is an exception.\n    \"\"\"\n\n    exception = None\n    try:\n        return_value = task.result()\n        raise AssertionError(\n            f\"The engine background task should never finish without an \"\n            f\"exception. {return_value}\")\n    except asyncio.exceptions.CancelledError:\n        # We assume that if the task is cancelled, we are gracefully shutting\n        # down. This should only happen on program exit.\n        logger.info(\"Engine is gracefully shutting down.\")\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            \"Task finished unexpectedly. This should never happen! \"\n            \"Please open an issue on Github. See stack trace above for the \"\n            \"actual cause.\") from e\n\n\nSTOP_ITERATION = Exception()  # Sentinel\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or EmbeddingRequestOutputs for a request\n    that can be iterated over asynchronously via an async generator.\"\"\"\n\n    def __init__(self, request_id: str, cancel: Callable[[str], None]) -> None:\n        self.request_id = request_id\n        self._cancel = cancel\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, EmbeddingRequestOutput,\n                              Exception]) -> None:\n        if self._finished:\n            return\n        self._queue.put_nowait(item)\n\n    def finish(\n        self,\n        exception: Optional[Union[BaseException, Type[BaseException]]] = None,\n    ) -> None:\n        if not self._finished:\n            self._finished = True\n            self._queue.put_nowait(\n                exception if exception is not None else STOP_ITERATION)\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    async def generator(\n        self\n    ) -> AsyncGenerator[Union[RequestOutput, EmbeddingRequestOutput], None]:\n        try:\n            while not self._finished:\n                result = await self._queue.get()\n                if isinstance(result, Exception):\n                    if result == STOP_ITERATION:\n                        return\n                    raise result\n                yield result\n        except GeneratorExit:\n            self._cancel(self.request_id)\n            raise asyncio.CancelledError from None\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._aborted_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self.abort_request(request_id, exception=exc)\n        else:\n            # NB: tuple() used here because self.abort_request pops the stream\n            # out of self._request_streams, so we can't iterate on it directly\n            for rid in tuple(self._request_streams.keys()):\n                self.abort_request(rid, exception=exc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: BaseException,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id, exception=exception)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    async def step_async(\n        self, virtual_engine: int\n    ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler[\n            virtual_engine].schedule()\n\n        if not scheduler_outputs.is_empty():\n            # Execute the model.\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                virtual_engine=virtual_engine,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids)\n            output = await self.model_executor.execute_model_async(\n                execute_model_req)\n        else:\n            output = []\n\n        request_outputs = self._process_model_outputs(\n            output, scheduler_outputs.scheduled_seq_groups,\n            scheduler_outputs.ignored_seq_groups, seq_group_metadata_list)\n\n        # Log stats.\n        self.do_log_stats(scheduler_outputs, output)\n\n        # Tracing\n        self.do_tracing(scheduler_outputs)\n\n        return request_outputs\n\n    async def stop_remote_worker_execution_loop_async(self) -> None:\n        \"\"\"Stop the remote worker execution loop.\"\"\"\n        await self.model_executor.stop_remote_worker_execution_loop_async()\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        request_id: str,\n        lora_request: Optional[LoRARequest],\n    ) -> List[int]:\n        \"\"\"Async version of :meth:`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group(\"prompts must be None if \"\n                                             \"skip_tokenizer_init is True\")\n\n        return await tokenizer.encode_async(request_id=request_id,\n                                            prompt=prompt,\n                                            lora_request=lora_request)\n\n    async def _extract_prompt_components_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> PromptComponents:\n        \"\"\"Async version of :meth:`_extract_prompt_components`.\"\"\"\n        if isinstance(inputs, str):\n            prompt = inputs\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt,\n                request_id=request_id,\n                lora_request=lora_request,\n            )\n            multi_modal_data = None\n        elif isinstance(inputs, dict):\n            if \"prompt_token_ids\" in inputs:\n                prompt = None\n                prompt_token_ids = inputs[\"prompt_token_ids\"]\n            else:\n                # NOTE: This extra assignment is required to pass mypy\n                prompt = parsed_prompt = inputs[\"prompt\"]\n                prompt_token_ids = await self._tokenize_prompt_async(\n                    parsed_prompt,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                )\n\n            multi_modal_data = inputs.get(\"multi_modal_data\")\n        else:\n            assert_never(inputs)\n\n        return prompt, prompt_token_ids, multi_modal_data\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n    ) -> EncoderDecoderLLMInputs:\n        \"\"\"Async version of :meth:`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_comps: PromptComponents\n        decoder_comps: DecoderPromptComponents\n\n        if is_explicit_encoder_decoder_prompt(inputs):\n            encoder_task = self._extract_prompt_components_async(\n                inputs[\"encoder_prompt\"],\n                request_id=request_id,\n            )\n\n            if (decoder_input := inputs[\"decoder_prompt\"]) is None:\n                encoder_comps = await encoder_task\n                decoder_comps = None, None, None\n            else:\n                decoder_task = self._extract_prompt_components_async(\n                    decoder_input,\n                    request_id=request_id,\n                )\n\n                encoder_comps, decoder_comps = await asyncio.gather(\n                    encoder_task, decoder_task)\n        else:\n            encoder_comps = await self._extract_prompt_components_async(\n                inputs,\n                request_id=request_id,\n            )\n\n            decoder_comps = None, None, None\n\n        return self._build_enc_dec_llm_inputs(encoder_comps, decoder_comps)\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> LLMInputs:\n        \"\"\"Async version of :meth:`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._extract_prompt_components_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def process_model_inputs_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> Union[LLMInputs, EncoderDecoderLLMInputs]:\n        \"\"\"Async version of :meth:`process_model_inputs`.\"\"\"\n        if self.is_encoder_decoder_model():\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            model_inputs = await self._process_encoder_decoder_prompt_async(\n                inputs,\n                request_id=request_id,\n            )\n        else:\n            if is_explicit_encoder_decoder_prompt(inputs):\n                raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                                 \"to decoder-only models\")\n\n            # Decoder-only operation\n            model_inputs = await self._process_decoder_only_prompt_async(\n                inputs,\n                request_id=request_id,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return self.input_processor(model_inputs)\n\n    async def add_request_async(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        \"\"\"Async version of :meth:`add_request`.\"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        processed_inputs = await self.process_model_inputs_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n        )\n\n    async def check_health_async(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for :class:`LLMEngine`.\n\n    This class is used to wrap the :class:`LLMEngine` class to make it\n    asynchronous. It uses asyncio to create a background loop that keeps\n    processing incoming requests. The :class:`LLMEngine` is kicked by the\n    generate method when there are requests in the waiting queue. The generate\n    method yields the outputs from the :class:`LLMEngine` to the caller.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for :class:`LLMEngine`.\n        **kwargs: Arguments for :class:`LLMEngine`.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.engine = self._init_engine(*args, **kwargs)\n\n        if self.engine_use_ray:\n            print_warning_once(\n                \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n                \"be removed in a future update. \"\n                \"See https://github.com/vllm-project/vllm/issues/7045.\")\n\n            if envs.VLLM_ALLOW_ENGINE_USE_RAY:\n                print_warning_once(\n                    \"VLLM_ALLOW_ENGINE_USE_RAY is set, force engine use Ray\")\n            else:\n                raise ValueError(\"`--engine-use-ray` is deprecated. \"\n                                 \"Set `VLLM_ALLOW_ENGINE_USE_RAY=1` to \"\n                                 \"force use it\")\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    @classmethod\n    def _get_executor_cls(\n            cls, engine_config: EngineConfig) -> Type[ExecutorAsyncBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorAsyncBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorAsyncBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutorAsync\n                executor_class = RayTPUExecutorAsync\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutorAsync\n                executor_class = TPUExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif engine_config.device_config.device_type == \"openvino\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with \"\n                \"the OpenVINO backend.\")\n            from vllm.executor.openvino_executor import OpenVINOExecutorAsync\n            executor_class = OpenVINOExecutorAsync\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend is None:\n                from vllm.executor.xpu_executor import XPUExecutorAsync\n                executor_class = XPUExecutorAsync\n            elif distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutorAsync\n                executor_class = RayXPUExecutorAsync\n            else:\n                raise RuntimeError(\n                    \"Not supported distributed execution model on XPU device.\")\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> \"PreTrainedTokenizer\":\n        if self.engine_use_ray:\n            return await self.engine.get_tokenizer.remote(  # type: ignore\n                lora_request)\n\n        return await (self.engine.get_tokenizer_group().\n                      get_lora_tokenizer_async(lora_request))\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_log_task_completion, error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def shutdown_background_loop(self) -> None:\n        \"\"\"\n        Shut down the background loop.\n\n        This method needs to be called during cleanup to remove\n        references to `self` and properly GC the resources held\n        by the async LLM engine (e.g., the executors as well as\n        their resources).\n        \"\"\"\n        if self._background_loop_unshielded is not None:\n            self._background_loop_unshielded.cancel()\n            self._background_loop_unshielded = None\n        self.background_loop = None\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = kwargs[\"cache_config\"]\n            parallel_config = kwargs[\"parallel_config\"]\n            if (parallel_config.tensor_parallel_size == 1\n                    and parallel_config.pipeline_parallel_size == 1):\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self, virtual_engine: int) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, aborted_requests = (\n            self._request_tracker.get_new_and_aborted_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            try:\n                if self.engine_use_ray:\n                    await self.engine.add_request.remote(  # type: ignore\n                        **new_request)\n                else:\n                    await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if aborted_requests:\n            await self._engine_abort(aborted_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await\n```\n# Function Signature: async def add_request(\n# Natural Language Annotations: # This method does not need to be async, but kept that way\n# for backwards compatibility.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def add_request(\n        self,\n        request", "  ```\nasync def add_request(\n    self,\n    request", "  ```\nasync def add_request(self, request_ids: List"]}, "vllm-vllm/engine/async_llm_engine.py-abort_request": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/async_llm_engine.py:\n```\nimport asyncio\nimport time\nfrom functools import partial\nfrom typing import (AsyncGenerator, Callable, Dict, Iterable, List, Mapping,\n                    Optional, Set, Tuple, Type, Union)\n\nfrom transformers import PreTrainedTokenizer\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, EngineConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_timeout import asyncio_timeout\nfrom vllm.engine.llm_engine import (DecoderPromptComponents, LLMEngine,\n                                    PromptComponents)\nfrom vllm.engine.metrics_types import StatLoggerBase\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster, ray\nfrom vllm.inputs import (EncoderDecoderLLMInputs, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import EmbeddingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import print_warning_once\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _log_task_completion(task: asyncio.Task,\n                         error_callback: Callable[[Exception], None]) -> None:\n    \"\"\"This function is only intended for the `engine.run_engine_loop()` task.\n\n    In particular, that task runs a `while True` loop that can only exit if\n    there is an exception.\n    \"\"\"\n\n    exception = None\n    try:\n        return_value = task.result()\n        raise AssertionError(\n            f\"The engine background task should never finish without an \"\n            f\"exception. {return_value}\")\n    except asyncio.exceptions.CancelledError:\n        # We assume that if the task is cancelled, we are gracefully shutting\n        # down. This should only happen on program exit.\n        logger.info(\"Engine is gracefully shutting down.\")\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            \"Task finished unexpectedly. This should never happen! \"\n            \"Please open an issue on Github. See stack trace above for the \"\n            \"actual cause.\") from e\n\n\nSTOP_ITERATION = Exception()  # Sentinel\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or EmbeddingRequestOutputs for a request\n    that can be iterated over asynchronously via an async generator.\"\"\"\n\n    def __init__(self, request_id: str, cancel: Callable[[str], None]) -> None:\n        self.request_id = request_id\n        self._cancel = cancel\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, EmbeddingRequestOutput,\n                              Exception]) -> None:\n        if self._finished:\n            return\n        self._queue.put_nowait(item)\n\n    def finish(\n        self,\n        exception: Optional[Union[BaseException, Type[BaseException]]] = None,\n    ) -> None:\n        if not self._finished:\n            self._finished = True\n            self._queue.put_nowait(\n                exception if exception is not None else STOP_ITERATION)\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    async def generator(\n        self\n    ) -> AsyncGenerator[Union[RequestOutput, EmbeddingRequestOutput], None]:\n        try:\n            while not self._finished:\n                result = await self._queue.get()\n                if isinstance(result, Exception):\n                    if result == STOP_ITERATION:\n                        return\n                    raise result\n                yield result\n        except GeneratorExit:\n            self._cancel(self.request_id)\n            raise asyncio.CancelledError from None\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._aborted_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self.abort_request(request_id, exception=exc)\n        else:\n            # NB: tuple() used here because self.abort_request pops the stream\n            # out of self._request_streams, so we can't iterate on it directly\n            for rid in tuple(self._request_streams.keys()):\n                self.abort_request(rid, exception=exc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: BaseException,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id, exception=exception)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    async def step_async(\n        self, virtual_engine: int\n    ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler[\n            virtual_engine].schedule()\n\n        if not scheduler_outputs.is_empty():\n            # Execute the model.\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                virtual_engine=virtual_engine,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids)\n            output = await self.model_executor.execute_model_async(\n                execute_model_req)\n        else:\n            output = []\n\n        request_outputs = self._process_model_outputs(\n            output, scheduler_outputs.scheduled_seq_groups,\n            scheduler_outputs.ignored_seq_groups, seq_group_metadata_list)\n\n        # Log stats.\n        self.do_log_stats(scheduler_outputs, output)\n\n        # Tracing\n        self.do_tracing(scheduler_outputs)\n\n        return request_outputs\n\n    async def stop_remote_worker_execution_loop_async(self) -> None:\n        \"\"\"Stop the remote worker execution loop.\"\"\"\n        await self.model_executor.stop_remote_worker_execution_loop_async()\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        request_id: str,\n        lora_request: Optional[LoRARequest],\n    ) -> List[int]:\n        \"\"\"Async version of :meth:`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group(\"prompts must be None if \"\n                                             \"skip_tokenizer_init is True\")\n\n        return await tokenizer.encode_async(request_id=request_id,\n                                            prompt=prompt,\n                                            lora_request=lora_request)\n\n    async def _extract_prompt_components_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> PromptComponents:\n        \"\"\"Async version of :meth:`_extract_prompt_components`.\"\"\"\n        if isinstance(inputs, str):\n            prompt = inputs\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt,\n                request_id=request_id,\n                lora_request=lora_request,\n            )\n            multi_modal_data = None\n        elif isinstance(inputs, dict):\n            if \"prompt_token_ids\" in inputs:\n                prompt = None\n                prompt_token_ids = inputs[\"prompt_token_ids\"]\n            else:\n                # NOTE: This extra assignment is required to pass mypy\n                prompt = parsed_prompt = inputs[\"prompt\"]\n                prompt_token_ids = await self._tokenize_prompt_async(\n                    parsed_prompt,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                )\n\n            multi_modal_data = inputs.get(\"multi_modal_data\")\n        else:\n            assert_never(inputs)\n\n        return prompt, prompt_token_ids, multi_modal_data\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n    ) -> EncoderDecoderLLMInputs:\n        \"\"\"Async version of :meth:`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_comps: PromptComponents\n        decoder_comps: DecoderPromptComponents\n\n        if is_explicit_encoder_decoder_prompt(inputs):\n            encoder_task = self._extract_prompt_components_async(\n                inputs[\"encoder_prompt\"],\n                request_id=request_id,\n            )\n\n            if (decoder_input := inputs[\"decoder_prompt\"]) is None:\n                encoder_comps = await encoder_task\n                decoder_comps = None, None, None\n            else:\n                decoder_task = self._extract_prompt_components_async(\n                    decoder_input,\n                    request_id=request_id,\n                )\n\n                encoder_comps, decoder_comps = await asyncio.gather(\n                    encoder_task, decoder_task)\n        else:\n            encoder_comps = await self._extract_prompt_components_async(\n                inputs,\n                request_id=request_id,\n            )\n\n            decoder_comps = None, None, None\n\n        return self._build_enc_dec_llm_inputs(encoder_comps, decoder_comps)\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> LLMInputs:\n        \"\"\"Async version of :meth:`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._extract_prompt_components_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def process_model_inputs_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> Union[LLMInputs, EncoderDecoderLLMInputs]:\n        \"\"\"Async version of :meth:`process_model_inputs`.\"\"\"\n        if self.is_encoder_decoder_model():\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            model_inputs = await self._process_encoder_decoder_prompt_async(\n                inputs,\n                request_id=request_id,\n            )\n        else:\n            if is_explicit_encoder_decoder_prompt(inputs):\n                raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                                 \"to decoder-only models\")\n\n            # Decoder-only operation\n            model_inputs = await self._process_decoder_only_prompt_async(\n                inputs,\n                request_id=request_id,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return self.input_processor(model_inputs)\n\n    async def add_request_async(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        \"\"\"Async version of :meth:`add_request`.\"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        processed_inputs = await self.process_model_inputs_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n        )\n\n    async def check_health_async(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for :class:`LLMEngine`.\n\n    This class is used to wrap the :class:`LLMEngine` class to make it\n    asynchronous. It uses asyncio to create a background loop that keeps\n    processing incoming requests. The :class:`LLMEngine` is kicked by the\n    generate method when there are requests in the waiting queue. The generate\n    method yields the outputs from the :class:`LLMEngine` to the caller.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for :class:`LLMEngine`.\n        **kwargs: Arguments for :class:`LLMEngine`.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.engine = self._init_engine(*args, **kwargs)\n\n        if self.engine_use_ray:\n            print_warning_once(\n                \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n                \"be removed in a future update. \"\n                \"See https://github.com/vllm-project/vllm/issues/7045.\")\n\n            if envs.VLLM_ALLOW_ENGINE_USE_RAY:\n                print_warning_once(\n                    \"VLLM_ALLOW_ENGINE_USE_RAY is set, force engine use Ray\")\n            else:\n                raise ValueError(\"`--engine-use-ray` is deprecated. \"\n                                 \"Set `VLLM_ALLOW_ENGINE_USE_RAY=1` to \"\n                                 \"force use it\")\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    @classmethod\n    def _get_executor_cls(\n            cls, engine_config: EngineConfig) -> Type[ExecutorAsyncBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorAsyncBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorAsyncBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutorAsync\n                executor_class = RayTPUExecutorAsync\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutorAsync\n                executor_class = TPUExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif engine_config.device_config.device_type == \"openvino\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with \"\n                \"the OpenVINO backend.\")\n            from vllm.executor.openvino_executor import OpenVINOExecutorAsync\n            executor_class = OpenVINOExecutorAsync\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend is None:\n                from vllm.executor.xpu_executor import XPUExecutorAsync\n                executor_class = XPUExecutorAsync\n            elif distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutorAsync\n                executor_class = RayXPUExecutorAsync\n            else:\n                raise RuntimeError(\n                    \"Not supported distributed execution model on XPU device.\")\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> \"PreTrainedTokenizer\":\n        if self.engine_use_ray:\n            return await self.engine.get_tokenizer.remote(  # type: ignore\n                lora_request)\n\n        return await (self.engine.get_tokenizer_group().\n                      get_lora_tokenizer_async(lora_request))\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_log_task_completion, error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def shutdown_background_loop(self) -> None:\n        \"\"\"\n        Shut down the background loop.\n\n        This method needs to be called during cleanup to remove\n        references to `self` and properly GC the resources held\n        by the async LLM engine (e.g., the executors as well as\n        their resources).\n        \"\"\"\n        if self._background_loop_unshielded is not None:\n            self._background_loop_unshielded.cancel()\n            self._background_loop_unshielded = None\n        self.background_loop = None\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = kwargs[\"cache_config\"]\n            parallel_config = kwargs[\"parallel_config\"]\n            if (parallel_config.tensor_parallel_size == 1\n                    and parallel_config.pipeline_parallel_size == 1):\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self, virtual_engine: int) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, aborted_requests = (\n            self._request_tracker.get_new_and_aborted_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            try:\n                if self.engine_use_ray:\n                    await self.engine.add_request.remote(  # type: ignore\n                        **new_request)\n                else:\n                    await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if aborted_requests:\n            await self._engine_abort(aborted_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await\n```\n# Function Signature: def abort_request(self,\n# Natural Language Annotations: \"\"\"Abort a request during next background loop iteration.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfrom typing import Optional\n\n\nclass SomeClass:\n   ", "  ```\nasync def abort_request(\n        self,\n        request", "  ```\nfrom vllm.engine.request_tracker import Request"]}, "vllm-vllm/engine/async_llm_engine.py-get_new_and_aborted_requests": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/async_llm_engine.py:\n```\nimport asyncio\nimport time\nfrom functools import partial\nfrom typing import (AsyncGenerator, Callable, Dict, Iterable, List, Mapping,\n                    Optional, Set, Tuple, Type, Union)\n\nfrom transformers import PreTrainedTokenizer\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, EngineConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_timeout import asyncio_timeout\nfrom vllm.engine.llm_engine import (DecoderPromptComponents, LLMEngine,\n                                    PromptComponents)\nfrom vllm.engine.metrics_types import StatLoggerBase\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster, ray\nfrom vllm.inputs import (EncoderDecoderLLMInputs, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import EmbeddingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import print_warning_once\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _log_task_completion(task: asyncio.Task,\n                         error_callback: Callable[[Exception], None]) -> None:\n    \"\"\"This function is only intended for the `engine.run_engine_loop()` task.\n\n    In particular, that task runs a `while True` loop that can only exit if\n    there is an exception.\n    \"\"\"\n\n    exception = None\n    try:\n        return_value = task.result()\n        raise AssertionError(\n            f\"The engine background task should never finish without an \"\n            f\"exception. {return_value}\")\n    except asyncio.exceptions.CancelledError:\n        # We assume that if the task is cancelled, we are gracefully shutting\n        # down. This should only happen on program exit.\n        logger.info(\"Engine is gracefully shutting down.\")\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            \"Task finished unexpectedly. This should never happen! \"\n            \"Please open an issue on Github. See stack trace above for the \"\n            \"actual cause.\") from e\n\n\nSTOP_ITERATION = Exception()  # Sentinel\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or EmbeddingRequestOutputs for a request\n    that can be iterated over asynchronously via an async generator.\"\"\"\n\n    def __init__(self, request_id: str, cancel: Callable[[str], None]) -> None:\n        self.request_id = request_id\n        self._cancel = cancel\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, EmbeddingRequestOutput,\n                              Exception]) -> None:\n        if self._finished:\n            return\n        self._queue.put_nowait(item)\n\n    def finish(\n        self,\n        exception: Optional[Union[BaseException, Type[BaseException]]] = None,\n    ) -> None:\n        if not self._finished:\n            self._finished = True\n            self._queue.put_nowait(\n                exception if exception is not None else STOP_ITERATION)\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    async def generator(\n        self\n    ) -> AsyncGenerator[Union[RequestOutput, EmbeddingRequestOutput], None]:\n        try:\n            while not self._finished:\n                result = await self._queue.get()\n                if isinstance(result, Exception):\n                    if result == STOP_ITERATION:\n                        return\n                    raise result\n                yield result\n        except GeneratorExit:\n            self._cancel(self.request_id)\n            raise asyncio.CancelledError from None\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._aborted_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self.abort_request(request_id, exception=exc)\n        else:\n            # NB: tuple() used here because self.abort_request pops the stream\n            # out of self._request_streams, so we can't iterate on it directly\n            for rid in tuple(self._request_streams.keys()):\n                self.abort_request(rid, exception=exc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: BaseException,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id, exception=exception)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    async def step_async(\n        self, virtual_engine: int\n    ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler[\n            virtual_engine].schedule()\n\n        if not scheduler_outputs.is_empty():\n            # Execute the model.\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                virtual_engine=virtual_engine,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids)\n            output = await self.model_executor.execute_model_async(\n                execute_model_req)\n        else:\n            output = []\n\n        request_outputs = self._process_model_outputs(\n            output, scheduler_outputs.scheduled_seq_groups,\n            scheduler_outputs.ignored_seq_groups, seq_group_metadata_list)\n\n        # Log stats.\n        self.do_log_stats(scheduler_outputs, output)\n\n        # Tracing\n        self.do_tracing(scheduler_outputs)\n\n        return request_outputs\n\n    async def stop_remote_worker_execution_loop_async(self) -> None:\n        \"\"\"Stop the remote worker execution loop.\"\"\"\n        await self.model_executor.stop_remote_worker_execution_loop_async()\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        request_id: str,\n        lora_request: Optional[LoRARequest],\n    ) -> List[int]:\n        \"\"\"Async version of :meth:`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group(\"prompts must be None if \"\n                                             \"skip_tokenizer_init is True\")\n\n        return await tokenizer.encode_async(request_id=request_id,\n                                            prompt=prompt,\n                                            lora_request=lora_request)\n\n    async def _extract_prompt_components_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> PromptComponents:\n        \"\"\"Async version of :meth:`_extract_prompt_components`.\"\"\"\n        if isinstance(inputs, str):\n            prompt = inputs\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt,\n                request_id=request_id,\n                lora_request=lora_request,\n            )\n            multi_modal_data = None\n        elif isinstance(inputs, dict):\n            if \"prompt_token_ids\" in inputs:\n                prompt = None\n                prompt_token_ids = inputs[\"prompt_token_ids\"]\n            else:\n                # NOTE: This extra assignment is required to pass mypy\n                prompt = parsed_prompt = inputs[\"prompt\"]\n                prompt_token_ids = await self._tokenize_prompt_async(\n                    parsed_prompt,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                )\n\n            multi_modal_data = inputs.get(\"multi_modal_data\")\n        else:\n            assert_never(inputs)\n\n        return prompt, prompt_token_ids, multi_modal_data\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n    ) -> EncoderDecoderLLMInputs:\n        \"\"\"Async version of :meth:`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_comps: PromptComponents\n        decoder_comps: DecoderPromptComponents\n\n        if is_explicit_encoder_decoder_prompt(inputs):\n            encoder_task = self._extract_prompt_components_async(\n                inputs[\"encoder_prompt\"],\n                request_id=request_id,\n            )\n\n            if (decoder_input := inputs[\"decoder_prompt\"]) is None:\n                encoder_comps = await encoder_task\n                decoder_comps = None, None, None\n            else:\n                decoder_task = self._extract_prompt_components_async(\n                    decoder_input,\n                    request_id=request_id,\n                )\n\n                encoder_comps, decoder_comps = await asyncio.gather(\n                    encoder_task, decoder_task)\n        else:\n            encoder_comps = await self._extract_prompt_components_async(\n                inputs,\n                request_id=request_id,\n            )\n\n            decoder_comps = None, None, None\n\n        return self._build_enc_dec_llm_inputs(encoder_comps, decoder_comps)\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> LLMInputs:\n        \"\"\"Async version of :meth:`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._extract_prompt_components_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def process_model_inputs_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> Union[LLMInputs, EncoderDecoderLLMInputs]:\n        \"\"\"Async version of :meth:`process_model_inputs`.\"\"\"\n        if self.is_encoder_decoder_model():\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            model_inputs = await self._process_encoder_decoder_prompt_async(\n                inputs,\n                request_id=request_id,\n            )\n        else:\n            if is_explicit_encoder_decoder_prompt(inputs):\n                raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                                 \"to decoder-only models\")\n\n            # Decoder-only operation\n            model_inputs = await self._process_decoder_only_prompt_async(\n                inputs,\n                request_id=request_id,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return self.input_processor(model_inputs)\n\n    async def add_request_async(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        \"\"\"Async version of :meth:`add_request`.\"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        processed_inputs = await self.process_model_inputs_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n        )\n\n    async def check_health_async(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for :class:`LLMEngine`.\n\n    This class is used to wrap the :class:`LLMEngine` class to make it\n    asynchronous. It uses asyncio to create a background loop that keeps\n    processing incoming requests. The :class:`LLMEngine` is kicked by the\n    generate method when there are requests in the waiting queue. The generate\n    method yields the outputs from the :class:`LLMEngine` to the caller.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for :class:`LLMEngine`.\n        **kwargs: Arguments for :class:`LLMEngine`.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.engine = self._init_engine(*args, **kwargs)\n\n        if self.engine_use_ray:\n            print_warning_once(\n                \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n                \"be removed in a future update. \"\n                \"See https://github.com/vllm-project/vllm/issues/7045.\")\n\n            if envs.VLLM_ALLOW_ENGINE_USE_RAY:\n                print_warning_once(\n                    \"VLLM_ALLOW_ENGINE_USE_RAY is set, force engine use Ray\")\n            else:\n                raise ValueError(\"`--engine-use-ray` is deprecated. \"\n                                 \"Set `VLLM_ALLOW_ENGINE_USE_RAY=1` to \"\n                                 \"force use it\")\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    @classmethod\n    def _get_executor_cls(\n            cls, engine_config: EngineConfig) -> Type[ExecutorAsyncBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorAsyncBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorAsyncBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutorAsync\n                executor_class = RayTPUExecutorAsync\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutorAsync\n                executor_class = TPUExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif engine_config.device_config.device_type == \"openvino\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with \"\n                \"the OpenVINO backend.\")\n            from vllm.executor.openvino_executor import OpenVINOExecutorAsync\n            executor_class = OpenVINOExecutorAsync\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend is None:\n                from vllm.executor.xpu_executor import XPUExecutorAsync\n                executor_class = XPUExecutorAsync\n            elif distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutorAsync\n                executor_class = RayXPUExecutorAsync\n            else:\n                raise RuntimeError(\n                    \"Not supported distributed execution model on XPU device.\")\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> \"PreTrainedTokenizer\":\n        if self.engine_use_ray:\n            return await self.engine.get_tokenizer.remote(  # type: ignore\n                lora_request)\n\n        return await (self.engine.get_tokenizer_group().\n                      get_lora_tokenizer_async(lora_request))\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_log_task_completion, error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def shutdown_background_loop(self) -> None:\n        \"\"\"\n        Shut down the background loop.\n\n        This method needs to be called during cleanup to remove\n        references to `self` and properly GC the resources held\n        by the async LLM engine (e.g., the executors as well as\n        their resources).\n        \"\"\"\n        if self._background_loop_unshielded is not None:\n            self._background_loop_unshielded.cancel()\n            self._background_loop_unshielded = None\n        self.background_loop = None\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = kwargs[\"cache_config\"]\n            parallel_config = kwargs[\"parallel_config\"]\n            if (parallel_config.tensor_parallel_size == 1\n                    and parallel_config.pipeline_parallel_size == 1):\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self, virtual_engine: int) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, aborted_requests = (\n            self._request_tracker.get_new_and_aborted_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            try:\n                if self.engine_use_ray:\n                    await self.engine.add_request.remote(  # type: ignore\n                        **new_request)\n                else:\n                    await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if aborted_requests:\n            await self._engine_abort(aborted_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await\n```\n# Function Signature: def get_new_and_aborted_requests(self) -> Tuple[List[Dict], Set[str]]:\n# Natural Language Annotations: \"\"\"Get the new requests and finished requests to be\n        sent to the engine.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_new_and_aborted_requests(self", "  ```\ndef get_new_and_aborted_requests(self", "  ```\ndef get_new_and_aborted_requests(self"]}, "vllm-vllm/engine/async_llm_engine.py-from_engine_args": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/async_llm_engine.py:\n```\nimport asyncio\nimport time\nfrom functools import partial\nfrom typing import (AsyncGenerator, Callable, Dict, Iterable, List, Mapping,\n                    Optional, Set, Tuple, Type, Union)\n\nfrom transformers import PreTrainedTokenizer\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, EngineConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_timeout import asyncio_timeout\nfrom vllm.engine.llm_engine import (DecoderPromptComponents, LLMEngine,\n                                    PromptComponents)\nfrom vllm.engine.metrics_types import StatLoggerBase\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster, ray\nfrom vllm.inputs import (EncoderDecoderLLMInputs, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import EmbeddingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import print_warning_once\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _log_task_completion(task: asyncio.Task,\n                         error_callback: Callable[[Exception], None]) -> None:\n    \"\"\"This function is only intended for the `engine.run_engine_loop()` task.\n\n    In particular, that task runs a `while True` loop that can only exit if\n    there is an exception.\n    \"\"\"\n\n    exception = None\n    try:\n        return_value = task.result()\n        raise AssertionError(\n            f\"The engine background task should never finish without an \"\n            f\"exception. {return_value}\")\n    except asyncio.exceptions.CancelledError:\n        # We assume that if the task is cancelled, we are gracefully shutting\n        # down. This should only happen on program exit.\n        logger.info(\"Engine is gracefully shutting down.\")\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            \"Task finished unexpectedly. This should never happen! \"\n            \"Please open an issue on Github. See stack trace above for the \"\n            \"actual cause.\") from e\n\n\nSTOP_ITERATION = Exception()  # Sentinel\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or EmbeddingRequestOutputs for a request\n    that can be iterated over asynchronously via an async generator.\"\"\"\n\n    def __init__(self, request_id: str, cancel: Callable[[str], None]) -> None:\n        self.request_id = request_id\n        self._cancel = cancel\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, EmbeddingRequestOutput,\n                              Exception]) -> None:\n        if self._finished:\n            return\n        self._queue.put_nowait(item)\n\n    def finish(\n        self,\n        exception: Optional[Union[BaseException, Type[BaseException]]] = None,\n    ) -> None:\n        if not self._finished:\n            self._finished = True\n            self._queue.put_nowait(\n                exception if exception is not None else STOP_ITERATION)\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    async def generator(\n        self\n    ) -> AsyncGenerator[Union[RequestOutput, EmbeddingRequestOutput], None]:\n        try:\n            while not self._finished:\n                result = await self._queue.get()\n                if isinstance(result, Exception):\n                    if result == STOP_ITERATION:\n                        return\n                    raise result\n                yield result\n        except GeneratorExit:\n            self._cancel(self.request_id)\n            raise asyncio.CancelledError from None\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._aborted_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self.abort_request(request_id, exception=exc)\n        else:\n            # NB: tuple() used here because self.abort_request pops the stream\n            # out of self._request_streams, so we can't iterate on it directly\n            for rid in tuple(self._request_streams.keys()):\n                self.abort_request(rid, exception=exc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: BaseException,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id, exception=exception)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    async def step_async(\n        self, virtual_engine: int\n    ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler[\n            virtual_engine].schedule()\n\n        if not scheduler_outputs.is_empty():\n            # Execute the model.\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                virtual_engine=virtual_engine,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids)\n            output = await self.model_executor.execute_model_async(\n                execute_model_req)\n        else:\n            output = []\n\n        request_outputs = self._process_model_outputs(\n            output, scheduler_outputs.scheduled_seq_groups,\n            scheduler_outputs.ignored_seq_groups, seq_group_metadata_list)\n\n        # Log stats.\n        self.do_log_stats(scheduler_outputs, output)\n\n        # Tracing\n        self.do_tracing(scheduler_outputs)\n\n        return request_outputs\n\n    async def stop_remote_worker_execution_loop_async(self) -> None:\n        \"\"\"Stop the remote worker execution loop.\"\"\"\n        await self.model_executor.stop_remote_worker_execution_loop_async()\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        request_id: str,\n        lora_request: Optional[LoRARequest],\n    ) -> List[int]:\n        \"\"\"Async version of :meth:`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group(\"prompts must be None if \"\n                                             \"skip_tokenizer_init is True\")\n\n        return await tokenizer.encode_async(request_id=request_id,\n                                            prompt=prompt,\n                                            lora_request=lora_request)\n\n    async def _extract_prompt_components_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> PromptComponents:\n        \"\"\"Async version of :meth:`_extract_prompt_components`.\"\"\"\n        if isinstance(inputs, str):\n            prompt = inputs\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt,\n                request_id=request_id,\n                lora_request=lora_request,\n            )\n            multi_modal_data = None\n        elif isinstance(inputs, dict):\n            if \"prompt_token_ids\" in inputs:\n                prompt = None\n                prompt_token_ids = inputs[\"prompt_token_ids\"]\n            else:\n                # NOTE: This extra assignment is required to pass mypy\n                prompt = parsed_prompt = inputs[\"prompt\"]\n                prompt_token_ids = await self._tokenize_prompt_async(\n                    parsed_prompt,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                )\n\n            multi_modal_data = inputs.get(\"multi_modal_data\")\n        else:\n            assert_never(inputs)\n\n        return prompt, prompt_token_ids, multi_modal_data\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n    ) -> EncoderDecoderLLMInputs:\n        \"\"\"Async version of :meth:`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_comps: PromptComponents\n        decoder_comps: DecoderPromptComponents\n\n        if is_explicit_encoder_decoder_prompt(inputs):\n            encoder_task = self._extract_prompt_components_async(\n                inputs[\"encoder_prompt\"],\n                request_id=request_id,\n            )\n\n            if (decoder_input := inputs[\"decoder_prompt\"]) is None:\n                encoder_comps = await encoder_task\n                decoder_comps = None, None, None\n            else:\n                decoder_task = self._extract_prompt_components_async(\n                    decoder_input,\n                    request_id=request_id,\n                )\n\n                encoder_comps, decoder_comps = await asyncio.gather(\n                    encoder_task, decoder_task)\n        else:\n            encoder_comps = await self._extract_prompt_components_async(\n                inputs,\n                request_id=request_id,\n            )\n\n            decoder_comps = None, None, None\n\n        return self._build_enc_dec_llm_inputs(encoder_comps, decoder_comps)\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> LLMInputs:\n        \"\"\"Async version of :meth:`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._extract_prompt_components_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def process_model_inputs_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> Union[LLMInputs, EncoderDecoderLLMInputs]:\n        \"\"\"Async version of :meth:`process_model_inputs`.\"\"\"\n        if self.is_encoder_decoder_model():\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            model_inputs = await self._process_encoder_decoder_prompt_async(\n                inputs,\n                request_id=request_id,\n            )\n        else:\n            if is_explicit_encoder_decoder_prompt(inputs):\n                raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                                 \"to decoder-only models\")\n\n            # Decoder-only operation\n            model_inputs = await self._process_decoder_only_prompt_async(\n                inputs,\n                request_id=request_id,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return self.input_processor(model_inputs)\n\n    async def add_request_async(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        \"\"\"Async version of :meth:`add_request`.\"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        processed_inputs = await self.process_model_inputs_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n        )\n\n    async def check_health_async(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for :class:`LLMEngine`.\n\n    This class is used to wrap the :class:`LLMEngine` class to make it\n    asynchronous. It uses asyncio to create a background loop that keeps\n    processing incoming requests. The :class:`LLMEngine` is kicked by the\n    generate method when there are requests in the waiting queue. The generate\n    method yields the outputs from the :class:`LLMEngine` to the caller.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for :class:`LLMEngine`.\n        **kwargs: Arguments for :class:`LLMEngine`.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.engine = self._init_engine(*args, **kwargs)\n\n        if self.engine_use_ray:\n            print_warning_once(\n                \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n                \"be removed in a future update. \"\n                \"See https://github.com/vllm-project/vllm/issues/7045.\")\n\n            if envs.VLLM_ALLOW_ENGINE_USE_RAY:\n                print_warning_once(\n                    \"VLLM_ALLOW_ENGINE_USE_RAY is set, force engine use Ray\")\n            else:\n                raise ValueError(\"`--engine-use-ray` is deprecated. \"\n                                 \"Set `VLLM_ALLOW_ENGINE_USE_RAY=1` to \"\n                                 \"force use it\")\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    @classmethod\n    def _get_executor_cls(\n            cls, engine_config: EngineConfig) -> Type[ExecutorAsyncBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorAsyncBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorAsyncBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutorAsync\n                executor_class = RayTPUExecutorAsync\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutorAsync\n                executor_class = TPUExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif engine_config.device_config.device_type == \"openvino\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with \"\n                \"the OpenVINO backend.\")\n            from vllm.executor.openvino_executor import OpenVINOExecutorAsync\n            executor_class = OpenVINOExecutorAsync\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend is None:\n                from vllm.executor.xpu_executor import XPUExecutorAsync\n                executor_class = XPUExecutorAsync\n            elif distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutorAsync\n                executor_class = RayXPUExecutorAsync\n            else:\n                raise RuntimeError(\n                    \"Not supported distributed execution model on XPU device.\")\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> \"PreTrainedTokenizer\":\n        if self.engine_use_ray:\n            return await self.engine.get_tokenizer.remote(  # type: ignore\n                lora_request)\n\n        return await (self.engine.get_tokenizer_group().\n                      get_lora_tokenizer_async(lora_request))\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_log_task_completion, error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def shutdown_background_loop(self) -> None:\n        \"\"\"\n        Shut down the background loop.\n\n        This method needs to be called during cleanup to remove\n        references to `self` and properly GC the resources held\n        by the async LLM engine (e.g., the executors as well as\n        their resources).\n        \"\"\"\n        if self._background_loop_unshielded is not None:\n            self._background_loop_unshielded.cancel()\n            self._background_loop_unshielded = None\n        self.background_loop = None\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = kwargs[\"cache_config\"]\n            parallel_config = kwargs[\"parallel_config\"]\n            if (parallel_config.tensor_parallel_size == 1\n                    and parallel_config.pipeline_parallel_size == 1):\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self, virtual_engine: int) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, aborted_requests = (\n            self._request_tracker.get_new_and_aborted_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            try:\n                if self.engine_use_ray:\n                    await self.engine.add_request.remote(  # type: ignore\n                        **new_request)\n                else:\n                    await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if aborted_requests:\n            await self._engine_abort(aborted_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await\n```\n# Function Signature: def from_engine_args(\n# Natural Language Annotations: \"\"\"Creates an async LLM engine from the engine arguments.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport asyncio\nimport time\nfrom functools import", "  ```\nclass AsyncLLMEngine():\n      def __init__(", "  ```\ndef from_engine_args(engine_args: AsyncEngine"]}, "vllm-vllm/engine/async_llm_engine.py-generate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/async_llm_engine.py:\n```\nimport asyncio\nimport time\nfrom functools import partial\nfrom typing import (AsyncGenerator, Callable, Dict, Iterable, List, Mapping,\n                    Optional, Set, Tuple, Type, Union)\n\nfrom transformers import PreTrainedTokenizer\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, EngineConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_timeout import asyncio_timeout\nfrom vllm.engine.llm_engine import (DecoderPromptComponents, LLMEngine,\n                                    PromptComponents)\nfrom vllm.engine.metrics_types import StatLoggerBase\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster, ray\nfrom vllm.inputs import (EncoderDecoderLLMInputs, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import EmbeddingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import print_warning_once\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _log_task_completion(task: asyncio.Task,\n                         error_callback: Callable[[Exception], None]) -> None:\n    \"\"\"This function is only intended for the `engine.run_engine_loop()` task.\n\n    In particular, that task runs a `while True` loop that can only exit if\n    there is an exception.\n    \"\"\"\n\n    exception = None\n    try:\n        return_value = task.result()\n        raise AssertionError(\n            f\"The engine background task should never finish without an \"\n            f\"exception. {return_value}\")\n    except asyncio.exceptions.CancelledError:\n        # We assume that if the task is cancelled, we are gracefully shutting\n        # down. This should only happen on program exit.\n        logger.info(\"Engine is gracefully shutting down.\")\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            \"Task finished unexpectedly. This should never happen! \"\n            \"Please open an issue on Github. See stack trace above for the \"\n            \"actual cause.\") from e\n\n\nSTOP_ITERATION = Exception()  # Sentinel\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or EmbeddingRequestOutputs for a request\n    that can be iterated over asynchronously via an async generator.\"\"\"\n\n    def __init__(self, request_id: str, cancel: Callable[[str], None]) -> None:\n        self.request_id = request_id\n        self._cancel = cancel\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, EmbeddingRequestOutput,\n                              Exception]) -> None:\n        if self._finished:\n            return\n        self._queue.put_nowait(item)\n\n    def finish(\n        self,\n        exception: Optional[Union[BaseException, Type[BaseException]]] = None,\n    ) -> None:\n        if not self._finished:\n            self._finished = True\n            self._queue.put_nowait(\n                exception if exception is not None else STOP_ITERATION)\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    async def generator(\n        self\n    ) -> AsyncGenerator[Union[RequestOutput, EmbeddingRequestOutput], None]:\n        try:\n            while not self._finished:\n                result = await self._queue.get()\n                if isinstance(result, Exception):\n                    if result == STOP_ITERATION:\n                        return\n                    raise result\n                yield result\n        except GeneratorExit:\n            self._cancel(self.request_id)\n            raise asyncio.CancelledError from None\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._aborted_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self.abort_request(request_id, exception=exc)\n        else:\n            # NB: tuple() used here because self.abort_request pops the stream\n            # out of self._request_streams, so we can't iterate on it directly\n            for rid in tuple(self._request_streams.keys()):\n                self.abort_request(rid, exception=exc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: BaseException,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id, exception=exception)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    async def step_async(\n        self, virtual_engine: int\n    ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler[\n            virtual_engine].schedule()\n\n        if not scheduler_outputs.is_empty():\n            # Execute the model.\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                virtual_engine=virtual_engine,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids)\n            output = await self.model_executor.execute_model_async(\n                execute_model_req)\n        else:\n            output = []\n\n        request_outputs = self._process_model_outputs(\n            output, scheduler_outputs.scheduled_seq_groups,\n            scheduler_outputs.ignored_seq_groups, seq_group_metadata_list)\n\n        # Log stats.\n        self.do_log_stats(scheduler_outputs, output)\n\n        # Tracing\n        self.do_tracing(scheduler_outputs)\n\n        return request_outputs\n\n    async def stop_remote_worker_execution_loop_async(self) -> None:\n        \"\"\"Stop the remote worker execution loop.\"\"\"\n        await self.model_executor.stop_remote_worker_execution_loop_async()\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        request_id: str,\n        lora_request: Optional[LoRARequest],\n    ) -> List[int]:\n        \"\"\"Async version of :meth:`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group(\"prompts must be None if \"\n                                             \"skip_tokenizer_init is True\")\n\n        return await tokenizer.encode_async(request_id=request_id,\n                                            prompt=prompt,\n                                            lora_request=lora_request)\n\n    async def _extract_prompt_components_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> PromptComponents:\n        \"\"\"Async version of :meth:`_extract_prompt_components`.\"\"\"\n        if isinstance(inputs, str):\n            prompt = inputs\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt,\n                request_id=request_id,\n                lora_request=lora_request,\n            )\n            multi_modal_data = None\n        elif isinstance(inputs, dict):\n            if \"prompt_token_ids\" in inputs:\n                prompt = None\n                prompt_token_ids = inputs[\"prompt_token_ids\"]\n            else:\n                # NOTE: This extra assignment is required to pass mypy\n                prompt = parsed_prompt = inputs[\"prompt\"]\n                prompt_token_ids = await self._tokenize_prompt_async(\n                    parsed_prompt,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                )\n\n            multi_modal_data = inputs.get(\"multi_modal_data\")\n        else:\n            assert_never(inputs)\n\n        return prompt, prompt_token_ids, multi_modal_data\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n    ) -> EncoderDecoderLLMInputs:\n        \"\"\"Async version of :meth:`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_comps: PromptComponents\n        decoder_comps: DecoderPromptComponents\n\n        if is_explicit_encoder_decoder_prompt(inputs):\n            encoder_task = self._extract_prompt_components_async(\n                inputs[\"encoder_prompt\"],\n                request_id=request_id,\n            )\n\n            if (decoder_input := inputs[\"decoder_prompt\"]) is None:\n                encoder_comps = await encoder_task\n                decoder_comps = None, None, None\n            else:\n                decoder_task = self._extract_prompt_components_async(\n                    decoder_input,\n                    request_id=request_id,\n                )\n\n                encoder_comps, decoder_comps = await asyncio.gather(\n                    encoder_task, decoder_task)\n        else:\n            encoder_comps = await self._extract_prompt_components_async(\n                inputs,\n                request_id=request_id,\n            )\n\n            decoder_comps = None, None, None\n\n        return self._build_enc_dec_llm_inputs(encoder_comps, decoder_comps)\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> LLMInputs:\n        \"\"\"Async version of :meth:`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._extract_prompt_components_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def process_model_inputs_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> Union[LLMInputs, EncoderDecoderLLMInputs]:\n        \"\"\"Async version of :meth:`process_model_inputs`.\"\"\"\n        if self.is_encoder_decoder_model():\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            model_inputs = await self._process_encoder_decoder_prompt_async(\n                inputs,\n                request_id=request_id,\n            )\n        else:\n            if is_explicit_encoder_decoder_prompt(inputs):\n                raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                                 \"to decoder-only models\")\n\n            # Decoder-only operation\n            model_inputs = await self._process_decoder_only_prompt_async(\n                inputs,\n                request_id=request_id,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return self.input_processor(model_inputs)\n\n    async def add_request_async(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        \"\"\"Async version of :meth:`add_request`.\"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        processed_inputs = await self.process_model_inputs_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n        )\n\n    async def check_health_async(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for :class:`LLMEngine`.\n\n    This class is used to wrap the :class:`LLMEngine` class to make it\n    asynchronous. It uses asyncio to create a background loop that keeps\n    processing incoming requests. The :class:`LLMEngine` is kicked by the\n    generate method when there are requests in the waiting queue. The generate\n    method yields the outputs from the :class:`LLMEngine` to the caller.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for :class:`LLMEngine`.\n        **kwargs: Arguments for :class:`LLMEngine`.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.engine = self._init_engine(*args, **kwargs)\n\n        if self.engine_use_ray:\n            print_warning_once(\n                \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n                \"be removed in a future update. \"\n                \"See https://github.com/vllm-project/vllm/issues/7045.\")\n\n            if envs.VLLM_ALLOW_ENGINE_USE_RAY:\n                print_warning_once(\n                    \"VLLM_ALLOW_ENGINE_USE_RAY is set, force engine use Ray\")\n            else:\n                raise ValueError(\"`--engine-use-ray` is deprecated. \"\n                                 \"Set `VLLM_ALLOW_ENGINE_USE_RAY=1` to \"\n                                 \"force use it\")\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    @classmethod\n    def _get_executor_cls(\n            cls, engine_config: EngineConfig) -> Type[ExecutorAsyncBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorAsyncBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorAsyncBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutorAsync\n                executor_class = RayTPUExecutorAsync\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutorAsync\n                executor_class = TPUExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif engine_config.device_config.device_type == \"openvino\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with \"\n                \"the OpenVINO backend.\")\n            from vllm.executor.openvino_executor import OpenVINOExecutorAsync\n            executor_class = OpenVINOExecutorAsync\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend is None:\n                from vllm.executor.xpu_executor import XPUExecutorAsync\n                executor_class = XPUExecutorAsync\n            elif distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutorAsync\n                executor_class = RayXPUExecutorAsync\n            else:\n                raise RuntimeError(\n                    \"Not supported distributed execution model on XPU device.\")\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> \"PreTrainedTokenizer\":\n        if self.engine_use_ray:\n            return await self.engine.get_tokenizer.remote(  # type: ignore\n                lora_request)\n\n        return await (self.engine.get_tokenizer_group().\n                      get_lora_tokenizer_async(lora_request))\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_log_task_completion, error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def shutdown_background_loop(self) -> None:\n        \"\"\"\n        Shut down the background loop.\n\n        This method needs to be called during cleanup to remove\n        references to `self` and properly GC the resources held\n        by the async LLM engine (e.g., the executors as well as\n        their resources).\n        \"\"\"\n        if self._background_loop_unshielded is not None:\n            self._background_loop_unshielded.cancel()\n            self._background_loop_unshielded = None\n        self.background_loop = None\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = kwargs[\"cache_config\"]\n            parallel_config = kwargs[\"parallel_config\"]\n            if (parallel_config.tensor_parallel_size == 1\n                    and parallel_config.pipeline_parallel_size == 1):\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self, virtual_engine: int) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, aborted_requests = (\n            self._request_tracker.get_new_and_aborted_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            try:\n                if self.engine_use_ray:\n                    await self.engine.add_request.remote(  # type: ignore\n                        **new_request)\n                else:\n                    await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if aborted_requests:\n            await self._engine_abort(aborted_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await\n```\n# Function Signature: async def generate(\n# Natural Language Annotations: \"\"\"Generate outputs for a request.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            inputs: The inputs to the LLM. See\n                :class:`~vllm.inputs.PromptInputs`\n                for more details about the format of each input.\n            sampling_params: The sampling parameters of the request.\n            request_id: The unique id of the request.\n            lora_request: LoRA request to use for generation, if any.\n            trace_headers: OpenTelemetry trace headers.\n            prompt_adapter_request: Prompt Adapter request to use \n                                            for generation, if any.\n\n        Yields:\n            The output `RequestOutput` objects from the LLMEngine\n            for the request.\n\n        Details:\n            - If the engine is not running, start the background loop,\n              which iteratively invokes\n              :meth:`~vllm.engine.async_llm_engine.AsyncLLMEngine.engine_step`\n              to process the waiting requests.\n            - Add the request to the engine's `RequestTracker`.\n              On the next background loop, this request will be sent to\n              the underlying engine.\n              Also, a corresponding `AsyncStream` will be created.\n            - Wait for the request outputs from `AsyncStream` and yield them.\n\n        Example:\n            >>> # Please refer to entrypoints/api_server.py for\n            >>> # the complete example.\n            >>>\n            >>> # initialize the engine and the example input\n            >>> engine = AsyncLLMEngine.from_engine_args(engine_args)\n            >>> example_input = {\n            >>>     \"prompt\": \"What is LLM?\",\n            >>>     \"stream\": False, # assume the non-streaming case\n            >>>     \"temperature\": 0.0,\n            >>>     \"request_id\": 0,\n            >>> }\n            >>>\n            >>> # start the generation\n            >>> results_generator = engine.generate(\n            >>>    example_input[\"prompt\"],\n            >>>    SamplingParams(temperature=example_input[\"temperature\"]),\n            >>>    example_input[\"request_id\"])\n            >>>\n            >>> # get the results\n            >>> final_output = None\n            >>> async for request_output in results_generator:\n            >>>     if await request.is_disconnected():\n            >>>         # Abort the request if the client disconnects.\n            >>>         await engine.abort(request_id)\n            >>>         # Return or raise an error\n            >>>         ...\n            >>>     final_output = request_output\n            >>>\n            >>> # Process and return the final output\n            >>> ...\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def generate(\n    self,\n    inputs: Prom", "  ```\ndef generate(\n        self,\n        inputs: Prompt", "  ```\nasync def generate(self, inputs: PromptInputs,"]}, "vllm-vllm/engine/async_llm_engine.py-encode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/async_llm_engine.py:\n```\nimport asyncio\nimport time\nfrom functools import partial\nfrom typing import (AsyncGenerator, Callable, Dict, Iterable, List, Mapping,\n                    Optional, Set, Tuple, Type, Union)\n\nfrom transformers import PreTrainedTokenizer\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, EngineConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_timeout import asyncio_timeout\nfrom vllm.engine.llm_engine import (DecoderPromptComponents, LLMEngine,\n                                    PromptComponents)\nfrom vllm.engine.metrics_types import StatLoggerBase\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster, ray\nfrom vllm.inputs import (EncoderDecoderLLMInputs, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import EmbeddingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import print_warning_once\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _log_task_completion(task: asyncio.Task,\n                         error_callback: Callable[[Exception], None]) -> None:\n    \"\"\"This function is only intended for the `engine.run_engine_loop()` task.\n\n    In particular, that task runs a `while True` loop that can only exit if\n    there is an exception.\n    \"\"\"\n\n    exception = None\n    try:\n        return_value = task.result()\n        raise AssertionError(\n            f\"The engine background task should never finish without an \"\n            f\"exception. {return_value}\")\n    except asyncio.exceptions.CancelledError:\n        # We assume that if the task is cancelled, we are gracefully shutting\n        # down. This should only happen on program exit.\n        logger.info(\"Engine is gracefully shutting down.\")\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            \"Task finished unexpectedly. This should never happen! \"\n            \"Please open an issue on Github. See stack trace above for the \"\n            \"actual cause.\") from e\n\n\nSTOP_ITERATION = Exception()  # Sentinel\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or EmbeddingRequestOutputs for a request\n    that can be iterated over asynchronously via an async generator.\"\"\"\n\n    def __init__(self, request_id: str, cancel: Callable[[str], None]) -> None:\n        self.request_id = request_id\n        self._cancel = cancel\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, EmbeddingRequestOutput,\n                              Exception]) -> None:\n        if self._finished:\n            return\n        self._queue.put_nowait(item)\n\n    def finish(\n        self,\n        exception: Optional[Union[BaseException, Type[BaseException]]] = None,\n    ) -> None:\n        if not self._finished:\n            self._finished = True\n            self._queue.put_nowait(\n                exception if exception is not None else STOP_ITERATION)\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    async def generator(\n        self\n    ) -> AsyncGenerator[Union[RequestOutput, EmbeddingRequestOutput], None]:\n        try:\n            while not self._finished:\n                result = await self._queue.get()\n                if isinstance(result, Exception):\n                    if result == STOP_ITERATION:\n                        return\n                    raise result\n                yield result\n        except GeneratorExit:\n            self._cancel(self.request_id)\n            raise asyncio.CancelledError from None\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._aborted_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self.abort_request(request_id, exception=exc)\n        else:\n            # NB: tuple() used here because self.abort_request pops the stream\n            # out of self._request_streams, so we can't iterate on it directly\n            for rid in tuple(self._request_streams.keys()):\n                self.abort_request(rid, exception=exc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: BaseException,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id, exception=exception)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    async def step_async(\n        self, virtual_engine: int\n    ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler[\n            virtual_engine].schedule()\n\n        if not scheduler_outputs.is_empty():\n            # Execute the model.\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                virtual_engine=virtual_engine,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids)\n            output = await self.model_executor.execute_model_async(\n                execute_model_req)\n        else:\n            output = []\n\n        request_outputs = self._process_model_outputs(\n            output, scheduler_outputs.scheduled_seq_groups,\n            scheduler_outputs.ignored_seq_groups, seq_group_metadata_list)\n\n        # Log stats.\n        self.do_log_stats(scheduler_outputs, output)\n\n        # Tracing\n        self.do_tracing(scheduler_outputs)\n\n        return request_outputs\n\n    async def stop_remote_worker_execution_loop_async(self) -> None:\n        \"\"\"Stop the remote worker execution loop.\"\"\"\n        await self.model_executor.stop_remote_worker_execution_loop_async()\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        request_id: str,\n        lora_request: Optional[LoRARequest],\n    ) -> List[int]:\n        \"\"\"Async version of :meth:`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group(\"prompts must be None if \"\n                                             \"skip_tokenizer_init is True\")\n\n        return await tokenizer.encode_async(request_id=request_id,\n                                            prompt=prompt,\n                                            lora_request=lora_request)\n\n    async def _extract_prompt_components_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> PromptComponents:\n        \"\"\"Async version of :meth:`_extract_prompt_components`.\"\"\"\n        if isinstance(inputs, str):\n            prompt = inputs\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt,\n                request_id=request_id,\n                lora_request=lora_request,\n            )\n            multi_modal_data = None\n        elif isinstance(inputs, dict):\n            if \"prompt_token_ids\" in inputs:\n                prompt = None\n                prompt_token_ids = inputs[\"prompt_token_ids\"]\n            else:\n                # NOTE: This extra assignment is required to pass mypy\n                prompt = parsed_prompt = inputs[\"prompt\"]\n                prompt_token_ids = await self._tokenize_prompt_async(\n                    parsed_prompt,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                )\n\n            multi_modal_data = inputs.get(\"multi_modal_data\")\n        else:\n            assert_never(inputs)\n\n        return prompt, prompt_token_ids, multi_modal_data\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n    ) -> EncoderDecoderLLMInputs:\n        \"\"\"Async version of :meth:`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_comps: PromptComponents\n        decoder_comps: DecoderPromptComponents\n\n        if is_explicit_encoder_decoder_prompt(inputs):\n            encoder_task = self._extract_prompt_components_async(\n                inputs[\"encoder_prompt\"],\n                request_id=request_id,\n            )\n\n            if (decoder_input := inputs[\"decoder_prompt\"]) is None:\n                encoder_comps = await encoder_task\n                decoder_comps = None, None, None\n            else:\n                decoder_task = self._extract_prompt_components_async(\n                    decoder_input,\n                    request_id=request_id,\n                )\n\n                encoder_comps, decoder_comps = await asyncio.gather(\n                    encoder_task, decoder_task)\n        else:\n            encoder_comps = await self._extract_prompt_components_async(\n                inputs,\n                request_id=request_id,\n            )\n\n            decoder_comps = None, None, None\n\n        return self._build_enc_dec_llm_inputs(encoder_comps, decoder_comps)\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> LLMInputs:\n        \"\"\"Async version of :meth:`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._extract_prompt_components_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def process_model_inputs_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> Union[LLMInputs, EncoderDecoderLLMInputs]:\n        \"\"\"Async version of :meth:`process_model_inputs`.\"\"\"\n        if self.is_encoder_decoder_model():\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            model_inputs = await self._process_encoder_decoder_prompt_async(\n                inputs,\n                request_id=request_id,\n            )\n        else:\n            if is_explicit_encoder_decoder_prompt(inputs):\n                raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                                 \"to decoder-only models\")\n\n            # Decoder-only operation\n            model_inputs = await self._process_decoder_only_prompt_async(\n                inputs,\n                request_id=request_id,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return self.input_processor(model_inputs)\n\n    async def add_request_async(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        \"\"\"Async version of :meth:`add_request`.\"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        processed_inputs = await self.process_model_inputs_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n        )\n\n    async def check_health_async(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for :class:`LLMEngine`.\n\n    This class is used to wrap the :class:`LLMEngine` class to make it\n    asynchronous. It uses asyncio to create a background loop that keeps\n    processing incoming requests. The :class:`LLMEngine` is kicked by the\n    generate method when there are requests in the waiting queue. The generate\n    method yields the outputs from the :class:`LLMEngine` to the caller.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for :class:`LLMEngine`.\n        **kwargs: Arguments for :class:`LLMEngine`.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.engine = self._init_engine(*args, **kwargs)\n\n        if self.engine_use_ray:\n            print_warning_once(\n                \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n                \"be removed in a future update. \"\n                \"See https://github.com/vllm-project/vllm/issues/7045.\")\n\n            if envs.VLLM_ALLOW_ENGINE_USE_RAY:\n                print_warning_once(\n                    \"VLLM_ALLOW_ENGINE_USE_RAY is set, force engine use Ray\")\n            else:\n                raise ValueError(\"`--engine-use-ray` is deprecated. \"\n                                 \"Set `VLLM_ALLOW_ENGINE_USE_RAY=1` to \"\n                                 \"force use it\")\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    @classmethod\n    def _get_executor_cls(\n            cls, engine_config: EngineConfig) -> Type[ExecutorAsyncBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorAsyncBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorAsyncBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutorAsync\n                executor_class = RayTPUExecutorAsync\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutorAsync\n                executor_class = TPUExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif engine_config.device_config.device_type == \"openvino\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with \"\n                \"the OpenVINO backend.\")\n            from vllm.executor.openvino_executor import OpenVINOExecutorAsync\n            executor_class = OpenVINOExecutorAsync\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend is None:\n                from vllm.executor.xpu_executor import XPUExecutorAsync\n                executor_class = XPUExecutorAsync\n            elif distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutorAsync\n                executor_class = RayXPUExecutorAsync\n            else:\n                raise RuntimeError(\n                    \"Not supported distributed execution model on XPU device.\")\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        return executor_class\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> \"PreTrainedTokenizer\":\n        if self.engine_use_ray:\n            return await self.engine.get_tokenizer.remote(  # type: ignore\n                lora_request)\n\n        return await (self.engine.get_tokenizer_group().\n                      get_lora_tokenizer_async(lora_request))\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_log_task_completion, error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def shutdown_background_loop(self) -> None:\n        \"\"\"\n        Shut down the background loop.\n\n        This method needs to be called during cleanup to remove\n        references to `self` and properly GC the resources held\n        by the async LLM engine (e.g., the executors as well as\n        their resources).\n        \"\"\"\n        if self._background_loop_unshielded is not None:\n            self._background_loop_unshielded.cancel()\n            self._background_loop_unshielded = None\n        self.background_loop = None\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = kwargs[\"cache_config\"]\n            parallel_config = kwargs[\"parallel_config\"]\n            if (parallel_config.tensor_parallel_size == 1\n                    and parallel_config.pipeline_parallel_size == 1):\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self, virtual_engine: int) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, aborted_requests = (\n            self._request_tracker.get_new_and_aborted_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            try:\n                if self.engine_use_ray:\n                    await self.engine.add_request.remote(  # type: ignore\n                        **new_request)\n                else:\n                    await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if aborted_requests:\n            await self._engine_abort(aborted_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await\n```\n# Function Signature: async def encode(\n# Natural Language Annotations: \"\"\"Generate outputs for a request from an embedding model.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            inputs: The inputs to the LLM. See\n                :class:`~vllm.inputs.PromptInputs`\n                for more details about the format of each input.\n            pooling_params: The pooling parameters of the request.\n            request_id: The unique id of the request.\n            lora_request: LoRA request to use for generation, if any.\n            trace_headers: OpenTelemetry trace headers.\n\n        Yields:\n            The output `EmbeddingRequestOutput` objects from the LLMEngine\n            for the request.\n\n        Details:\n            - If the engine is not running, start the background loop,\n              which iteratively invokes\n              :meth:`~vllm.engine.async_llm_engine.AsyncLLMEngine.engine_step`\n              to process the waiting requests.\n            - Add the request to the engine's `RequestTracker`.\n              On the next background loop, this request will be sent to\n              the underlying engine.\n              Also, a corresponding `AsyncStream` will be created.\n            - Wait for the request outputs from `AsyncStream` and yield them.\n\n        Example:\n            >>> # Please refer to entrypoints/api_server.py for\n            >>> # the complete example.\n            >>>\n            >>> # initialize the engine and the example input\n            >>> engine = AsyncLLMEngine.from_engine_args(engine_args)\n            >>> example_input = {\n            >>>     \"input\": \"What is LLM?\",\n            >>>     \"request_id\": 0,\n            >>> }\n            >>>\n            >>> # start the generation\n            >>> results_generator = engine.encode(\n            >>>    example_input[\"input\"],\n            >>>    PoolingParams(),\n            >>>    example_input[\"request_id\"])\n            >>>\n            >>> # get the results\n            >>> final_output = None\n            >>> async for request_output in results_generator:\n            >>>     if await request.is_disconnected():\n            >>>         # Abort the request if the client disconnects.\n            >>>         await engine.abort(request_id)\n            >>>         # Return or raise an error\n            >>>         ...\n            >>>     final_output = request_output\n            >>>\n            >>> # Process and return the final output\n            >>> ...\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def encode(\n    self,\n    inputs: Pro", "  ```\nasync def encode(\n    self,\n    inputs: Prom", "  ```\nasync def encode(\n    self,\n    inputs: Prom"]}, "vllm-vllm/engine/output_processor/stop_checker.py-maybe_stop_sequence": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/output_processor/stop_checker.py:\n```\nfrom typing import Callable, Optional\n\nfrom transformers import PreTrainedTokenizer\n\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import Sequence, SequenceStatus\n\n\nclass StopChecker:\n    \"\"\"LLMEngine helper class which separates out the logic involving stop\n    checking. This checks things such as: whether the eos token was emitted,\n    whether the max_tokens has been consumed, whether a stop string has been\n    emitted, or if we have exceeded the max model len.\n    \"\"\"\n\n    def __init__(self, max_model_len: int,\n                 get_tokenizer_for_seq: Callable[[Sequence],\n                                                 PreTrainedTokenizer]):\n        # Do not use it directly, but use `self._get_max_model_len`.\n        self._max_model_len = max_model_len\n        self.get_tokenizer_for_seq = get_tokenizer_for_seq\n\n    def _get_max_model_len(self, lora_req: Optional[LoRARequest]):\n        if lora_req and lora_req.long_lora_max_len:\n            return lora_req.long_lora_max_len\n        else:\n            return self._max_model_len\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @staticmethod\n    def _check_stop_strings(seq: Sequence, new_char_count: int,\n                            sampling_params: SamplingParams) -> Optional[str]:\n        \"\"\"Check if any stop strings are matched and truncate sequence\n        output text accordingly.\n\n        Returns the stop string if matched or else None.\n        \"\"\"\n        if not new_char_count:\n            return None\n\n        for stop_str in sampling_params.stop:\n            stop_string_len = len(stop_str)\n            # Avoid searching already-searched text.\n            stop_index = seq.output_text.find(\n                stop_str, -new_char_count - stop_string_len)\n            if stop_index == -1:\n                continue\n\n            if sampling_params.include_stop_str_in_output:\n                # Truncate to end of stop string.\n                stop_index += stop_string_len\n                if stop_index >= len(seq.output_text):\n                    # No truncation required.\n                    return stop_str\n\n            # Truncate the output text to either the beginning\n            # or end of the stop string.\n            seq.output_text = seq.output_text[:stop_index]\n            return stop_str\n        return None\n\n```\n# Function Signature: def maybe_stop_sequence(\n# Natural Language Annotations: \"\"\"Stop the finished sequences.\n\n       new_char_count is the number of chars added to the\n           sequence's output text for the newly generated token\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef maybe_stop_sequence(seq: Sequence, new", "  ```\nfrom typing import Optional\n\nfrom vllm.lora", "  ```\ndef maybe_stop_sequence(\n    seq: Sequence"]}, "vllm-vllm/engine/output_processor/multi_step.py-process_outputs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/engine/output_processor/multi_step.py:\n```\nimport functools\nfrom typing import Callable, List\n\nfrom transformers import PreTrainedTokenizer\n\nfrom vllm.core.scheduler import Scheduler\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.logger import init_logger\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (Sequence, SequenceGroup, SequenceGroupOutput,\n                           SequenceOutput, SequenceStatus)\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.utils import Counter\n\nlogger = init_logger(__name__)\n\n\nclass MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n    \"\"\"SequenceGroupOutputProcessor which handles logic related to\n    detokenization and stopping conditions. It specializes to \"multi-step\n    decoding\", where vLLM's worker may generate multiple tokens per invocation.\n    This is currently mutually exclusive with advanced sampling techniques like\n    beam search, which motivates the separation of this logic from the single\n    step output processor.\n\n    This class is responsible for things such as correctly appending all new\n    token ids to their sequence, detokenizing new token ids, truncating new\n    output tokens after an eos token, and correctly handling the case where the\n    number of new output tokens per sequence differs in a single batch.\n    \"\"\"\n\n    def __init__(\n        self,\n        detokenizer: Detokenizer,\n        scheduler: List[Scheduler],\n        seq_counter: Counter,\n        get_tokenizer_for_seq: Callable[[Sequence], PreTrainedTokenizer],\n        stop_checker: StopChecker,\n    ):\n        self.detokenizer = detokenizer\n        self.scheduler = scheduler\n        self.seq_counter = seq_counter\n        self.get_tokenizer_for_seq = get_tokenizer_for_seq\n        self.stop_checker = stop_checker\n\n    def process_prompt_logprob(self, seq_group: SequenceGroup,\n                               outputs: List[SequenceGroupOutput]) -> None:\n        # TODO(sang): Prompt logprob currently not implemented in multi step\n        # workers.\n        self._log_prompt_logprob_unsupported_warning_once()\n\n    @staticmethod\n    @functools.lru_cache()\n    def _log_prompt_logprob_unsupported_warning_once():\n        logger.warning(\n            \"Prompt logprob is not supported by multi step workers. \"\n            \"(e.g., speculative decode uses multi step workers).\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _process_seq_outputs(self, seq: Sequence,\n                             valid_samples: List[SequenceOutput],\n                             sampling_params: SamplingParams) -> None:\n        output_token_ids = [sample.output_token for sample in valid_samples]\n        output_logprobs = [sample.logprobs for sample in valid_samples]\n\n        # Truncate to max_tokens if necessary.\n        remaining_tokens = sampling_params.max_tokens - (seq.get_output_len() +\n                                                         len(output_token_ids))\n        if remaining_tokens < 0:\n            valid_samples = valid_samples[:remaining_tokens]\n            output_token_ids = output_token_ids[:remaining_tokens]\n\n        # Truncate any tokens after EOS. This is required as spec decode\n        # generates a fixed number of tokens without evaluating stopping\n        # conditions within the block. This can cause an eos token to be\n        # unintentionally ignored.\n        if not sampling_params.ignore_eos:\n            eos_token_id = self.get_tokenizer_for_seq(seq).eos_token_id\n            # Avoiding .index calls as exception throwing in the happy path\n            # is expensive.\n            for i in range(len(output_token_ids)):\n                if output_token_ids[i] == eos_token_id:\n                    output_token_ids = output_token_ids[:i + 1]\n                    valid_samples = valid_samples[:i + 1]\n                    break\n\n        # Incrementally append tokens to the sequence, as if we had only one new\n        # token.\n        for output_token_id, output_logprob in zip(output_token_ids,\n                                                   output_logprobs):\n            seq.append_token_id(\n                token_id=output_token_id,\n                logprobs=output_logprob,\n            )\n\n            new_char_count = 0\n            if sampling_params.detokenize:\n                new_char_count = self.detokenizer.decode_sequence_inplace(\n                    seq, sampling_params)\n\n            # TODO(sang): Support lora.\n            self.stop_checker.maybe_stop_sequence(\n                seq,\n                new_char_count=new_char_count,\n                sampling_params=sampling_params,\n            )\n            if seq.is_finished():\n                break\n\n        if seq.is_finished():\n            for scheduler in self.scheduler:\n                scheduler.free_seq(seq)\n\n```\n# Function Signature: def process_outputs(self, sequence_group: SequenceGroup,\n# Natural Language Annotations: \"\"\"Append new tokens in the outputs to sequences in the sequence group.\n\n        This only supports sequence groups of size 1. It supports greater than\n        one new token per sequence.\n\n        This applies logic like stop condition checking and detokenization,\n        including freeing finished sequences. It also handles cases where there\n        are tokens emitted after the EOS token.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef process_outputs(self, sequence_group: Se", "  ```\ndef process_outputs(self, sequence_group: Se", "  ```\ndef process_outputs(self, sequence_group: Se"]}, "vllm-vllm/lora/models.py-from_local_checkpoint": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/lora/models.py:\n```\nimport copy\nimport json\nimport math\nimport os\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Dict, List, Optional, Type\n\nimport safetensors.torch\nimport torch\nfrom torch import nn\n\nfrom vllm.adapter_commons.models import (AdapterLRUCache, AdapterModel,\n                                         AdapterModelManager)\nfrom vllm.adapter_commons.utils import (add_adapter, deactivate_adapter,\n                                        get_adapter, list_adapters,\n                                        remove_adapter, set_adapter_mapping)\nfrom vllm.config import LoRAConfig\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import (BaseLayerWithLoRA,\n                              LinearScalingRotaryEmbeddingWithLora,\n                              LoRAMapping)\nfrom vllm.lora.lora import LoRALayerWeights, PackedLoRALayerWeights\nfrom vllm.lora.punica import PunicaWrapper\nfrom vllm.lora.utils import (from_layer, from_layer_logits_processor,\n                             parse_fine_tuned_lora_name, replace_submodule)\nfrom vllm.model_executor.models.interfaces import SupportsLoRA\nfrom vllm.model_executor.models.utils import PPMissingLayer\nfrom vllm.utils import is_pin_memory_available\n\nlogger = init_logger(__name__)\n\n_GLOBAL_LORA_ID = 0\n\n\n@dataclass\nclass LongContextLoRAContext:\n    \"\"\"Context for lora adapters that support long context.\"\"\"\n    # The scaling factors to support long context lora fine tuned models.\n    scaling_factors: List[float]\n    # dimension to apply rotary embedding.\n    rot_dim: int\n    # offsets to the sin_cos_cache for each lora_id loaded.\n    # This value is dynamically modified.\n    offsets_by_lora_id: Dict[int, int] = field(default_factory=dict)\n\n\ndef get_lora_id():\n    global _GLOBAL_LORA_ID\n    _GLOBAL_LORA_ID += 1\n    return _GLOBAL_LORA_ID\n\n\nclass LoRAModel(AdapterModel):\n    \"\"\"A LoRA fine-tuned model.\"\"\"\n\n    def __init__(\n        self,\n        lora_model_id: int,\n        rank: int,\n        loras: Dict[str, LoRALayerWeights],\n        scaling_factor: Optional[float] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            lora_model_id: The integer id for the lora model.\n            rank: lora rank.\n            loras: module name -> weights for lora-replaced layers.\n            scaling_factor: Scaling factor to support long context lora model.\n                None if the lora is not tuned for long context support.\n        \"\"\"\n        self.id = lora_model_id\n        # Scaling factor for long context lora model. None if it is not\n        # fine tuned for the long context.\n        self.scaling_factor = scaling_factor\n        assert (lora_model_id >\n                0), f\"a valid lora id should be greater than 0, got {self.id}\"\n        self.rank = rank\n        self.loras: Dict[str, LoRALayerWeights] = loras\n\n    def clone(self, lora_model_id: int) -> \"LoRAModel\":\n        \"\"\"Return a copy of the object with different ids.\n\n        Will share the underlying tensors.\"\"\"\n        return self.__class__(\n            lora_model_id,\n            rank=self.rank,\n            loras=self.loras.copy(),\n        )\n\n    @property\n    def extra_vocab_size(self) -> int:\n        return max(lora.extra_vocab_size\n                   for lora in self.loras.values()) if self.loras else 0\n\n    def get_lora(self, module_name: str) -> Optional[LoRALayerWeights]:\n        \"\"\"Get LoRA for a given module by name\"\"\"\n        return self.loras.get(module_name, None)\n\n    # (yard1): TODO see if we can derive target_embedding_padding automatically\n    @classmethod\n    def from_lora_tensors(\n        cls,\n        lora_model_id: int,\n        rank: int,\n        lora_alpha: int,\n        tensors: Dict[str, torch.Tensor],\n        device: str = \"cuda\",\n        dtype: Optional[torch.dtype] = None,\n        embeddings: Optional[Dict[str, torch.Tensor]] = None,\n        target_embedding_padding: Optional[int] = None,\n        scaling_factor: Optional[float] = None,\n        embedding_modules: Optional[Dict[str, str]] = None,\n        embedding_padding_modules: Optional[List[str]] = None,\n    ) -> \"LoRAModel\":\n        \"\"\"Create a LoRAModel from a dictionary of tensors.\"\"\"\n        pin_memory = str(device) == \"cpu\" and is_pin_memory_available()\n        loras: Dict[str, LoRALayerWeights] = {}\n        for tensor_name, tensor in tensors.items():\n            module_name, is_lora_a = parse_fine_tuned_lora_name(tensor_name)\n            if module_name not in loras:\n                lora_embeddings_tensor = None\n                if embeddings:\n                    assert embedding_modules is not None\n                    embeddings_module = next(\n                        (k for k in embedding_modules if k in module_name),\n                        None)\n                    if embeddings_module:\n                        lora_embeddings_tensor = embeddings[\n                            embedding_modules[embeddings_module]].to(\n                                device=device, dtype=dtype)\n                        if pin_memory:\n                            lora_embeddings_tensor = (\n                                lora_embeddings_tensor.pin_memory())\n                loras[module_name] = LoRALayerWeights(module_name, rank,\n                                                      lora_alpha, None, None,\n                                                      lora_embeddings_tensor)\n            if is_lora_a:\n                loras[module_name].lora_a = tensor.to(device=device,\n                                                      dtype=dtype).t()\n                if pin_memory:\n                    loras[module_name].lora_a = loras[\n                        module_name].lora_a.pin_memory()\n            else:\n                loras[module_name].lora_b = tensor.to(device=device,\n                                                      dtype=dtype).t()\n                assert embedding_padding_modules is not None\n                if any(name in module_name\n                       for name in embedding_padding_modules\n                       ) and target_embedding_padding is not None:\n                    lora_b = loras[module_name].lora_b\n                    assert target_embedding_padding >= lora_b.shape[1]\n                    addition = target_embedding_padding - lora_b.shape[1]\n                    loras[module_name].lora_b = torch.nn.functional.pad(\n                        lora_b, (0, addition))\n                if pin_memory:\n                    loras[module_name].lora_b = loras[\n                        module_name].lora_b.pin_memory()\n\n        for lora in loras.values():\n            lora.optimize()\n        return cls(lora_model_id, rank, loras, scaling_factor=scaling_factor)\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass LoRAModelManager(AdapterModelManager):\n    \"\"\"A manager that manages multiple LoRA-fine-tuned models.\"\"\"\n\n    def __init__(\n        self,\n        model: SupportsLoRA,\n        max_num_seqs: int,\n        max_num_batched_tokens: int,\n        vocab_size: int,\n        lora_config: LoRAConfig,\n    ):\n        \"\"\"Create a LoRAModelManager and adapter for a given model.\n\n        Args:\n            model: the model to be adapted.\n            max_num_seqs: the maximum number of sequences model can run in a\n                single batch.\n            max_num_batched_tokens: the maximum number of tokens model can run\n                in a single batch.\n            vocab_size: the vocab size of the model.\n            lora_config: the LoRA configuration.\n        \"\"\"\n        self.lora_config = lora_config\n        self.max_num_seqs = max_num_seqs\n        assert self.capacity >= self.lora_slots\n        self.max_num_batched_tokens = math.ceil(max_num_batched_tokens / 8) * 8\n        self.lora_index_to_id: List[Optional[int]] = [None] * self.lora_slots\n        self.vocab_size = vocab_size\n        self.long_lora_context: Optional[LongContextLoRAContext] = None\n        self.punica_wrapper = PunicaWrapper(max_num_batched_tokens,\n                                            max_batches=self.max_num_seqs,\n                                            device=\"cuda\")\n        # Scaling factor -> offset to the sin_cos_cache to it.\n        # Used for long context lora.\n        self.scaling_factor_to_offset: Dict[float, int] = {}\n        super().__init__(model)\n        if hasattr(self.model, \"supported_lora_modules\"):\n            self.supported_lora_modules = copy.deepcopy(\n                self.model.supported_lora_modules)\n            if lora_config.long_lora_scaling_factors:\n                # We need to replace rotary emb layer to do batch computation\n                # for long lora.\n                self.supported_lora_modules.append(\"rotary_emb\")\n            self.packed_modules_mapping = copy.deepcopy(\n                self.model.packed_modules_mapping)\n        self.packed_modules: Dict[str, List[str]] = {}\n        self.modules: Dict[str, \"BaseLayerWithLoRA\"] = {}\n        # Dict instead of a Set for compatibility with LRUCache.\n        self._last_mapping: Optional[LoRAMapping] = None\n        self._create_lora_modules()\n        self.model.lora_manager = self\n        self.adapter_type = 'LoRa'\n\n    @property\n    def capacity(self) -> int:\n        return self.lora_config.max_cpu_loras\n\n    @property\n    def lora_slots(self) -> int:\n        return self.lora_config.max_loras\n\n    @property\n    def adapter_slots(self) -> int:\n        return self.lora_slots\n\n    def activate_adapter(\n        self,\n        lora_id: int,\n    ) -> bool:\n        \"\"\"Move LoRA into a GPU buffer to be used in the forward pass.\"\"\"\n        if lora_id in self._active_adapters:\n            return False\n        first_free_slot = next(\n            ((i, lora_id) for i, lora_id in enumerate(self.lora_index_to_id)\n             if lora_id is None), None)\n        if first_free_slot is None:\n            raise ValueError(\"No free lora slots\")\n        index, _ = first_free_slot\n        self._active_adapters[lora_id] = None\n        lora_model = self._registered_adapters[lora_id]\n        logger.debug(\"Activating LoRA. int id: %d, slot index: %d\",\n                     lora_model.id, index)\n        self.lora_index_to_id[index] = lora_model.id\n        for module_name, module in self.modules.items():\n            module_lora = lora_model.get_lora(module_name)\n            if module_lora:\n                module_lora.optimize()\n                module.set_lora(index, module_lora.lora_a, module_lora.lora_b,\n                                module_lora.embeddings_tensor)\n            else:\n                module.reset_lora(index)\n        return True\n\n    def _deactivate_adapter(self, lora_id: int):\n        try:\n            index = self.lora_index_to_id.index(lora_id)\n            self.lora_index_to_id[index] = None\n        except ValueError:\n            pass\n\n    def _set_long_lora_context(self, lora: LoRAModel):\n        if self.long_lora_context is None:\n            return\n\n        if lora.scaling_factor is None:\n            return\n\n        if (lora.scaling_factor not in self.scaling_factor_to_offset):\n            raise ValueError(f\"Long LoRA scaling factor {lora.scaling_factor}\"\n                             \" has not been initialized.\")\n\n        offsets = self.scaling_factor_to_offset.get(lora.scaling_factor)\n        if offsets:\n            self.long_lora_context.offsets_by_lora_id[lora.id] = offsets\n\n    def _add_adapter(self, lora: LoRAModel):\n        self._create_merged_loras_inplace(lora)\n        self._registered_adapters[lora.id] = lora\n        self._set_long_lora_context(lora)\n\n    def pin_adapter(self, lora_id: int) -> bool:\n        \"\"\"Pin a LoRAModel in the manager cache.\"\"\"\n        raise NotImplementedError(\n            \"Pinning is not supported in LoRAModelManager.\"\n            \"Use LRUCacheLoRAModelManager for pinning\")  # type: ignore\n\n    def _set_adapter_mapping(self, mapping: LoRAMapping) -> None:\n        # update lora states\n        self.punica_wrapper.update_metadata(\n            mapping,\n            self.lora_index_to_id,\n            self.lora_slots + 1,\n            self.vocab_size,\n            self.lora_config.lora_extra_vocab_size,\n            self.long_lora_context,\n        )\n\n    def remove_all_adapters(self):\n        \"\"\"Remove all LoRAModels from the manager.\"\"\"\n        self._registered_adapters.clear()\n        self.lora_index_to_id = [None] * self.lora_slots\n        self._active_adapters.clear()\n\n    def _create_lora_modules(self):\n        for module_name, module in self.model.named_modules(\n                remove_duplicate=False):\n            if isinstance(module, PPMissingLayer):\n                continue\n            if not self._match_target_modules(module_name):\n                continue\n            parts = module_name.split(\".\")[-1]\n            packed_moduled_lst = self.packed_modules_mapping.get(parts, [])\n            new_module = replace_submodule(\n                self.model, module_name,\n                from_layer(module, self.lora_slots, self.lora_config,\n                           packed_moduled_lst, self.model.config))\n            # LinearScalingRotaryEmbeddingWithLora is used to handle\n            # long context lora. Register relevant metadata.\n            if isinstance(new_module, LinearScalingRotaryEmbeddingWithLora):\n                self.long_lora_context = LongContextLoRAContext(\n                    new_module.scaling_factors, new_module.rotary_dim)\n                self.scaling_factor_to_offset = \\\n                    new_module.scaling_factor_to_offset\n            # (yard1): TODO make this more robust\n            if \"lm_head\" in module_name:\n                logits_processor_module = self.model.get_submodule(\n                    \"logits_processor\")\n                new_module = replace_submodule(\n                    self.model, \"logits_processor\",\n                    from_layer_logits_processor(logits_processor_module,\n                                                module, self.lora_slots,\n                                                self.lora_config,\n                                                self.model.config))\n            self.register_module(module_name, new_module)\n            self._register_packed_modules(module_name)\n            # All lora layers share the same punica_wrapper based on reference.\n            new_module.set_mapping(self.punica_wrapper)\n\n    def register_module(self, module_name: str, module: \"BaseLayerWithLoRA\"):\n        assert isinstance(module, BaseLayerWithLoRA)\n        self.modules[module_name] = module\n\n    def create_dummy_lora(\n            self,\n            lora_id: int,\n            rank: int,\n            scaling_factor: Optional[float],\n            embedding_modules: Optional[Dict[str, str]] = None) -> LoRAModel:\n        \"\"\"Create zero-initialized LoRAModel for warmup.\"\"\"\n        model = LoRAModel(lora_id, rank, {}, scaling_factor)\n        for module_name, module in self.model.named_modules():\n            if not self._match_target_modules(module_name) or not isinstance(\n                    module, BaseLayerWithLoRA) or isinstance(\n                        module, LinearScalingRotaryEmbeddingWithLora):\n                continue\n            parts = module_name.split(\".\")\n            if module_name not in self.packed_modules:\n                assert embedding_modules is not None\n                if parts[-1] in embedding_modules:\n                    input_dim = (module.base_layer.org_vocab_size +\n                                 self.lora_config.lora_extra_vocab_size if\n                                 hasattr(module.base_layer, \"org_vocab_size\")\n                                 else module.base_layer.weight.shape[1])\n                    output_dim = module.base_layer.embedding_dim if hasattr(\n                        module.base_layer,\n                        \"embedding_dim\") else module.base_layer.weight.shape[0]\n                    embeddings_tensor_dim = (module.base_layer.embedding_dim if\n                                             hasattr(module.base_layer,\n                                                     \"embedding_dim\") else\n                                             module.base_layer.weight.shape[1])\n                    lora = LoRALayerWeights.create_dummy_lora_weights(\n                        module_name,\n                        input_dim,\n                        output_dim,\n                        rank,\n                        module.lora_a_stacked.dtype,\n                        \"cpu\",\n                        embeddings_tensor_dim=embeddings_tensor_dim)\n                else:\n                    lora = LoRALayerWeights.create_dummy_lora_weights(\n                        module_name,\n                        module.lora_a_stacked.shape[-1],\n                        module.lora_b_stacked.shape[-2],\n                        rank,\n                        module.lora_a_stacked.dtype,\n                        \"cpu\",\n                    )\n                lora.optimize()\n            else:\n                parts = module_name.split(\".\")\n                replacements = self.packed_modules_mapping[parts[-1]]\n                subloras: List[Optional[\"LoRALayerWeights\"]] = []\n                for i, r in enumerate(replacements):\n                    lora = LoRALayerWeights.create_dummy_lora_weights(\n                        module_name + \".\" + r,\n                        module.lora_a_stacked[i].shape[-1],\n                        module.lora_b_stacked[i].shape[-2],\n                        rank,\n                        module.lora_a_stacked[i].dtype,\n                        \"cpu\",\n                    )\n                    lora.optimize()\n                    subloras.append(lora)\n                lora = PackedLoRALayerWeights.pack(subloras)\n            model.loras[module_name] = lora\n        return model\n\n    def _match_target_modules(self, module_name: str):\n        return any(\n            re.match(\n                r\".*\\.{target_module}$\".format(target_module=target_module),\n                module_name) or target_module == module_name\n            for target_module in self.supported_lora_modules)\n\n    def _register_packed_modules(self, module_full_name: str) -> None:\n        parts = module_full_name.split(\".\")\n        module_name = parts[-1]\n        replacements = self.packed_modules_mapping.get(module_name, [])\n        # When replacements is less than or equal to 1, it indicates that this\n        # module is not a packed module.\n        if len(replacements) <= 1:\n            return\n        prefix = \".\".join(parts[:-1])\n        self.packed_modules[module_full_name] = [\n            prefix + \".\" + r if prefix else r for r in replacements\n        ]\n\n    def _create_merged_loras_inplace(self, lora_model: LoRAModel) -> None:\n        for module_name, new_module_names in self.packed_modules.items():\n            replacement_loras: List[Optional[LoRALayerWeights]] = []\n            has_replacement = False\n            for r in new_module_names:\n                lora = lora_model.get_lora(r)\n                replacement_loras.append(lora)\n                if lora:\n                    has_replacement = True\n            if not has_replacement:\n                continue\n            for i in range(len(replacement_loras)):\n                if replacement_loras[i]:\n                    continue\n                replacement_loras[i] = None\n            lora_model.loras[module_name] = PackedLoRALayerWeights.pack(\n                replacement_loras)\n\n    def deactivate_adapter(self, adapter_id: int) -> bool:\n        return deactivate_adapter(adapter_id, self._active_adapters,\n                                  self._deactivate_adapter)\n\n    def add_adapter(self, adapter: LoRAModel) -> bool:\n        logger.debug(\n            \"Adding lora. Model id: %d, \"\n            \"int id: %d, \"\n            \"scaling factor: %s\", adapter.id, adapter.id,\n            adapter.scaling_factor)\n        return add_adapter(adapter, self._registered_adapters, self.capacity,\n                           self._add_adapter)\n\n    def set_adapter_mapping(self, mapping: LoRAMapping) -> None:\n        self._last_mapping = set_adapter_mapping(mapping, self._last_mapping,\n                                                 self._set_adapter_mapping)\n\n    def remove_adapter(self, adapter_id: int) -> bool:\n        return remove_adapter(adapter_id, self._registered_adapters,\n                              self.deactivate_adapter)\n\n    def list_adapters(self) -> Dict[int, Any]:\n        return list_adapters(self._registered_adapters)\n\n    def get_adapter(self, adapter_id: int) -> Optional[Any]:\n        return get_adapter(adapter_id, self._registered_adapters)\n\n\nclass LoRALRUCache(AdapterLRUCache[LoRAModel]):\n\n    def __init__(self, capacity: int, deactivate_lora_fn: Callable[[int],\n                                                                   bool]):\n        super().__init__(capacity, deactivate_lora_fn)\n\n\nclass LRUCacheLoRAModelManager(LoRAModelManager):\n    \"\"\"A model manager that manages multiple LoRAs with LRU cache.\"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        max_num_seqs: int,\n        max_num_batched_tokens: int,\n        vocab_size: int,\n        lora_config: LoRAConfig,\n    ):\n        super().__init__(model, max_num_seqs, max_num_batched_tokens,\n                         vocab_size, lora_config)\n        self._registered_adapters: LoRALRUCache = LoRALRUCache(\n            self.capacity, self.deactivate_adapter)\n        self._active_adapters: LoRALRUCache = LoRALRUCache(\n            self.lora_slots, self._deactivate_adapter)\n\n    def list_adapters(self) -> Dict[int, LoRAModel]:\n        \"\"\"List all registered LoRAModels.\"\"\"\n        return dict(self._registered_adapters.cache)\n\n    def add_adapter(self, lora: LoRAModel) -> bool:\n        \"\"\"Add a LoRAModel to the manager.\"\"\"\n        logger.debug(\n            \"Adding lora. Model id: %d, \"\n            \"int id: %d, \"\n            \"scaling factor: %s\", lora.id, lora.id, lora.scaling_factor)\n        if lora.id not in self._registered_adapters:\n            self._add_adapter(lora)\n            was_added = True\n        else:\n            # We always touch to update the LRU cache order\n            self._registered_adapters.touch(lora.id)\n            was_added = False\n        return was_added\n\n    def activate_adapter(\n        self,\n        lora_id: int,\n    ) -> bool:\n        if lora_id not in self._active_adapters and len(\n                self._active_adapters) >= self.lora_slots:\n            self._active_adapters.remove_oldest()\n        result = super().activate_adapter(lora_id)\n        # We always touch to update the LRU cache order\n        self._active_adapters.touch(lora_id)\n        return result\n\n    def remove_oldest_adapter(self) -> bool:\n        if len(self._registered_adapters) > 0:\n            self._registered_adapters.remove_oldest()\n            return True\n        return False\n\n    def pin_adapter(self, lora_id: int) -> bool:\n        \"\"\"Pin a LoRAModel in the manager cache.\"\"\"\n        self._pin_lora_in_cpu_cache(lora_id)\n        self._pin_lora_in_gpu_cache(lora_id)\n        return True\n\n    def _pin_lora_in_cpu_cache(self, lora_id: int):\n        try:\n            self._registered_adapters.pin(lora_id)\n        except ValueError as err:\n            raise ValueError(\"Pinning failed. \"\n                             f\"LoRA {lora_id} is not registered.\") from err\n\n    def _pin_lora_in_gpu_cache(self, lora_id: int):\n        if lora_id not in self._active_adapters:\n            # move lora to gpu if not already active\n            self.activate_adapter(lora_id)\n\n        self._active_adapters.pin(lora_id)\n\n\ndef create_lora_manager(\n        model: nn.Module,\n        max_num_seqs: int,\n        max_num_batched_tokens: int,\n        vocab_size: int,\n        lora_config: LoRAConfig,\n        lora_manager_cls: Type[LoRAModelManager] = LoRAModelManager,\n        **kwargs) -> LoRAModelManager:\n    \"\"\"Create a LoRA adapter for\n```\n# Function Signature: def from_local_checkpoint(\n# Natural Language Annotations: \"\"\"Create a LoRAModel from a local checkpoint.\n        \n        Args:\n            lora_dir: The local path that has lora data.\n            expected_lora_modules: Name of modules that are expected to be\n                replaced by lora.\n            max_position_embeddings: Max position embedding length. Used to\n                scaling the largest context length. If None, the lora model's\n                context length is not scaled.\n            lora_model_id: Lora model id. If not given, automatically set by\n                a global counter.\n            device: Device where the lora model is loaded.\n            dtype: dtype of the lora model weights.\n\n        Returns:\n            Loaded LoRA Model.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n@dataclass\nclass LoRAModel(AdapterModel):\n", "  ```\ndef from_local_checkpoint(\n    lora_", "  ```\nimport json\nimport os\nimport re\nfrom typing import Any"]}, "vllm-vllm/entrypoints/openai/serving_chat.py-create_chat_completion": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/entrypoints/openai/serving_chat.py:\n```\nimport asyncio\nimport time\nfrom typing import AsyncGenerator, AsyncIterator, Dict, List, Optional\nfrom typing import Sequence as GenericSequence\nfrom typing import Union\n\nfrom fastapi import Request\nfrom transformers import PreTrainedTokenizer\n\nfrom vllm.config import ModelConfig\nfrom vllm.engine.protocol import AsyncEngineClient\nfrom vllm.entrypoints.chat_utils import (ConversationMessage,\n                                         apply_chat_template,\n                                         load_chat_template,\n                                         parse_chat_messages)\nfrom vllm.entrypoints.logger import RequestLogger\nfrom vllm.entrypoints.openai.protocol import (\n    ChatCompletionLogProb, ChatCompletionLogProbs,\n    ChatCompletionLogProbsContent, ChatCompletionNamedToolChoiceParam,\n    ChatCompletionRequest, ChatCompletionResponse,\n    ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice,\n    ChatCompletionStreamResponse, ChatMessage, DeltaMessage, ErrorResponse,\n    FunctionCall, ToolCall, UsageInfo)\nfrom vllm.entrypoints.openai.serving_engine import (LoRAModulePath,\n                                                    OpenAIServing,\n                                                    PromptAdapterPath)\nfrom vllm.inputs import PromptInputs\nfrom vllm.logger import init_logger\nfrom vllm.multimodal import MultiModalDataDict\nfrom vllm.outputs import RequestOutput\nfrom vllm.sequence import Logprob\nfrom vllm.tracing import (contains_trace_headers, extract_trace_headers,\n                          log_tracing_disabled_warning)\nfrom vllm.utils import iterate_with_cancellation, random_uuid\n\nlogger = init_logger(__name__)\n\n\nclass OpenAIServingChat(OpenAIServing):\n\n    def __init__(\n        self,\n        async_engine_client: AsyncEngineClient,\n        model_config: ModelConfig,\n        served_model_names: List[str],\n        response_role: str,\n        *,\n        lora_modules: Optional[List[LoRAModulePath]],\n        prompt_adapters: Optional[List[PromptAdapterPath]],\n        request_logger: Optional[RequestLogger],\n        chat_template: Optional[str],\n        return_tokens_as_token_ids: bool = False,\n    ):\n        super().__init__(async_engine_client=async_engine_client,\n                         model_config=model_config,\n                         served_model_names=served_model_names,\n                         lora_modules=lora_modules,\n                         prompt_adapters=prompt_adapters,\n                         request_logger=request_logger,\n                         return_tokens_as_token_ids=return_tokens_as_token_ids)\n\n        self.response_role = response_role\n\n        # If this is None we use the tokenizer's default chat template\n        self.chat_template = load_chat_template(chat_template)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_chat_request_role(self, request: ChatCompletionRequest) -> str:\n        if request.add_generation_prompt:\n            return self.response_role\n        else:\n            return request.messages[-1][\"role\"]\n\n    async def chat_completion_stream_generator(\n        self,\n        request: ChatCompletionRequest,\n        result_generator: AsyncIterator[RequestOutput],\n        request_id: str,\n        conversation: List[ConversationMessage],\n        tokenizer: PreTrainedTokenizer,\n    ) -> AsyncGenerator[str, None]:\n        model_name = self.served_model_names[0]\n        created_time = int(time.time())\n        chunk_object_type = \"chat.completion.chunk\"\n        first_iteration = True\n\n        # Send response for each token for each request.n (index)\n        num_choices = 1 if request.n is None else request.n\n        previous_texts = [\"\"] * num_choices\n        previous_num_tokens = [0] * num_choices\n        finish_reason_sent = [False] * num_choices\n\n        try:\n            async for res in result_generator:\n                # We need to do it here, because if there are exceptions in\n                # the result_generator, it needs to be sent as the FIRST\n                # response (by the try...catch).\n                if first_iteration:\n                    # Send first response for each request.n (index) with\n                    # the role\n                    role = self.get_chat_request_role(request)\n                    for i in range(num_choices):\n                        choice_data = ChatCompletionResponseStreamChoice(\n                            index=i,\n                            delta=DeltaMessage(role=role),\n                            logprobs=None,\n                            finish_reason=None)\n                        chunk = ChatCompletionStreamResponse(\n                            id=request_id,\n                            object=chunk_object_type,\n                            created=created_time,\n                            choices=[choice_data],\n                            model=model_name)\n                        if (request.stream_options\n                                and request.stream_options.include_usage):\n                            if (request.stream_options.continuous_usage_stats):\n                                prompt_tokens = len(res.prompt_token_ids)\n                                usage = UsageInfo(prompt_tokens=prompt_tokens,\n                                                  completion_tokens=0,\n                                                  total_tokens=prompt_tokens)\n                                chunk.usage = usage\n                            else:\n                                chunk.usage = None\n\n                        data = chunk.model_dump_json(exclude_unset=True)\n                        yield f\"data: {data}\\n\\n\"\n\n                    # Send response to echo the input portion of the\n                    # last message\n                    if request.echo:\n                        last_msg_content = \"\"\n                        if conversation and conversation[-1].get(\n                                \"content\") and conversation[-1].get(\n                                    \"role\") == role:\n                            last_msg_content = conversation[-1][\"content\"]\n\n                        if last_msg_content:\n                            for i in range(num_choices):\n                                choice_data = (\n                                    ChatCompletionResponseStreamChoice(\n                                        index=i,\n                                        delta=DeltaMessage(\n                                            content=last_msg_content),\n                                        logprobs=None,\n                                        finish_reason=None))\n                                chunk = ChatCompletionStreamResponse(\n                                    id=request_id,\n                                    object=chunk_object_type,\n                                    created=created_time,\n                                    choices=[choice_data],\n                                    model=model_name)\n                                if (request.stream_options and\n                                        request.stream_options.include_usage):\n                                    if (request.stream_options.\n                                            continuous_usage_stats):\n                                        prompt_tokens = len(\n                                            res.prompt_token_ids)\n                                        usage = UsageInfo(\n                                            prompt_tokens=prompt_tokens,\n                                            completion_tokens=0,\n                                            total_tokens=prompt_tokens)\n                                        chunk.usage = usage\n                                    else:\n                                        chunk.usage = None\n\n                                data = chunk.model_dump_json(\n                                    exclude_unset=True)\n                                yield f\"data: {data}\\n\\n\"\n                    first_iteration = False\n\n                for output in res.outputs:\n                    i = output.index\n\n                    if finish_reason_sent[i]:\n                        continue\n\n                    delta_token_ids = output.token_ids[previous_num_tokens[i]:]\n                    out_logprobs = output.logprobs[\n                        previous_num_tokens[i]:] if output.logprobs else None\n\n                    if request.logprobs and request.top_logprobs is not None:\n                        assert out_logprobs is not None, (\n                            \"Did not output logprobs\")\n                        logprobs = self._create_chat_logprobs(\n                            token_ids=delta_token_ids,\n                            top_logprobs=out_logprobs,\n                            tokenizer=tokenizer,\n                            num_output_top_logprobs=request.top_logprobs,\n                        )\n                    else:\n                        logprobs = None\n\n                    delta_text = output.text[len(previous_texts[i]):]\n                    previous_texts[i] = output.text\n                    previous_num_tokens[i] = len(output.token_ids)\n\n                    if request.tool_choice and type(\n                            request.tool_choice\n                    ) is ChatCompletionNamedToolChoiceParam:\n                        delta_message = DeltaMessage(tool_calls=[\n                            ToolCall(function=FunctionCall(\n                                name=request.tool_choice.function.name,\n                                arguments=delta_text))\n                        ])\n                    else:\n                        delta_message = DeltaMessage(content=delta_text)\n\n                    if output.finish_reason is None:\n                        # Send token-by-token response for each request.n\n\n                        choice_data = ChatCompletionResponseStreamChoice(\n                            index=i,\n                            delta=delta_message,\n                            logprobs=logprobs,\n                            finish_reason=None)\n                        chunk = ChatCompletionStreamResponse(\n                            id=request_id,\n                            object=chunk_object_type,\n                            created=created_time,\n                            choices=[choice_data],\n                            model=model_name)\n                        if (request.stream_options\n                                and request.stream_options.include_usage):\n                            if (request.stream_options.continuous_usage_stats):\n                                prompt_tokens = len(res.prompt_token_ids)\n                                completion_tokens = len(output.token_ids)\n                                usage = UsageInfo(\n                                    prompt_tokens=prompt_tokens,\n                                    completion_tokens=completion_tokens,\n                                    total_tokens=prompt_tokens +\n                                    completion_tokens,\n                                )\n                                chunk.usage = usage\n                            else:\n                                chunk.usage = None\n\n                        data = chunk.model_dump_json(exclude_unset=True)\n                        yield f\"data: {data}\\n\\n\"\n                    else:\n                        # Send the finish response for each request.n only once\n                        prompt_tokens = len(res.prompt_token_ids)\n                        choice_data = ChatCompletionResponseStreamChoice(\n                            index=i,\n                            delta=delta_message,\n                            logprobs=logprobs,\n                            finish_reason=output.finish_reason,\n                            stop_reason=output.stop_reason)\n                        chunk = ChatCompletionStreamResponse(\n                            id=request_id,\n                            object=chunk_object_type,\n                            created=created_time,\n                            choices=[choice_data],\n                            model=model_name)\n                        if (request.stream_options\n                                and request.stream_options.include_usage):\n                            if (request.stream_options.continuous_usage_stats):\n                                prompt_tokens = len(res.prompt_token_ids)\n                                completion_tokens = len(output.token_ids)\n                                usage = UsageInfo(\n                                    prompt_tokens=prompt_tokens,\n                                    completion_tokens=completion_tokens,\n                                    total_tokens=prompt_tokens +\n                                    completion_tokens,\n                                )\n                                chunk.usage = usage\n                            else:\n                                chunk.usage = None\n                        data = chunk.model_dump_json(exclude_unset=True)\n                        yield f\"data: {data}\\n\\n\"\n                        finish_reason_sent[i] = True\n\n            if (request.stream_options\n                    and request.stream_options.include_usage):\n                final_usage = UsageInfo(\n                    prompt_tokens=prompt_tokens,\n                    completion_tokens=previous_num_tokens[i],\n                    total_tokens=prompt_tokens + previous_num_tokens[i],\n                )\n\n                final_usage_chunk = ChatCompletionStreamResponse(\n                    id=request_id,\n                    object=chunk_object_type,\n                    created=created_time,\n                    choices=[],\n                    model=model_name,\n                    usage=final_usage)\n                final_usage_data = (final_usage_chunk.model_dump_json(\n                    exclude_unset=True, exclude_none=True))\n                yield f\"data: {final_usage_data}\\n\\n\"\n\n        except ValueError as e:\n            # TODO: Use a vllm-specific Validation Error\n            data = self.create_streaming_error_response(str(e))\n            yield f\"data: {data}\\n\\n\"\n        # Send the final done message after all response.n are finished\n        yield \"data: [DONE]\\n\\n\"\n\n    async def chat_completion_full_generator(\n        self,\n        request: ChatCompletionRequest,\n        result_generator: AsyncIterator[RequestOutput],\n        request_id: str,\n        conversation: List[ConversationMessage],\n        tokenizer: PreTrainedTokenizer,\n    ) -> Union[ErrorResponse, ChatCompletionResponse]:\n\n        model_name = self.served_model_names[0]\n        created_time = int(time.time())\n        final_res: Optional[RequestOutput] = None\n\n        try:\n            async for res in result_generator:\n                final_res = res\n        except asyncio.CancelledError:\n            return self.create_error_response(\"Client disconnected\")\n\n        assert final_res is not None\n\n        choices: List[ChatCompletionResponseChoice] = []\n\n        role = self.get_chat_request_role(request)\n        for output in final_res.outputs:\n            token_ids = output.token_ids\n            out_logprobs = output.logprobs\n\n            if request.logprobs and request.top_logprobs is not None:\n                assert out_logprobs is not None, \"Did not output logprobs\"\n                logprobs = self._create_chat_logprobs(\n                    token_ids=token_ids,\n                    top_logprobs=out_logprobs,\n                    num_output_top_logprobs=request.top_logprobs,\n                    tokenizer=tokenizer,\n                )\n            else:\n                logprobs = None\n\n            if request.tool_choice and type(\n                    request.tool_choice) is ChatCompletionNamedToolChoiceParam:\n                message = ChatMessage(\n                    role=role,\n                    content=\"\",\n                    tool_calls=[\n                        ToolCall(function=FunctionCall(\n                            name=request.tool_choice.function.name,\n                            arguments=output.text))\n                    ])\n            elif not request.tool_choice or request.tool_choice == \"none\":\n                message = ChatMessage(role=role, content=output.text)\n\n            choice_data = ChatCompletionResponseChoice(\n                index=output.index,\n                message=message,\n                logprobs=logprobs,\n                finish_reason=output.finish_reason,\n                stop_reason=output.stop_reason)\n            choices.append(choice_data)\n\n        if request.echo:\n            last_msg_content = \"\"\n            if conversation and conversation[-1].get(\n                    \"content\") and conversation[-1].get(\"role\") == role:\n                last_msg_content = conversation[-1][\"content\"]\n\n            for choice in choices:\n                full_message = last_msg_content + choice.message.content\n                choice.message.content = full_message\n\n        num_prompt_tokens = len(final_res.prompt_token_ids)\n        num_generated_tokens = sum(\n            len(output.token_ids) for output in final_res.outputs)\n        usage = UsageInfo(\n            prompt_tokens=num_prompt_tokens,\n            completion_tokens=num_generated_tokens,\n            total_tokens=num_prompt_tokens + num_generated_tokens,\n        )\n        response = ChatCompletionResponse(\n            id=request_id,\n            created=created_time,\n            model=model_name,\n            choices=choices,\n            usage=usage,\n            prompt_logprobs=final_res.prompt_logprobs,\n        )\n\n        return response\n\n    def _get_top_logprobs(\n            self, logprobs: Dict[int, Logprob], top_logprobs: Optional[int],\n            tokenizer: PreTrainedTokenizer) -> List[ChatCompletionLogProb]:\n        return [\n            ChatCompletionLogProb(token=(token := self._get_decoded_token(\n                p[1],\n                p[0],\n                tokenizer,\n                return_as_token_id=self.return_tokens_as_token_ids)),\n                                  logprob=max(p[1].logprob, -9999.0),\n                                  bytes=list(\n                                      token.encode(\"utf-8\", errors=\"replace\")))\n            for i, p in enumerate(logprobs.items())\n            if top_logprobs and i < top_logprobs\n        ]\n\n    def _create_chat_logprobs(\n        self,\n        token_ids: GenericSequence[int],\n        top_logprobs: GenericSequence[Optional[Dict[int, Logprob]]],\n        tokenizer: PreTrainedTokenizer,\n        num_output_top_logprobs: Optional[int] = None,\n    ) -> ChatCompletionLogProbs:\n        \"\"\"Create OpenAI-style logprobs.\"\"\"\n\n        logprobs_content = []\n\n        for i, token_id in enumerate(token_ids):\n            step_top_logprobs = top_logprobs[i]\n            if step_top_logprobs is None:\n                token = tokenizer.decode(token_id)\n                if self.return_tokens_as_token_ids:\n                    token = f\"token_id:{token_id}\"\n                logprobs_content.append(\n                    ChatCompletionLogProbsContent(\n                        token=token,\n                        bytes=list(token.encode(\"utf-8\", errors=\"replace\"))))\n            else:\n                logprobs_content.append(\n                    ChatCompletionLogProbsContent(\n                        token=self._get_decoded_token(\n                            step_top_logprobs[token_id], token_id, tokenizer,\n                            self.return_tokens_as_token_ids),\n                        logprob=max(step_top_logprobs[token_id].logprob,\n                                    -9999.0),\n                        bytes=list(\n                            step_top_logprobs[token_id].decoded_token.encode(\n                                \"utf-8\", errors=\"replace\")),\n                        top_logprobs=self._get_top_logprobs(\n                            step_top_logprobs, num_output_top_logprobs,\n                            tokenizer)))\n\n        return ChatCompletionLogProbs(content=logprobs_content)\n\n```\n# Function Signature: async def create_chat_completion(\n# Natural Language Annotations: \"\"\"Completion API similar to OpenAI's API.\n\n        See https://platform.openai.com/docs/api-reference/chat/create\n        for the API specification. This API mimics the OpenAI\n        ChatCompletion API.\n\n        NOTE: Currently we do not support the following feature:\n            - function_call (Users should implement this by themselves)\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def create_chat_completion(request: Chat", "  ```\nasync def create_chat_completion(\n    chat_", "  ```\nasync def create_chat_completion(request: Chat"]}, "vllm-vllm/multimodal/utils.py-async_fetch_image": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/vllm/vllm/multimodal/utils.py:\n```\nimport base64\nfrom io import BytesIO\nfrom typing import Tuple, Union\n\nimport librosa\nimport numpy as np\nimport soundfile\nfrom PIL import Image\n\nfrom vllm.connections import global_http_connection\nfrom vllm.envs import VLLM_AUDIO_FETCH_TIMEOUT, VLLM_IMAGE_FETCH_TIMEOUT\nfrom vllm.multimodal.base import MultiModalDataDict\n\n\ndef _load_image_from_bytes(b: bytes):\n    image = Image.open(BytesIO(b))\n    image.load()\n    return image\n\n\ndef _load_image_from_data_url(image_url: str):\n    # Only split once and assume the second part is the base64 encoded image\n    _, image_base64 = image_url.split(\",\", 1)\n    return load_image_from_base64(image_base64)\n\n\ndef fetch_image(image_url: str, *, image_mode: str = \"RGB\") -> Image.Image:\n    \"\"\"\n    Load a PIL image from a HTTP or base64 data URL.\n\n    By default, the image is converted into RGB format.\n    \"\"\"\n    if image_url.startswith('http'):\n        image_raw = global_http_connection.get_bytes(\n            image_url, timeout=VLLM_IMAGE_FETCH_TIMEOUT)\n        image = _load_image_from_bytes(image_raw)\n\n    elif image_url.startswith('data:image'):\n        image = _load_image_from_data_url(image_url)\n    else:\n        raise ValueError(\"Invalid 'image_url': A valid 'image_url' must start \"\n                         \"with either 'data:image' or 'http'.\")\n\n    return image.convert(image_mode)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef fetch_audio(audio_url: str) -> Tuple[np.ndarray, Union[int, float]]:\n    \"\"\"\n    Load audio from a URL.\n    \"\"\"\n    if audio_url.startswith(\"http\"):\n        audio_bytes = global_http_connection.get_bytes(\n            audio_url, timeout=VLLM_AUDIO_FETCH_TIMEOUT)\n    elif audio_url.startswith(\"data:audio\"):\n        _, audio_base64 = audio_url.split(\",\", 1)\n        audio_bytes = base64.b64decode(audio_base64)\n    else:\n        raise ValueError(\"Invalid 'audio_url': A valid 'audio_url' must start \"\n                         \"with either 'data:audio' or 'http'.\")\n\n    return librosa.load(BytesIO(audio_bytes), sr=None)\n\n\nasync def async_fetch_audio(\n        audio_url: str) -> Tuple[np.ndarray, Union[int, float]]:\n    \"\"\"\n    Asynchronously fetch audio from a URL.\n    \"\"\"\n    if audio_url.startswith(\"http\"):\n        audio_bytes = await global_http_connection.async_get_bytes(\n            audio_url, timeout=VLLM_AUDIO_FETCH_TIMEOUT)\n    elif audio_url.startswith(\"data:audio\"):\n        _, audio_base64 = audio_url.split(\",\", 1)\n        audio_bytes = base64.b64decode(audio_base64)\n    else:\n        raise ValueError(\"Invalid 'audio_url': A valid 'audio_url' must start \"\n                         \"with either 'data:audio' or 'http'.\")\n\n    return librosa.load(BytesIO(audio_bytes), sr=None)\n\n\nasync def async_get_and_parse_audio(audio_url: str) -> MultiModalDataDict:\n    audio, sr = await async_fetch_audio(audio_url)\n    return {\"audio\": (audio, sr)}\n\n\nasync def async_get_and_parse_image(image_url: str) -> MultiModalDataDict:\n    image = await async_fetch_image(image_url)\n    return {\"image\": image}\n\n\ndef encode_audio_base64(\n    audio: np.ndarray,\n    sampling_rate: int,\n) -> str:\n    \"\"\"Encode audio as base64.\"\"\"\n    buffered = BytesIO()\n    soundfile.write(buffered, audio, sampling_rate, format=\"WAV\")\n\n    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n\n\ndef encode_image_base64(\n    image: Image.Image,\n    *,\n    image_mode: str = \"RGB\",\n    format: str = \"JPEG\",\n) -> str:\n    \"\"\"\n    Encode a pillow image to base64 format.\n\n    By default, the image is converted into RGB format before being encoded.\n    \"\"\"\n    buffered = BytesIO()\n    image = image.convert(image_mode)\n    image.save(buffered, format)\n    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n\n\ndef load_image_from_base64(image: Union[bytes, str]) -> Image.Image:\n    \"\"\"Load image from base64 format.\"\"\"\n    return _load_image_from_bytes(base64.b64decode(image))\n\n\ndef rescale_image_size(image: Image.Image,\n                       size_factor: float,\n                       transpose: int = -1) -> Image.Image:\n    \"\"\"Rescale the dimensions of an image by a constant factor.\"\"\"\n    new_width = int(image.width * size_factor)\n    new_height = int(image.height * size_factor)\n    image = image.resize((new_width, new_height))\n    if transpose >= 0:\n        image = image.transpose(Image.Transpose(transpose))\n    return image\n\n```\n# Function Signature: async def async_fetch_image(image_url: str,\n# Natural Language Annotations: \"\"\"\n    Asynchronously load a PIL image from a HTTP or base64 data URL.\n\n    By default, the image is converted into RGB format.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def async_fetch_image(image_url: str", "  ```python\nimport asyncio\nimport base64\nfrom io", "  ```\nasync def async_fetch_image(image_url: str"]}, "marimo-marimo/_islands/island_generator.py-add_code": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_islands/island_generator.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport json\nfrom textwrap import dedent\nfrom typing import TYPE_CHECKING, List, Optional, Union, cast\n\nfrom marimo import __version__, _loggers\nfrom marimo._ast.app import App, InternalApp, _AppConfig\nfrom marimo._ast.cell import Cell, CellConfig\nfrom marimo._ast.compiler import compile_cell\nfrom marimo._messaging.cell_output import CellOutput\nfrom marimo._output.formatting import as_html\nfrom marimo._output.utils import uri_encode_component\nfrom marimo._plugins.stateless.json_output import json_output\nfrom marimo._plugins.ui import code_editor\nfrom marimo._server.export import run_app_until_completion\nfrom marimo._server.file_manager import AppFileManager\nfrom marimo._server.file_router import AppFileRouter\nfrom marimo._utils.marimo_path import MarimoPath\n\nif TYPE_CHECKING:\n    from marimo._server.session.session_view import SessionView\n\nLOGGER = _loggers.marimo_logger()\n\n\nclass MarimoIslandStub:\n    def __init__(\n        self,\n        display_code: bool = False,\n        display_output: bool = True,\n        is_reactive: bool = True,\n        *,\n        cell_id: str,\n        app_id: str,\n        code: str,\n    ):\n        self._cell_id = cell_id\n        self._app_id = app_id\n        self._code = code\n        self._display_code = display_code\n        self._display_output = display_output\n        self._is_reactive = is_reactive\n\n        self._internal_app: Optional[InternalApp] = None\n        self._session_view: Optional[SessionView] = None\n        self._output: Optional[CellOutput] = None\n\n    @property\n    def output(self) -> Optional[CellOutput]:\n        # Leave output accessible for direct use for non-interactive cases e.g.\n        # pdf.\n        if self._output is None:\n            if self._session_view is not None:\n                outputs = self._session_view.get_cell_outputs([self._cell_id])\n                self._output = outputs.get(self._cell_id, None)\n        return self._output\n\n    @property\n    def code(self) -> str:\n        return self._code\n\n    def render(\n        self,\n        display_code: Optional[bool] = None,\n        display_output: Optional[bool] = None,\n        is_reactive: Optional[bool] = None,\n    ) -> str:\n        \"\"\"\n        Render the HTML island code for the cell.\n        Note: This will override construction defaults.\n\n        *Args:*\n\n        - display_code (bool): Whether to display the code in HTML.\n        - display_output (bool): Whether to include the output in the HTML.\n        - is_reactive (bool): Whether this code block will run with pyodide.\n\n        *Returns:*\n\n        - str: The HTML code.\n        \"\"\"\n\n        is_reactive = (\n            is_reactive if is_reactive is not None else self._is_reactive\n        )\n        display_code = (\n            display_code if display_code is not None else self._display_code\n        )\n        display_output = (\n            display_output\n            if display_output is not None\n            else self._display_output\n        )\n\n        if not (display_code or display_output or is_reactive):\n            raise ValueError(\"You must include either code or output\")\n\n        output = handle_mimetypes(self.output) if self.output else None\n\n        # Specifying display_code=False will hide the code block, but still\n        # make it present for reactivity, unless reactivity is disabled.\n        if display_code:\n            # TODO: Allow for non-disabled code editors.\n            code_block = as_html(code_editor(self.code, disabled=False)).text\n        else:\n            code_block = (\n                \"<marimo-cell-code hidden>\"\n                f\"{uri_encode_component(self.code) if is_reactive else ''}\"\n                \"</marimo-cell-code>\"\n            )\n\n        # Cell may not have output\n        # (e.g. imports, but still needs to be included)\n        return remove_empty_lines(\n            dedent(\n                f\"\"\"\n        <marimo-island\n            data-app-id=\"{self._app_id}\"\n            data-cell-id=\"{self._cell_id}\"\n            data-reactive=\"{json.dumps(is_reactive)}\"\n        >\n            <marimo-cell-output>\n            {output if output and display_output else \"\"}\n            </marimo-cell-output>\n            {code_block}\n        </marimo-island>\n        \"\"\"\n            ).strip()\n        )\n\n\nclass MarimoIslandGenerator:\n    \"\"\"\n    Generates Marimo islands for embedding in other pages.\n\n    This is a great way to use another SSG framework that converts\n    Python code to HTML using marimo-islands.\n\n    Generally you will want to:\n\n    1. Find all the code snippets and add them to the generator.\n    2. Build the app.\n    3. Replace all code snippets with the rendered HTML.\n    4. Include the header in the <head> tag.\n\n    # Example\n\n    ```python\n    from marimo import MarimoIslandGenerator\n\n    generator = MarimoIslandGenerator()\n    block1 = generator.add_code(\"import marimo as mo\")\n    block2 = generator.add_code(\"mo.md('Hello, islands!')\")\n\n    # Build the app\n    app = await generator.build()\n\n    # Render the app\n    output = f\\\"\\\"\\\"\n    <html>\n        <head>\n            {generator.render_head()}\n        </head>\n        <body>\n            {block1.render(display_output=False)}\n            {block2.render()}\n        </body>\n    </html>\n    \\\"\\\"\\\"\n    ```\n    \"\"\"\n\n    def __init__(self, app_id: str = \"main\"):\n        self.has_run = False\n        self._app_id = app_id\n        self._app = InternalApp(App())\n        self._stubs: List[MarimoIslandStub] = []\n        self._config = _AppConfig()\n\n    @staticmethod\n    def from_file(\n        filename: str,\n        display_code: bool = False,\n    ) -> MarimoIslandGenerator:\n        \"\"\"\n        Create a MarimoIslandGenerator and populate MarimoIslandStubs\n        using code cells from a marimo *.py file.\n\n        *Args:*\n\n        - filename (str): Marimo .py filename to convert to reactive HTML.\n        - display_code (bool): Whether to display the code in HTML snippets.\n        \"\"\"\n        path = MarimoPath(filename)\n        file_router = AppFileRouter.from_filename(path)\n        file_key = file_router.get_unique_file_key()\n        assert file_key is not None\n        file_manager = file_router.get_file_manager(file_key)\n\n        generator = MarimoIslandGenerator()\n        stubs = []\n        for cell_data in file_manager.app.cell_manager.cell_data():\n            stubs.append(\n                generator.add_code(\n                    cell_data.code,\n                    display_code=display_code,\n                )\n            )\n\n        generator._stubs = stubs\n        generator._config = file_manager.app.config\n\n        return generator\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def render_head(\n        self,\n        *,\n        version_override: str = __version__,\n        _development_url: Union[str | bool] = False,\n    ) -> str:\n        \"\"\"\n        Render the header for the app.\n        This should be included in the <head> tag of the page.\n\n        *Args:*\n\n        - version_override (str): Marimo version to use for loaded js/css.\n        - _development_url (str): If True, uses local marimo islands js.\n        \"\"\"\n\n        # This loads:\n        # - The marimo islands js\n        # - The marimo islands css\n        # - Preconnects to Google Fonts (https://stackoverflow.com/questions/73838138)\n        # - Fonts from Google Fonts\n        #   (otherwise they would get bundled in the css)\n        # - Fonts from KaTeX\n        #   (otherwise they would get bundled in the css)\n\n        base_url = f\"https://cdn.jsdelivr.net/npm/@marimo-team/islands@{version_override}\"\n        # This should be kept in sync fonts.css in the frontend\n        # Since this is embedded on other pages, we want display=swap\n        # for the most compatible font loading\n        font_url = \"https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500;700&amp;family=Lora&amp;family=PT+Sans:wght@400;700&amp;display=swap\"\n\n        fonts = f\"\"\"\n            <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\" />\n            <link\n                rel=\"preconnect\"\n                href=\"https://fonts.gstatic.com\"\n                crossorigin\n            />\n            <link href=\"{font_url}\" rel=\"stylesheet\" />\n            <link\n                rel=\"stylesheet\"\n                href=\"https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css\"\n                integrity=\"sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww\"\n                crossorigin=\"anonymous\"\n            />\n        \"\"\".strip()\n\n        if _development_url:\n            base_url = \"http://localhost:5174\"\n            if isinstance(_development_url, str):\n                base_url = _development_url\n            return dedent(\n                f\"\"\"\n                <script\n                    type=\"module\"\n                    src=\"{base_url}/src/core/islands/main.ts\"\n                ></script>\n                {fonts}\n                \"\"\"\n            ).strip()\n\n        return dedent(\n            f\"\"\"\n            <script type=\"module\" src=\"{base_url}/dist/main.js\"></script>\n            <link\n                href=\"{base_url}/dist/style.css\"\n                rel=\"stylesheet\"\n                crossorigin=\"anonymous\"\n            />\n            {fonts}\n            \"\"\"\n        ).strip()\n\n    def render_init_island(self) -> str:\n        \"\"\"\n        Renders a static html MarimoIsland str which displays a spinning\n        initialization loader while Pyodide loads and disappears once\n        the kernel is ready to use.\n        \"\"\"\n\n        init_cell_id = self._app.cell_manager.create_cell_id()\n        init_input = \"<marimo-cell-code hidden> '' </marimo-cell-code>\"\n        init_output = \"\"\"\n        <div class=\"marimo\" style=\"--tw-bg-opacity: 0;\">\n          <div class=\"flex flex-col items-center justify-center\">\n            <svg\n              xmlns=\"http://www.w3.org/2000/svg\"\n              width=\"24\"\n              height=\"24\"\n              viewBox=\"0 0 24 24\"\n              fill=\"none\"\n              stroke=\"currentColor\"\n              stroke-width=\"1\"\n              stroke-linecap=\"round\"\n              stroke-linejoin=\"round\"\n              class=\"size-20 animate-spin text-primary\"\n            >\n              <path d=\"M21 12a9 9 0 1 1-6.219-8.56\"></path>\n            </svg>\n            <div>Initializing...</div>\n          </div>\n        </div>\n        \"\"\"\n        init_island = dedent(\n            f\"\"\"\n            <marimo-island\n                data-app-id=\"{self._app_id}\"\n                data-cell-id=\"{init_cell_id}\"\n                data-reactive=\"{json.dumps(True)}\"\n            >\n                <marimo-cell-output>\n                {init_output}\n                </marimo-cell-output>\n                {init_input}\n            </marimo-island>\n            \"\"\"\n        ).strip()\n\n        return init_island\n\n    def render_body(\n        self,\n        *,\n        include_init_island: bool = True,\n        max_width: Optional[str] = None,\n        margin: Optional[str] = None,\n        style: Optional[str] = None,\n    ) -> str:\n        \"\"\"\n        Render the body for the app.\n        This should be included in the <body> tag of the page.\n\n        *Args:*\n        - include_init_island (bool): If True, adds initialization loader.\n        - max_width (str): CSS style max_width property.\n        - margin (str): CSS style margin property.\n        - style (str): CSS style. Overrides max_width and margin.\n        \"\"\"\n\n        rendered_stubs = []\n        for stub in self._stubs:\n            rendered_stubs.append(stub.render())\n\n        if include_init_island:\n            init_island = self.render_init_island()\n            rendered_stubs = [init_island] + rendered_stubs\n\n        body = \"\\n\".join(rendered_stubs)\n\n        if margin is None:\n            margin = \"auto\"\n        if max_width is None:\n            width = self._config.width\n            if width == \"compact\" or width == \"normal\":\n                max_width = \"740px\"\n            elif width == \"medium\":\n                max_width = \"1110px\"\n            else:\n                max_width = \"none\"\n\n        if style is None:\n            style = f\"margin: {margin}; max-width: {max_width};\"\n\n        return dedent(\n            f\"\"\"\n                <div style=\"{style}\">\n                  {body}\n                </div>\n                \"\"\"\n        ).strip()\n\n    def render_html(\n        self,\n        *,\n        version_override: str = __version__,\n        _development_url: Union[str | bool] = False,\n        include_init_island: bool = True,\n        max_width: Optional[str] = None,\n        margin: Optional[str] = None,\n        style: Optional[str] = None,\n    ) -> str:\n        \"\"\"\n        Render reactive html for the app.\n\n        *Args:*\n\n        - version_override (str): Marimo version to use for loaded js/css.\n        - _development_url (str): If True, uses local marimo islands js.\n        - include_init_island (bool): If True, adds initialization loader.\n        - max_width (str): CSS style max_width property.\n        - margin (str): CSS style margin property.\n        - style (str): CSS style. Overrides max_width and margin.\n        \"\"\"\n        head = self.render_head(\n            version_override=version_override,\n            _development_url=_development_url,\n        )\n        body = self.render_body(\n            include_init_island=include_init_island,\n            max_width=max_width,\n            margin=margin,\n            style=style,\n        )\n        title = (\n            self._app_id\n            if self._config.app_title is None\n            else self._config.app_title\n        )\n\n        return dedent(\n            f\"\"\"<!doctype html>\n                <html lang=\"en\">\n                    <head>\n                      <meta charset=\"UTF-8\" />\n                      <title> {title} </title>\n                        {head}\n                    </head>\n                    <body>\n                      {body}\n                    </body>\n                </html>\n                \"\"\"\n        ).strip()\n\n\ndef remove_empty_lines(text: str) -> str:\n    return \"\\n\".join([line for line in text.split(\"\\n\") if line.strip() != \"\"])\n\n\ndef handle_mimetypes(output: CellOutput) -> str:\n    data = output.data\n    if not isinstance(data, str):\n        return f\"{data}\"\n    mimetype = output.mimetype\n    # Since raw data, without wrapping in an image tag, this is just a huge\n    # blob.\n    if mimetype.startswith(\"image/\"):\n        data = f\"<img src='{data}'/>\"\n    elif mimetype == \"application/json\":\n        data = f\"{json_output(json.loads(data))}\"\n    # TODO: Errors are displayed as just json strings until reactivity kicks\n    # in. Ideally, handle application/vnd.marimo+error\n    return data\n\n```\n# Function Signature: def add_code(\n# Natural Language Annotations: \"\"\"Add a code cell to the app.\n\n        *Args:*\n\n        - code (str): The code to add to the app.\n        - display_code (bool): Whether to display the code in the HTML.\n        - display_output (bool): Whether to display the output in the HTML.\n        - is_raw (bool): Whether to handled the code without formatting.\n        - is_reactive (bool): Whether this code block will run with pyodide.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```python\ndef add_code(\n    code: str,\n", "  ```\ndef add_code(code, display_code=False,", "  ```python\ndef add_code(\n    self,\n    code"]}, "marimo-marimo/_islands/island_generator.py-build": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_islands/island_generator.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport json\nfrom textwrap import dedent\nfrom typing import TYPE_CHECKING, List, Optional, Union, cast\n\nfrom marimo import __version__, _loggers\nfrom marimo._ast.app import App, InternalApp, _AppConfig\nfrom marimo._ast.cell import Cell, CellConfig\nfrom marimo._ast.compiler import compile_cell\nfrom marimo._messaging.cell_output import CellOutput\nfrom marimo._output.formatting import as_html\nfrom marimo._output.utils import uri_encode_component\nfrom marimo._plugins.stateless.json_output import json_output\nfrom marimo._plugins.ui import code_editor\nfrom marimo._server.export import run_app_until_completion\nfrom marimo._server.file_manager import AppFileManager\nfrom marimo._server.file_router import AppFileRouter\nfrom marimo._utils.marimo_path import MarimoPath\n\nif TYPE_CHECKING:\n    from marimo._server.session.session_view import SessionView\n\nLOGGER = _loggers.marimo_logger()\n\n\nclass MarimoIslandStub:\n    def __init__(\n        self,\n        display_code: bool = False,\n        display_output: bool = True,\n        is_reactive: bool = True,\n        *,\n        cell_id: str,\n        app_id: str,\n        code: str,\n    ):\n        self._cell_id = cell_id\n        self._app_id = app_id\n        self._code = code\n        self._display_code = display_code\n        self._display_output = display_output\n        self._is_reactive = is_reactive\n\n        self._internal_app: Optional[InternalApp] = None\n        self._session_view: Optional[SessionView] = None\n        self._output: Optional[CellOutput] = None\n\n    @property\n    def output(self) -> Optional[CellOutput]:\n        # Leave output accessible for direct use for non-interactive cases e.g.\n        # pdf.\n        if self._output is None:\n            if self._session_view is not None:\n                outputs = self._session_view.get_cell_outputs([self._cell_id])\n                self._output = outputs.get(self._cell_id, None)\n        return self._output\n\n    @property\n    def code(self) -> str:\n        return self._code\n\n    def render(\n        self,\n        display_code: Optional[bool] = None,\n        display_output: Optional[bool] = None,\n        is_reactive: Optional[bool] = None,\n    ) -> str:\n        \"\"\"\n        Render the HTML island code for the cell.\n        Note: This will override construction defaults.\n\n        *Args:*\n\n        - display_code (bool): Whether to display the code in HTML.\n        - display_output (bool): Whether to include the output in the HTML.\n        - is_reactive (bool): Whether this code block will run with pyodide.\n\n        *Returns:*\n\n        - str: The HTML code.\n        \"\"\"\n\n        is_reactive = (\n            is_reactive if is_reactive is not None else self._is_reactive\n        )\n        display_code = (\n            display_code if display_code is not None else self._display_code\n        )\n        display_output = (\n            display_output\n            if display_output is not None\n            else self._display_output\n        )\n\n        if not (display_code or display_output or is_reactive):\n            raise ValueError(\"You must include either code or output\")\n\n        output = handle_mimetypes(self.output) if self.output else None\n\n        # Specifying display_code=False will hide the code block, but still\n        # make it present for reactivity, unless reactivity is disabled.\n        if display_code:\n            # TODO: Allow for non-disabled code editors.\n            code_block = as_html(code_editor(self.code, disabled=False)).text\n        else:\n            code_block = (\n                \"<marimo-cell-code hidden>\"\n                f\"{uri_encode_component(self.code) if is_reactive else ''}\"\n                \"</marimo-cell-code>\"\n            )\n\n        # Cell may not have output\n        # (e.g. imports, but still needs to be included)\n        return remove_empty_lines(\n            dedent(\n                f\"\"\"\n        <marimo-island\n            data-app-id=\"{self._app_id}\"\n            data-cell-id=\"{self._cell_id}\"\n            data-reactive=\"{json.dumps(is_reactive)}\"\n        >\n            <marimo-cell-output>\n            {output if output and display_output else \"\"}\n            </marimo-cell-output>\n            {code_block}\n        </marimo-island>\n        \"\"\"\n            ).strip()\n        )\n\n\nclass MarimoIslandGenerator:\n    \"\"\"\n    Generates Marimo islands for embedding in other pages.\n\n    This is a great way to use another SSG framework that converts\n    Python code to HTML using marimo-islands.\n\n    Generally you will want to:\n\n    1. Find all the code snippets and add them to the generator.\n    2. Build the app.\n    3. Replace all code snippets with the rendered HTML.\n    4. Include the header in the <head> tag.\n\n    # Example\n\n    ```python\n    from marimo import MarimoIslandGenerator\n\n    generator = MarimoIslandGenerator()\n    block1 = generator.add_code(\"import marimo as mo\")\n    block2 = generator.add_code(\"mo.md('Hello, islands!')\")\n\n    # Build the app\n    app = await generator.build()\n\n    # Render the app\n    output = f\\\"\\\"\\\"\n    <html>\n        <head>\n            {generator.render_head()}\n        </head>\n        <body>\n            {block1.render(display_output=False)}\n            {block2.render()}\n        </body>\n    </html>\n    \\\"\\\"\\\"\n    ```\n    \"\"\"\n\n    def __init__(self, app_id: str = \"main\"):\n        self.has_run = False\n        self._app_id = app_id\n        self._app = InternalApp(App())\n        self._stubs: List[MarimoIslandStub] = []\n        self._config = _AppConfig()\n\n    @staticmethod\n    def from_file(\n        filename: str,\n        display_code: bool = False,\n    ) -> MarimoIslandGenerator:\n        \"\"\"\n        Create a MarimoIslandGenerator and populate MarimoIslandStubs\n        using code cells from a marimo *.py file.\n\n        *Args:*\n\n        - filename (str): Marimo .py filename to convert to reactive HTML.\n        - display_code (bool): Whether to display the code in HTML snippets.\n        \"\"\"\n        path = MarimoPath(filename)\n        file_router = AppFileRouter.from_filename(path)\n        file_key = file_router.get_unique_file_key()\n        assert file_key is not None\n        file_manager = file_router.get_file_manager(file_key)\n\n        generator = MarimoIslandGenerator()\n        stubs = []\n        for cell_data in file_manager.app.cell_manager.cell_data():\n            stubs.append(\n                generator.add_code(\n                    cell_data.code,\n                    display_code=display_code,\n                )\n            )\n\n        generator._stubs = stubs\n        generator._config = file_manager.app.config\n\n        return generator\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def render_head(\n        self,\n        *,\n        version_override: str = __version__,\n        _development_url: Union[str | bool] = False,\n    ) -> str:\n        \"\"\"\n        Render the header for the app.\n        This should be included in the <head> tag of the page.\n\n        *Args:*\n\n        - version_override (str): Marimo version to use for loaded js/css.\n        - _development_url (str): If True, uses local marimo islands js.\n        \"\"\"\n\n        # This loads:\n        # - The marimo islands js\n        # - The marimo islands css\n        # - Preconnects to Google Fonts (https://stackoverflow.com/questions/73838138)\n        # - Fonts from Google Fonts\n        #   (otherwise they would get bundled in the css)\n        # - Fonts from KaTeX\n        #   (otherwise they would get bundled in the css)\n\n        base_url = f\"https://cdn.jsdelivr.net/npm/@marimo-team/islands@{version_override}\"\n        # This should be kept in sync fonts.css in the frontend\n        # Since this is embedded on other pages, we want display=swap\n        # for the most compatible font loading\n        font_url = \"https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500;700&amp;family=Lora&amp;family=PT+Sans:wght@400;700&amp;display=swap\"\n\n        fonts = f\"\"\"\n            <link rel=\"preconnect\" href=\"https://fonts.googleapis.com\" />\n            <link\n                rel=\"preconnect\"\n                href=\"https://fonts.gstatic.com\"\n                crossorigin\n            />\n            <link href=\"{font_url}\" rel=\"stylesheet\" />\n            <link\n                rel=\"stylesheet\"\n                href=\"https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css\"\n                integrity=\"sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww\"\n                crossorigin=\"anonymous\"\n            />\n        \"\"\".strip()\n\n        if _development_url:\n            base_url = \"http://localhost:5174\"\n            if isinstance(_development_url, str):\n                base_url = _development_url\n            return dedent(\n                f\"\"\"\n                <script\n                    type=\"module\"\n                    src=\"{base_url}/src/core/islands/main.ts\"\n                ></script>\n                {fonts}\n                \"\"\"\n            ).strip()\n\n        return dedent(\n            f\"\"\"\n            <script type=\"module\" src=\"{base_url}/dist/main.js\"></script>\n            <link\n                href=\"{base_url}/dist/style.css\"\n                rel=\"stylesheet\"\n                crossorigin=\"anonymous\"\n            />\n            {fonts}\n            \"\"\"\n        ).strip()\n\n    def render_init_island(self) -> str:\n        \"\"\"\n        Renders a static html MarimoIsland str which displays a spinning\n        initialization loader while Pyodide loads and disappears once\n        the kernel is ready to use.\n        \"\"\"\n\n        init_cell_id = self._app.cell_manager.create_cell_id()\n        init_input = \"<marimo-cell-code hidden> '' </marimo-cell-code>\"\n        init_output = \"\"\"\n        <div class=\"marimo\" style=\"--tw-bg-opacity: 0;\">\n          <div class=\"flex flex-col items-center justify-center\">\n            <svg\n              xmlns=\"http://www.w3.org/2000/svg\"\n              width=\"24\"\n              height=\"24\"\n              viewBox=\"0 0 24 24\"\n              fill=\"none\"\n              stroke=\"currentColor\"\n              stroke-width=\"1\"\n              stroke-linecap=\"round\"\n              stroke-linejoin=\"round\"\n              class=\"size-20 animate-spin text-primary\"\n            >\n              <path d=\"M21 12a9 9 0 1 1-6.219-8.56\"></path>\n            </svg>\n            <div>Initializing...</div>\n          </div>\n        </div>\n        \"\"\"\n        init_island = dedent(\n            f\"\"\"\n            <marimo-island\n                data-app-id=\"{self._app_id}\"\n                data-cell-id=\"{init_cell_id}\"\n                data-reactive=\"{json.dumps(True)}\"\n            >\n                <marimo-cell-output>\n                {init_output}\n                </marimo-cell-output>\n                {init_input}\n            </marimo-island>\n            \"\"\"\n        ).strip()\n\n        return init_island\n\n    def render_body(\n        self,\n        *,\n        include_init_island: bool = True,\n        max_width: Optional[str] = None,\n        margin: Optional[str] = None,\n        style: Optional[str] = None,\n    ) -> str:\n        \"\"\"\n        Render the body for the app.\n        This should be included in the <body> tag of the page.\n\n        *Args:*\n        - include_init_island (bool): If True, adds initialization loader.\n        - max_width (str): CSS style max_width property.\n        - margin (str): CSS style margin property.\n        - style (str): CSS style. Overrides max_width and margin.\n        \"\"\"\n\n        rendered_stubs = []\n        for stub in self._stubs:\n            rendered_stubs.append(stub.render())\n\n        if include_init_island:\n            init_island = self.render_init_island()\n            rendered_stubs = [init_island] + rendered_stubs\n\n        body = \"\\n\".join(rendered_stubs)\n\n        if margin is None:\n            margin = \"auto\"\n        if max_width is None:\n            width = self._config.width\n            if width == \"compact\" or width == \"normal\":\n                max_width = \"740px\"\n            elif width == \"medium\":\n                max_width = \"1110px\"\n            else:\n                max_width = \"none\"\n\n        if style is None:\n            style = f\"margin: {margin}; max-width: {max_width};\"\n\n        return dedent(\n            f\"\"\"\n                <div style=\"{style}\">\n                  {body}\n                </div>\n                \"\"\"\n        ).strip()\n\n    def render_html(\n        self,\n        *,\n        version_override: str = __version__,\n        _development_url: Union[str | bool] = False,\n        include_init_island: bool = True,\n        max_width: Optional[str] = None,\n        margin: Optional[str] = None,\n        style: Optional[str] = None,\n    ) -> str:\n        \"\"\"\n        Render reactive html for the app.\n\n        *Args:*\n\n        - version_override (str): Marimo version to use for loaded js/css.\n        - _development_url (str): If True, uses local marimo islands js.\n        - include_init_island (bool): If True, adds initialization loader.\n        - max_width (str): CSS style max_width property.\n        - margin (str): CSS style margin property.\n        - style (str): CSS style. Overrides max_width and margin.\n        \"\"\"\n        head = self.render_head(\n            version_override=version_override,\n            _development_url=_development_url,\n        )\n        body = self.render_body(\n            include_init_island=include_init_island,\n            max_width=max_width,\n            margin=margin,\n            style=style,\n        )\n        title = (\n            self._app_id\n            if self._config.app_title is None\n            else self._config.app_title\n        )\n\n        return dedent(\n            f\"\"\"<!doctype html>\n                <html lang=\"en\">\n                    <head>\n                      <meta charset=\"UTF-8\" />\n                      <title> {title} </title>\n                        {head}\n                    </head>\n                    <body>\n                      {body}\n                    </body>\n                </html>\n                \"\"\"\n        ).strip()\n\n\ndef remove_empty_lines(text: str) -> str:\n    return \"\\n\".join([line for line in text.split(\"\\n\") if line.strip() != \"\"])\n\n\ndef handle_mimetypes(output: CellOutput) -> str:\n    data = output.data\n    if not isinstance(data, str):\n        return f\"{data}\"\n    mimetype = output.mimetype\n    # Since raw data, without wrapping in an image tag, this is just a huge\n    # blob.\n    if mimetype.startswith(\"image/\"):\n        data = f\"<img src='{data}'/>\"\n    elif mimetype == \"application/json\":\n        data = f\"{json_output(json.loads(data))}\"\n    # TODO: Errors are displayed as just json strings until reactivity kicks\n    # in. Ideally, handle application/vnd.marimo+error\n    return data\n\n```\n# Function Signature: async def build(self) -> App:\n# Natural Language Annotations: \"\"\"\n        Build the app. This should be called after adding all the code cells.\n\n        *Returns:*\n\n        - App: The built app.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def build(self) -> App:\n    if not", "  ```\nasync def build(self) -> App:\n    # Copy", "  ```\nasync def build(self) -> App:\n    \"\"\"\n"]}, "marimo-marimo/_utils/parse_dataclass.py-build_dataclass": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_utils/parse_dataclass.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport dataclasses\nimport json\nfrom enum import Enum\nfrom typing import (\n    Any,\n    Literal,\n    Optional,\n    Type,\n    TypeVar,\n    Union,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\nT = TypeVar(\"T\")\n\n\ndef to_snake(string: str) -> str:\n    # basic conversion of javascript camel case to snake\n    # does not handle contiguous caps\n    return \"\".join(\n        [\"_\" + i.lower() if i.isupper() else i for i in string]\n    ).lstrip(\"_\")\n\n\ndef _build_value(value: Any, cls: Type[T]) -> T:\n    # origin_cls is not None if cls is a container (such as list, tuple, set,\n    # ...)\n    origin_cls = get_origin(cls)\n    if origin_cls is Optional:\n        (arg_type,) = get_args(cls)\n        if value is None:\n            return None  # type: ignore[return-value]\n        else:\n            return _build_value(value, arg_type)  # type: ignore # noqa: E501\n    elif origin_cls in (list, set):\n        (arg_type,) = get_args(cls)\n        return origin_cls(_build_value(v, arg_type) for v in value)  # type: ignore # noqa: E501\n    elif origin_cls is tuple:\n        arg_types = get_args(cls)\n        if len(arg_types) == 2 and isinstance(arg_types[1], type(Ellipsis)):\n            return origin_cls(_build_value(v, arg_types[0]) for v in value)  # type: ignore # noqa: E501\n        else:\n            return origin_cls(  # type: ignore # noqa: E501\n                _build_value(v, t) for v, t in zip(value, arg_types)\n            )\n    elif origin_cls is dict:\n        key_type, value_type = get_args(cls)\n        return origin_cls(  # type: ignore[no-any-return]\n            **{\n                _build_value(k, key_type): _build_value(v, value_type)\n                for k, v in value.items()\n            }\n        )\n    elif origin_cls == Union:\n        arg_types = get_args(cls)\n        for arg_type in arg_types:\n            try:\n                return _build_value(value, arg_type)  # type: ignore # noqa: E501\n            # catch expected exceptions when conversion fails\n            except (TypeError, ValueError):\n                continue\n            except:\n                raise\n        raise ValueError(f\"Value '{value}' does not fit any type of the union\")\n    elif origin_cls is Literal:\n        # if its a single Literal of an enum, we can just return the enum\n        arg_types = get_args(cls)\n        first_arg_type = arg_types[0]\n        if (\n            len(arg_types) == 1\n            and isinstance(first_arg_type, Enum)\n            and first_arg_type.value == value\n        ):\n            return first_arg_type  # type: ignore[return-value]\n        if value not in arg_types:\n            raise ValueError(\n                f\"Value '{value}' does not fit any type of the literal\"\n            )\n        return value  # type: ignore[no-any-return]\n    elif type(cls) == type(Enum) and issubclass(cls, Enum):\n        return cls(value)  # type: ignore[return-value]\n    elif dataclasses.is_dataclass(cls):\n        return build_dataclass(value, cls)  # type: ignore[return-value]\n    else:\n        return value  # type: ignore[no-any-return]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def build_dataclass(values: dict[Any, Any], cls: Type[T]) -> T:\n# Natural Language Annotations: \"\"\"Returns instance of dataclass [cls] instantiated from [values].\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef build_dataclass(values: dict[Any, Any", "  ```\ndef build_dataclass(values: dict[Any, Any", "  ```\nimport dataclasses\n\ndef build_dataclass(values:"]}, "marimo-marimo/_utils/parse_dataclass.py-parse_raw": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_utils/parse_dataclass.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport dataclasses\nimport json\nfrom enum import Enum\nfrom typing import (\n    Any,\n    Literal,\n    Optional,\n    Type,\n    TypeVar,\n    Union,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\nT = TypeVar(\"T\")\n\n\ndef to_snake(string: str) -> str:\n    # basic conversion of javascript camel case to snake\n    # does not handle contiguous caps\n    return \"\".join(\n        [\"_\" + i.lower() if i.isupper() else i for i in string]\n    ).lstrip(\"_\")\n\n\ndef _build_value(value: Any, cls: Type[T]) -> T:\n    # origin_cls is not None if cls is a container (such as list, tuple, set,\n    # ...)\n    origin_cls = get_origin(cls)\n    if origin_cls is Optional:\n        (arg_type,) = get_args(cls)\n        if value is None:\n            return None  # type: ignore[return-value]\n        else:\n            return _build_value(value, arg_type)  # type: ignore # noqa: E501\n    elif origin_cls in (list, set):\n        (arg_type,) = get_args(cls)\n        return origin_cls(_build_value(v, arg_type) for v in value)  # type: ignore # noqa: E501\n    elif origin_cls is tuple:\n        arg_types = get_args(cls)\n        if len(arg_types) == 2 and isinstance(arg_types[1], type(Ellipsis)):\n            return origin_cls(_build_value(v, arg_types[0]) for v in value)  # type: ignore # noqa: E501\n        else:\n            return origin_cls(  # type: ignore # noqa: E501\n                _build_value(v, t) for v, t in zip(value, arg_types)\n            )\n    elif origin_cls is dict:\n        key_type, value_type = get_args(cls)\n        return origin_cls(  # type: ignore[no-any-return]\n            **{\n                _build_value(k, key_type): _build_value(v, value_type)\n                for k, v in value.items()\n            }\n        )\n    elif origin_cls == Union:\n        arg_types = get_args(cls)\n        for arg_type in arg_types:\n            try:\n                return _build_value(value, arg_type)  # type: ignore # noqa: E501\n            # catch expected exceptions when conversion fails\n            except (TypeError, ValueError):\n                continue\n            except:\n                raise\n        raise ValueError(f\"Value '{value}' does not fit any type of the union\")\n    elif origin_cls is Literal:\n        # if its a single Literal of an enum, we can just return the enum\n        arg_types = get_args(cls)\n        first_arg_type = arg_types[0]\n        if (\n            len(arg_types) == 1\n            and isinstance(first_arg_type, Enum)\n            and first_arg_type.value == value\n        ):\n            return first_arg_type  # type: ignore[return-value]\n        if value not in arg_types:\n            raise ValueError(\n                f\"Value '{value}' does not fit any type of the literal\"\n            )\n        return value  # type: ignore[no-any-return]\n    elif type(cls) == type(Enum) and issubclass(cls, Enum):\n        return cls(value)  # type: ignore[return-value]\n    elif dataclasses.is_dataclass(cls):\n        return build_dataclass(value, cls)  # type: ignore[return-value]\n    else:\n        return value  # type: ignore[no-any-return]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def parse_raw(message: Union[bytes, dict[Any, Any]], cls: Type[T]) -> T:\n# Natural Language Annotations: \"\"\"Utility to parse a message as JSON, and instantiate into supplied type.\n\n    `cls` must be a dataclass.\n\n    Supported collection types in the dataclass:\n    - List, Tuple, Set, Dict\n    - for Python 3.8 compatibility, must use collection types from\n      the typing module (e.g., typing.List[int] instead of list[int])\n\n    Transforms all fields in the parsed JSON from camel case to snake case.\n\n    Args:\n    ----\n    message: the message to parse\n    cls: the type to instantiate\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef parse_raw(message: Union[bytes, dict[", "  ```\ndef parse_raw(message: Union[bytes, dict[", "  ```\n@dataclass\nclass MarimoMessage {\n    def parse"]}, "marimo-marimo/_plugins/stateless/icon.py-icon": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_plugins/stateless/icon.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nfrom typing import Literal, Optional, Union\n\nfrom marimo._output.builder import h\nfrom marimo._output.hypertext import Html\nfrom marimo._output.rich_help import mddoc\nfrom marimo._output.utils import create_style\n\n\n@mddoc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef _space_to_string(value: Union[str, int, float, None]) -> Optional[str]:\n    if value is None:\n        return None\n    if isinstance(value, str):\n        return value\n    else:\n        return f\"{value}px\"\n\n```\n# Function Signature: def icon(\n# Natural Language Annotations: \"\"\"\n    Displays an icon. These icons are referenced by name from the\n    [Iconify](https://iconify.design/) library.\n\n    They are named in the format `icon-set:icon-name`, e.g.\n    `lucide:leaf`.\n\n    Icons are lazily loaded from a CDN, so they will not be loaded when\n    not connected to the internet.\n\n    These can be used in buttons, tabs, and other UI elements.\n\n    **Examples.**\n\n    ```python\n    mo.md(f\"# {mo.icon('lucide:leaf')} Leaf\")\n\n    mo.ui.button(\n        label=f\"{mo.icon('lucide:rocket')} Submit\",\n    )\n    ```\n\n    **Args.**\n\n    - `icon_name`: the name of the icon to display\n    - `size`: the size of the icon in pixels\n    - `color`: the color of the icon\n    - `inline`: whether to display the icon inline or as a block element\n    - `flip`: whether to flip the icon horizontally, vertically, or both\n    - `rotate`: whether to rotate the icon 90, 180, or 270 degrees\n    - `style`: a dictionary of CSS styles to apply to the icon\n\n    **Returns.**\n\n    - An `Html` object.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport marimo\n\n\n@mddoc\ndef icon", "  ```\ndef icon(\n    icon_name: str,\n   ", "  ```\ndef icon(icon_name: str, size: Union["]}, "marimo-marimo/_plugins/stateless/image.py-image": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_plugins/stateless/image.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport io\nimport os\nfrom typing import Any, Optional, Union\n\nimport marimo._output.data.data as mo_data\nfrom marimo._dependencies.dependencies import DependencyManager\nfrom marimo._output.builder import h\nfrom marimo._output.hypertext import Html\nfrom marimo._output.rich_help import mddoc\nfrom marimo._output.utils import create_style\nfrom marimo._plugins.core.media import io_to_data_url\n\nImage = Union[str, bytes, io.BytesIO, io.BufferedReader]\n# Union[list, torch.Tensor, jax.numpy.ndarray,\n#             np.ndarray, scipy.sparse.spmatrix]\nTensor = Any\nImageLike = Union[Image, Tensor]\n\n\ndef _normalize_image(src: ImageLike) -> Image:\n    \"\"\"\n    Normalize an image-like object to a standard format.\n\n    This function handles a variety of input types, including lists, arrays,\n    and tensors, and converts them to a BytesIO object representing a PNG\n    image.\n\n    Typical convention for handling images is to use `PIL`, which is exactly\n    what `matplotlib` does behind the scenes. `PIL` requires a `ndarray`\n    (validated with the numpy specific `__array_interface__` attribute). In\n    turn, numpy can cast lists, and objects with the `__array__` method (like\n    jax, torch tensors). `scipy.sparse` breaks this convention but does have a\n    `toarray` method, which is general enough that a specific check is\n    performed here.\n\n    **Args.**\n\n    - `src`: An image-like object. This can be a list, array, tensor, or a\n        file-like object.\n\n    **Returns.**\n\n    A BytesIO object or other Image type.\n\n    **Raises.**\n\n    - `ModuleNotFoundError`: If the required `PIL` or `numpy` packages are not\n        available.\n    - `ValueError`: If the input is not a valid image-like object.\n    \"\"\"\n    if (\n        isinstance(src, list)\n        or hasattr(src, \"__array__\")\n        or hasattr(src, \"toarray\")\n    ):\n        DependencyManager.require_pillow(\n            \"to render images from arrays in `mo.image`\"\n        )\n        from PIL import Image as _Image\n\n        if not hasattr(src, \"__array_interface__\"):\n            DependencyManager.require_numpy(\n                \"to render images from generic arrays in `mo.image`\"\n            )\n            import numpy\n\n            # Capture those sparse cases\n            if hasattr(src, \"toarray\"):\n                src = src.toarray()\n            src = numpy.array(src)\n        src = (src - src.min()) / (src.max() - src.min()) * 255.0\n        img = _Image.fromarray(src.astype(\"uint8\"))\n        # io.BytesIO is one of the Image types.\n        normalized_src: Image = io.BytesIO()\n        img.save(normalized_src, format=\"PNG\")\n        return normalized_src\n    # Verify that this is a image object\n    if not isinstance(src, (str, bytes, io.BytesIO, io.BufferedReader)):\n        raise ValueError(\n            f\"Expected an image object, but got {type(src)} instead.\"\n        )\n    return src\n\n\n@mddoc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def image(\n# Natural Language Annotations: \"\"\"Render an image as HTML.\n\n    **Examples.**\n\n    ```python3\n    # Render an image from a local file\n    mo.image(src=\"path/to/image.png\")\n    ```\n\n    ```python3\n    # Render an image from a URL\n    mo.image(\n        src=\"https://marimo.io/logo.png\",\n        alt=\"Marimo logo\",\n        width=100,\n        height=100,\n        rounded=True,\n    )\n    ```\n\n    **Args.**\n\n    - `src`: a path or URL to an image, a file-like object\n        (opened in binary mode), or array-like object.\n    - `alt`: the alt text of the image\n    - `width`: the width of the image in pixels\n    - `height`: the height of the image in pixels\n    - `rounded`: whether to round the corners of the image\n    - `style`: a dictionary of CSS styles to apply to the image\n\n    **Returns.**\n\n    `Html` object\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef image(\n    src: Optional[ImageLike] =", "  ```\n@mddoc\ndef image(\n    src: Image", "  ```\n@mddoc\ndef image(\n    src: Image"]}, "marimo-marimo/_plugins/stateless/flex.py-hstack": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_plugins/stateless/flex.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Literal, Optional\n\nfrom marimo._output.builder import h\nfrom marimo._output.formatting import as_html\nfrom marimo._output.hypertext import Html\nfrom marimo._output.rich_help import mddoc\nfrom marimo._output.utils import create_style\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n\n\ndef _flex(\n    items: Sequence[object],\n    direction: Literal[\"row\", \"column\"],\n    justify: Literal[\n        \"start\", \"center\", \"end\", \"space-between\", \"space-around\"\n    ],\n    align: Optional[Literal[\"start\", \"end\", \"center\", \"stretch\"]],\n    wrap: bool,\n    gap: float,\n    child_flexes: Optional[Sequence[Optional[float]]],\n) -> Html:\n    justify_content_map = {\n        \"start\": \"flex-start\",\n        \"center\": \"center\",\n        \"end\": \"flex-end\",\n        \"space-between\": \"space-between\",\n        \"space-around\": \"space-around\",\n        None: \"space-between\",\n    }\n    align_items_map = {\n        \"start\": \"flex-start\",\n        \"center\": \"center\",\n        \"end\": \"flex-end\",\n        \"stretch\": \"stretch\",\n        None: \"normal\",\n    }\n    style = create_style(\n        {\n            \"display\": \"flex\",\n            \"flex\": \"1\",\n            \"flex-direction\": direction,\n            \"justify-content\": justify_content_map[justify],\n            \"align-items\": align_items_map[align],\n            \"flex-wrap\": \"wrap\" if wrap else \"nowrap\",\n            \"gap\": f\"{gap}rem\",\n        }\n    )\n\n    def create_style_for_item(idx: int) -> Optional[str]:\n        if child_flexes is None:\n            return \"\"\n        child_flex = child_flexes[idx]\n        if child_flex is None:\n            return \"\"\n        return create_style({\"flex\": f\"{child_flex}\"})\n\n    grid_items = [\n        h.div(as_html(item).text, style=create_style_for_item(i))\n        for i, item in enumerate(items)\n    ]\n    return Html(h.div(grid_items, style=style))\n\n\n@mddoc\ndef vstack(\n    items: Sequence[object],\n    *,\n    align: Optional[Literal[\"start\", \"end\", \"center\", \"stretch\"]] = None,\n    justify: Literal[\n        \"start\", \"center\", \"end\", \"space-between\", \"space-around\"\n    ] = \"start\",\n    gap: float = 0.5,\n    heights: Optional[Literal[\"equal\"] | Sequence[float]] = None,\n) -> Html:\n    \"\"\"Stack items vertically, in a column.\n\n    Combine with `hstack` to build a grid of items.\n\n    **Example.**\n\n    ```python3\n    # Build a column of items\n    mo.vstack([mo.md(\"...\"), mo.ui.text_area()])\n    ```\n\n    ```python3\n    # Build a grid.\n    mo.vstack(\n        [\n            mo.hstack([mo.md(\"...\"), mo.ui.text_area()]),\n            mo.hstack([mo.ui.checkbox(), mo.ui.text(), mo.ui.date()]),\n        ]\n    )\n    ```\n\n    **Args.**\n\n    - `items`: A list of items.\n    - `align`: Align items horizontally: start, end, center, or stretch.\n    - `justify`: Justify items vertically: start, center, end,\n    - `gap`: Gap between items as a float in rem. 1rem is 16px by default.\n    - `heights`: \"equal\" to give items equal height;\n        or a list of relative heights with same length as `items`,\n        eg, [1, 2] means the second item is twice as tall\n        as the first; or `None` for a sensible default\n\n    **Returns.**\n\n    - An `Html` object.\n    \"\"\"\n    return _flex(\n        items,\n        direction=\"column\",\n        justify=justify,\n        align=align,\n        wrap=False,\n        gap=gap,\n        child_flexes=[1 for _ in range(len(items))]\n        if heights == \"equal\"\n        else heights,\n    )\n\n\n@mddoc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# TODO(akshayka): Implement as a stateless plugin in frontend.\n# Unused, but may be nice to keep around in case we want to add `mo.grid`\ndef _spaced(\n    items: Sequence[object],\n    justify: Literal[\"left\", \"right\", \"center\", \"normal\"] = \"center\",\n    items_per_row: Optional[int] = None,\n    column_gap: float = 1,\n    row_gap: float = 1,\n) -> Html:\n    \"\"\"Space items evenly in row-major order.\n\n    A grid built with this function has a fixed number of items per row.\n    For more flexibility, use `hstack` and `vstack`.\n\n    **Args.**\n\n    - `items`: Items to arrange\n    - `justify`: Justify items normally, left, right, or center.\n    - `items_per_row`: Number of items to place in each row\n    - `column_gap`: Minimum gap in rem between columns\n    - `row_gap`: Minimum gap in rem between rows\n\n    **Returns.**\n\n    - An `Html` object.\n    \"\"\"\n    items_per_row = len(items) if items_per_row is None else items_per_row\n    style = create_style(\n        {\n            \"display\": \"grid\",\n            \"grid-template-columns\": f\"repeat({items_per_row}, 1fr)\",\n            \"column-gap\": f\"{column_gap}rem\",\n            \"row-gap\": f\"{row_gap}rem\",\n            \"justify-items\": justify,\n            \"overflow\": \"auto\",\n        }\n    )\n    grid_items = [h.div(as_html(item).text) for item in items]\n    return Html(h.div(grid_items, style=style))\n\n```\n# Function Signature: def hstack(\n# Natural Language Annotations: \"\"\"Stack items horizontally, in a row.\n\n    Combine with `vstack` to build a grid.\n\n    **Example.**\n\n    ```python3\n    # Build a row of items\n    mo.hstack([mo.md(\"...\"), mo.ui.text_area()])\n    ```\n\n    ```python3\n    # Build a grid.\n    mo.hstack(\n        [\n            mo.vstack([mo.md(\"...\"), mo.ui.text_area()]),\n            mo.vstack([mo.ui.checkbox(), mo.ui.text(), mo.ui.date()]),\n        ]\n    )\n    ```\n\n    **Args.**\n\n    - `items`: A list of items.\n    - `justify`: Justify items horizontally: start, center, end,\n        space-between, or space-around.\n    - `align`: Align items vertically: start, end, center, or stretch.\n    - `wrap`: Wrap items or not.\n    - `gap`: Gap between items as a float in rem. 1rem is 16px by default.\n    - `widths`: \"equal\" to give items equal width; or a list of relative widths\n      with same length as `items`, eg, [1, 2] means the second item is twice as\n      wide as the first; or `None` for a sensible default\n\n    **Returns.**\n\n    - An `Html` object.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef hstack(items: List[object], justify: Optional", "  ```\ndef hstack(\n    items: List,\n    justify", "  ```\nimport styled_components as sc\nfrom styles import flexColumn"]}, "marimo-marimo/_plugins/stateless/status/_progress.py-update_progress": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_plugins/stateless/status/_progress.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport time\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Iterable,\n    Optional,\n    TypeVar,\n)\n\nimport marimo._runtime.output._output as output\nfrom marimo._messaging.mimetypes import KnownMimeType\nfrom marimo._output.hypertext import Html\nfrom marimo._output.rich_help import mddoc\nfrom marimo._plugins.core.web_component import build_stateless_plugin\nfrom marimo._utils.debounce import debounce\n\nif TYPE_CHECKING:\n    from collections.abc import Collection\n\nS = TypeVar(\"S\")\nT = TypeVar(\"T\")\n\n\ndef _remove_none_values(d: dict[S, T]) -> dict[S, T]:\n    return {k: v for k, v in d.items() if v is not None}\n\n\nclass _Progress(Html):\n    \"\"\"A mutable class to represent a progress indicator in the UI.\"\"\"\n\n    def __init__(\n        self,\n        title: Optional[str],\n        subtitle: Optional[str],\n        total: Optional[int],\n        show_rate: bool,\n        show_eta: bool,\n    ) -> None:\n        self.title = title\n        self.subtitle = subtitle\n        self.total = total\n        self.current = 0\n        self.closed = False\n        # We show a loading spinner if total not known\n        self.loading_spinner = total is None\n        self.show_rate = show_rate\n        self.show_eta = show_eta\n        self.start_time = time.time()\n        super().__init__(self._get_text())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @debounce(0.15)\n    def debounced_flush(self) -> None:\n        \"\"\"Flush the output to the UI\"\"\"\n        output.flush()\n\n    def clear(self) -> None:\n        if self.closed:\n            raise RuntimeError(\n                \"Progress indicators cannot be updated after exiting \"\n                \"the context manager that created them. \"\n            )\n        output.remove(self)\n\n    def close(self) -> None:\n        output.flush()  # Flush one last time before closing\n        self.closed = True\n\n    def _get_text(self) -> str:\n        return build_stateless_plugin(\n            component_name=\"marimo-progress\",\n            args=_remove_none_values(\n                {\n                    \"title\": self.title,\n                    \"subtitle\": self.subtitle,\n                    \"total\": self.total,\n                    # 'progress' is True is we don't know the total,\n                    # which shows a loading spinner\n                    \"progress\": True if self.loading_spinner else self.current,\n                    \"rate\": self._get_rate(),\n                    \"eta\": self._get_eta(),\n                }\n            ),\n        )\n\n    def _get_rate(self) -> Optional[float]:\n        if self.show_rate:\n            diff = time.time() - self.start_time\n            if diff == 0:\n                return None\n            rate = self.current / diff\n            return round(rate, 2)\n        else:\n            return None\n\n    def _get_eta(self) -> Optional[float]:\n        if self.show_eta and self.total is not None:\n            rate = self._get_rate()\n            if rate is not None and rate > 0:\n                return round((self.total - self.current) / rate, 2)\n            else:\n                return None\n        else:\n            return None\n\n\nclass ProgressBar(_Progress):\n    def __init__(\n        self,\n        title: str | None,\n        subtitle: str | None,\n        total: int,\n        show_rate: bool,\n        show_eta: bool,\n    ) -> None:\n        super().__init__(\n            title=title,\n            subtitle=subtitle,\n            total=total,\n            show_rate=show_rate,\n            show_eta=show_eta,\n        )\n\n    def update(\n        self,\n        increment: int = 1,\n        title: str | None = None,\n        subtitle: str | None = None,\n    ) -> None:\n        super().update_progress(\n            increment=increment, title=title, subtitle=subtitle\n        )\n\n\n# TODO(akshayka): Add a `done()` method that turns the spinner into a checkmark\nclass Spinner(_Progress):\n    \"\"\"A spinner output representing a loading state\"\"\"\n\n    def __init__(self, title: str | None, subtitle: str | None) -> None:\n        super().__init__(\n            title=title,\n            subtitle=subtitle,\n            total=None,\n            show_rate=False,\n            show_eta=False,\n        )\n\n    def update(\n        self, title: str | None = None, subtitle: str | None = None\n    ) -> None:\n        \"\"\"Update the title and subtitle of the spinner\n\n        This method updates a spinner output in-place. Must be used\n        in the cell the spinner was created.\n\n        **Example.**\n\n        ```python\n        with mo.status.spinner(\"Hang tight!\") as _spinner:\n            ...\n            _spinner.update(title=\"Almost done!\")\n        # Optionally, remove the spinner from the output\n        # _spinner.clear()\n        ```\n        \"\"\"\n        super().update_progress(increment=1, title=title, subtitle=subtitle)\n\n\n@mddoc\nclass spinner:\n    \"\"\"Show a loading spinner\n\n    Use `mo.status.spinner()` as a context manager to show a loading spinner.\n    You can optionally pass a title and subtitle.\n\n    **Example.**\n\n    ```python\n    with mo.status.spinner(subtitle=\"Loading data ...\") as _spinner:\n        data = expensive_function()\n        _spinner.update(subtitle=\"Crunching numbers ...\")\n        ...\n\n    mo.ui.table(data)\n    ```\n\n    You can also show the spinner without a context manager:\n\n    ```python\n    mo.status.spinner(title=\"Loading ...\") if condition else mo.md(\"Done!\")\n    ```\n\n    **Args:**\n\n    - `title`: optional title\n    - `subtitle`: optional subtitle\n    - `remove_on_exit`: if True, the spinner is removed from output on exit\n    \"\"\"\n\n    def __init__(\n        self,\n        title: Optional[str] = None,\n        subtitle: Optional[str] = None,\n        remove_on_exit: bool = True,\n    ):\n        self.title = title\n        self.subtitle = subtitle\n        self.remove_on_exit = remove_on_exit\n        self.spinner = Spinner(title=self.title, subtitle=self.subtitle)\n\n    def __enter__(self) -> Spinner:\n        output.append(self.spinner)\n        return self.spinner\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        if self.remove_on_exit:\n            self.spinner.clear()\n        # TODO(akshayka): else consider transitioning to a done state\n        self.spinner.close()\n\n    def _mime_(self) -> tuple[KnownMimeType, str]:\n        return self.spinner._mime_()\n\n\nclass progress_bar:\n    \"\"\"Iterate over a collection and show a progress bar\n\n    **Example.**\n\n    ```python\n    for i in mo.status.progress_bar(range(10)):\n        ...\n    ```\n\n    You can optionally provide a title and subtitle to show\n    during iteration, and a title/subtitle to show upon completion.\n\n    You can also use progress_bar with a context manager and manually update\n    the bar:\n\n    ```python\n    with mo.status.progress_bar(total=10) as bar:\n        for i in range(10):\n            ...\n            bar.update()\n    ```\n\n    The `update` method accepts the optional keyword\n    arguments `increment` (defaults to `1`), `title`,\n    and `subtitle`.\n\n    For performance reasons, the progress bar is only updated in the UI\n    every 150ms.\n\n    **Args.**\n\n    - `collection`: optional collection to iterate over\n    - `title`: optional title\n    - `subtitle`: optional subtitle\n    - `completion_title`: optional title to show during completion\n    - `completion_subtitle`: optional subtitle to show during completion\n    - `total`: optional total number of items to iterate over\n    - `show_rate`: if True, show the rate of progress (items per second)\n    - `show_eta`: if True, show the estimated time of completion\n    \"\"\"\n\n    def __init__(\n        self,\n        collection: Optional[Collection[S | int]] = None,\n        *,\n        title: Optional[str] = None,\n        subtitle: Optional[str] = None,\n        completion_title: Optional[str] = None,\n        completion_subtitle: Optional[str] = None,\n        total: Optional[int] = None,\n        show_rate: bool = True,\n        show_eta: bool = True,\n    ):\n        self.completion_title = completion_title\n        self.completion_subtitle = completion_subtitle\n\n        if collection is not None:\n            self.collection = collection\n\n            try:\n                total = total or len(collection)\n                self.step = (\n                    collection.step if isinstance(collection, range) else 1\n                )\n            except TypeError:  # if collection is a generator\n                raise TypeError(\n                    \"fail to determine length of collection, use `total`\"\n                    + \"to specify\"\n                ) from None\n\n        elif total is None:\n            raise ValueError(\n                \"`total` is required when using as a context manager\"\n            )\n\n        self.progress = ProgressBar(\n            title=title,\n            subtitle=subtitle,\n            total=total,\n            show_rate=show_rate,\n            show_eta=show_eta,\n        )\n        output.append(self.progress)\n\n    def __iter__(self) -> Iterable[S | int]:\n        for item in self.collection:\n            yield item\n            self.progress.update(increment=self.step)\n        self._finish()\n\n    def __enter__(self) -> ProgressBar:\n        return self.progress\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        self._finish()\n\n    def _finish(self) -> None:\n        self.progress.update(\n            increment=0,\n            title=self.completion_title,\n            subtitle=self.completion_subtitle,\n        )\n        self.progress.close()\n\n```\n# Function Signature: def update_progress(\n# Natural Language Annotations: \"\"\"Update the progress indicator.\n\n        **Example.**\n\n        ```python\n        # Increment by 1\n        progress.update()\n\n        # Increment by 10 and update title and subtitle\n        progress.update(10, title=\"Loading\", subtitle=\"Still going...\")\n        ```\n\n        **Args.**\n\n        - increment: amount to increment by. Defaults to 1.\n        - title: new title. Defaults to None.\n        - subtitle: new subtitle. Defaults to None.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef update_progress(\n    increment: int = 1", "  ```\ndef update_progress(\n    self,\n    increment:", "  ```\ndef update_progress(\n    self,\n    increment:"]}, "marimo-marimo/_plugins/core/web_component.py-build_ui_plugin": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_plugins/core/web_component.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport json\nimport re\nfrom html import escape, unescape\nfrom typing import (\n    TYPE_CHECKING,\n    Mapping,\n    Sequence,\n    TypeVar,\n    Union,\n    cast,\n)\n\nif TYPE_CHECKING:\n    import sys\n\n    if sys.version_info < (3, 10):\n        from typing_extensions import TypeAlias\n    else:\n        from typing import TypeAlias\n\n    from typing import Optional\n\nfrom marimo._output.md import md\nfrom marimo._output.mime import MIME\nfrom marimo._plugins.core.json_encoder import WebComponentEncoder\n\nJSONType: TypeAlias = Union[\n    Mapping[str, \"JSONType\"],\n    Sequence[\"JSONType\"],\n    str,\n    int,\n    float,\n    bool,\n    MIME,  # MIME is a JSONType since we have a custom JSONEncoder for it\n    None,\n]\n\nS = TypeVar(\"S\", bound=JSONType)\n\n\ndef _build_attr(name: str, value: JSONType) -> str:\n    processed = escape(json.dumps(value, cls=WebComponentEncoder))\n    # manual escapes for things html.escape doesn't escape\n    #\n    # - backslashes, when unescaped can lead to problems\n    # when embedding in markdown\n    # - dollar sign, when unescaped can incorrectly be recognized as\n    # latex delimiter when embedding into markdown\n    processed = processed.replace(\"\\\\\", \"&#92;\").replace(\"$\", \"&#36;\")\n    return f\"data-{name}='{processed}'\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef build_stateless_plugin(\n    component_name: str,\n    args: dict[str, JSONType],\n    slotted_html: str = \"\",\n) -> str:\n    \"\"\"\n    Build HTML for a stateless plugin.\n\n    Args:\n    ----\n    component_name: tag name of the component\n    args: mapping from arg names to JSON-serializable value\n    slotted_html: HTML to slot in the component\n\n    Returns:\n    -------\n    HTML text for the component\n    \"\"\"\n    attrs = [_build_attr(name, value) for name, value in args.items()]\n    return (\n        f\"<{component_name} {' '.join(attrs)}>\"\n        f\"{slotted_html}\"\n        f\"</{component_name}>\"\n    )\n\n\ndef parse_initial_value(text: str) -> JSONType:\n    \"\"\"Get initial value from HTML for a UI element.\"\"\"\n    match = re.search(\"data-initial-value='(.*?)'\", text)\n    if match is None:\n        raise ValueError(\"Invalid component HTML: \", text)\n    return cast(JSONType, json.loads(unescape(match.groups()[0])))\n\n```\n# Function Signature: def build_ui_plugin(\n# Natural Language Annotations: \"\"\"\n    Build HTML for a UI (stateful) plugin.\n\n    Args:\n    ----\n    component_name: tag name of the component\n    initial_value: JSON-serializable initial value of the component\n    label: markdown string that component may use a text label\n    args: mapping from arg names to JSON-serializable value\n    slotted_html: HTML to slot in the component\n\n    Returns:\n    -------\n    HTML text for the component\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef build_ui_plugin(\n    component_name:", "  ```\ndef build_ui_plugin(\n    component_name:", "  ```\ndef build_ui_plugin(\n    component_name:"]}, "marimo-marimo/_output/formatting.py-as_html": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_output/formatting.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\n\"\"\"Formatting protocol\n\nThis module defines a protocol for implementing objects that can be displayed\nusing marimo's media viewer.\n\nTo register a formatter for a type, user have two options:\n    1. Implement a method _mime_ on the class that takes an instance\n       and returns a (mime, data) tuple (i.e., implement the protocol MIME)\n    2. Register a formatter function that takes a value and returns\n       a (mime, data) tuple.\n\nThe function get_formatter(value: T) can be used to obtain a function that\ninstantiates a (mime, data) tuple for a value, with registered formatters\ntaking precedence over the MIME protocol.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nimport json\nimport traceback\nimport types\nfrom dataclasses import dataclass\nfrom html import escape\nfrom typing import Any, Callable, Optional, Tuple, Type, TypeVar, cast\n\nfrom marimo import _loggers as loggers\nfrom marimo._messaging.mimetypes import KnownMimeType\nfrom marimo._output.hypertext import Html\nfrom marimo._output.rich_help import mddoc\nfrom marimo._output.utils import flatten_string\nfrom marimo._plugins.stateless.json_output import json_output\nfrom marimo._plugins.stateless.plain_text import plain_text\n\nT = TypeVar(\"T\")\n\n# we use Tuple instead of the builtin tuple for py3.8 compatibility\nFormatter = Callable[[T], Tuple[KnownMimeType, str]]\nFORMATTERS: dict[Type[Any], Formatter[Any]] = {}\nOPINIONATED_FORMATTERS: dict[Type[Any], Formatter[Any]] = {}\nLOGGER = loggers.marimo_logger()\n\n\ndef formatter(t: Type[Any]) -> Callable[[Formatter[T]], Formatter[T]]:\n    \"\"\"Register a formatter function for a type\n\n    Decorator to register a custom formatter for a given type.\n\n    For example, to register a formatter for a class Foo with a string\n    attribute data:\n\n    ```\n    @formatter(Foo)\n    def show_foo(foo: Foo) -> tuple[str, str]:\n        return (\"text/html\", f\"<p>{foo.data}</p>\")\n    ```\n    \"\"\"\n\n    def register_format(f: Formatter[T]) -> Formatter[T]:\n        FORMATTERS[t] = f\n        return f\n\n    return register_format\n\n\ndef opinionated_formatter(\n    t: Type[Any],\n) -> Callable[[Formatter[T]], Formatter[T]]:\n    \"\"\"Register an opinionated formatter function for a type\n\n    Decorator to register a custom formatter for a given type.\n\n    For example, to register a formatter for a class Foo with a string\n    attribute data:\n\n    ```\n    @opinionated_formatter(Foo)\n    def show_df(foo: Foo) -> tuple[str, str]:\n        return table(foo)._mime_()\n    ```\n    \"\"\"\n\n    def register_format(f: Formatter[T]) -> Formatter[T]:\n        OPINIONATED_FORMATTERS[t] = f\n        return f\n\n    return register_format\n\n\ndef get_formatter(\n    obj: T,\n    # Include opinionated formatters by default\n    # (e.g., for pandas, polars, arrow, etc.)\n    include_opinionated: bool = True,\n) -> Optional[Formatter[T]]:\n    from marimo._runtime.context import ContextNotInitializedError, get_context\n\n    try:\n        get_context()\n    except ContextNotInitializedError:\n        if not FORMATTERS:\n            from marimo._output.formatters.formatters import (\n                register_formatters,\n            )\n\n            # Install formatters when marimo is being used without\n            # a kernel (eg, in a unit test or when run as a Python script)\n            register_formatters()\n\n    if isinstance(obj, Plain):\n        child_formatter = get_formatter(obj.child, include_opinionated=False)\n        if child_formatter:\n\n            def plain_formatter(obj: T) -> tuple[KnownMimeType, str]:\n                assert child_formatter is not None\n                return child_formatter(cast(Plain, obj).child)\n\n            return plain_formatter\n\n    if include_opinionated:\n        if type(obj) in OPINIONATED_FORMATTERS:\n            return OPINIONATED_FORMATTERS[type(obj)]\n\n    if type(obj) in FORMATTERS:\n        return FORMATTERS[type(obj)]\n    elif any(isinstance(obj, t) for t in FORMATTERS.keys()):\n        # we avoid using the walrus operator (matched_type := t) above\n        # to keep compatibility with Python < 3.8\n        for t in FORMATTERS.keys():\n            if isinstance(obj, t):\n                return FORMATTERS[t]\n    elif hasattr(obj, \"_mime_\"):\n        method = obj._mime_\n        if inspect.isclass(obj) and not isinstance(method, (types.MethodType)):\n            return None\n        if callable(method):\n\n            def f(obj: T) -> tuple[KnownMimeType, str]:\n                return obj._mime_()  # type: ignore\n\n            return f\n    elif hasattr(obj, \"_repr_html_\"):\n        method = obj._repr_html_\n        if inspect.isclass(obj) and not isinstance(method, (types.MethodType)):\n            return None\n        if callable(method):\n\n            def f(obj: T) -> tuple[KnownMimeType, str]:\n                return (\"text/html\", obj._repr_html_())  # type: ignore\n\n            return f\n    return None\n\n\n@dataclass\nclass FormattedOutput:\n    mimetype: KnownMimeType\n    data: str\n    traceback: Optional[str] = None\n\n\ndef try_format(obj: Any) -> FormattedOutput:\n    obj = \"\" if obj is None else obj\n    if (formatter := get_formatter(obj)) is not None:\n        try:\n            mimetype, data = formatter(obj)\n            return FormattedOutput(mimetype=mimetype, data=data)\n        except BaseException:  # noqa: E722\n            # Catching base exception so we're robust to bugs in libraries\n            return FormattedOutput(\n                mimetype=\"text/plain\",\n                data=\"\",\n                traceback=traceback.format_exc(),\n            )\n\n    from marimo._runtime.context import ContextNotInitializedError, get_context\n\n    glbls = {}\n    try:\n        ctx = get_context()\n    except ContextNotInitializedError:\n        pass\n    else:\n        glbls = ctx.globals\n\n    tb = None\n    try:\n        # convert the object to a string using the kernel globals;\n        # some libraries like duckdb introspect globals() ...\n        data = eval(\"str(obj)\", glbls, {\"obj\": obj})\n    except Exception:\n        tb = traceback.format_exc()\n        return FormattedOutput(\n            mimetype=\"text/plain\",\n            data=\"\",\n            traceback=tb,\n        )\n    else:\n        return (\n            FormattedOutput(\n                mimetype=\"text/html\",\n                data=plain_text(escape(data)).text,\n                traceback=tb,\n            )\n            if data\n            else FormattedOutput(\n                mimetype=\"text/plain\",\n                data=\"\",\n                traceback=tb,\n            )\n        )\n\n\n@mddoc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@mddoc\ndef plain(value: Any) -> Plain:\n    \"\"\"\n    Wrap a value to indicate that it should be displayed\n    without any opinionated formatting.\n\n    This is the best way to opt out of marimo's\n    default dataframe rendering.\n\n    **Example.**\n\n    ```python\n    df = data.cars()\n    mo.plain(df)\n    ```\n\n    **Args.**\n\n    - `value`: Any value\n    \"\"\"\n    return Plain(value)\n\n\nclass Plain:\n    \"\"\"\n    Wrapper around a value to indicate that it should be displayed\n    without any opinionated formatting.\n    \"\"\"\n\n    def __init__(self, child: Any):\n        self.child = child\n\n```\n# Function Signature: def as_html(value: object) -> Html:\n# Natural Language Annotations: \"\"\"Convert a value to HTML that can be embedded into markdown\n\n    This function returns an `Html` object representing `value`. Use it to\n    embed values into Markdown or other HTML strings.\n\n    **Example.**\n\n    ```python3\n    import matplotlib.pyplot as plt\n    plt.plot([1, 2])\n    axis = plt.gca()\n    mo.md(\n        f\\\"\\\"\\\"\n        Here is a plot:\n\n        {mo.as_html(axis)}\n        \\\"\\\"\\\"\n    )\n    ```\n\n    **Args.**\n\n    - `value`: An object\n\n    **Returns.**\n\n    - An `Html` object\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef as_html(value: Any) -> Html:\n", "  ```\ndef as_html(value: object) -> Html:\n", "  ```\ndef as_html(value):\n    the_html ="]}, "marimo-marimo/_output/md.py-md": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_output/md.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nfrom inspect import cleandoc\n\nimport markdown  # type: ignore\n\nfrom marimo._output.hypertext import Html\nfrom marimo._output.md_extensions.external_links import ExternalLinksExtension\nfrom marimo._output.rich_help import mddoc\n\nextension_configs = {\n    \"pymdownx.arithmatex\": {\n        # Use \"generic\" mode, no preview, since we don't use MathJax\n        \"preview\": False,\n        \"generic\": True,\n        # The default \"\\\\(\" causes problems when passing\n        # html-escaped `md` output back into `md`\n        \"tex_inline_wrap\": [\"||(\", \"||)\"],\n        \"tex_block_wrap\": [\"||[\", \"||]\"],\n        # Wrap latex in a custom element\n        \"block_tag\": \"marimo-tex\",\n        \"inline_tag\": \"marimo-tex\",\n    },\n    \"pymdownx.superfences\": {\n        \"disable_indented_code_blocks\": True,\n        \"css_class\": \"codehilite\",\n    },\n    \"footnotes\": {\n        \"UNIQUE_IDS\": True,\n    },\n}\n\n\ndef _md(text: str, apply_markdown_class: bool = True) -> Html:\n    # cleandoc uniformly strips leading whitespace; useful for\n    # indented multiline strings\n    text = cleandoc(text)\n    # markdown.markdown appends a newline, hence strip\n    html_text = markdown.markdown(\n        text,\n        extensions=[\n            # Syntax highlighting\n            \"codehilite\",\n            # Markdown tables\n            \"tables\",\n            # LaTeX\n            \"pymdownx.arithmatex\",\n            # Subscripts and strikethrough\n            \"pymdownx.tilde\",\n            # Better code blocks\n            \"pymdownx.superfences\",\n            # Table of contents\n            # This adds ids to the HTML headers\n            \"toc\",\n            # Footnotes\n            \"footnotes\",\n            # Admonitions\n            \"admonition\",\n            # Links\n            ExternalLinksExtension(),\n        ],\n        extension_configs=extension_configs,  # type: ignore[arg-type]\n    ).strip()\n    # replace <p> tags with <span> as HTML doesn't allow nested <div>s in <p>s\n    html_text = html_text.replace(\"<p>\", '<span class=\"paragraph\">').replace(\n        \"</p>\", \"</span>\"\n    )\n\n    if apply_markdown_class:\n        return Html('<span class=\"markdown\">' + html_text + \"</span>\")\n    else:\n        return Html(html_text)\n\n\n@mddoc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def md(text: str) -> Html:\n# Natural Language Annotations: r\"\"\"Write markdown\n\n    This function takes a string of markdown as input and returns an Html\n    object. Output the object as the last expression of a cell to render\n    the markdown in your app.\n\n    **Interpolation.**\n\n    You can interpolate Python values into your markdown strings, for example\n    using f-strings. Html objects and UI elements can be directly interpolated.\n    For example:\n\n    ```python3\n    text_input = mo.ui.text()\n    md(f\"Enter some text: {text_input}\")\n    ```\n\n    For other objects, like plots, use marimo's `as_html` method to embed\n    them in markdown:\n\n    ```python3\n    import matplotlib.pyplot as plt\n\n    plt.plot([1, 2])\n    axis = plt.gca()\n    md(f\"Here's a plot: {mo.as_html(axis)}\")\n    ```\n\n    **LaTeX.**\n\n    Enclose LaTeX in single '\\$' signs for inline math, and double '\\$\\$' for\n    display math or square brackets for display math. (Use raw strings,\n    prefixed with an \"r\", to use single backslashes.) For example:\n\n    ```python3\n    mo.md(\n        r'''\n        The exponential function $f(x) = e^x$ can be represented as\n\n        \\[\n            f(x) = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\ldots.\n        \\]\n        '''\n    )\n    ```\n    renders:\n\n    The exponential function $f(x) = e^x$ can be represented as\n\n    $$\n    f(x) = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\ldots.\n    $$\n\n\n    **Args**:\n\n    - `text`: a string of markdown\n\n    **Returns**:\n\n    - An `Html` object.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef md(text: str) -> Html:\n    html", "  ```\ndef md(text: str) -> Html:\n    #", "  ```\nimport matplotlib.pyplot as plt\n\ndef md(text"]}, "marimo-marimo/_output/formatters/formatters.py-register_formatters": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_output/formatters/formatters.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport sys\nfrom typing import Any, Callable, Sequence\n\nfrom marimo._output.formatters.altair_formatters import AltairFormatter\nfrom marimo._output.formatters.anywidget_formatters import AnyWidgetFormatter\nfrom marimo._output.formatters.bokeh_formatters import BokehFormatter\nfrom marimo._output.formatters.cell import CellFormatter\nfrom marimo._output.formatters.df_formatters import (\n    PolarsFormatter,\n    PyArrowFormatter,\n)\nfrom marimo._output.formatters.formatter_factory import FormatterFactory\nfrom marimo._output.formatters.holoviews_formatters import HoloViewsFormatter\nfrom marimo._output.formatters.ipython_formatters import IPythonFormatter\nfrom marimo._output.formatters.leafmap_formatters import LeafmapFormatter\nfrom marimo._output.formatters.matplotlib_formatters import MatplotlibFormatter\nfrom marimo._output.formatters.pandas_formatters import PandasFormatter\nfrom marimo._output.formatters.plotly_formatters import PlotlyFormatter\nfrom marimo._output.formatters.seaborn_formatters import SeabornFormatter\nfrom marimo._output.formatters.structures import StructuresFormatter\n\n# Map from formatter factory's package name to formatter, for third-party\n# modules. These formatters will be registered if and when their associated\n# packages are imported.\nTHIRD_PARTY_FACTORIES: dict[str, FormatterFactory] = {\n    AltairFormatter.package_name(): AltairFormatter(),\n    MatplotlibFormatter.package_name(): MatplotlibFormatter(),\n    PandasFormatter.package_name(): PandasFormatter(),\n    PolarsFormatter.package_name(): PolarsFormatter(),\n    PyArrowFormatter.package_name(): PyArrowFormatter(),\n    PlotlyFormatter.package_name(): PlotlyFormatter(),\n    SeabornFormatter.package_name(): SeabornFormatter(),\n    LeafmapFormatter.package_name(): LeafmapFormatter(),\n    BokehFormatter.package_name(): BokehFormatter(),\n    HoloViewsFormatter.package_name(): HoloViewsFormatter(),\n    IPythonFormatter.package_name(): IPythonFormatter(),\n    AnyWidgetFormatter.package_name(): AnyWidgetFormatter(),\n}\n\n# Formatters for builtin types and other things that don't require a\n# third-party module import. These formatters' register methods need to be\n# fast: we don't want their registration to noticeably delay program start-up.\nNATIVE_FACTORIES: Sequence[FormatterFactory] = [\n    CellFormatter(),\n    StructuresFormatter(),\n]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def register_formatters() -> None:\n# Natural Language Annotations: \"\"\"Register formatters with marimo.\n\n    marimo comes packaged with rich formatters for a number of third-party\n    libraries. This function hooks into Python's import system to register\n    these formatters with the kernel if and when a supported third-party\n    library is imported into a marimo notebook.\n\n    Hooking into the import system is more complicated than the alternative\n    of checking whether a package is installed (by importing it) and then\n    registering its formatters at kernel start-up. However, because some\n    packages imports take a long time, this alternative would add considerable\n    delay at program start-up, as the kernel would block as it registered\n    all formatters before running the notebook. Hooking into the import\n    system makes formatter registration completely lazy, improving\n    UX at the cost of increased complexity that we have to maintain. In this\n    case, the trade-off is worth it.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef register_formatters() -> None:\n    \"\"\"Register", "  ```\ndef register_formatters():\n    \"\"\"Register formatters with", "  ```\nimport sys\nfrom typing import Any, Callable, Sequence"]}, "marimo-marimo/_data/series.py-get_category_series_info": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_data/series.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport datetime\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Union\n\nfrom marimo._dependencies.dependencies import DependencyManager\n\nif TYPE_CHECKING:\n    import pandas as pd\n    import polars as pl\n\nDataFrameSeries = Union[\"pd.Series[Any]\", \"pl.Series\"]\n\n\n@dataclass\nclass NumberSeriesInfo:\n    \"\"\"\n    Represents a summary of a numeric series.\n    \"\"\"\n\n    min: float\n    max: float\n    label: str\n\n\n@dataclass\nclass CategorySeriesInfo:\n    \"\"\"\n    Represents a summary of a categorical series.\n    \"\"\"\n\n    categories: list[str]\n    label: str\n\n\n@dataclass\nclass DateSeriesInfo:\n    \"\"\"\n    Represents a summary of a date series.\n    \"\"\"\n\n    min: str\n    max: str\n    label: str\n\n\ndef _get_name(series: DataFrameSeries) -> str:\n    return str(series.name) if series.name is not None else \"\"\n\n\ndef get_number_series_info(series: Any) -> NumberSeriesInfo:\n    \"\"\"\n    Get the summary of a numeric series.\n    \"\"\"\n\n    def validate_number(value: Any) -> float:\n        value = float(value)\n        if not isinstance(value, (int, float)):\n            raise ValueError(\"Expected a number. Got: \" + str(type(value)))\n        return value\n\n    if DependencyManager.has_pandas():\n        import pandas as pd\n\n        if isinstance(series, pd.Series):\n            return NumberSeriesInfo(\n                min=validate_number(series.min()),\n                max=validate_number(series.max()),\n                label=_get_name(series),\n            )\n\n    if DependencyManager.has_polars():\n        import polars as pl\n\n        if isinstance(series, pl.Series):\n            return NumberSeriesInfo(\n                min=validate_number(series.min()),\n                max=validate_number(series.max()),\n                label=_get_name(series),\n            )\n\n    raise ValueError(\"Unsupported series type. Expected pandas or polars.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef get_date_series_info(series: Any) -> DateSeriesInfo:\n    \"\"\"\n    Get the summary of a date series.\n    \"\"\"\n\n    def validate_date(value: Any) -> str:\n        if not isinstance(value, datetime.date):\n            raise ValueError(\"Expected a date. Got: \" + str(type(value)))\n        return value.strftime(\"%Y-%m-%d\")\n\n    if DependencyManager.has_pandas():\n        import pandas as pd\n\n        if isinstance(series, pd.Series):\n            return DateSeriesInfo(\n                min=validate_date(series.min()),\n                max=validate_date(series.max()),\n                label=_get_name(series),\n            )\n\n    if DependencyManager.has_polars():\n        import polars as pl\n\n        if isinstance(series, pl.Series):\n            return DateSeriesInfo(\n                min=validate_date(series.min()),\n                max=validate_date(series.max()),\n                label=_get_name(series),\n            )\n\n    raise ValueError(\"Unsupported series type. Expected pandas or polars.\")\n\n```\n# Function Signature: def get_category_series_info(series: Any) -> CategorySeriesInfo:\n# Natural Language Annotations: \"\"\"\n    Get the summary of a categorical series.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_category_series_info(series: Any)", "  ```\n@dataclass\nclass CategorySeriesInfo:\n    \"\"\"\n", "  ```\ndef get_category_series_info(series: Any)"]}, "marimo-marimo/_data/preview_column.py-get_column_preview": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_data/preview_column.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nfrom typing import Any, Optional\n\nfrom marimo import _loggers\nfrom marimo._data.charts import get_chart_builder\nfrom marimo._data.models import ColumnSummary\nfrom marimo._dependencies.dependencies import DependencyManager\nfrom marimo._messaging.ops import DataColumnPreview\nfrom marimo._plugins.ui._impl.tables.table_manager import TableManager\nfrom marimo._plugins.ui._impl.tables.utils import get_table_manager_or_none\nfrom marimo._runtime.requests import PreviewDatasetColumnRequest\n\nLOGGER = _loggers.marimo_logger()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef _get_altair_chart(\n    request: PreviewDatasetColumnRequest,\n    table: TableManager[Any],\n    summary: ColumnSummary,\n) -> tuple[Optional[str], Optional[str], bool]:\n    # We require altair to render the chart\n    if not DependencyManager.has_altair() or not table.supports_altair():\n        return None, None, False\n\n    import altair as alt  # type: ignore[import-not-found,import-untyped,unused-ignore] # noqa: E501\n    from altair import (  # type: ignore[import-not-found,import-untyped,unused-ignore] # noqa: E501\n        MaxRowsError,\n    )\n\n    (column_type, _external_type) = table.get_field_types()[\n        request.column_name\n    ]\n\n    # For categorical columns with more than 10 unique values,\n    # we limit the chart to 10 items\n    should_limit_to_10_items = False\n    if (\n        column_type == \"string\"\n        and summary.unique is not None\n        and summary.unique > 10\n    ):\n        should_limit_to_10_items = True\n\n    chart_builder = get_chart_builder(column_type, should_limit_to_10_items)\n    code = chart_builder.altair_code(\n        request.table_name,\n        request.column_name,\n    )\n\n    chart_max_rows_errors = False\n    try:\n        column_data = table.select_columns([request.column_name]).data\n        # Date types don't serialize well to csv,\n        # so we don't transform them\n        if column_type == \"date\":\n            # Default max_rows is 5_000, but we can support more.\n            with alt.data_transformers.enable(\"default\", max_rows=20_000):\n                chart_json = chart_builder.altair_json(\n                    column_data,\n                    request.column_name,\n                )\n        else:\n            with alt.data_transformers.enable(\"marimo_inline_csv\"):\n                chart_json = chart_builder.altair_json(\n                    column_data,\n                    request.column_name,\n                )\n    except MaxRowsError:\n        chart_json = None\n        chart_max_rows_errors = True\n\n    return chart_json, code, chart_max_rows_errors\n\n```\n# Function Signature: def get_column_preview(\n# Natural Language Annotations: \"\"\"\n    Get a preview of the column in the dataset.\n\n    This may return a chart and a aggregation summary of the column.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_column_preview(\n    request: Preview", "  ```\ndef get_column_preview(\n    request: Preview", "  ```\nimport altair as alt\nfrom altair import HorizonChain"]}, "marimo-marimo/_server/sessions.py-start_file_watcher": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_server/sessions.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\n\"\"\"Client session management\n\nThis module encapsulates session management: each client gets a unique session,\nand each session wraps a Python kernel and a websocket connection through which\nthe kernel can send messages to the frontend. Sessions do not share kernels or\nwebsockets.\n\nIn run mode, in which we may have many clients connected to the server, a\nsession is closed as soon as its websocket connection is severed. In edit mode,\nin which we have at most one connected client, a session may be kept around\neven if its socket is closed.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport multiprocessing as mp\nimport os\nimport queue\nimport shutil\nimport signal\nimport subprocess\nimport sys\nimport threading\nfrom multiprocessing import connection\nfrom multiprocessing.queues import Queue as MPQueue\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\nfrom uuid import uuid4\n\nfrom marimo import _loggers\nfrom marimo._ast.cell import CellConfig, CellId_t\nfrom marimo._cli.print import red\nfrom marimo._config.manager import UserConfigManager\nfrom marimo._messaging.ops import (\n    Alert,\n    FocusCell,\n    MessageOperation,\n    Reload,\n    UpdateCellCodes,\n)\nfrom marimo._messaging.types import KernelMessage\nfrom marimo._output.formatters.formatters import register_formatters\nfrom marimo._runtime import requests, runtime\nfrom marimo._runtime.requests import (\n    AppMetadata,\n    CreationRequest,\n    ExecuteMultipleRequest,\n    ExecutionRequest,\n    SerializedCLIArgs,\n    SerializedQueryParams,\n    SetUIElementValueRequest,\n)\nfrom marimo._server.exceptions import InvalidSessionException\nfrom marimo._server.file_manager import AppFileManager\nfrom marimo._server.file_router import AppFileRouter, MarimoFileKey\nfrom marimo._server.ids import ConsumerId, SessionId\nfrom marimo._server.model import ConnectionState, SessionConsumer, SessionMode\nfrom marimo._server.models.models import InstantiateRequest\nfrom marimo._server.recents import RecentFilesManager\nfrom marimo._server.session.session_view import SessionView\nfrom marimo._server.tokens import AuthToken, SkewProtectionToken\nfrom marimo._server.types import QueueType\nfrom marimo._server.utils import print_tabbed\nfrom marimo._utils.disposable import Disposable\nfrom marimo._utils.distributor import Distributor\nfrom marimo._utils.file_watcher import FileWatcher\nfrom marimo._utils.paths import import_files\nfrom marimo._utils.repr import format_repr\nfrom marimo._utils.typed_connection import TypedConnection\n\nLOGGER = _loggers.marimo_logger()\nSESSION_MANAGER: Optional[\"SessionManager\"] = None\n\n\nclass QueueManager:\n    \"\"\"Manages queues for a session.\"\"\"\n\n    def __init__(self, use_multiprocessing: bool):\n        context = mp.get_context(\"spawn\") if use_multiprocessing else None\n\n        # Control messages for the kernel (run, set UI element, set config, etc\n        # ) are sent through the control queue\n        self.control_queue: QueueType[requests.ControlRequest] = (\n            context.Queue() if context is not None else queue.Queue()\n        )\n\n        # Set UI element queues are stored in both the control queue and\n        # this queue, so that the backend can merge/batch set-ui-element\n        # requests.\n        self.set_ui_element_queue: QueueType[\n            requests.SetUIElementValueRequest\n        ] = context.Queue() if context is not None else queue.Queue()\n\n        # Code completion requests are sent through a separate queue\n        self.completion_queue: QueueType[requests.CodeCompletionRequest] = (\n            context.Queue() if context is not None else queue.Queue()\n        )\n\n        self.win32_interrupt_queue: QueueType[bool] | None\n        if sys.platform == \"win32\":\n            self.win32_interrupt_queue = (\n                context.Queue() if context is not None else queue.Queue()\n            )\n        else:\n            self.win32_interrupt_queue = None\n\n        # Input messages for the user's Python code are sent through the\n        # input queue\n        self.input_queue: QueueType[str] = (\n            context.Queue(maxsize=1)\n            if context is not None\n            else queue.Queue(maxsize=1)\n        )\n\n    def close_queues(self) -> None:\n        if isinstance(self.control_queue, MPQueue):\n            # cancel join thread because we don't care if the queues still have\n            # things in it: don't want to make the child process wait for the\n            # queues to empty\n            self.control_queue.cancel_join_thread()\n            self.control_queue.close()\n        else:\n            # kernel thread cleans up read/write conn and IOloop handler on\n            # exit; we don't join the thread because we don't want to block\n            self.control_queue.put(requests.StopRequest())\n\n        if isinstance(self.set_ui_element_queue, MPQueue):\n            self.set_ui_element_queue.cancel_join_thread()\n            self.set_ui_element_queue.close()\n\n        if isinstance(self.input_queue, MPQueue):\n            # again, don't make the child process wait for the queues to empty\n            self.input_queue.cancel_join_thread()\n            self.input_queue.close()\n\n        if isinstance(self.completion_queue, MPQueue):\n            self.completion_queue.cancel_join_thread()\n            self.completion_queue.close()\n\n        if isinstance(self.win32_interrupt_queue, MPQueue):\n            self.win32_interrupt_queue.cancel_join_thread()\n            self.win32_interrupt_queue.close()\n\n\nclass KernelManager:\n    def __init__(\n        self,\n        queue_manager: QueueManager,\n        mode: SessionMode,\n        configs: dict[CellId_t, CellConfig],\n        app_metadata: AppMetadata,\n        user_config_manager: UserConfigManager,\n        virtual_files_supported: bool,\n    ) -> None:\n        self.kernel_task: Optional[threading.Thread] | Optional[mp.Process]\n        self.queue_manager = queue_manager\n        self.mode = mode\n        self.configs = configs\n        self.app_metadata = app_metadata\n        self.user_config_manager = user_config_manager\n        self._read_conn: Optional[TypedConnection[KernelMessage]] = None\n        self._virtual_files_supported = virtual_files_supported\n\n    def start_kernel(self) -> None:\n        # Need to use a socket for windows compatibility\n        listener = connection.Listener(family=\"AF_INET\")\n\n        # We use a process in edit mode so that we can interrupt the app\n        # with a SIGINT; we don't mind the additional memory consumption,\n        # since there's only one client sess\n        is_edit_mode = self.mode == SessionMode.EDIT\n        if is_edit_mode:\n            self.kernel_task = mp.Process(\n                target=runtime.launch_kernel,\n                args=(\n                    self.queue_manager.control_queue,\n                    self.queue_manager.set_ui_element_queue,\n                    self.queue_manager.completion_queue,\n                    self.queue_manager.input_queue,\n                    listener.address,\n                    is_edit_mode,\n                    self.configs,\n                    self.app_metadata,\n                    self.user_config_manager.config,\n                    self._virtual_files_supported,\n                    self.queue_manager.win32_interrupt_queue,\n                ),\n                # The process can't be a daemon, because daemonic processes\n                # can't create children\n                # https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process.daemon  # noqa: E501\n                daemon=False,\n            )\n        else:\n            # We use threads in run mode to minimize memory consumption;\n            # launching a process would copy the entire program state,\n            # which (as of writing) is around 150MB\n\n            # We can't terminate threads, so we have to wait until they\n            # naturally exit before cleaning up resources\n            def launch_kernel_with_cleanup(*args: Any) -> None:\n                runtime.launch_kernel(*args)\n                if not self.kernel_connection.closed:\n                    self.kernel_connection.close()\n\n            # install formatter import hooks, which will be shared by all\n            # threads (in edit mode, the single kernel process installs\n            # formatters ...)\n            register_formatters()\n\n            # Make threads daemons so killing the server immediately brings\n            # down all client sessions\n            self.kernel_task = threading.Thread(\n                target=launch_kernel_with_cleanup,\n                args=(\n                    self.queue_manager.control_queue,\n                    self.queue_manager.set_ui_element_queue,\n                    self.queue_manager.completion_queue,\n                    self.queue_manager.input_queue,\n                    listener.address,\n                    is_edit_mode,\n                    self.configs,\n                    self.app_metadata,\n                    self.user_config_manager.config,\n                    self._virtual_files_supported,\n                    # win32 interrupt queue\n                    None,\n                ),\n                # daemon threads can create child processes, unlike\n                # daemon processes\n                daemon=True,\n            )\n\n        self.kernel_task.start()  # type: ignore\n        # First thing kernel does is connect to the socket, so it's safe to\n        # call accept\n        self._read_conn = TypedConnection[KernelMessage].of(listener.accept())\n\n    def is_alive(self) -> bool:\n        return self.kernel_task is not None and self.kernel_task.is_alive()\n\n    def interrupt_kernel(self) -> None:\n        if (\n            isinstance(self.kernel_task, mp.Process)\n            and self.kernel_task.pid is not None\n        ):\n            q = self.queue_manager.win32_interrupt_queue\n            if sys.platform == \"win32\" and q is not None:\n                LOGGER.debug(\"Queueing interrupt request for kernel.\")\n                q.put_nowait(True)\n            else:\n                LOGGER.debug(\"Sending SIGINT to kernel\")\n                os.kill(self.kernel_task.pid, signal.SIGINT)\n\n    def close_kernel(self) -> None:\n        assert self.kernel_task is not None, \"kernel not started\"\n\n        if isinstance(self.kernel_task, mp.Process):\n            self.queue_manager.close_queues()\n            if self.kernel_task.is_alive():\n                self.kernel_task.terminate()\n            self.kernel_connection.close()\n        elif self.kernel_task.is_alive():\n            # We don't join the kernel thread because we don't want to server\n            # to block on it finishing\n            self.queue_manager.control_queue.put(requests.StopRequest())\n\n    @property\n    def kernel_connection(self) -> TypedConnection[KernelMessage]:\n        assert self._read_conn is not None, \"connection not started\"\n        return self._read_conn\n\n\nclass Room:\n    \"\"\"\n    A room is a collection of SessionConsumers\n    that can be used to broadcast messages to all\n    of them.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.main_consumer: Optional[SessionConsumer] = None\n        self.consumers: Dict[SessionConsumer, ConsumerId] = {}\n        self.disposables: Dict[SessionConsumer, Disposable] = {}\n\n    def add_consumer(\n        self,\n        consumer: SessionConsumer,\n        dispose: Disposable,\n        consumer_id: ConsumerId,\n        # Whether the consumer is the main session consumer\n        # We only allow one main consumer, the rest are kiosk consumers\n        main: bool,\n    ) -> None:\n        self.consumers[consumer] = consumer_id\n        self.disposables[consumer] = dispose\n        if main:\n            assert (\n                self.main_consumer is None\n            ), \"Main session consumer already exists\"\n            self.main_consumer = consumer\n\n    def remove_consumer(self, consumer: SessionConsumer) -> None:\n        if consumer not in self.consumers:\n            LOGGER.debug(\n                \"Attempted to remove a consumer that was not in room.\"\n            )\n            return\n\n        if consumer == self.main_consumer:\n            self.main_consumer = None\n        self.consumers.pop(consumer)\n        disposable = self.disposables.pop(consumer)\n        try:\n            consumer.on_stop()\n        finally:\n            disposable.dispose()\n\n    def broadcast(self, operation: MessageOperation) -> None:\n        for consumer in self.consumers:\n            consumer.write_operation(operation)\n\n    def close(self) -> None:\n        for consumer in self.consumers:\n            disposable = self.disposables.pop(consumer)\n            consumer.on_stop()\n            disposable.dispose()\n        self.consumers = {}\n        self.main_consumer = None\n\n\nclass Session:\n    \"\"\"A client session.\n\n    Each session has its own Python kernel, for editing and running the app,\n    and its own websocket, for sending messages to the client.\n    \"\"\"\n\n    TTL_SECONDS = 120\n\n    @classmethod\n    def create(\n        cls,\n        initialization_id: str,\n        session_consumer: SessionConsumer,\n        mode: SessionMode,\n        app_metadata: AppMetadata,\n        app_file_manager: AppFileManager,\n        user_config_manager: UserConfigManager,\n        virtual_files_supported: bool,\n    ) -> Session:\n        \"\"\"\n        Create a new session.\n        \"\"\"\n        configs = app_file_manager.app.cell_manager.config_map()\n        use_multiprocessing = mode == SessionMode.EDIT\n        queue_manager = QueueManager(use_multiprocessing)\n        kernel_manager = KernelManager(\n            queue_manager,\n            mode,\n            configs,\n            app_metadata,\n            user_config_manager,\n            virtual_files_supported=virtual_files_supported,\n        )\n        return cls(\n            initialization_id,\n            session_consumer,\n            queue_manager,\n            kernel_manager,\n            app_file_manager,\n        )\n\n    def __init__(\n        self,\n        initialization_id: str,\n        session_consumer: SessionConsumer,\n        queue_manager: QueueManager,\n        kernel_manager: KernelManager,\n        app_file_manager: AppFileManager,\n    ) -> None:\n        \"\"\"Initialize kernel and client connection to it.\"\"\"\n        # This is some unique ID that we can use to identify the session\n        # We don't use the session_id because this can change if the\n        # session is resumed\n        self.initialization_id = initialization_id\n        self._queue_manager: QueueManager\n        self.app_file_manager = app_file_manager\n        self.room = Room()\n        self._queue_manager = queue_manager\n        self.kernel_manager = kernel_manager\n        self.session_view = SessionView()\n\n        self.kernel_manager.start_kernel()\n        # Reads from the kernel connection and distributes the\n        # messages to each subscriber.\n        self.message_distributor = Distributor[KernelMessage](\n            self.kernel_manager.kernel_connection\n        )\n        self.message_distributor.add_consumer(\n            lambda msg: self.session_view.add_raw_operation(msg[1])\n        )\n        self.connect_consumer(session_consumer, main=True)\n\n        self.message_distributor.start()\n        self.heartbeat_task: Optional[asyncio.Task[Any]] = None\n        self._start_heartbeat()\n        self._closed = False\n\n    def _start_heartbeat(self) -> None:\n        def _check_alive() -> None:\n            if not self.kernel_manager.is_alive():\n                LOGGER.debug(\"Closing session because kernel died\")\n                self.close()\n                print()\n                print_tabbed(red(\"The Python kernel died unexpectedly.\"))\n                print()\n                sys.exit()\n\n        # Start a heartbeat task, which checks if the kernel is alive\n        # every second\n\n        async def _heartbeat() -> None:\n            while True:\n                await asyncio.sleep(1)\n                _check_alive()\n\n        try:\n            loop = asyncio.get_event_loop()\n            self.heartbeat_task = loop.create_task(_heartbeat())\n        except RuntimeError:\n            # This can happen if there is no event loop running\n            self.heartbeat_task = None\n\n    def try_interrupt(self) -> None:\n        \"\"\"Try to interrupt the kernel.\"\"\"\n        self.kernel_manager.interrupt_kernel()\n\n    def put_control_request(self, request: requests.ControlRequest) -> None:\n        \"\"\"Put a control request in the control queue.\"\"\"\n        self._queue_manager.control_queue.put(request)\n        if isinstance(request, SetUIElementValueRequest):\n            self._queue_manager.set_ui_element_queue.put(request)\n        # Propagate the control request to the room\n        if isinstance(request, ExecuteMultipleRequest):\n            self.room.broadcast(\n                UpdateCellCodes(\n                    cell_ids=request.cell_ids,\n                    codes=request.codes,\n                )\n            )\n            if len(request.cell_ids) == 1:\n                self.room.broadcast(FocusCell(cell_id=request.cell_ids[0]))\n        self.session_view.add_control_request(request)\n\n    def put_completion_request(\n        self, request: requests.CodeCompletionRequest\n    ) -> None:\n        \"\"\"Put a code completion request in the completion queue.\"\"\"\n        self._queue_manager.completion_queue.put(request)\n\n    def put_input(self, text: str) -> None:\n        \"\"\"Put an input() request in the input queue.\"\"\"\n        self._queue_manager.input_queue.put(text)\n        self.session_view.add_stdin(text)\n\n    def disconnect_consumer(self, session_consumer: SessionConsumer) -> None:\n        \"\"\"\n        Stop the session consumer but keep the kernel running.\n\n        This will disconnect the main session consumer,\n        or a kiosk consumer.\n        \"\"\"\n        self.room.remove_consumer(session_consumer)\n\n    def maybe_disconnect_consumer(self) -> None:\n        \"\"\"\n        Disconnect the main session consumer if it connected.\n        \"\"\"\n        if self.room.main_consumer is not None:\n            self.disconnect_consumer(self.room.main_consumer)\n\n    def connect_consumer(\n        self, session_consumer: SessionConsumer, *, main: bool\n    ) -> None:\n        \"\"\"\n        Connect or resume the session with a new consumer.\n\n        If its the main consumer and one already exists,\n        an exception is raised.\n        \"\"\"\n        subscribe = session_consumer.on_start()\n        unsubscribe_consumer = self.message_distributor.add_consumer(subscribe)\n        self.room.add_consumer(\n            session_consumer,\n            unsubscribe_consumer,\n            session_consumer.consumer_id,\n            main=main,\n        )\n\n    def get_current_state(self) -> SessionView:\n        \"\"\"Return the current state of the session.\"\"\"\n        return self.session_view\n\n    def connection_state(self) -> ConnectionState:\n        \"\"\"Return the connection state of the session.\"\"\"\n        if self._closed:\n            return ConnectionState.CLOSED\n        if self.room.main_consumer is None:\n            return ConnectionState.ORPHANED\n        return self.room.main_consumer.connection_state()\n\n    def write_operation(self, operation: MessageOperation) -> None:\n        \"\"\"Write an operation to the session consumer and the session view.\"\"\"\n        self.session_view.add_operation(operation)\n        self.room.broadcast(operation)\n\n    def close(self) -> None:\n        \"\"\"\n        Close the session.\n\n        This will close the session consumer, kernel, and all kiosk consumers.\n        \"\"\"\n        self._closed = True\n        # Close the room\n        self.room.close()\n        # Close the kernel\n        self.message_distributor.stop()\n        if self.heartbeat_task:\n            self.heartbeat_task.cancel()\n        self.kernel_manager.close_kernel()\n\n    def instantiate(self, request: InstantiateRequest) -> None:\n        \"\"\"Instantiate the app.\"\"\"\n        execution_requests = tuple(\n            ExecutionRequest(cell_id=cell_data.cell_id, code=cell_data.code)\n            for cell_data in self.app_file_manager.app.cell_manager.cell_data()\n        )\n\n        self.put_control_request(\n            CreationRequest(\n                execution_requests=execution_requests,\n                set_ui_element_value_request=SetUIElementValueRequest(\n                    object_ids=request.object_ids,\n                    values=request.values,\n                    token=str(uuid4()),\n                ),\n            )\n        )\n\n    def __repr__(self) -> str:\n        return format_repr(\n            self,\n            {\n                \"connection_state\": self.connection_state(),\n                \"room\": self.room,\n            },\n        )\n\n\nclass SessionManager:\n    \"\"\"Mapping from client session IDs to sessions.\n\n    Maintains a mapping from client session IDs to client sessions;\n    there is exactly one session per client.\n\n    The SessionManager also encapsulates state common to all sessions:\n    - the app filename\n    - the app mode (edit or run)\n    - the auth token\n    - the skew-protection token\n    \"\"\"\n\n    def __init__(\n        self,\n        file_router: AppFileRouter,\n        mode: SessionMode,\n        development_mode: bool,\n        quiet: bool,\n        include_code: bool,\n        lsp_server: LspServer,\n        user_config_manager: UserConfigManager,\n        cli_args: SerializedCLIArgs,\n        auth_token: Optional[AuthToken],\n    ) -> None:\n        self.file_router = file_router\n        self.mode = mode\n        self.development_mode = development_mode\n        self.quiet = quiet\n        self.sessions: dict[SessionId, Session] = {}\n        self.include_code = include_code\n        self.lsp_server = lsp_server\n        self.watcher: Optional[FileWatcher] = None\n        self.recents = RecentFilesManager()\n        self.user_config_manager = user_config_manager\n        self.cli_args = cli_args\n\n        # Auth token and Skew-protection token\n        if auth_token is not None:\n            self.auth_token = auth_token\n            self.skew_protection_token = SkewProtectionToken.random()\n        elif mode == SessionMode.EDIT:\n            # In edit mode, if no auth token is provided,\n            # generate a random token\n            self.auth_token = AuthToken.random()\n            self.skew_protection_token = SkewProtectionToken.random()\n        else:\n            app = file_router.get_single_app_file_manager(\n                default_width=user_config_manager.get_config()[\"display\"][\n                    \"default_width\"\n                ]\n            ).app\n            codes = \"\".join(code for code in app.cell_manager.codes())\n            # Because run-mode is read-only and we could have multiple\n            # servers for the same app (going to sleep or autoscaling),\n            # we default to a token based on the app's code\n            self.auth_token = AuthToken.from_code(codes)\n            self.skew_protection_token = SkewProtectionToken.from_code(codes)\n\n    def app_manager(self, key: MarimoFileKey) -> AppFileManager:\n        \"\"\"\n        Get the app manager for the given key.\n        \"\"\"\n        return self.file_router.get_file_manager(\n            key,\n            default_width=self.user_config_manager.get_config()[\"display\"][\n                \"default_width\"\n            ],\n        )\n\n    def create_session(\n        self,\n        session_id: SessionId,\n        session_consumer: SessionConsumer,\n        query_params: SerializedQueryParams,\n        file_key: MarimoFileKey,\n    ) -> Session:\n        \"\"\"Create a new session\"\"\"\n        LOGGER.debug(\"Creating new session for id %s\", session_id)\n        if session_id not in self.sessions:\n            app_file_manager = self.file_router.get_file_manager(\n                file_key,\n                default_width=self.user_config_manager.get_config()[\"display\"][\n                    \"default_width\"\n                ],\n            )\n\n            if app_file_manager.path:\n                self.recents.touch(app_file_manager.path)\n\n            self.sessions[session_id] = Session.create(\n                initialization_id=file_key,\n                session_consumer=session_consumer,\n                mode=self.mode,\n                app_metadata=AppMetadata(\n                    query_params=query_params,\n                    filename=app_file_manager.path,\n                    cli_args=self.cli_args,\n                ),\n                app_file_manager=app_file_manager,\n                user_config_manager=self.user_config_manager,\n                virtual_files_supported=True,\n            )\n        return self.sessions[session_id]\n\n    def get_session(self, session_id: SessionId) -> Optional[Session]:\n        session = self.sessions.get(session_id)\n        if session:\n            return session\n\n        # Search for kiosk sessions\n        for session in self.sessions.values():\n            if session_id in session.room.consumers.values():\n                return session\n\n        return None\n\n    def get_session_by_file_key(\n        self, file_key: MarimoFileKey\n    ) -> Optional[Session]:\n        for session in self.sessions.values():\n            if session.initialization_id == file_key:\n                return session\n        return None\n\n    def maybe_resume_session(\n        self, new_session_id: SessionId, file_key: MarimoFileKey\n    ) -> Optional[Session]:\n        \"\"\"\n        Try to resume a session if one is resumable.\n        If it is resumable, return the session and update the session id.\n        \"\"\"\n\n        # If in run mode, only resume the session if it is orphaned and has\n        # the same session id, otherwise we want to create a new session\n        if self.mode == SessionMode.RUN:\n            maybe_session = self.get_session(new_session_id)\n            if (\n                maybe_session\n                and maybe_session.connection_state()\n                == ConnectionState.ORPHANED\n```\n# Function Signature: def start_file_watcher(self) -> Disposable:\n# Natural Language Annotations: \"\"\"Starts the file watcher if it is not already started\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport asyncio\n\nclass Disposable:\n   ", "  ```\nimport asyncio\nfrom typing import Dict, Optional\n", "  ```py\ndef start_file_watcher(self) -> Dis"]}, "marimo-marimo/_server/file_manager.py-save_app_config": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_server/file_manager.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport os\nfrom typing import Any, Dict, Optional\n\nfrom marimo import _loggers\nfrom marimo._ast import codegen\nfrom marimo._ast.app import App, InternalApp, _AppConfig\nfrom marimo._ast.cell import CellConfig\nfrom marimo._config.config import WidthType\nfrom marimo._runtime.layout.layout import (\n    LayoutConfig,\n    read_layout_config,\n    save_layout_config,\n)\nfrom marimo._server.api.status import HTTPException, HTTPStatus\nfrom marimo._server.models.models import SaveNotebookRequest\nfrom marimo._server.utils import canonicalize_filename\n\nLOGGER = _loggers.marimo_logger()\n\n\nclass AppFileManager:\n    def __init__(\n        self, filename: Optional[str], default_width: WidthType | None = None\n    ) -> None:\n        self.filename = filename\n        self._default_width: WidthType | None = default_width\n        self.app = self._load_app(self.path)\n\n    @staticmethod\n    def from_app(app: InternalApp) -> AppFileManager:\n        manager = AppFileManager(None)\n        manager.app = app\n        return manager\n\n    def reload(self) -> None:\n        \"\"\"Reload the app from the file.\"\"\"\n        self.app = self._load_app(self.path)\n\n    def _is_same_path(self, filename: str) -> bool:\n        if self.filename is None:\n            return False\n        return os.path.abspath(self.filename) == os.path.abspath(filename)\n\n    def _assert_path_does_not_exist(self, filename: str) -> None:\n        if os.path.exists(filename):\n            raise HTTPException(\n                status_code=HTTPStatus.BAD_REQUEST,\n                detail=\"File {0} already exists\".format(filename),\n            )\n\n    def _assert_path_is_the_same(self, filename: str) -> None:\n        if self.filename is not None and not self._is_same_path(filename):\n            raise HTTPException(\n                status_code=HTTPStatus.BAD_REQUEST,\n                detail=\"Save handler cannot rename files.\",\n            )\n\n    def _create_file(\n        self,\n        filename: str,\n        contents: str = \"\",\n    ) -> None:\n        try:\n            with open(filename, \"w\", encoding=\"utf-8\") as f:\n                f.write(contents)\n        except Exception as err:\n            raise HTTPException(\n                status_code=HTTPStatus.SERVER_ERROR,\n                detail=\"Failed to save file {0}\".format(filename),\n            ) from err\n\n    def _rename_file(self, new_filename: str) -> None:\n        assert self.filename is not None\n        try:\n            os.rename(self.filename, new_filename)\n        except Exception as err:\n            raise HTTPException(\n                status_code=HTTPStatus.SERVER_ERROR,\n                detail=\"Failed to rename from {0} to {1}\".format(\n                    self.filename, new_filename\n                ),\n            ) from err\n\n    def _save_file(\n        self,\n        filename: str,\n        codes: list[str],\n        names: list[str],\n        configs: list[CellConfig],\n        app_config: _AppConfig,\n        # Whether or not to persist the app to the file system\n        persist: bool,\n    ) -> str:\n        LOGGER.debug(\"Saving app to %s\", filename)\n        if filename.endswith(\".md\"):\n            # TODO: Remember just proof of concept, potentially needs\n            # restructuring.\n            from marimo._server.export.exporter import Exporter\n\n            contents, _ = Exporter().export_as_md(self)\n        else:\n            # Header might be better kept on the AppConfig side, opposed to\n            # reparsing it. Also would allow for md equivalent in a field like\n            # `description`.\n            header_comments = codegen.get_header_comments(filename)\n            # try to save the app under the name `filename`\n            contents = codegen.generate_filecontents(\n                codes,\n                names,\n                cell_configs=configs,\n                config=app_config,\n                header_comments=header_comments,\n            )\n\n        if persist:\n            self._create_file(filename, contents)\n\n        if self._is_unnamed():\n            self.rename(filename)\n\n        return contents\n\n    def _load_app(self, path: Optional[str]) -> InternalApp:\n        \"\"\"Read the app from the file.\"\"\"\n        app = codegen.get_app(path)\n        if app is None:\n            kwargs = (\n                {\"width\": self._default_width}\n                if self._default_width is not None\n                # App decides its own default width\n                else {}\n            )\n            empty_app = InternalApp(App(**kwargs))\n            empty_app.cell_manager.register_cell(\n                cell_id=None,\n                code=\"\",\n                config=CellConfig(),\n            )\n            return empty_app\n        return InternalApp(app)\n\n    def rename(self, new_filename: str) -> None:\n        \"\"\"Rename the file.\"\"\"\n        new_filename = canonicalize_filename(new_filename)\n\n        if self._is_same_path(new_filename):\n            return\n\n        self._assert_path_does_not_exist(new_filename)\n\n        need_save = False\n        # Check if filename is not None to satisfy mypy's type checking.\n        # This ensures that filename is treated as a non-optional str,\n        # preventing potential type errors in subsequent code.\n        if self._is_named() and self.filename is not None:\n            # Force a save after rename in case filetype changed.\n            need_save = self.filename[-3:] != new_filename[-3:]\n            self._rename_file(new_filename)\n        else:\n            self._create_file(new_filename)\n\n        self.filename = new_filename\n        if need_save:\n            self._save_file(\n                self.filename,\n                list(self.app.cell_manager.codes()),\n                list(self.app.cell_manager.names()),\n                list(self.app.cell_manager.configs()),\n                self.app.config,\n                persist=True,\n            )\n\n    def read_layout_config(self) -> Optional[LayoutConfig]:\n        if self.app.config.layout_file is not None and isinstance(\n            self.filename, str\n        ):\n            app_dir = os.path.dirname(self.filename)\n            layout = read_layout_config(app_dir, self.app.config.layout_file)\n            return layout\n\n        return None\n\n    @property\n    def path(self) -> Optional[str]:\n        if self.filename is None:\n            return None\n        try:\n            return os.path.abspath(self.filename)\n        except AttributeError:\n            return None\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def save(self, request: SaveNotebookRequest) -> str:\n        \"\"\"Save the current app.\"\"\"\n        cell_ids, codes, configs, names, filename, layout = (\n            request.cell_ids,\n            request.codes,\n            request.configs,\n            request.names,\n            request.filename,\n            request.layout,\n        )\n        filename = canonicalize_filename(filename)\n        self.app.with_data(\n            cell_ids=cell_ids,\n            codes=codes,\n            names=names,\n            configs=configs,\n        )\n\n        if self._is_named() and not self._is_same_path(filename):\n            raise HTTPException(\n                status_code=HTTPStatus.BAD_REQUEST,\n                detail=\"Save handler cannot rename files.\",\n            )\n\n        # save layout\n        if layout is not None:\n            app_dir = os.path.dirname(filename)\n            app_name = os.path.basename(filename)\n            layout_filename = save_layout_config(\n                app_dir, app_name, LayoutConfig(**layout)\n            )\n            self.app.update_config({\"layout_file\": layout_filename})\n        else:\n            # Remove the layout from the config\n            # We don't remove the layout file from the disk to avoid\n            # deleting state that the user might want to keep\n            self.app.update_config({\"layout_file\": None})\n        return self._save_file(\n            filename,\n            codes,\n            names,\n            configs,\n            self.app.config,\n            persist=request.persist,\n        )\n\n    def to_code(self) -> str:\n        \"\"\"Read the contents of the unsaved file.\"\"\"\n        contents = codegen.generate_filecontents(\n            codes=list(self.app.cell_manager.codes()),\n            names=list(self.app.cell_manager.names()),\n            cell_configs=list(self.app.cell_manager.configs()),\n            config=self.app.config,\n        )\n        return contents\n\n    def _is_unnamed(self) -> bool:\n        return self.filename is None\n\n    def _is_named(self) -> bool:\n        return self.filename is not None\n\n    def read_file(self) -> str:\n        \"\"\"Read the contents of the file.\"\"\"\n        if self.filename is None:\n            raise HTTPException(\n                status_code=HTTPStatus.BAD_REQUEST,\n                detail=\"Cannot read code from an unnamed notebook\",\n            )\n        with open(self.filename, \"r\", encoding=\"utf-8\") as f:\n            contents = f.read().strip()\n        return contents\n\n```\n# Function Signature: def save_app_config(self, config: Dict[str, Any]) -> str:\n# Natural Language Annotations: \"\"\"Save the app configuration.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef save_app_config(self, config: Dict", "  ```\ndef save_app_config(self, config: Dict", "  ```\ndef save_app_config(self, config: Dict"]}, "marimo-marimo/_server/session/session_view.py-add_stdin": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_server/session/session_view.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport time\nfrom dataclasses import dataclass\nfrom typing import Any, Literal, Optional, Union\n\nfrom marimo._ast.cell import CellId_t\nfrom marimo._data.models import DataTable\nfrom marimo._messaging.cell_output import CellChannel, CellOutput\nfrom marimo._messaging.ops import (\n    CellOp,\n    Datasets,\n    Interrupted,\n    MessageOperation,\n    Variables,\n    VariableValue,\n    VariableValues,\n)\nfrom marimo._runtime.requests import (\n    ControlRequest,\n    CreationRequest,\n    ExecuteMultipleRequest,\n    ExecutionRequest,\n    SetUIElementValueRequest,\n)\nfrom marimo._utils.parse_dataclass import parse_raw\n\n\nclass SessionView:\n    \"\"\"\n    This stores the current view of the session.\n\n    Which are the cell's outputs, status, and console.\n    \"\"\"\n\n    def __init__(self) -> None:\n        # List of operations we care about keeping track of.\n        self.cell_operations: dict[CellId_t, CellOp] = {}\n        # The most recent datasets operation.\n        self.datasets: Datasets = Datasets(tables=[])\n        # The most recent Variables operation.\n        self.variable_operations: Variables = Variables(variables=[])\n        # Map of variable name to value.\n        self.variable_values: dict[str, VariableValue] = {}\n        # Map of object id to value.\n        self.ui_values: dict[str, Any] = {}\n        # Map of cell id to the last code that was executed in that cell.\n        self.last_executed_code: dict[CellId_t, str] = {}\n        # Map of cell id to the last cell execution time\n        self.last_execution_time: dict[CellId_t, float] = {}\n\n    def _add_ui_value(self, name: str, value: Any) -> None:\n        self.ui_values[name] = value\n\n    def _add_last_run_code(self, req: ExecutionRequest) -> None:\n        self.last_executed_code[req.cell_id] = req.code\n\n    def add_raw_operation(self, raw_operation: Any) -> None:\n        # parse_raw only accepts a dataclass, so we wrap MessageOperation in a\n        # dataclass.\n        @dataclass\n        class _Container:\n            operation: MessageOperation\n\n        operation = parse_raw({\"operation\": raw_operation}, _Container)\n        self.add_operation(operation.operation)\n\n    def add_control_request(self, request: ControlRequest) -> None:\n        if isinstance(request, SetUIElementValueRequest):\n            for object_id, value in request.ids_and_values:\n                self._add_ui_value(object_id, value)\n        elif isinstance(request, ExecuteMultipleRequest):\n            for execution_request in request.execution_requests:\n                self._add_last_run_code(execution_request)\n        elif isinstance(request, CreationRequest):\n            for (\n                object_id,\n                value,\n            ) in request.set_ui_element_value_request.ids_and_values:\n                self._add_ui_value(object_id, value)\n            for execution_request in request.execution_requests:\n                self._add_last_run_code(execution_request)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_cell_outputs(\n        self, ids: list[CellId_t]\n    ) -> dict[CellId_t, CellOutput]:\n        \"\"\"Get the outputs for the given cell ids.\"\"\"\n        outputs: dict[CellId_t, CellOutput] = {}\n        for cell_id in ids:\n            cell_op = self.cell_operations.get(cell_id)\n            if cell_op is not None and cell_op.output is not None:\n                outputs[cell_id] = cell_op.output\n        return outputs\n\n\n\n\n\n\n\n\n\n\n\n\n    def save_execution_time(\n        self, operation: MessageOperation, event: Literal[\"start\", \"end\"]\n    ) -> None:\n        \"\"\"Updates execution time for given cell.\"\"\"\n        if not isinstance(operation, CellOp):\n            return\n        cell_id = operation.cell_id\n\n        if event == \"start\":\n            time_elapsed = operation.timestamp\n        elif event == \"end\":\n            start = self.last_execution_time.get(cell_id)\n            start = start if start else 0\n            time_elapsed = time.time() - start\n            time_elapsed = round(time_elapsed * 1000)\n\n        self.last_execution_time[cell_id] = time_elapsed\n\n    @property\n    def operations(self) -> list[MessageOperation]:\n        all_ops: list[MessageOperation] = []\n        if self.variable_operations.variables:\n            all_ops.append(self.variable_operations)\n        if self.variable_values:\n            all_ops.append(\n                VariableValues(variables=list(self.variable_values.values()))\n            )\n        if self.datasets.tables:\n            all_ops.append(self.datasets)\n        all_ops.extend(self.cell_operations.values())\n        return all_ops\n\n\ndef merge_cell_operation(\n    previous: Optional[CellOp],\n    next_: CellOp,\n) -> CellOp:\n    \"\"\"Merge two cell operations.\"\"\"\n    if previous is None:\n        return next_\n\n    assert previous.cell_id == next_.cell_id\n\n    if next_.status is None:\n        next_.status = previous.status\n\n    # If we went from queued to running, clear the console.\n    if next_.status == \"running\" and previous.status == \"queued\":\n        next_.console = []\n    else:\n        combined_console: list[CellOutput] = as_list(previous.console)\n        combined_console.extend(as_list(next_.console))\n        next_.console = combined_console\n\n    # If we went from running to running, use the previous timestamp.\n    if next_.status == \"running\" and previous.status == \"running\":\n        next_.timestamp = previous.timestamp\n\n    if next_.output is None:\n        next_.output = previous.output\n\n    return next_\n\n\ndef as_list(value: Union[Any, Optional[Any], list[Any]]) -> list[Any]:\n    if value is None:\n        return []\n    return value if isinstance(value, list) else [value]\n\n```\n# Function Signature: def add_stdin(self, stdin: str) -> None:\n# Natural Language Annotations: \"\"\"Add a stdin request to the session view.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef add_stdin(self, stdin: str)", "  ```\ndef add_stdin(self, stdin: str)", "  ```\ndef add_stdin(self, stdin: str)"]}, "marimo-marimo/_server/session/session_view.py-add_operation": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_server/session/session_view.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport time\nfrom dataclasses import dataclass\nfrom typing import Any, Literal, Optional, Union\n\nfrom marimo._ast.cell import CellId_t\nfrom marimo._data.models import DataTable\nfrom marimo._messaging.cell_output import CellChannel, CellOutput\nfrom marimo._messaging.ops import (\n    CellOp,\n    Datasets,\n    Interrupted,\n    MessageOperation,\n    Variables,\n    VariableValue,\n    VariableValues,\n)\nfrom marimo._runtime.requests import (\n    ControlRequest,\n    CreationRequest,\n    ExecuteMultipleRequest,\n    ExecutionRequest,\n    SetUIElementValueRequest,\n)\nfrom marimo._utils.parse_dataclass import parse_raw\n\n\nclass SessionView:\n    \"\"\"\n    This stores the current view of the session.\n\n    Which are the cell's outputs, status, and console.\n    \"\"\"\n\n    def __init__(self) -> None:\n        # List of operations we care about keeping track of.\n        self.cell_operations: dict[CellId_t, CellOp] = {}\n        # The most recent datasets operation.\n        self.datasets: Datasets = Datasets(tables=[])\n        # The most recent Variables operation.\n        self.variable_operations: Variables = Variables(variables=[])\n        # Map of variable name to value.\n        self.variable_values: dict[str, VariableValue] = {}\n        # Map of object id to value.\n        self.ui_values: dict[str, Any] = {}\n        # Map of cell id to the last code that was executed in that cell.\n        self.last_executed_code: dict[CellId_t, str] = {}\n        # Map of cell id to the last cell execution time\n        self.last_execution_time: dict[CellId_t, float] = {}\n\n    def _add_ui_value(self, name: str, value: Any) -> None:\n        self.ui_values[name] = value\n\n    def _add_last_run_code(self, req: ExecutionRequest) -> None:\n        self.last_executed_code[req.cell_id] = req.code\n\n    def add_raw_operation(self, raw_operation: Any) -> None:\n        # parse_raw only accepts a dataclass, so we wrap MessageOperation in a\n        # dataclass.\n        @dataclass\n        class _Container:\n            operation: MessageOperation\n\n        operation = parse_raw({\"operation\": raw_operation}, _Container)\n        self.add_operation(operation.operation)\n\n    def add_control_request(self, request: ControlRequest) -> None:\n        if isinstance(request, SetUIElementValueRequest):\n            for object_id, value in request.ids_and_values:\n                self._add_ui_value(object_id, value)\n        elif isinstance(request, ExecuteMultipleRequest):\n            for execution_request in request.execution_requests:\n                self._add_last_run_code(execution_request)\n        elif isinstance(request, CreationRequest):\n            for (\n                object_id,\n                value,\n            ) in request.set_ui_element_value_request.ids_and_values:\n                self._add_ui_value(object_id, value)\n            for execution_request in request.execution_requests:\n                self._add_last_run_code(execution_request)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_cell_outputs(\n        self, ids: list[CellId_t]\n    ) -> dict[CellId_t, CellOutput]:\n        \"\"\"Get the outputs for the given cell ids.\"\"\"\n        outputs: dict[CellId_t, CellOutput] = {}\n        for cell_id in ids:\n            cell_op = self.cell_operations.get(cell_id)\n            if cell_op is not None and cell_op.output is not None:\n                outputs[cell_id] = cell_op.output\n        return outputs\n\n\n\n\n\n\n\n\n\n\n\n\n    def save_execution_time(\n        self, operation: MessageOperation, event: Literal[\"start\", \"end\"]\n    ) -> None:\n        \"\"\"Updates execution time for given cell.\"\"\"\n        if not isinstance(operation, CellOp):\n            return\n        cell_id = operation.cell_id\n\n        if event == \"start\":\n            time_elapsed = operation.timestamp\n        elif event == \"end\":\n            start = self.last_execution_time.get(cell_id)\n            start = start if start else 0\n            time_elapsed = time.time() - start\n            time_elapsed = round(time_elapsed * 1000)\n\n        self.last_execution_time[cell_id] = time_elapsed\n\n    @property\n    def operations(self) -> list[MessageOperation]:\n        all_ops: list[MessageOperation] = []\n        if self.variable_operations.variables:\n            all_ops.append(self.variable_operations)\n        if self.variable_values:\n            all_ops.append(\n                VariableValues(variables=list(self.variable_values.values()))\n            )\n        if self.datasets.tables:\n            all_ops.append(self.datasets)\n        all_ops.extend(self.cell_operations.values())\n        return all_ops\n\n\ndef merge_cell_operation(\n    previous: Optional[CellOp],\n    next_: CellOp,\n) -> CellOp:\n    \"\"\"Merge two cell operations.\"\"\"\n    if previous is None:\n        return next_\n\n    assert previous.cell_id == next_.cell_id\n\n    if next_.status is None:\n        next_.status = previous.status\n\n    # If we went from queued to running, clear the console.\n    if next_.status == \"running\" and previous.status == \"queued\":\n        next_.console = []\n    else:\n        combined_console: list[CellOutput] = as_list(previous.console)\n        combined_console.extend(as_list(next_.console))\n        next_.console = combined_console\n\n    # If we went from running to running, use the previous timestamp.\n    if next_.status == \"running\" and previous.status == \"running\":\n        next_.timestamp = previous.timestamp\n\n    if next_.output is None:\n        next_.output = previous.output\n\n    return next_\n\n\ndef as_list(value: Union[Any, Optional[Any], list[Any]]) -> list[Any]:\n    if value is None:\n        return []\n    return value if isinstance(value, list) else [value]\n\n```\n# Function Signature: def add_operation(self, operation: MessageOperation) -> None:\n# Natural Language Annotations: \"\"\"Add an operation to the session view.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef add_operation(self, operation: MessageOperation) ->", "  ```\ndef add_operation(self, operation: MessageOperation) ->", "  ```\ndef add_operation(self, operation: MessageOperation) ->"]}, "marimo-marimo/_server/session/session_view.py-get_cell_console_outputs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_server/session/session_view.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport time\nfrom dataclasses import dataclass\nfrom typing import Any, Literal, Optional, Union\n\nfrom marimo._ast.cell import CellId_t\nfrom marimo._data.models import DataTable\nfrom marimo._messaging.cell_output import CellChannel, CellOutput\nfrom marimo._messaging.ops import (\n    CellOp,\n    Datasets,\n    Interrupted,\n    MessageOperation,\n    Variables,\n    VariableValue,\n    VariableValues,\n)\nfrom marimo._runtime.requests import (\n    ControlRequest,\n    CreationRequest,\n    ExecuteMultipleRequest,\n    ExecutionRequest,\n    SetUIElementValueRequest,\n)\nfrom marimo._utils.parse_dataclass import parse_raw\n\n\nclass SessionView:\n    \"\"\"\n    This stores the current view of the session.\n\n    Which are the cell's outputs, status, and console.\n    \"\"\"\n\n    def __init__(self) -> None:\n        # List of operations we care about keeping track of.\n        self.cell_operations: dict[CellId_t, CellOp] = {}\n        # The most recent datasets operation.\n        self.datasets: Datasets = Datasets(tables=[])\n        # The most recent Variables operation.\n        self.variable_operations: Variables = Variables(variables=[])\n        # Map of variable name to value.\n        self.variable_values: dict[str, VariableValue] = {}\n        # Map of object id to value.\n        self.ui_values: dict[str, Any] = {}\n        # Map of cell id to the last code that was executed in that cell.\n        self.last_executed_code: dict[CellId_t, str] = {}\n        # Map of cell id to the last cell execution time\n        self.last_execution_time: dict[CellId_t, float] = {}\n\n    def _add_ui_value(self, name: str, value: Any) -> None:\n        self.ui_values[name] = value\n\n    def _add_last_run_code(self, req: ExecutionRequest) -> None:\n        self.last_executed_code[req.cell_id] = req.code\n\n    def add_raw_operation(self, raw_operation: Any) -> None:\n        # parse_raw only accepts a dataclass, so we wrap MessageOperation in a\n        # dataclass.\n        @dataclass\n        class _Container:\n            operation: MessageOperation\n\n        operation = parse_raw({\"operation\": raw_operation}, _Container)\n        self.add_operation(operation.operation)\n\n    def add_control_request(self, request: ControlRequest) -> None:\n        if isinstance(request, SetUIElementValueRequest):\n            for object_id, value in request.ids_and_values:\n                self._add_ui_value(object_id, value)\n        elif isinstance(request, ExecuteMultipleRequest):\n            for execution_request in request.execution_requests:\n                self._add_last_run_code(execution_request)\n        elif isinstance(request, CreationRequest):\n            for (\n                object_id,\n                value,\n            ) in request.set_ui_element_value_request.ids_and_values:\n                self._add_ui_value(object_id, value)\n            for execution_request in request.execution_requests:\n                self._add_last_run_code(execution_request)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_cell_outputs(\n        self, ids: list[CellId_t]\n    ) -> dict[CellId_t, CellOutput]:\n        \"\"\"Get the outputs for the given cell ids.\"\"\"\n        outputs: dict[CellId_t, CellOutput] = {}\n        for cell_id in ids:\n            cell_op = self.cell_operations.get(cell_id)\n            if cell_op is not None and cell_op.output is not None:\n                outputs[cell_id] = cell_op.output\n        return outputs\n\n\n\n\n\n\n\n\n\n\n\n\n    def save_execution_time(\n        self, operation: MessageOperation, event: Literal[\"start\", \"end\"]\n    ) -> None:\n        \"\"\"Updates execution time for given cell.\"\"\"\n        if not isinstance(operation, CellOp):\n            return\n        cell_id = operation.cell_id\n\n        if event == \"start\":\n            time_elapsed = operation.timestamp\n        elif event == \"end\":\n            start = self.last_execution_time.get(cell_id)\n            start = start if start else 0\n            time_elapsed = time.time() - start\n            time_elapsed = round(time_elapsed * 1000)\n\n        self.last_execution_time[cell_id] = time_elapsed\n\n    @property\n    def operations(self) -> list[MessageOperation]:\n        all_ops: list[MessageOperation] = []\n        if self.variable_operations.variables:\n            all_ops.append(self.variable_operations)\n        if self.variable_values:\n            all_ops.append(\n                VariableValues(variables=list(self.variable_values.values()))\n            )\n        if self.datasets.tables:\n            all_ops.append(self.datasets)\n        all_ops.extend(self.cell_operations.values())\n        return all_ops\n\n\ndef merge_cell_operation(\n    previous: Optional[CellOp],\n    next_: CellOp,\n) -> CellOp:\n    \"\"\"Merge two cell operations.\"\"\"\n    if previous is None:\n        return next_\n\n    assert previous.cell_id == next_.cell_id\n\n    if next_.status is None:\n        next_.status = previous.status\n\n    # If we went from queued to running, clear the console.\n    if next_.status == \"running\" and previous.status == \"queued\":\n        next_.console = []\n    else:\n        combined_console: list[CellOutput] = as_list(previous.console)\n        combined_console.extend(as_list(next_.console))\n        next_.console = combined_console\n\n    # If we went from running to running, use the previous timestamp.\n    if next_.status == \"running\" and previous.status == \"running\":\n        next_.timestamp = previous.timestamp\n\n    if next_.output is None:\n        next_.output = previous.output\n\n    return next_\n\n\ndef as_list(value: Union[Any, Optional[Any], list[Any]]) -> list[Any]:\n    if value is None:\n        return []\n    return value if isinstance(value, list) else [value]\n\n```\n# Function Signature: def get_cell_console_outputs(\n# Natural Language Annotations: \"\"\"Get the console outputs for the given cell ids.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_cell_console_outputs(ids: list", "  ```\ndef get_cell_console_outputs(\n    self", "  ```\ndef get_cell_console_outputs(\n    self"]}, "marimo-marimo/_runtime/dataflow.py-register_cell": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_runtime/dataflow.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport threading\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any, Callable, List, Tuple\n\nfrom marimo import _loggers\nfrom marimo._ast.cell import (\n    CellId_t,\n    CellImpl,\n)\nfrom marimo._ast.compiler import code_key\nfrom marimo._ast.visitor import Name, VariableData\nfrom marimo._runtime.executor import execute_cell, execute_cell_async\nfrom marimo._utils.variables import is_mangled_local\n\nif TYPE_CHECKING:\n    from collections.abc import Collection\n\nEdge = Tuple[CellId_t, CellId_t]\n# EdgeWithVar uses a list rather than a set for the variables linking the cells\n# as sets are not JSON-serializable (required by static_notebook_template()).\n# The first entry is the source node; the second entry is a list of defs from\n# the source read by the destination; and the third entry is the destination\n# node.\nEdgeWithVar = Tuple[CellId_t, List[str], CellId_t]\n\nLOGGER = _loggers.marimo_logger()\n\n\n# TODO(akshayka): Add method disable_cell, enable_cell which handle\n# state transitions on cells\n@dataclass(frozen=True)\nclass DirectedGraph:\n    # Nodes in the graph\n    cells: dict[CellId_t, CellImpl] = field(default_factory=dict)\n\n    # Edge (u, v) means v is a child of u, i.e., v has a reference\n    # to something defined in u\n    children: dict[CellId_t, set[CellId_t]] = field(default_factory=dict)\n\n    # Reversed edges (parent pointers) for convenience\n    parents: dict[CellId_t, set[CellId_t]] = field(default_factory=dict)\n\n    # Cells that define the same name\n    #\n    # siblings[cell_id] is a set of cell ids, one for each cell that shares a\n    # definition with cell_id.\n    #\n    # If this dict is non-empty, then the marimo program contains multiply\n    # defined names (and is therefore in an error state)\n    siblings: dict[CellId_t, set[CellId_t]] = field(default_factory=dict)\n\n    # A mapping from defs to the cells that define them\n    definitions: dict[Name, set[CellId_t]] = field(default_factory=dict)\n\n    # The set of cycles in the graph\n    cycles: set[tuple[Edge, ...]] = field(default_factory=set)\n\n    # This lock must be acquired during methods that mutate the graph; it's\n    # only needed because a graph is shared between the kernel and the code\n    # completion service. It should almost always be uncontended.\n    lock: threading.Lock = field(default_factory=threading.Lock)\n\n    def is_cell_cached(self, cell_id: CellId_t, code: str) -> bool:\n        \"\"\"Whether a cell with id `cell_id` and code `code` is in the graph.\"\"\"\n        return (\n            cell_id in self.cells and code_key(code) == self.cells[cell_id].key\n        )\n\n    def get_defining_cells(self, name: Name) -> set[CellId_t]:\n        \"\"\"Get all cells that define name.\n\n        This is a singleton for well-formed graphs.\n        \"\"\"\n        return self.definitions[name]\n\n    def get_referring_cells(self, name: Name) -> set[CellId_t]:\n        \"\"\"Get all cells that have a ref to `name`.\"\"\"\n        return set([cid for cid in self.cells if name in self.cells[cid].refs])\n\n    def get_path(self, source: CellId_t, dst: CellId_t) -> list[Edge]:\n        \"\"\"Get a path from `source` to `dst`, if any.\"\"\"\n        if source == dst:\n            return []\n\n        queue: list[tuple[CellId_t, list[Edge]]] = [(source, [])]\n        found = set()\n        while queue:\n            node, path = queue.pop(0)\n            found.add(node)\n            for cid in self.children[node]:\n                if cid not in found:\n                    next_path = path + [(node, cid)]\n                    if cid == dst:\n                        return next_path\n                    queue.append((cid, next_path))\n        return []\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def is_any_ancestor_stale(self, cell_id: CellId_t) -> bool:\n        return any(self.cells[cid].stale for cid in self.ancestors(cell_id))\n\n    def is_any_ancestor_disabled(self, cell_id: CellId_t) -> bool:\n        return any(\n            self.cells[cid].config.disabled for cid in self.ancestors(cell_id)\n        )\n\n    def disable_cell(self, cell_id: CellId_t) -> None:\n        \"\"\"\n        Disables a cell in the graph.\n\n        Does not mutate the graph (but does mutate cell statuses).\n\n        Returns the ids of descendants that are disabled transitively.\n        \"\"\"\n        if cell_id not in self.cells:\n            raise ValueError(f\"Cell {cell_id} not found\")\n\n        for cid in transitive_closure(self, set([cell_id])) - set([cell_id]):\n            cell = self.cells[cid]\n            cell.set_status(status=\"disabled-transitively\")\n\n    def enable_cell(self, cell_id: CellId_t) -> set[CellId_t]:\n        \"\"\"\n        Enables a cell in the graph.\n\n        Does not mutate the graph (but does mutate cell statuses).\n\n        Returns:\n        - set of cells that were stale and should be re-run\n        \"\"\"\n        if cell_id not in self.cells:\n            raise ValueError(f\"Cell {cell_id} not found\")\n\n        cells_to_run: set[CellId_t] = set()\n        for cid in transitive_closure(self, set([cell_id])):\n            if not self.is_disabled(cid):\n                child = self.cells[cid]\n                if child.stale:\n                    # cell was previously disabled, is no longer\n                    # disabled, and is stale: needs to run.\n                    cells_to_run.add(cid)\n                if child.disabled_transitively:\n                    # cell is no longer disabled: status -> idle\n                    child.set_status(\"idle\")\n        return cells_to_run\n\n    def delete_cell(self, cell_id: CellId_t) -> set[CellId_t]:\n        \"\"\"Removes a cell from the graph.\n\n        Mutates the graph, acquiring `self.lock`.\n\n        Returns the ids of the children of the removed cell.\n        \"\"\"\n        with self.lock:\n            if cell_id not in self.cells:\n                raise ValueError(f\"Cell {cell_id} not found\")\n\n            # Removing this cell from its defs' definer sets\n            for name in self.cells[cell_id].defs:\n                name_defs = self.definitions[name]\n                name_defs.remove(cell_id)\n                if not name_defs:\n                    # No more cells define this name, so we remove it from the\n                    # graph\n                    del self.definitions[name]\n\n            # Remove cycles that are broken from removing this cell.\n            edges = [(cell_id, child) for child in self.children[cell_id]] + [\n                (parent, cell_id) for parent in self.parents[cell_id]\n            ]\n            for e in edges:\n                broken_cycles = [c for c in self.cycles if e in c]\n                for c in broken_cycles:\n                    self.cycles.remove(c)\n\n            # Grab a reference to children before we remove it from our map.\n            children = self.children[cell_id]\n\n            # Purge this cell from the graph.\n            del self.cells[cell_id]\n            del self.children[cell_id]\n            del self.parents[cell_id]\n            del self.siblings[cell_id]\n\n            for elems in self.parents.values():\n                if cell_id in elems:\n                    elems.remove(cell_id)\n            for elems in self.children.values():\n                if cell_id in elems:\n                    elems.remove(cell_id)\n            for elems in self.siblings.values():\n                if cell_id in elems:\n                    elems.remove(cell_id)\n\n            return children\n\n    def is_disabled(self, cell_id: CellId_t) -> bool:\n        if cell_id not in self.cells:\n            raise ValueError(f\"Cell {cell_id} not in graph.\")\n        cell = self.cells[cell_id]\n        if cell.config.disabled:\n            return True\n        seen: set[CellId_t] = set()\n        queue = [cell_id]\n        while queue:\n            cid = queue.pop()\n            seen.add(cid)\n            for parent_id in self.parents[cid]:\n                if parent_id in seen:\n                    continue\n                elif self.cells[parent_id].config.disabled:\n                    return True\n                else:\n                    queue.append(parent_id)\n        return False\n\n    def get_multiply_defined(self) -> list[Name]:\n        names = []\n        for name, definers in self.definitions.items():\n            if len(definers) > 1:\n                names.append(name)\n        return names\n\n    def get_deleted_nonlocal_ref(self) -> list[Name]:\n        names = []\n        for cell in self.cells.values():\n            for ref in cell.deleted_refs:\n                if ref in self.definitions:\n                    names.append(ref)\n        return names\n\n    def descendants(self, cell_id: CellId_t) -> set[CellId_t]:\n        return transitive_closure(self, set([cell_id]), inclusive=False)\n\n    def ancestors(self, cell_id: CellId_t) -> set[CellId_t]:\n        return transitive_closure(\n            self, set([cell_id]), children=False, inclusive=False\n        )\n\n    def set_stale(self, cell_ids: set[CellId_t]) -> None:\n        for cid in transitive_closure(self, cell_ids):\n            self.cells[cid].set_stale(stale=True)\n\n    def get_stale(self) -> set[CellId_t]:\n        return set([cid for cid, cell in self.cells.items() if cell.stale])\n\n    def get_transitive_references(\n        self,\n        refs: set[Name],\n        inclusive: bool = True,\n        predicate: Callable[[Name, VariableData], bool] | None = None,\n    ) -> set[Name]:\n        \"\"\"Return a set of the passed-in cells' references and their\n        references on the block (function / class) level.\n\n        If inclusive, includes the references of the passed-in cells in the\n        set.\n\n        If predicate, only references satisfying predicate(ref) are included\n        \"\"\"\n        # TODO: Consider caching on the graph level and updating on register /\n        # delete\n        processed = set()\n        queue = set(refs & self.definitions.keys())\n        predicate = predicate or (lambda *_: True)\n\n        while queue:\n            # Should ideally be one cell per ref, but for completion, stay\n            # agnostic to potenital cycles.\n            cells = set().union(*[self.definitions[ref] for ref in queue])\n            for cell_id in cells:\n                data = self.cells[cell_id].variable_data\n                variables = set(data.keys())\n                # intersection of variables and queue\n                newly_processed = variables & queue\n                processed.update(newly_processed)\n                queue.difference_update(newly_processed)\n                for variable in newly_processed:\n                    if predicate(variable, data[variable]):\n                        to_process = data[variable].required_refs - processed\n                        queue.update(to_process & self.definitions.keys())\n                        # Private variables referenced by public functions have\n                        # to be included.\n                        for maybe_private in (\n                            to_process - self.definitions.keys()\n                        ):\n                            if is_mangled_local(maybe_private, cell_id):\n                                processed.add(maybe_private)\n\n        if inclusive:\n            return processed | refs\n        return processed - refs\n\n\ndef transitive_closure(\n    graph: DirectedGraph,\n    cell_ids: set[CellId_t],\n    children: bool = True,\n    inclusive: bool = True,\n    predicate: Callable[[CellImpl], bool] | None = None,\n) -> set[CellId_t]:\n    \"\"\"Return a set of the passed-in cells and their descendants or ancestors\n\n    If children is True, returns descendants; otherwise, returns ancestors\n\n    If inclusive, includes passed-in cells in the set.\n\n    If predicate, only cells satisfying predicate(cell) are included\n    \"\"\"\n    seen = set()\n    cells = set()\n    queue = list(cell_ids)\n    predicate = predicate or (lambda _: True)\n\n    def relatives(cid: CellId_t) -> set[CellId_t]:\n        return graph.children[cid] if children else graph.parents[cid]\n\n    while queue:\n        cid = queue.pop(0)\n        seen.add(cid)\n        cell = graph.cells[cid]\n        if inclusive and predicate(cell):\n            cells.add(cid)\n        elif cid not in cell_ids and predicate(cell):\n            cells.add(cid)\n        for relative in relatives(cid):\n            if relative not in seen:\n                queue.append(relative)\n    return cells\n\n\ndef induced_subgraph(\n    graph: DirectedGraph, cell_ids: Collection[CellId_t]\n) -> tuple[dict[CellId_t, set[CellId_t]], dict[CellId_t, set[CellId_t]]]:\n    \"\"\"Return parents and children for each node in `cell_ids`\n\n    Represents the subgraph induced by `cell_ids`.\n    \"\"\"\n    parents = {}\n    children = {}\n    for cid in cell_ids:\n        parents[cid] = set(p for p in graph.parents[cid] if p in cell_ids)\n        children[cid] = set(c for c in graph.children[cid] if c in cell_ids)\n    return parents, children\n\n\ndef get_cycles(\n    graph: DirectedGraph, cell_ids: Collection[CellId_t]\n) -> list[tuple[Edge, ...]]:\n    \"\"\"Get all cycles among `cell_ids`.\"\"\"\n    _, induced_children = induced_subgraph(graph, cell_ids)\n    induced_edges = set(\n        [(u, v) for u in induced_children for v in induced_children[u]]\n    )\n    return [c for c in graph.cycles if all(e in induced_edges for e in c)]\n\n\ndef topological_sort(\n    graph: DirectedGraph, cell_ids: Collection[CellId_t]\n) -> list[CellId_t]:\n    \"\"\"Sort `cell_ids` in a topological order.\"\"\"\n    parents, children = induced_subgraph(graph, cell_ids)\n    roots = [cid for cid in cell_ids if not parents[cid]]\n    sorted_cell_ids = []\n    while roots:\n        cid = roots.pop(0)\n        sorted_cell_ids.append(cid)\n        for child in children[cid]:\n            parents[child].remove(cid)\n            if not parents[child]:\n                roots.append(child)\n    # TODO make sure parents for each id is empty, otherwise cycle\n    return sorted_cell_ids\n\n\nclass Runner:\n    \"\"\"Utility for running individual cells in a graph\n\n    This class provides methods to a run a cell in the graph and obtain its\n    output (last expression) and the values of its defs.\n\n    If needed, the runner will recursively compute the values of the cell's\n    refs by executing its ancestors. Refs can also be substituted by the\n    caller.\n\n    TODO(akshayka): Add an API for caching defs across cell runs.\n    \"\"\"\n\n    def __init__(self, graph: DirectedGraph) -> None:\n        self._graph = graph\n\n    @staticmethod\n    def _returns(cell_impl: CellImpl, glbls: dict[str, Any]) -> dict[str, Any]:\n        return {name: glbls[name] for name in cell_impl.defs if name in glbls}\n\n    @staticmethod\n    def _substitute_refs(\n        cell_impl: CellImpl,\n        glbls: dict[str, Any],\n        kwargs: dict[str, Any],\n    ) -> None:\n        for argname, argvalue in kwargs.items():\n            if argname in cell_impl.refs:\n                glbls[argname] = argvalue\n            else:\n                raise ValueError(\n                    f\"Cell got unexpected argument {argname}\"\n                    f\"The allowed arguments are {cell_impl.refs}.\"\n                )\n\n    def _get_ancestors(\n        self, cell_impl: CellImpl, kwargs: dict[str, Any]\n    ) -> set[CellId_t]:\n        # Get the transitive closure of parents defining unsubstituted refs\n        graph = self._graph\n        substitutions = set(kwargs.values())\n        unsubstituted_refs = cell_impl.refs - substitutions\n        parent_ids = set(\n            [\n                parent_id\n                for parent_id in graph.parents[cell_impl.cell_id]\n                if graph.cells[parent_id].defs.intersection(unsubstituted_refs)\n            ]\n        )\n        return transitive_closure(graph, parent_ids, children=False)\n\n    @staticmethod\n    def _validate_kwargs(cell_impl: CellImpl, kwargs: dict[str, Any]) -> None:\n        for argname in kwargs:\n            if argname not in cell_impl.refs:\n                raise ValueError(\n                    f\"Cell got unexpected argument {argname}\"\n                    f\"The allowed arguments are {cell_impl.refs}.\"\n                )\n\n    def is_coroutine(self, cell_id: CellId_t) -> bool:\n        return self._graph.cells[cell_id].is_coroutine() or any(\n            self._graph.cells[cid].is_coroutine()\n            for cid in self._get_ancestors(\n                self._graph.cells[cell_id], kwargs={}\n            )\n        )\n\n    async def run_cell_async(\n        self, cell_id: CellId_t, kwargs: dict[str, Any]\n    ) -> tuple[Any, dict[str, Any]]:\n        \"\"\"Run a possibly async cell and its ancestors\n\n        Substitutes kwargs as refs for the cell, omitting ancestors that\n        whose refs are substituted.\n        \"\"\"\n        graph = self._graph\n        cell_impl = graph.cells[cell_id]\n        Runner._validate_kwargs(cell_impl, kwargs)\n        ancestor_ids = self._get_ancestors(cell_impl, kwargs)\n\n        glbls: dict[str, Any] = {}\n        for cid in topological_sort(graph, ancestor_ids):\n            await execute_cell_async(graph.cells[cid], glbls, graph)\n\n        Runner._substitute_refs(cell_impl, glbls, kwargs)\n        output = await execute_cell_async(\n            graph.cells[cell_impl.cell_id], glbls, graph\n        )\n        defs = Runner._returns(cell_impl, glbls)\n        return output, defs\n\n    def run_cell_sync(\n        self, cell_id: CellId_t, kwargs: dict[str, Any]\n    ) -> tuple[Any, dict[str, Any]]:\n        \"\"\"Run a synchronous cell and its ancestors\n\n        Substitutes kwargs as refs for the cell, omitting ancestors that\n        whose refs are substituted.\n\n        Raises a `RuntimeError` if the cell or any of its unsubstituted\n        ancestors are coroutine functions.\n        \"\"\"\n        graph = self._graph\n        cell_impl = graph.cells[cell_id]\n        if cell_impl.is_coroutine():\n            raise RuntimeError(\n                \"A coroutine function can't be run synchronously. \"\n                \"Use `run_async()` instead\"\n            )\n\n        Runner._validate_kwargs(cell_impl, kwargs)\n        ancestor_ids = self._get_ancestors(cell_impl, kwargs)\n\n        if any(graph.cells[cid].is_coroutine() for cid in ancestor_ids):\n            raise RuntimeError(\n                \"Cell has an ancestor that is a \"\n                \"coroutine (async) cell. Use `run_async()` instead\"\n            )\n\n        glbls: dict[str, Any] = {}\n        for cid in topological_sort(graph, ancestor_ids):\n            execute_cell(graph.cells[cid], glbls, graph)\n\n        self._substitute_refs(cell_impl, glbls, kwargs)\n        output = execute_cell(graph.cells[cell_impl.cell_id], glbls, graph)\n        defs = Runner._returns(cell_impl, glbls)\n        return output, defs\n\n```\n# Function Signature: def register_cell(self, cell_id: CellId_t, cell: CellImpl) -> None:\n# Natural Language Annotations: \"\"\"Add a cell to the graph.\n\n        Mutates the graph, acquiring `self.lock`.\n\n        Requires that `cell_id` is not already in the graph.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef register_cell(self, cell_id: CellId", "  ```\ndef register_cell(self, cell_id: CellId", "  ```\ndef register_cell(self, cell_id: CellId"]}, "marimo-marimo/_runtime/validate_graph.py-check_for_errors": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_runtime/validate_graph.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport itertools\nfrom collections import defaultdict\n\nfrom marimo._ast.cell import CellId_t\nfrom marimo._messaging.errors import (\n    CycleError,\n    DeleteNonlocalError,\n    Error,\n    MultipleDefinitionError,\n)\nfrom marimo._runtime.dataflow import DirectedGraph\n\n\ndef check_for_multiple_definitions(\n    graph: DirectedGraph,\n) -> dict[CellId_t, list[MultipleDefinitionError]]:\n    \"\"\"Check whether multiple cells define the same global name.\"\"\"\n    errors = defaultdict(list)\n    defs = sorted(\n        list(set().union(*(cell.defs for _, cell in graph.cells.items())))\n    )\n    for name in defs:\n        defining_cells = graph.definitions[name]\n        if len(defining_cells) > 1:\n            for cid in defining_cells:\n                errors[cid].append(\n                    MultipleDefinitionError(\n                        name=str(name),\n                        cells=tuple(sorted(defining_cells - set([cid]))),\n                    )\n                )\n    return errors\n\n\ndef check_for_delete_nonlocal(\n    graph: DirectedGraph,\n) -> dict[CellId_t, list[DeleteNonlocalError]]:\n    \"\"\"Check whether cells delete their refs.\"\"\"\n    errors = defaultdict(list)\n    for cid in graph.cells.keys():\n        for name in graph.cells[cid].deleted_refs:\n            if name in graph.definitions:\n                errors[cid].append(\n                    DeleteNonlocalError(\n                        name=str(name),\n                        cells=tuple(graph.definitions[name]),\n                    )\n                )\n    return errors\n\n\ndef check_for_cycles(graph: DirectedGraph) -> dict[CellId_t, list[CycleError]]:\n    \"\"\"Return cycle errors, if any.\"\"\"\n    errors = defaultdict(list)\n    for cycle in graph.cycles:\n        nodes_in_cycle: set[CellId_t] = set(sum(cycle, ()))\n        # before reporting the cells in the cycle to the user,\n        # we first annotate the cycle with the variable names\n        # that link its cells\n        cycle_with_vars = tuple(\n            (\n                edge[0],\n                sorted(graph.cells[edge[0]].defs & graph.cells[edge[1]].refs),\n                edge[1],\n            )\n            for edge in cycle\n        )\n        for cid in nodes_in_cycle:\n            errors[cid].append(CycleError(edges_with_vars=cycle_with_vars))\n    return errors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def check_for_errors(\n# Natural Language Annotations: \"\"\"\n    Check graph for violations of marimo semantics.\n\n    Return a dict of errors in the graph, with an entry for each cell\n    that is involved in an error.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef check_for_errors(graph: DirectedGraph)", "  ```\ndef check_for_errors(graph: DirectedGraph)", "  ```\ndef check_for_errors(graph: DirectedGraph)"]}, "marimo-marimo/_runtime/runtime.py-_maybe_register_cell": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_runtime/runtime.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport asyncio\nimport builtins\nimport contextlib\nimport dataclasses\nimport io\nimport itertools\nimport os\nimport pathlib\nimport signal\nimport sys\nimport threading\nimport time\nimport traceback\nfrom multiprocessing import connection\nfrom typing import TYPE_CHECKING, Any, Callable, Iterator, Optional, cast\nfrom uuid import uuid4\n\nfrom marimo import _loggers\nfrom marimo._ast.cell import CellConfig, CellId_t\nfrom marimo._ast.compiler import compile_cell\nfrom marimo._ast.visitor import Name\nfrom marimo._config.config import ExecutionType, MarimoConfig, OnCellChangeType\nfrom marimo._data.preview_column import get_column_preview\nfrom marimo._messaging.cell_output import CellChannel\nfrom marimo._messaging.errors import (\n    Error,\n    MarimoStrictExecutionError,\n    MarimoSyntaxError,\n    UnknownError,\n)\nfrom marimo._messaging.ops import (\n    Alert,\n    CellOp,\n    CompletedRun,\n    DataColumnPreview,\n    FunctionCallResult,\n    HumanReadableStatus,\n    InstallingPackageAlert,\n    MissingPackageAlert,\n    PackageStatusType,\n    RemoveUIElements,\n    VariableDeclaration,\n    Variables,\n    VariableValue,\n    VariableValues,\n)\nfrom marimo._messaging.streams import (\n    ThreadSafeStderr,\n    ThreadSafeStdin,\n    ThreadSafeStdout,\n    ThreadSafeStream,\n)\nfrom marimo._messaging.tracebacks import write_traceback\nfrom marimo._messaging.types import (\n    KernelMessage,\n    Stderr,\n    Stdin,\n    Stdout,\n    Stream,\n)\nfrom marimo._output.rich_help import mddoc\nfrom marimo._plugins.core.web_component import JSONType\nfrom marimo._plugins.ui._core.ui_element import MarimoConvertValueException\nfrom marimo._runtime import dataflow, handlers, marimo_pdb, patches\nfrom marimo._runtime.complete import complete, completion_worker\nfrom marimo._runtime.context import (\n    ContextNotInitializedError,\n    ExecutionContext,\n    get_context,\n)\nfrom marimo._runtime.context.kernel_context import initialize_kernel_context\nfrom marimo._runtime.control_flow import MarimoInterrupt\nfrom marimo._runtime.input_override import input_override\nfrom marimo._runtime.packages.module_registry import ModuleRegistry\nfrom marimo._runtime.packages.package_manager import PackageManager\nfrom marimo._runtime.packages.package_managers import create_package_manager\nfrom marimo._runtime.packages.utils import is_python_isolated\nfrom marimo._runtime.params import CLIArgs, QueryParams\nfrom marimo._runtime.redirect_streams import redirect_streams\nfrom marimo._runtime.reload.autoreload import ModuleReloader\nfrom marimo._runtime.reload.module_watcher import ModuleWatcher\nfrom marimo._runtime.requests import (\n    AppMetadata,\n    CodeCompletionRequest,\n    ControlRequest,\n    CreationRequest,\n    DeleteCellRequest,\n    ExecuteMultipleRequest,\n    ExecuteStaleRequest,\n    ExecutionRequest,\n    FunctionCallRequest,\n    InstallMissingPackagesRequest,\n    PreviewDatasetColumnRequest,\n    SetCellConfigRequest,\n    SetUIElementValueRequest,\n    SetUserConfigRequest,\n    StopRequest,\n)\nfrom marimo._runtime.runner import cell_runner\nfrom marimo._runtime.runner.hooks import (\n    ON_FINISH_HOOKS,\n    POST_EXECUTION_HOOKS,\n    PRE_EXECUTION_HOOKS,\n    PREPARATION_HOOKS,\n)\nfrom marimo._runtime.runner.hooks_on_finish import OnFinishHookType\nfrom marimo._runtime.runner.hooks_post_execution import PostExecutionHookType\nfrom marimo._runtime.runner.hooks_pre_execution import PreExecutionHookType\nfrom marimo._runtime.runner.hooks_preparation import PreparationHookType\nfrom marimo._runtime.state import State\nfrom marimo._runtime.utils.set_ui_element_request_manager import (\n    SetUIElementRequestManager,\n)\nfrom marimo._runtime.validate_graph import check_for_errors\nfrom marimo._runtime.win32_interrupt_handler import Win32InterruptHandler\nfrom marimo._server.types import QueueType\nfrom marimo._utils.platform import is_pyodide\nfrom marimo._utils.signals import restore_signals\nfrom marimo._utils.typed_connection import TypedConnection\nfrom marimo._utils.variables import is_local\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Sequence\n    from types import ModuleType\n\n    from marimo._plugins.ui._core.ui_element import UIElement\n\nLOGGER = _loggers.marimo_logger()\n\n\n@mddoc\ndef defs() -> tuple[str, ...]:\n    \"\"\"Get the definitions of the currently executing cell.\n\n    **Returns**:\n\n    - tuple of the currently executing cell's defs.\n    \"\"\"\n    try:\n        ctx = get_context()\n    except ContextNotInitializedError:\n        return tuple()\n\n    if ctx.execution_context is not None:\n        return tuple(\n            sorted(\n                defn\n                for defn in ctx.graph.cells[ctx.execution_context.cell_id].defs\n            )\n        )\n    return tuple()\n\n\n@mddoc\ndef refs() -> tuple[str, ...]:\n    \"\"\"Get the references of the currently executing cell.\n\n    **Returns**:\n\n    - tuple of the currently executing cell's refs.\n    \"\"\"\n    try:\n        ctx = get_context()\n    except ContextNotInitializedError:\n        return tuple()\n\n    # builtins that have not been shadowed by the user\n    unshadowed_builtins = set(builtins.__dict__.keys()).difference(\n        set(ctx.graph.definitions.keys())\n    )\n\n    if ctx.execution_context is not None:\n        return tuple(\n            sorted(\n                defn\n                for defn in ctx.graph.cells[ctx.execution_context.cell_id].refs\n                # exclude builtins that have not been shadowed\n                if defn not in unshadowed_builtins\n            )\n        )\n    return tuple()\n\n\n@mddoc\ndef query_params() -> QueryParams:\n    \"\"\"Get the query parameters of a marimo app.\n\n    **Examples**:\n\n    Keep the text input in sync with the URL query parameters.\n\n    ```python3\n    # In it's own cell\n    query_params = mo.query_params()\n\n    # In another cell\n    search = mo.ui.text(\n        value=query_params[\"search\"] or \"\",\n        on_change=lambda value: query_params.set(\"search\", value),\n    )\n    search\n    ```\n\n    You can also set the query parameters reactively:\n\n    ```python3\n    toggle = mo.ui.switch(label=\"Toggle me\")\n    toggle\n\n    # In another cell\n    query_params[\"is_enabled\"] = toggle.value\n    ```\n\n    **Returns**:\n\n    - A `QueryParams` object containing the query parameters.\n      You can directly interact with this object like a dictionary.\n      If you mutate this object, changes will be persisted to the frontend\n      query parameters and any other cells referencing the query parameters\n      will automatically re-run.\n    \"\"\"\n    return get_context().query_params\n\n\n@mddoc\ndef cli_args() -> CLIArgs:\n    \"\"\"Get the command line arguments of a marimo notebook.\n\n        **Examples**:\n\n    `marimo edit notebook.py -- -size 10`\n\n        ```python3\n        # Access the command line arguments\n        size = mo.cli_args().get(\"size\") or 100\n\n        for i in range(size):\n            print(i)\n        ```\n\n        **Returns**:\n\n        - A dictionary containing the command line arguments.\n          This dictionary is read-only and cannot be mutated.\n    \"\"\"\n    return get_context().cli_args\n\n\n@dataclasses.dataclass\nclass CellMetadata:\n    \"\"\"CellMetadata\n\n    Metadata the kernel needs to persist, even when a cell is removed\n    from the graph or when a cell can't be formed from user code due to syntax\n    errors.\n    \"\"\"\n\n    config: CellConfig = dataclasses.field(default_factory=CellConfig)\n\n\nclass Kernel:\n    \"\"\"Kernel that manages the dependency graph and its execution.\n\n    Args:\n    - cell_configs: initial configuration for each cell\n    - app_metadata: metadata about the notebook\n    - user_config: the initial user configuration\n    - stream: object used to communicate with the server/outside world\n    - stdout: replacement for sys.stdout\n    - stderr: replacement for sys.stderr\n    - stdin: replacement for sys.stdin\n    - module: module in which to execute code\n    - enqueue_control_request: callback to enqueue control requests\n    - debugger_override: a replacement for the built-in Pdb\n    \"\"\"\n\n    def __init__(\n        self,\n        cell_configs: dict[CellId_t, CellConfig],\n        app_metadata: AppMetadata,\n        user_config: MarimoConfig,\n        stream: Stream,\n        stdout: Stdout | None,\n        stderr: Stderr | None,\n        stdin: Stdin | None,\n        module: ModuleType,\n        enqueue_control_request: Callable[[ControlRequest], None],\n        preparation_hooks: list[PreparationHookType] | None = None,\n        pre_execution_hooks: list[PreExecutionHookType] | None = None,\n        post_execution_hooks: list[PostExecutionHookType] | None = None,\n        on_finish_hooks: list[OnFinishHookType] | None = None,\n        debugger_override: marimo_pdb.MarimoPdb | None = None,\n    ) -> None:\n        self.app_metadata = app_metadata\n        self.query_params = QueryParams(app_metadata.query_params)\n        self.cli_args = CLIArgs(app_metadata.cli_args)\n        self.stream = stream\n        self.stdout = stdout\n        self.stderr = stderr\n        self.stdin = stdin\n        self.enqueue_control_request = enqueue_control_request\n\n        self._preparation_hooks = (\n            preparation_hooks\n            if preparation_hooks is not None\n            else PREPARATION_HOOKS\n        )\n        self._pre_execution_hooks = (\n            pre_execution_hooks\n            if pre_execution_hooks is not None\n            else PRE_EXECUTION_HOOKS\n        )\n        self._post_execution_hooks = (\n            post_execution_hooks\n            if post_execution_hooks is not None\n            else POST_EXECUTION_HOOKS\n        )\n        self._on_finish_hooks = (\n            on_finish_hooks if on_finish_hooks is not None else ON_FINISH_HOOKS\n        )\n\n        self._globals_lock = threading.RLock()\n        self._completion_worker_started = False\n\n        self.debugger = debugger_override\n        if self.debugger is not None:\n            patches.patch_pdb(self.debugger)\n\n        self._module = module\n        if self.app_metadata.filename is not None:\n            # TODO(akshayka): When a file is renamed / moved to another folder,\n            # we need to update sys.path.\n            try:\n                notebook_directory = str(\n                    pathlib.Path(self.app_metadata.filename).parent.absolute()\n                )\n                if notebook_directory not in sys.path:\n                    sys.path.insert(0, notebook_directory)\n            except Exception as e:\n                LOGGER.warning(\n                    \"Failed to add directory to path (error %e)\", str(e)\n                )\n        elif \"\" not in sys.path:\n            # an empty string represents ...\n            #   the current directory, when using\n            #      marimo edit filename.py / marimo run\n            #   the marimo home directory, when using\n            #      marimo edit (ie homepage)\n            sys.path.insert(0, \"\")\n\n        self.graph = dataflow.DirectedGraph()\n        self.cell_metadata: dict[CellId_t, CellMetadata] = {\n            cell_id: CellMetadata(config=config)\n            for cell_id, config in cell_configs.items()\n        }\n        self.module_registry = ModuleRegistry(\n            self.graph, excluded_modules=set()\n        )\n        self.package_manager: PackageManager | None = None\n        self.module_reloader: ModuleReloader | None = None\n        self.module_watcher: ModuleWatcher | None = None\n        # Load runtime settings from user config\n        self.reactive_execution_mode: OnCellChangeType = user_config[\n            \"runtime\"\n        ][\"on_cell_change\"]\n        self.execution_type: ExecutionType = user_config.get(\n            \"experimental\", {}\n        ).get(\"execution_type\", \"relaxed\")\n        self._update_runtime_from_user_config(user_config)\n\n        # Set up the execution context\n        self.execution_context: Optional[ExecutionContext] = None\n        # initializers to override construction of ui elements\n        self.ui_initializers: dict[str, Any] = {}\n        # errored cells\n        self.errors: dict[CellId_t, tuple[Error, ...]] = {}\n        # Mapping from state to the cell when its setter\n        # was invoked. New state updates evict older ones.\n        self.state_updates: dict[State[Any], CellId_t] = {}\n\n        if not is_pyodide():\n            patches.patch_micropip(self.globals)\n        exec(\"import marimo as __marimo__\", self.globals)\n\n    def lazy(self) -> bool:\n        return self.reactive_execution_mode == \"lazy\"\n\n    def _execute_stale_cells_callback(self) -> None:\n        return self.enqueue_control_request(ExecuteStaleRequest())\n\n    def _execute_install_missing_packages_callback(\n        self, package_manager: str\n    ) -> None:\n        return self.enqueue_control_request(\n            InstallMissingPackagesRequest(manager=package_manager)\n        )\n\n    def _update_runtime_from_user_config(self, config: MarimoConfig) -> None:\n        package_manager = config[\"package_management\"][\"manager\"]\n        autoreload_mode = config[\"runtime\"][\"auto_reload\"]\n        self.reactive_execution_mode = config[\"runtime\"][\"on_cell_change\"]\n\n        if (\n            self.package_manager is None\n            or package_manager != self.package_manager.name\n        ):\n            self.package_manager = create_package_manager(package_manager)\n\n        if autoreload_mode == \"lazy\" or autoreload_mode == \"autorun\":\n            if self.module_reloader is None:\n                self.module_reloader = ModuleReloader()\n\n            if (\n                self.module_watcher is not None\n                and self.module_watcher.mode != autoreload_mode\n            ):\n                self.module_watcher.stop()\n                self.module_watcher = None\n\n            if self.module_watcher is None:\n                self.module_watcher = ModuleWatcher(\n                    self.graph,\n                    reloader=self.module_reloader,\n                    enqueue_run_stale_cells=self._execute_stale_cells_callback,\n                    mode=autoreload_mode,\n                    stream=self.stream,\n                )\n        else:\n            self.module_reloader = None\n            if self.module_watcher is not None:\n                self.module_watcher.stop()\n\n        self.user_config = config\n\n    @property\n    def globals(self) -> dict[Any, Any]:\n        return self._module.__dict__\n\n    @contextlib.contextmanager\n    def lock_globals(self) -> Iterator[None]:\n        # The only other thread accessing globals is the completion worker. If\n        # we haven't started a completion worker, there's no need to lock\n        # globals.\n        if self._completion_worker_started:\n            with self._globals_lock:\n                yield\n        else:\n            yield\n\n    def start_completion_worker(\n        self, completion_queue: QueueType[CodeCompletionRequest]\n    ) -> None:\n        \"\"\"Must be called after context is initialized\"\"\"\n        threading.Thread(\n            target=completion_worker,\n            args=(\n                completion_queue,\n                self.graph,\n                self.globals,\n                self._globals_lock,\n                get_context().stream,\n            ),\n            daemon=True,\n        ).start()\n        self._completion_worker_started = True\n\n    def code_completion(\n        self, request: CodeCompletionRequest, docstrings_limit: int\n    ) -> None:\n        complete(\n            request,\n            self.graph,\n            self.globals,\n            self._globals_lock,\n            get_context().stream,\n            docstrings_limit,\n        )\n\n    @contextlib.contextmanager\n    def _install_execution_context(\n        self, cell_id: CellId_t, setting_element_value: bool = False\n    ) -> Iterator[ExecutionContext]:\n        self.execution_context = ExecutionContext(\n            cell_id, setting_element_value\n        )\n        with get_context().provide_ui_ids(str(cell_id)), redirect_streams(\n            cell_id,\n            stream=self.stream,\n            stdout=self.stdout,\n            stderr=self.stderr,\n            stdin=self.stdin,\n        ):\n            modules = None\n            try:\n                if self.module_reloader is not None:\n                    # Reload modules if they have changed\n                    modules = set(sys.modules)\n                    self.module_reloader.check(\n                        modules=sys.modules, reload=True\n                    )\n                yield self.execution_context\n            finally:\n                self.execution_context = None\n                if self.module_reloader is not None and modules is not None:\n                    # Note timestamps for newly loaded modules\n                    new_modules = set(sys.modules) - modules\n                    self.module_reloader.check(\n                        modules={m: sys.modules[m] for m in new_modules},\n                        reload=False,\n                    )\n\n    def _try_registering_cell(\n        self,\n        cell_id: CellId_t,\n        code: str,\n    ) -> Optional[Error]:\n        \"\"\"Attempt to register a cell with given id and code.\n\n        Precondition: a cell with the supplied id must not already exist in the\n        graph.\n\n        If cell was unable to be registered, returns an Error object.\n        \"\"\"\n        error: Optional[Error] = None\n        try:\n            cell = compile_cell(code, cell_id=cell_id)\n        except Exception as e:\n            cell = None\n            if isinstance(e, SyntaxError):\n                tmpio = io.StringIO()\n                traceback.print_exc(file=tmpio, limit=0)\n                tmpio.seek(0)\n                syntax_error = tmpio.read().split(\"\\n\")\n                # first line has the form File XXX, line XXX\n                syntax_error[0] = syntax_error[0][\n                    syntax_error[0].find(\"line\") :\n                ]\n                error = MarimoSyntaxError(msg=\"\\n\".join(syntax_error))\n            else:\n                tmpio = io.StringIO()\n                traceback.print_exc(file=tmpio)\n                tmpio.seek(0)\n                error = UnknownError(msg=tmpio.read())\n\n        if cell_id in self.cell_metadata and cell is not None:\n            # If we already have a config for this cell id, restore it\n            # This can happen when a cell was previously deactivated (due to a\n            # syntax error or multiple definition error, for example) and then\n            # re-registered\n            cell.configure(self.cell_metadata[cell_id].config)\n        elif cell_id not in self.cell_metadata:\n            self.cell_metadata[cell_id] = CellMetadata()\n\n        if cell is not None:\n            self.graph.register_cell(cell_id, cell)\n            # leaky abstraction: the graph doesn't know about stale modules, so\n            # we have to check for them here.\n            module_reloader = self.module_reloader\n            if (\n                module_reloader is not None\n                and module_reloader.cell_uses_stale_modules(cell)\n            ):\n                self.graph.set_stale(set([cell.cell_id]))\n            LOGGER.debug(\"registered cell %s\", cell_id)\n            LOGGER.debug(\"parents: %s\", self.graph.parents[cell_id])\n            LOGGER.debug(\"children: %s\", self.graph.children[cell_id])\n\n        return error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _delete_names(\n        self, names: Iterable[Name], exclude_defs: set[Name]\n    ) -> None:\n        \"\"\"Delete `names` from kernel, except for `exclude_defs`\"\"\"\n        for name in names:\n            if name in exclude_defs:\n                continue\n\n            if name in self.globals:\n                del self.globals[name]\n\n            if (\n                \"__annotations__\" in self.globals\n                and name in self.globals[\"__annotations__\"]\n            ):\n                del self.globals[\"__annotations__\"][name]\n\n    def _invalidate_cell_state(\n        self,\n        cell_id: CellId_t,\n        exclude_defs: Optional[set[Name]] = None,\n        deletion: bool = False,\n    ) -> None:\n        \"\"\"Cleanup state associated with this cell.\n\n        Deletes a cell's defs from the kernel state, except for the names in\n        `exclude_defs`, and instructs the frontend to invalidate its UI\n        elements.\n        \"\"\"\n        cell = self.graph.cells[cell_id]\n        missing_modules_before_deletion = (\n            self.module_registry.missing_modules()\n        )\n        defs_to_delete = cell.defs\n        self._delete_names(\n            defs_to_delete, exclude_defs if exclude_defs is not None else set()\n        )\n\n        missing_modules_after_deletion = (\n            missing_modules_before_deletion & self.module_registry.modules()\n        )\n        if (\n            self.package_manager is not None\n            and missing_modules_after_deletion\n            != missing_modules_before_deletion\n        ):\n            if self.package_manager.should_auto_install():\n                self._execute_install_missing_packages_callback(\n                    self.package_manager.name\n                )\n            else:\n                # Deleting a cell can make the set of missing packages smaller\n                MissingPackageAlert(\n                    packages=list(\n                        sorted(\n                            self.package_manager.module_to_package(mod)\n                            for mod in missing_modules_after_deletion\n                        )\n                    ),\n                    isolated=is_python_isolated(),\n                ).broadcast()\n\n        cell.set_output(None)\n        get_context().cell_lifecycle_registry.dispose(\n            cell_id, deletion=deletion\n        )\n        for descendent in self.graph.descendants(cell_id):\n            get_context().cell_lifecycle_registry.dispose(\n                descendent, deletion=deletion\n            )\n        RemoveUIElements(cell_id=cell_id).broadcast()\n\n    def _deactivate_cell(self, cell_id: CellId_t) -> set[CellId_t]:\n        \"\"\"Deactivate: remove from graph, invalidate state, but keep metadata\n\n        Keeps the cell's config, in case we see the same cell again.\n\n        In contrast to deleting a cell, which fully scrubs the cell\n        from the kernel and graph.\n        \"\"\"\n        if cell_id not in self.errors:\n            self._invalidate_cell_state(cell_id, deletion=True)\n            return self.graph.delete_cell(cell_id)\n        else:\n            # An errored cell can be thought of as a cell that's in the graph\n            # but that has no state in the kernel (because it was never run).\n            # Its defs may overlap with defs of a non-errored cell, so we MUST\n            # NOT delete/cleanup its defs from the kernel (i.e., an errored\n            # cell shouldn't invalidate state of another cell).\n            self.graph.delete_cell(cell_id)\n            return set()\n\n    def _delete_cell(self, cell_id: CellId_t) -> set[CellId_t]:\n        \"\"\"Delete a cell from the kernel and the graph.\n\n        Deletion from the kernel involves removing cell's defs and\n        de-registering its UI Elements.\n\n        Deletion from graph is forwarded to graph object.\n        \"\"\"\n        del self.cell_metadata[cell_id]\n        return self._deactivate_cell(cell_id)\n\n    def mutate_graph(\n        self,\n        execution_requests: Sequence[ExecutionRequest],\n        deletion_requests: Sequence[DeleteCellRequest],\n    ) -> set[CellId_t]:\n        \"\"\"Add and remove cells to/from the graph.\n\n        This method adds the cells in `execution_requests` to the kernel's\n        graph (deleting old versions of these cells, if any), and removes the\n        cells in `deletion_requests` from the kernel's graph.\n\n        The mutations that this method makes to the graph renders the\n        kernel inconsistent (stale).\n\n        This method does not register errors for cells that were previously\n        valid and are not descendants of any of the newly registered cells.\n        This is important for multiple definition errors, since a user may\n        absent-mindedly redefine an existing name when creating a new cell:\n        such a mistake shouldn't invalidate the program state.\n\n        Returns\n        - set of cells that must be run to return kernel to consistent state\n        \"\"\"\n        LOGGER.debug(\"Current set of errors: %s\", self.errors)\n        cells_before_mutation = set(self.graph.cells.keys())\n\n```\n# Function Signature: def _maybe_register_cell(\n# Natural Language Annotations: \"\"\"Register a cell (given by id, code) if not already registered.\n\n        If a cell with id `cell_id` is already registered but with different\n        code, that cell is deleted from the graph and a new cell with the\n        same id but different code is registered.\n\n        Returns:\n        - a set of ids for cells that were previously children of `cell_id`;\n          only non-empty when `cell-id` was already registered but with\n          different code.\n        - an `Error` if the cell couldn't be registered, `None` otherwise\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _maybe_register_cell(\n    self, cell", "  ```\ndef _maybe_register_cell(\n        self,\n", "  ```\n@mddoc\ndef _maybe_register_cell("]}, "marimo-marimo/_runtime/runtime.py-run": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_runtime/runtime.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport asyncio\nimport builtins\nimport contextlib\nimport dataclasses\nimport io\nimport itertools\nimport os\nimport pathlib\nimport signal\nimport sys\nimport threading\nimport time\nimport traceback\nfrom multiprocessing import connection\nfrom typing import TYPE_CHECKING, Any, Callable, Iterator, Optional, cast\nfrom uuid import uuid4\n\nfrom marimo import _loggers\nfrom marimo._ast.cell import CellConfig, CellId_t\nfrom marimo._ast.compiler import compile_cell\nfrom marimo._ast.visitor import Name\nfrom marimo._config.config import ExecutionType, MarimoConfig, OnCellChangeType\nfrom marimo._data.preview_column import get_column_preview\nfrom marimo._messaging.cell_output import CellChannel\nfrom marimo._messaging.errors import (\n    Error,\n    MarimoStrictExecutionError,\n    MarimoSyntaxError,\n    UnknownError,\n)\nfrom marimo._messaging.ops import (\n    Alert,\n    CellOp,\n    CompletedRun,\n    DataColumnPreview,\n    FunctionCallResult,\n    HumanReadableStatus,\n    InstallingPackageAlert,\n    MissingPackageAlert,\n    PackageStatusType,\n    RemoveUIElements,\n    VariableDeclaration,\n    Variables,\n    VariableValue,\n    VariableValues,\n)\nfrom marimo._messaging.streams import (\n    ThreadSafeStderr,\n    ThreadSafeStdin,\n    ThreadSafeStdout,\n    ThreadSafeStream,\n)\nfrom marimo._messaging.tracebacks import write_traceback\nfrom marimo._messaging.types import (\n    KernelMessage,\n    Stderr,\n    Stdin,\n    Stdout,\n    Stream,\n)\nfrom marimo._output.rich_help import mddoc\nfrom marimo._plugins.core.web_component import JSONType\nfrom marimo._plugins.ui._core.ui_element import MarimoConvertValueException\nfrom marimo._runtime import dataflow, handlers, marimo_pdb, patches\nfrom marimo._runtime.complete import complete, completion_worker\nfrom marimo._runtime.context import (\n    ContextNotInitializedError,\n    ExecutionContext,\n    get_context,\n)\nfrom marimo._runtime.context.kernel_context import initialize_kernel_context\nfrom marimo._runtime.control_flow import MarimoInterrupt\nfrom marimo._runtime.input_override import input_override\nfrom marimo._runtime.packages.module_registry import ModuleRegistry\nfrom marimo._runtime.packages.package_manager import PackageManager\nfrom marimo._runtime.packages.package_managers import create_package_manager\nfrom marimo._runtime.packages.utils import is_python_isolated\nfrom marimo._runtime.params import CLIArgs, QueryParams\nfrom marimo._runtime.redirect_streams import redirect_streams\nfrom marimo._runtime.reload.autoreload import ModuleReloader\nfrom marimo._runtime.reload.module_watcher import ModuleWatcher\nfrom marimo._runtime.requests import (\n    AppMetadata,\n    CodeCompletionRequest,\n    ControlRequest,\n    CreationRequest,\n    DeleteCellRequest,\n    ExecuteMultipleRequest,\n    ExecuteStaleRequest,\n    ExecutionRequest,\n    FunctionCallRequest,\n    InstallMissingPackagesRequest,\n    PreviewDatasetColumnRequest,\n    SetCellConfigRequest,\n    SetUIElementValueRequest,\n    SetUserConfigRequest,\n    StopRequest,\n)\nfrom marimo._runtime.runner import cell_runner\nfrom marimo._runtime.runner.hooks import (\n    ON_FINISH_HOOKS,\n    POST_EXECUTION_HOOKS,\n    PRE_EXECUTION_HOOKS,\n    PREPARATION_HOOKS,\n)\nfrom marimo._runtime.runner.hooks_on_finish import OnFinishHookType\nfrom marimo._runtime.runner.hooks_post_execution import PostExecutionHookType\nfrom marimo._runtime.runner.hooks_pre_execution import PreExecutionHookType\nfrom marimo._runtime.runner.hooks_preparation import PreparationHookType\nfrom marimo._runtime.state import State\nfrom marimo._runtime.utils.set_ui_element_request_manager import (\n    SetUIElementRequestManager,\n)\nfrom marimo._runtime.validate_graph import check_for_errors\nfrom marimo._runtime.win32_interrupt_handler import Win32InterruptHandler\nfrom marimo._server.types import QueueType\nfrom marimo._utils.platform import is_pyodide\nfrom marimo._utils.signals import restore_signals\nfrom marimo._utils.typed_connection import TypedConnection\nfrom marimo._utils.variables import is_local\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Sequence\n    from types import ModuleType\n\n    from marimo._plugins.ui._core.ui_element import UIElement\n\nLOGGER = _loggers.marimo_logger()\n\n\n@mddoc\ndef defs() -> tuple[str, ...]:\n    \"\"\"Get the definitions of the currently executing cell.\n\n    **Returns**:\n\n    - tuple of the currently executing cell's defs.\n    \"\"\"\n    try:\n        ctx = get_context()\n    except ContextNotInitializedError:\n        return tuple()\n\n    if ctx.execution_context is not None:\n        return tuple(\n            sorted(\n                defn\n                for defn in ctx.graph.cells[ctx.execution_context.cell_id].defs\n            )\n        )\n    return tuple()\n\n\n@mddoc\ndef refs() -> tuple[str, ...]:\n    \"\"\"Get the references of the currently executing cell.\n\n    **Returns**:\n\n    - tuple of the currently executing cell's refs.\n    \"\"\"\n    try:\n        ctx = get_context()\n    except ContextNotInitializedError:\n        return tuple()\n\n    # builtins that have not been shadowed by the user\n    unshadowed_builtins = set(builtins.__dict__.keys()).difference(\n        set(ctx.graph.definitions.keys())\n    )\n\n    if ctx.execution_context is not None:\n        return tuple(\n            sorted(\n                defn\n                for defn in ctx.graph.cells[ctx.execution_context.cell_id].refs\n                # exclude builtins that have not been shadowed\n                if defn not in unshadowed_builtins\n            )\n        )\n    return tuple()\n\n\n@mddoc\ndef query_params() -> QueryParams:\n    \"\"\"Get the query parameters of a marimo app.\n\n    **Examples**:\n\n    Keep the text input in sync with the URL query parameters.\n\n    ```python3\n    # In it's own cell\n    query_params = mo.query_params()\n\n    # In another cell\n    search = mo.ui.text(\n        value=query_params[\"search\"] or \"\",\n        on_change=lambda value: query_params.set(\"search\", value),\n    )\n    search\n    ```\n\n    You can also set the query parameters reactively:\n\n    ```python3\n    toggle = mo.ui.switch(label=\"Toggle me\")\n    toggle\n\n    # In another cell\n    query_params[\"is_enabled\"] = toggle.value\n    ```\n\n    **Returns**:\n\n    - A `QueryParams` object containing the query parameters.\n      You can directly interact with this object like a dictionary.\n      If you mutate this object, changes will be persisted to the frontend\n      query parameters and any other cells referencing the query parameters\n      will automatically re-run.\n    \"\"\"\n    return get_context().query_params\n\n\n@mddoc\ndef cli_args() -> CLIArgs:\n    \"\"\"Get the command line arguments of a marimo notebook.\n\n        **Examples**:\n\n    `marimo edit notebook.py -- -size 10`\n\n        ```python3\n        # Access the command line arguments\n        size = mo.cli_args().get(\"size\") or 100\n\n        for i in range(size):\n            print(i)\n        ```\n\n        **Returns**:\n\n        - A dictionary containing the command line arguments.\n          This dictionary is read-only and cannot be mutated.\n    \"\"\"\n    return get_context().cli_args\n\n\n@dataclasses.dataclass\nclass CellMetadata:\n    \"\"\"CellMetadata\n\n    Metadata the kernel needs to persist, even when a cell is removed\n    from the graph or when a cell can't be formed from user code due to syntax\n    errors.\n    \"\"\"\n\n    config: CellConfig = dataclasses.field(default_factory=CellConfig)\n\n\nclass Kernel:\n    \"\"\"Kernel that manages the dependency graph and its execution.\n\n    Args:\n    - cell_configs: initial configuration for each cell\n    - app_metadata: metadata about the notebook\n    - user_config: the initial user configuration\n    - stream: object used to communicate with the server/outside world\n    - stdout: replacement for sys.stdout\n    - stderr: replacement for sys.stderr\n    - stdin: replacement for sys.stdin\n    - module: module in which to execute code\n    - enqueue_control_request: callback to enqueue control requests\n    - debugger_override: a replacement for the built-in Pdb\n    \"\"\"\n\n    def __init__(\n        self,\n        cell_configs: dict[CellId_t, CellConfig],\n        app_metadata: AppMetadata,\n        user_config: MarimoConfig,\n        stream: Stream,\n        stdout: Stdout | None,\n        stderr: Stderr | None,\n        stdin: Stdin | None,\n        module: ModuleType,\n        enqueue_control_request: Callable[[ControlRequest], None],\n        preparation_hooks: list[PreparationHookType] | None = None,\n        pre_execution_hooks: list[PreExecutionHookType] | None = None,\n        post_execution_hooks: list[PostExecutionHookType] | None = None,\n        on_finish_hooks: list[OnFinishHookType] | None = None,\n        debugger_override: marimo_pdb.MarimoPdb | None = None,\n    ) -> None:\n        self.app_metadata = app_metadata\n        self.query_params = QueryParams(app_metadata.query_params)\n        self.cli_args = CLIArgs(app_metadata.cli_args)\n        self.stream = stream\n        self.stdout = stdout\n        self.stderr = stderr\n        self.stdin = stdin\n        self.enqueue_control_request = enqueue_control_request\n\n        self._preparation_hooks = (\n            preparation_hooks\n            if preparation_hooks is not None\n            else PREPARATION_HOOKS\n        )\n        self._pre_execution_hooks = (\n            pre_execution_hooks\n            if pre_execution_hooks is not None\n            else PRE_EXECUTION_HOOKS\n        )\n        self._post_execution_hooks = (\n            post_execution_hooks\n            if post_execution_hooks is not None\n            else POST_EXECUTION_HOOKS\n        )\n        self._on_finish_hooks = (\n            on_finish_hooks if on_finish_hooks is not None else ON_FINISH_HOOKS\n        )\n\n        self._globals_lock = threading.RLock()\n        self._completion_worker_started = False\n\n        self.debugger = debugger_override\n        if self.debugger is not None:\n            patches.patch_pdb(self.debugger)\n\n        self._module = module\n        if self.app_metadata.filename is not None:\n            # TODO(akshayka): When a file is renamed / moved to another folder,\n            # we need to update sys.path.\n            try:\n                notebook_directory = str(\n                    pathlib.Path(self.app_metadata.filename).parent.absolute()\n                )\n                if notebook_directory not in sys.path:\n                    sys.path.insert(0, notebook_directory)\n            except Exception as e:\n                LOGGER.warning(\n                    \"Failed to add directory to path (error %e)\", str(e)\n                )\n        elif \"\" not in sys.path:\n            # an empty string represents ...\n            #   the current directory, when using\n            #      marimo edit filename.py / marimo run\n            #   the marimo home directory, when using\n            #      marimo edit (ie homepage)\n            sys.path.insert(0, \"\")\n\n        self.graph = dataflow.DirectedGraph()\n        self.cell_metadata: dict[CellId_t, CellMetadata] = {\n            cell_id: CellMetadata(config=config)\n            for cell_id, config in cell_configs.items()\n        }\n        self.module_registry = ModuleRegistry(\n            self.graph, excluded_modules=set()\n        )\n        self.package_manager: PackageManager | None = None\n        self.module_reloader: ModuleReloader | None = None\n        self.module_watcher: ModuleWatcher | None = None\n        # Load runtime settings from user config\n        self.reactive_execution_mode: OnCellChangeType = user_config[\n            \"runtime\"\n        ][\"on_cell_change\"]\n        self.execution_type: ExecutionType = user_config.get(\n            \"experimental\", {}\n        ).get(\"execution_type\", \"relaxed\")\n        self._update_runtime_from_user_config(user_config)\n\n        # Set up the execution context\n        self.execution_context: Optional[ExecutionContext] = None\n        # initializers to override construction of ui elements\n        self.ui_initializers: dict[str, Any] = {}\n        # errored cells\n        self.errors: dict[CellId_t, tuple[Error, ...]] = {}\n        # Mapping from state to the cell when its setter\n        # was invoked. New state updates evict older ones.\n        self.state_updates: dict[State[Any], CellId_t] = {}\n\n        if not is_pyodide():\n            patches.patch_micropip(self.globals)\n        exec(\"import marimo as __marimo__\", self.globals)\n\n    def lazy(self) -> bool:\n        return self.reactive_execution_mode == \"lazy\"\n\n    def _execute_stale_cells_callback(self) -> None:\n        return self.enqueue_control_request(ExecuteStaleRequest())\n\n    def _execute_install_missing_packages_callback(\n        self, package_manager: str\n    ) -> None:\n        return self.enqueue_control_request(\n            InstallMissingPackagesRequest(manager=package_manager)\n        )\n\n    def _update_runtime_from_user_config(self, config: MarimoConfig) -> None:\n        package_manager = config[\"package_management\"][\"manager\"]\n        autoreload_mode = config[\"runtime\"][\"auto_reload\"]\n        self.reactive_execution_mode = config[\"runtime\"][\"on_cell_change\"]\n\n        if (\n            self.package_manager is None\n            or package_manager != self.package_manager.name\n        ):\n            self.package_manager = create_package_manager(package_manager)\n\n        if autoreload_mode == \"lazy\" or autoreload_mode == \"autorun\":\n            if self.module_reloader is None:\n                self.module_reloader = ModuleReloader()\n\n            if (\n                self.module_watcher is not None\n                and self.module_watcher.mode != autoreload_mode\n            ):\n                self.module_watcher.stop()\n                self.module_watcher = None\n\n            if self.module_watcher is None:\n                self.module_watcher = ModuleWatcher(\n                    self.graph,\n                    reloader=self.module_reloader,\n                    enqueue_run_stale_cells=self._execute_stale_cells_callback,\n                    mode=autoreload_mode,\n                    stream=self.stream,\n                )\n        else:\n            self.module_reloader = None\n            if self.module_watcher is not None:\n                self.module_watcher.stop()\n\n        self.user_config = config\n\n    @property\n    def globals(self) -> dict[Any, Any]:\n        return self._module.__dict__\n\n    @contextlib.contextmanager\n    def lock_globals(self) -> Iterator[None]:\n        # The only other thread accessing globals is the completion worker. If\n        # we haven't started a completion worker, there's no need to lock\n        # globals.\n        if self._completion_worker_started:\n            with self._globals_lock:\n                yield\n        else:\n            yield\n\n    def start_completion_worker(\n        self, completion_queue: QueueType[CodeCompletionRequest]\n    ) -> None:\n        \"\"\"Must be called after context is initialized\"\"\"\n        threading.Thread(\n            target=completion_worker,\n            args=(\n                completion_queue,\n                self.graph,\n                self.globals,\n                self._globals_lock,\n                get_context().stream,\n            ),\n            daemon=True,\n        ).start()\n        self._completion_worker_started = True\n\n    def code_completion(\n        self, request: CodeCompletionRequest, docstrings_limit: int\n    ) -> None:\n        complete(\n            request,\n            self.graph,\n            self.globals,\n            self._globals_lock,\n            get_context().stream,\n            docstrings_limit,\n        )\n\n    @contextlib.contextmanager\n    def _install_execution_context(\n        self, cell_id: CellId_t, setting_element_value: bool = False\n    ) -> Iterator[ExecutionContext]:\n        self.execution_context = ExecutionContext(\n            cell_id, setting_element_value\n        )\n        with get_context().provide_ui_ids(str(cell_id)), redirect_streams(\n            cell_id,\n            stream=self.stream,\n            stdout=self.stdout,\n            stderr=self.stderr,\n            stdin=self.stdin,\n        ):\n            modules = None\n            try:\n                if self.module_reloader is not None:\n                    # Reload modules if they have changed\n                    modules = set(sys.modules)\n                    self.module_reloader.check(\n                        modules=sys.modules, reload=True\n                    )\n                yield self.execution_context\n            finally:\n                self.execution_context = None\n                if self.module_reloader is not None and modules is not None:\n                    # Note timestamps for newly loaded modules\n                    new_modules = set(sys.modules) - modules\n                    self.module_reloader.check(\n                        modules={m: sys.modules[m] for m in new_modules},\n                        reload=False,\n                    )\n\n    def _try_registering_cell(\n        self,\n        cell_id: CellId_t,\n        code: str,\n    ) -> Optional[Error]:\n        \"\"\"Attempt to register a cell with given id and code.\n\n        Precondition: a cell with the supplied id must not already exist in the\n        graph.\n\n        If cell was unable to be registered, returns an Error object.\n        \"\"\"\n        error: Optional[Error] = None\n        try:\n            cell = compile_cell(code, cell_id=cell_id)\n        except Exception as e:\n            cell = None\n            if isinstance(e, SyntaxError):\n                tmpio = io.StringIO()\n                traceback.print_exc(file=tmpio, limit=0)\n                tmpio.seek(0)\n                syntax_error = tmpio.read().split(\"\\n\")\n                # first line has the form File XXX, line XXX\n                syntax_error[0] = syntax_error[0][\n                    syntax_error[0].find(\"line\") :\n                ]\n                error = MarimoSyntaxError(msg=\"\\n\".join(syntax_error))\n            else:\n                tmpio = io.StringIO()\n                traceback.print_exc(file=tmpio)\n                tmpio.seek(0)\n                error = UnknownError(msg=tmpio.read())\n\n        if cell_id in self.cell_metadata and cell is not None:\n            # If we already have a config for this cell id, restore it\n            # This can happen when a cell was previously deactivated (due to a\n            # syntax error or multiple definition error, for example) and then\n            # re-registered\n            cell.configure(self.cell_metadata[cell_id].config)\n        elif cell_id not in self.cell_metadata:\n            self.cell_metadata[cell_id] = CellMetadata()\n\n        if cell is not None:\n            self.graph.register_cell(cell_id, cell)\n            # leaky abstraction: the graph doesn't know about stale modules, so\n            # we have to check for them here.\n            module_reloader = self.module_reloader\n            if (\n                module_reloader is not None\n                and module_reloader.cell_uses_stale_modules(cell)\n            ):\n                self.graph.set_stale(set([cell.cell_id]))\n            LOGGER.debug(\"registered cell %s\", cell_id)\n            LOGGER.debug(\"parents: %s\", self.graph.parents[cell_id])\n            LOGGER.debug(\"children: %s\", self.graph.children[cell_id])\n\n        return error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _delete_names(\n        self, names: Iterable[Name], exclude_defs: set[Name]\n    ) -> None:\n        \"\"\"Delete `names` from kernel, except for `exclude_defs`\"\"\"\n        for name in names:\n            if name in exclude_defs:\n                continue\n\n            if name in self.globals:\n                del self.globals[name]\n\n            if (\n                \"__annotations__\" in self.globals\n                and name in self.globals[\"__annotations__\"]\n            ):\n                del self.globals[\"__annotations__\"][name]\n\n    def _invalidate_cell_state(\n        self,\n        cell_id: CellId_t,\n        exclude_defs: Optional[set[Name]] = None,\n        deletion: bool = False,\n    ) -> None:\n        \"\"\"Cleanup state associated with this cell.\n\n        Deletes a cell's defs from the kernel state, except for the names in\n        `exclude_defs`, and instructs the frontend to invalidate its UI\n        elements.\n        \"\"\"\n        cell = self.graph.cells[cell_id]\n        missing_modules_before_deletion = (\n            self.module_registry.missing_modules()\n        )\n        defs_to_delete = cell.defs\n        self._delete_names(\n            defs_to_delete, exclude_defs if exclude_defs is not None else set()\n        )\n\n        missing_modules_after_deletion = (\n            missing_modules_before_deletion & self.module_registry.modules()\n        )\n        if (\n            self.package_manager is not None\n            and missing_modules_after_deletion\n            != missing_modules_before_deletion\n        ):\n            if self.package_manager.should_auto_install():\n                self._execute_install_missing_packages_callback(\n                    self.package_manager.name\n                )\n            else:\n                # Deleting a cell can make the set of missing packages smaller\n                MissingPackageAlert(\n                    packages=list(\n                        sorted(\n                            self.package_manager.module_to_package(mod)\n                            for mod in missing_modules_after_deletion\n                        )\n                    ),\n                    isolated=is_python_isolated(),\n                ).broadcast()\n\n        cell.set_output(None)\n        get_context().cell_lifecycle_registry.dispose(\n            cell_id, deletion=deletion\n        )\n        for descendent in self.graph.descendants(cell_id):\n            get_context().cell_lifecycle_registry.dispose(\n                descendent, deletion=deletion\n            )\n        RemoveUIElements(cell_id=cell_id).broadcast()\n\n    def _deactivate_cell(self, cell_id: CellId_t) -> set[CellId_t]:\n        \"\"\"Deactivate: remove from graph, invalidate state, but keep metadata\n\n        Keeps the cell's config, in case we see the same cell again.\n\n        In contrast to deleting a cell, which fully scrubs the cell\n        from the kernel and graph.\n        \"\"\"\n        if cell_id not in self.errors:\n            self._invalidate_cell_state(cell_id, deletion=True)\n            return self.graph.delete_cell(cell_id)\n        else:\n            # An errored cell can be thought of as a cell that's in the graph\n            # but that has no state in the kernel (because it was never run).\n            # Its defs may overlap with defs of a non-errored cell, so we MUST\n            # NOT delete/cleanup its defs from the kernel (i.e., an errored\n            # cell shouldn't invalidate state of another cell).\n            self.graph.delete_cell(cell_id)\n            return set()\n\n    def _delete_cell(self, cell_id: CellId_t) -> set[CellId_t]:\n        \"\"\"Delete a cell from the kernel and the graph.\n\n        Deletion from the kernel involves removing cell's defs and\n        de-registering its UI Elements.\n\n        Deletion from graph is forwarded to graph object.\n        \"\"\"\n        del self.cell_metadata[cell_id]\n        return self._deactivate_cell(cell_id)\n\n    def mutate_graph(\n        self,\n        execution_requests: Sequence[ExecutionRequest],\n        deletion_requests: Sequence[DeleteCellRequest],\n    ) -> set[CellId_t]:\n        \"\"\"Add and remove cells to/from the graph.\n\n        This method adds the cells in `execution_requests` to the kernel's\n        graph (deleting old versions of these cells, if any), and removes the\n        cells in `deletion_requests` from the kernel's graph.\n\n        The mutations that this method makes to the graph renders the\n        kernel inconsistent (stale).\n\n        This method does not register errors for cells that were previously\n        valid and are not descendants of any of the newly registered cells.\n        This is important for multiple definition errors, since a user may\n        absent-mindedly redefine an existing name when creating a new cell:\n        such a mistake shouldn't invalidate the program state.\n\n        Returns\n        - set of cells that must be run to return kernel to consistent state\n        \"\"\"\n        LOGGER.debug(\"Current set of errors: %s\", self.errors)\n        cells_before_mutation = set(self.graph.cells.keys())\n\n```\n# Function Signature: async def run(\n# Natural Language Annotations: \"\"\"Run cells and their descendants.\n\n\n        The cells may be cells already existing in the graph or new cells.\n        Adds the cells in `execution_requests` to the graph before running\n        them.\n\n        Cells may use top-level await, which is why this function is async.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def run(self, execution_requests: Sequence[", "  ```\nasync def run(\n    self,\n    execution_requests", "  ```\nimport logging\nimport sys\n\nfrom marimo._ast."]}, "marimo-marimo/_runtime/packages/utils.py-is_python_isolated": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_runtime/packages/utils.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nimport os\nimport sys\n\nfrom marimo._utils.platform import is_pyodide\n\n\ndef in_virtual_environment() -> bool:\n    \"\"\"Returns True if a venv/virtualenv is activated\"\"\"\n    # https://stackoverflow.com/questions/1871549/how-to-determine-if-python-is-running-inside-a-virtualenv/40099080#40099080  # noqa: E501\n    base_prefix = (\n        getattr(sys, \"base_prefix\", None)\n        or getattr(sys, \"real_prefix\", None)\n        or sys.prefix\n    )\n    return sys.prefix != base_prefix\n\n\ndef in_conda_env() -> bool:\n    return \"CONDA_DEFAULT_ENV\" in os.environ\n\n\n\n\n\n\n```\n# Function Signature: def is_python_isolated() -> bool:\n# Natural Language Annotations: \"\"\"Returns True if not using system Python\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef is_python_isolated() -> bool:\n", "  ```\ndef is_python_isolated() -> bool:\n", "  ```\ndef is_python_isolated() -> bool:\n"]}, "marimo-marimo/_runtime/packages/module_registry.py-missing_modules": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/marimo/marimo/_runtime/packages/module_registry.py:\n```\n# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport importlib.util\nimport sys\n\nfrom marimo._ast.cell import CellId_t\nfrom marimo._runtime.dataflow import DirectedGraph\n\n\ndef _is_module_installed(module_name: str) -> bool:\n    # importlib.util.find_spec retrieves a module's ModuleSpec, which\n    # is typically available as a dunder attribute on the module, i.e.\n    # module.__spec__. However, some packages are non-compliant and don't\n    # include a __spec__ attr (e.g., manim-slides), which can cause find_spec\n    # to throw if the module has already been imported.\n    #\n    # We don't actually need the spec, we just need to see if a package is\n    # available, so we first check if the module is in sys.modules without\n    # checking for a __spec__ attr.\n    return (\n        module_name in sys.modules\n        or importlib.util.find_spec(module_name) is not None\n    )\n\n\nclass ModuleRegistry:\n    def __init__(\n        self, graph: DirectedGraph, excluded_modules: set[str] | None = None\n    ) -> None:\n        self.graph = graph\n        # modules that do not have corresponding packages on package index\n        self.excluded_modules = (\n            excluded_modules if excluded_modules is not None else set()\n        )\n\n    def defining_cell(self, module_name: str) -> CellId_t | None:\n        \"\"\"Get the cell id of the cell importing module_name\"\"\"\n        for cell_id, cell in self.graph.cells.items():\n            if cell.namespace_to_variable(module_name) is not None:\n                return cell_id\n        return None\n\n    def modules(self) -> set[str]:\n        \"\"\"Modules imported by cells.\"\"\"\n        return set(\n            mod\n            for cell in self.graph.cells.values()\n            for mod in cell.imported_namespaces\n        )\n\n\n\n\n\n\n\n\n```\n# Function Signature: def missing_modules(self) -> set[str]:\n# Natural Language Annotations: \"\"\"Modules that will fail to import.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef missing_modules(self) -> set[str]:\n", "  ```\ndef missing_modules(self) -> set[str]:\n", "  ```\ndef missing_modules(self) -> set[str]:\n"]}, "TaskWeaver-taskweaver/misc/example.py-load_examples": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/TaskWeaver/taskweaver/misc/example.py:\n```\nimport glob\nfrom os import path\nfrom typing import List, Optional, Set\n\nfrom taskweaver.memory.conversation import Conversation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def load_examples(\n# Natural Language Annotations: \"\"\"\n    Load all the examples from a folder.\n\n    Args:\n        folder: the folder path.\n        role_set: the roles should be included in the examples.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport glob\nfrom os import path\nfrom typing import List,", "  ```\ndef load_examples(folder: str, role_set:", "  ```\ndef load_examples(folder: str, role_set:"]}, "TaskWeaver-taskweaver/code_interpreter/plugin_selection.py-add_selected_plugins": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/TaskWeaver/taskweaver/code_interpreter/plugin_selection.py:\n```\nimport os\nfrom typing import Dict, List\n\nfrom injector import inject\n\nfrom taskweaver.llm import LLMApi\nfrom taskweaver.memory.plugin import PluginEntry, PluginRegistry\nfrom taskweaver.utils import generate_md5_hash, write_yaml\n\n\nclass SelectedPluginPool:\n    def __init__(self):\n        self.selected_plugin_pool = []\n        self._previous_used_plugin_cache = []  # cache the plugins used in the previous code generation\n\n\n\n\n\n\n\n    def __len__(self) -> int:\n        return len(self.selected_plugin_pool)\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_plugins(self) -> List[PluginEntry]:\n        return self.selected_plugin_pool\n\n    @staticmethod\n    def merge_plugin_pool(pool1: List[PluginEntry], pool2: List[PluginEntry]) -> List[PluginEntry]:\n        \"\"\"\n        Merge two plugin pools and remove duplicates\n        \"\"\"\n        merged_list: List[PluginEntry] = pool1 + pool2\n        result: List[PluginEntry] = []\n\n        for item in merged_list:\n            is_duplicate = False\n            for existing_item in result:\n                if item.name == existing_item.name:\n                    is_duplicate = True\n                    break\n            if not is_duplicate:\n                result.append(item)\n        return result\n\n\nclass PluginSelector:\n    @inject\n    def __init__(\n        self,\n        plugin_registry: PluginRegistry,\n        llm_api: LLMApi,\n        plugin_only: bool = False,\n    ):\n        if plugin_only:\n            self.available_plugins = [p for p in plugin_registry.get_list() if p.plugin_only is True]\n        else:\n            self.available_plugins = plugin_registry.get_list()\n        self.llm_api = llm_api\n        self.plugin_embedding_dict: Dict[str, List[float]] = {}\n\n        self.exception_message_for_refresh = (\n            \"Please cd to the `script` directory and \"\n            \"run `python -m plugin_mgt --refresh` to refresh the plugin embedding.\"\n        )\n\n        self.meta_file_dir = os.path.join(os.path.dirname(plugin_registry.file_glob), \".meta\")\n        if not os.path.exists(self.meta_file_dir):\n            os.makedirs(self.meta_file_dir)\n\n    def refresh(self):\n        plugins_to_embedded = []\n        for idx, p in enumerate(self.available_plugins):\n            if (\n                len(p.meta_data.embedding) > 0\n                and p.meta_data.embedding_model == self.llm_api.embedding_service.config.embedding_model\n                and p.meta_data.md5hash == generate_md5_hash(p.spec.name + p.spec.description)\n            ):\n                continue\n            else:\n                plugins_to_embedded.append((idx, p.name + \": \" + p.spec.description))\n\n        if len(plugins_to_embedded) == 0:\n            print(\"All plugins are up-to-date.\")\n            return\n\n        plugin_embeddings = self.llm_api.get_embedding_list([text for idx, text in plugins_to_embedded])\n\n        for i, embedding in enumerate(plugin_embeddings):\n            p = self.available_plugins[plugins_to_embedded[i][0]]\n            p.meta_data.embedding = embedding\n            p.meta_data.embedding_model = self.llm_api.embedding_service.config.embedding_model\n            p.meta_data.md5hash = generate_md5_hash(p.spec.name + p.spec.description)\n            write_yaml(p.meta_data.path, p.meta_data.to_dict())\n\n    def load_plugin_embeddings(self):\n        for idx, p in enumerate(self.available_plugins):\n            # check if the plugin has embedding\n            assert len(p.meta_data.embedding) > 0, (\n                f\"Plugin {p.name} has no embedding. \" + self.exception_message_for_refresh\n            )\n            # check if the plugin is using the same embedding model as the current session\n            assert p.meta_data.embedding_model == self.llm_api.embedding_service.config.embedding_model, (\n                f\"Plugin {p.name} is using embedding model {p.meta_data.embedding_model}, \"\n                f\"which is different from the one used by current session\"\n                f\" ({self.llm_api.embedding_service.config.embedding_model}). \"\n                f\"Please use the same embedding model or refresh the plugin embedding.\"\n                + self.exception_message_for_refresh\n            )\n            # check if the plugin has been modified\n            assert p.meta_data.md5hash == generate_md5_hash(p.spec.name + p.spec.description), (\n                f\"Plugin {p.name} has been modified. \" + self.exception_message_for_refresh\n            )\n\n            self.plugin_embedding_dict[p.name] = p.meta_data.embedding\n\n    def plugin_select(self, user_query: str, top_k: int = 5) -> List[PluginEntry]:\n        if top_k >= len(self.available_plugins):\n            return self.available_plugins\n\n        import numpy as np\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        similarities = []\n        user_query_embedding = np.array(self.llm_api.get_embedding(user_query))\n\n        for p in self.available_plugins:\n            similarity = cosine_similarity(\n                user_query_embedding.reshape(\n                    1,\n                    -1,\n                ),\n                np.array(self.plugin_embedding_dict[p.name]).reshape(1, -1),\n            )\n            similarities.append((p, similarity))\n\n        plugins_rank = sorted(\n            similarities,\n            key=lambda x: x[1],\n            reverse=True,\n        )[:top_k]\n\n        selected_plugins = [p for p, sim in plugins_rank]\n\n        return selected_plugins\n\n```\n# Function Signature: def add_selected_plugins(self, external_plugin_pool: List[PluginEntry]):\n# Natural Language Annotations: \"\"\"\n        Add selected plugins to the pool\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef add_selected_plugins(self, external_plugin_", "  ```\ndef add_selected_plugins(self, external_plugin_", "  ```\ndef add_selected_plugins(self, external_plugin_"]}, "TaskWeaver-taskweaver/code_interpreter/plugin_selection.py-filter_unused_plugins": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/TaskWeaver/taskweaver/code_interpreter/plugin_selection.py:\n```\nimport os\nfrom typing import Dict, List\n\nfrom injector import inject\n\nfrom taskweaver.llm import LLMApi\nfrom taskweaver.memory.plugin import PluginEntry, PluginRegistry\nfrom taskweaver.utils import generate_md5_hash, write_yaml\n\n\nclass SelectedPluginPool:\n    def __init__(self):\n        self.selected_plugin_pool = []\n        self._previous_used_plugin_cache = []  # cache the plugins used in the previous code generation\n\n\n\n\n\n\n\n    def __len__(self) -> int:\n        return len(self.selected_plugin_pool)\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_plugins(self) -> List[PluginEntry]:\n        return self.selected_plugin_pool\n\n    @staticmethod\n    def merge_plugin_pool(pool1: List[PluginEntry], pool2: List[PluginEntry]) -> List[PluginEntry]:\n        \"\"\"\n        Merge two plugin pools and remove duplicates\n        \"\"\"\n        merged_list: List[PluginEntry] = pool1 + pool2\n        result: List[PluginEntry] = []\n\n        for item in merged_list:\n            is_duplicate = False\n            for existing_item in result:\n                if item.name == existing_item.name:\n                    is_duplicate = True\n                    break\n            if not is_duplicate:\n                result.append(item)\n        return result\n\n\nclass PluginSelector:\n    @inject\n    def __init__(\n        self,\n        plugin_registry: PluginRegistry,\n        llm_api: LLMApi,\n        plugin_only: bool = False,\n    ):\n        if plugin_only:\n            self.available_plugins = [p for p in plugin_registry.get_list() if p.plugin_only is True]\n        else:\n            self.available_plugins = plugin_registry.get_list()\n        self.llm_api = llm_api\n        self.plugin_embedding_dict: Dict[str, List[float]] = {}\n\n        self.exception_message_for_refresh = (\n            \"Please cd to the `script` directory and \"\n            \"run `python -m plugin_mgt --refresh` to refresh the plugin embedding.\"\n        )\n\n        self.meta_file_dir = os.path.join(os.path.dirname(plugin_registry.file_glob), \".meta\")\n        if not os.path.exists(self.meta_file_dir):\n            os.makedirs(self.meta_file_dir)\n\n    def refresh(self):\n        plugins_to_embedded = []\n        for idx, p in enumerate(self.available_plugins):\n            if (\n                len(p.meta_data.embedding) > 0\n                and p.meta_data.embedding_model == self.llm_api.embedding_service.config.embedding_model\n                and p.meta_data.md5hash == generate_md5_hash(p.spec.name + p.spec.description)\n            ):\n                continue\n            else:\n                plugins_to_embedded.append((idx, p.name + \": \" + p.spec.description))\n\n        if len(plugins_to_embedded) == 0:\n            print(\"All plugins are up-to-date.\")\n            return\n\n        plugin_embeddings = self.llm_api.get_embedding_list([text for idx, text in plugins_to_embedded])\n\n        for i, embedding in enumerate(plugin_embeddings):\n            p = self.available_plugins[plugins_to_embedded[i][0]]\n            p.meta_data.embedding = embedding\n            p.meta_data.embedding_model = self.llm_api.embedding_service.config.embedding_model\n            p.meta_data.md5hash = generate_md5_hash(p.spec.name + p.spec.description)\n            write_yaml(p.meta_data.path, p.meta_data.to_dict())\n\n    def load_plugin_embeddings(self):\n        for idx, p in enumerate(self.available_plugins):\n            # check if the plugin has embedding\n            assert len(p.meta_data.embedding) > 0, (\n                f\"Plugin {p.name} has no embedding. \" + self.exception_message_for_refresh\n            )\n            # check if the plugin is using the same embedding model as the current session\n            assert p.meta_data.embedding_model == self.llm_api.embedding_service.config.embedding_model, (\n                f\"Plugin {p.name} is using embedding model {p.meta_data.embedding_model}, \"\n                f\"which is different from the one used by current session\"\n                f\" ({self.llm_api.embedding_service.config.embedding_model}). \"\n                f\"Please use the same embedding model or refresh the plugin embedding.\"\n                + self.exception_message_for_refresh\n            )\n            # check if the plugin has been modified\n            assert p.meta_data.md5hash == generate_md5_hash(p.spec.name + p.spec.description), (\n                f\"Plugin {p.name} has been modified. \" + self.exception_message_for_refresh\n            )\n\n            self.plugin_embedding_dict[p.name] = p.meta_data.embedding\n\n    def plugin_select(self, user_query: str, top_k: int = 5) -> List[PluginEntry]:\n        if top_k >= len(self.available_plugins):\n            return self.available_plugins\n\n        import numpy as np\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        similarities = []\n        user_query_embedding = np.array(self.llm_api.get_embedding(user_query))\n\n        for p in self.available_plugins:\n            similarity = cosine_similarity(\n                user_query_embedding.reshape(\n                    1,\n                    -1,\n                ),\n                np.array(self.plugin_embedding_dict[p.name]).reshape(1, -1),\n            )\n            similarities.append((p, similarity))\n\n        plugins_rank = sorted(\n            similarities,\n            key=lambda x: x[1],\n            reverse=True,\n        )[:top_k]\n\n        selected_plugins = [p for p, sim in plugins_rank]\n\n        return selected_plugins\n\n```\n# Function Signature: def filter_unused_plugins(self, code: str):\n# Natural Language Annotations: \"\"\"\n        Filter out plugins that are not used in the code generated by LLM\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef filter_unused_plugins(self, code: str", "  ```\ndef filter_unused_plugins(self, code: str", "  ```\ndef filter_unused_plugins(self, code: str"]}, "TaskWeaver-taskweaver/module/prompt_util.py-remove_all_delimiters": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/TaskWeaver/taskweaver/module/prompt_util.py:\n```\nfrom typing import List, Tuple\n\n\nclass PromptUtil:\n    DELIMITER_TEMPORAL: Tuple[str, str] = (\"{{DELIMITER_START_TEMPORAL}}\", \"{{DELIMITER_END_TEMPORAL}}\")\n\n    @staticmethod\n    def wrap_text_with_delimiter(text, delimiter: Tuple[str, str]) -> str:\n        \"\"\"Wrap the provided text with the specified start and end delimiters.\"\"\"\n        return f\"{delimiter[0]}{text}{delimiter[1]}\"\n\n    @staticmethod\n    def get_all_delimiters() -> List[Tuple[str, str]]:\n        \"\"\"Get all the delimiters.\"\"\"\n        return [getattr(PromptUtil, attr) for attr in dir(PromptUtil) if attr.startswith(\"DELIMITER_\")]\n\n    @staticmethod\n    def remove_parts(text: str, delimiter: Tuple[str, str]) -> str:\n        \"\"\"Remove the parts of the text that are wrapped by the specified delimiters.\"\"\"\n        while True:\n            # Find the start of the temporal part\n            start_index = text.find(delimiter[0])\n            # Find the end of the temporal part\n            end_index = text.find(delimiter[1], start_index + len(delimiter[0]))\n\n            # Check if both markers are present\n            if start_index != -1 and end_index != -1:\n                # Ensure that the start marker comes before the end marker\n                if start_index < end_index:\n                    # Remove the temporal part including the markers\n                    text = text[:start_index] + text[end_index + len(delimiter[1]) :]\n                else:\n                    break\n            elif start_index == -1 and end_index == -1:\n                # No more markers found, break the loop\n                break\n            else:\n                # One of the markers is found without the other\n                break\n        return text\n\n    @staticmethod\n    def remove_delimiter(text: str, delimiter: Tuple[str, str]):\n        \"\"\"Remove the specified delimiter from the text.\"\"\"\n        text = text.replace(delimiter[0], \"\")\n        text = text.replace(delimiter[1], \"\")\n        return text\n\n    @staticmethod\n\n\n\n\n\n\n```\n# Function Signature: def remove_all_delimiters(text: str) -> str:\n# Natural Language Annotations: \"\"\"Remove all the delimiters from the text.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef remove_all_delimiters(text: str)", "  ```\ndef remove_all_delimiters(text: str)", "  ```\ndef remove_all_delimiters(text: str)"]}, "instructor-instructor/process_response.py-handle_response_model": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/instructor/instructor/process_response.py:\n```\n# type: ignore[all]\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\nfrom textwrap import dedent\nfrom instructor.dsl.iterable import IterableBase, IterableModel\nfrom instructor.dsl.parallel import ParallelBase, ParallelModel, handle_parallel_model\nfrom instructor.dsl.partial import PartialBase\nfrom instructor.dsl.simple_type import AdapterBase, ModelAdapter, is_simple_type\nfrom instructor.function_calls import OpenAISchema, openai_schema\nfrom instructor.utils import merge_consecutive_messages\nfrom openai.types.chat import ChatCompletion\nfrom pydantic import BaseModel, create_model\n\nimport json\nimport inspect\nimport logging\nfrom typing import (\n    get_args,\n    get_origin,\n    TypeVar,\n    Any,\n)\nfrom collections.abc import Generator\nfrom typing_extensions import ParamSpec\n\nfrom instructor.mode import Mode\n\nfrom .utils import transform_to_gemini_prompt\n\nlogger = logging.getLogger(\"instructor\")\n\nT_Model = TypeVar(\"T_Model\", bound=BaseModel)\nT_Retval = TypeVar(\"T_Retval\")\nT_ParamSpec = ParamSpec(\"T_ParamSpec\")\nT = TypeVar(\"T\")\n\n\nasync def process_response_async(\n    response: ChatCompletion,\n    *,\n    response_model: type[T_Model | OpenAISchema | BaseModel] | None,\n    stream: bool = False,\n    validation_context: dict[str, Any] | None = None,\n    strict: bool | None = None,\n    mode: Mode = Mode.TOOLS,\n) -> T_Model | ChatCompletion:\n    \"\"\"Processes a OpenAI response with the response model, if available.\n    It can use `validation_context` and `strict` to validate the response\n    via the pydantic model\n\n    Args:\n        response (ChatCompletion): The response from OpenAI's API\n        response_model (BaseModel): The response model to use for parsing the response\n        stream (bool): Whether the response is a stream\n        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.\n        strict (bool, optional): Whether to use strict json parsing. Defaults to None.\n    \"\"\"\n\n    logger.debug(\n        f\"Instructor Raw Response: {response}\",\n    )\n    if response_model is None:\n        return response\n\n    if (\n        inspect.isclass(response_model)\n        and issubclass(response_model, (IterableBase, PartialBase))\n        and stream\n    ):\n        model = await response_model.from_streaming_response_async(\n            response,\n            mode=mode,\n        )\n        return model\n\n    model = response_model.from_response(\n        response,\n        validation_context=validation_context,\n        strict=strict,\n        mode=mode,\n    )\n\n    # ? This really hints at the fact that we need a better way of\n    # ? attaching usage data and the raw response to the model we return.\n    if isinstance(model, IterableBase):\n        logger.debug(f\"Returning takes from IterableBase\")\n        return [task for task in model.tasks]\n\n    if isinstance(response_model, ParallelBase):\n        logger.debug(f\"Returning model from ParallelBase\")\n        return model\n\n    if isinstance(model, AdapterBase):\n        logger.debug(f\"Returning model from AdapterBase\")\n        return model.content\n\n    model._raw_response = response\n    return model\n\n\ndef process_response(\n    response: T_Model,\n    *,\n    response_model: type[OpenAISchema | BaseModel],\n    stream: bool,\n    validation_context: dict | None = None,\n    strict=None,\n    mode: Mode = Mode.TOOLS,\n) -> T_Model | Generator[T_Model, None, None] | ChatCompletion:\n    \"\"\"Processes a OpenAI response with the response model, if available.\n\n    Args:\n        response (T): The response from OpenAI's API\n        response_model (Type[T_Model]): The response model to use for parsing the response\n        stream (bool): Whether the response is a stream\n        validation_context (dict, optional): The validation context to use for validating the response. Defaults to None.\n        strict (_type_, optional): Whether to use strict json parsing. Defaults to None.\n        mode (Mode, optional): The openai completion mode. Defaults to Mode.FUNCTIONS.\n\n    Returns:\n        Union[T_Model, T]: The parsed response, if a response model is available, otherwise the response as is from the SDK\n    \"\"\"\n\n    logger.debug(\n        f\"Instructor Raw Response: {response}\",\n    )\n\n    if response_model is None:\n        logger.debug(\"No response model, returning response as is\")\n        return response\n\n    if (\n        inspect.isclass(response_model)\n        and issubclass(response_model, (IterableBase, PartialBase))\n        and stream\n    ):\n        model = response_model.from_streaming_response(\n            response,\n            mode=mode,\n        )\n        return model\n\n    model = response_model.from_response(\n        response,\n        validation_context=validation_context,\n        strict=strict,\n        mode=mode,\n    )\n\n    # ? This really hints at the fact that we need a better way of\n    # ? attaching usage data and the raw response to the model we return.\n    if isinstance(model, IterableBase):\n        logger.debug(f\"Returning takes from IterableBase\")\n        return [task for task in model.tasks]\n\n    if isinstance(response_model, ParallelBase):\n        logger.debug(f\"Returning model from ParallelBase\")\n        return model\n\n    if isinstance(model, AdapterBase):\n        logger.debug(f\"Returning model from AdapterBase\")\n        return model.content\n\n    model._raw_response = response\n    return model\n\n\ndef is_typed_dict(cls) -> bool:\n    return (\n        isinstance(cls, type)\n        and issubclass(cls, dict)\n        and hasattr(cls, \"__annotations__\")\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def handle_response_model(\n# Natural Language Annotations: \"\"\"Prepare the response model type hint, and returns the response_model\n    along with the new modified kwargs needed to be able to use the response_model\n    parameter with the patch function.\n\n\n    Args:\n        response_model (T): The response model to use for parsing the response\n        mode (Mode, optional): The openai completion mode. Defaults to Mode.TOOLS.\n\n    Raises:\n        NotImplementedError: When using stream=True with a non-iterable response_model\n        ValueError: When using an invalid patch mode\n\n    Returns:\n        Union[Type[OpenAISchema], dict]: The response model to use for parsing the response\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef handle_response_model(response_model: T,", "  ```\ndef handle_response_model(\n    response_model:", "  ```\ndef handle_response_model(\n    response_model:"]}, "reflex-reflex/app.py-_enable_state": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/app.py:\n```\n\"\"\"The main Reflex app.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport concurrent.futures\nimport contextlib\nimport copy\nimport functools\nimport io\nimport multiprocessing\nimport os\nimport platform\nimport sys\nfrom datetime import datetime\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n    get_args,\n    get_type_hints,\n)\n\nfrom fastapi import FastAPI, HTTPException, Request, UploadFile\nfrom fastapi.middleware import cors\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom rich.progress import MofNCompleteColumn, Progress, TimeElapsedColumn\nfrom socketio import ASGIApp, AsyncNamespace, AsyncServer\nfrom starlette_admin.contrib.sqla.admin import Admin\nfrom starlette_admin.contrib.sqla.view import ModelView\n\nfrom reflex import constants\nfrom reflex.admin import AdminDash\nfrom reflex.app_mixins import AppMixin, LifespanMixin, MiddlewareMixin\nfrom reflex.base import Base\nfrom reflex.compiler import compiler\nfrom reflex.compiler import utils as compiler_utils\nfrom reflex.compiler.compiler import ExecutorSafeFunctions\nfrom reflex.components.base.app_wrap import AppWrap\nfrom reflex.components.base.fragment import Fragment\nfrom reflex.components.component import (\n    Component,\n    ComponentStyle,\n    evaluate_style_namespaces,\n)\nfrom reflex.components.core.banner import connection_pulser, connection_toaster\nfrom reflex.components.core.breakpoints import set_breakpoints\nfrom reflex.components.core.client_side_routing import (\n    Default404Page,\n    wait_for_client_redirect,\n)\nfrom reflex.components.core.upload import Upload, get_upload_dir\nfrom reflex.components.radix import themes\nfrom reflex.config import get_config\nfrom reflex.event import Event, EventHandler, EventSpec\nfrom reflex.model import Model\nfrom reflex.page import (\n    DECORATED_PAGES,\n)\nfrom reflex.route import (\n    get_route_args,\n    replace_brackets_with_keywords,\n    verify_route_validity,\n)\nfrom reflex.state import (\n    BaseState,\n    RouterData,\n    State,\n    StateManager,\n    StateUpdate,\n    _substate_key,\n    code_uses_state_contexts,\n)\nfrom reflex.utils import codespaces, console, exceptions, format, prerequisites, types\nfrom reflex.utils.exec import is_prod_mode, is_testing_env, should_skip_compile\nfrom reflex.utils.imports import ImportVar\n\n# Define custom types.\nComponentCallable = Callable[[], Component]\nReducer = Callable[[Event], Coroutine[Any, Any, StateUpdate]]\n\n\ndef default_overlay_component() -> Component:\n    \"\"\"Default overlay_component attribute for App.\n\n    Returns:\n        The default overlay_component, which is a connection_modal.\n    \"\"\"\n    return Fragment.create(\n        connection_pulser(),\n        connection_toaster(),\n        *codespaces.codespaces_auto_redirect(),\n    )\n\n\nclass OverlayFragment(Fragment):\n    \"\"\"Alias for Fragment, used to wrap the overlay_component.\"\"\"\n\n    pass\n\n\nclass App(MiddlewareMixin, LifespanMixin, Base):\n    \"\"\"The main Reflex app that encapsulates the backend and frontend.\n\n    Every Reflex app needs an app defined in its main module.\n\n    ```python\n    # app.py\n    import reflex as rx\n\n    # Define state and pages\n    ...\n\n    app = rx.App(\n        # Set global level style.\n        style={...},\n        # Set the top level theme.\n        theme=rx.theme(accent_color=\"blue\"),\n    )\n    ```\n    \"\"\"\n\n    # The global [theme](https://reflex.dev/docs/styling/theming/#theme) for the entire app.\n    theme: Optional[Component] = themes.theme(accent_color=\"blue\")\n\n    # The [global style](https://reflex.dev/docs/styling/overview/#global-styles}) for the app.\n    style: ComponentStyle = {}\n\n    # A list of URLs to [stylesheets](https://reflex.dev/docs/styling/custom-stylesheets/) to include in the app.\n    stylesheets: List[str] = []\n\n    # A component that is present on every page (defaults to the Connection Error banner).\n    overlay_component: Optional[Union[Component, ComponentCallable]] = (\n        default_overlay_component\n    )\n\n    # Components to add to the head of every page.\n    head_components: List[Component] = []\n\n    # The Socket.IO AsyncServer instance.\n    sio: Optional[AsyncServer] = None\n\n    # The language to add to the html root tag of every page.\n    html_lang: Optional[str] = None\n\n    # Attributes to add to the html root tag of every page.\n    html_custom_attrs: Optional[Dict[str, str]] = None\n\n    # A map from a page route to the component to render. Users should use `add_page`. PRIVATE.\n    pages: Dict[str, Component] = {}\n\n    # The backend API object. PRIVATE.\n    api: FastAPI = None  # type: ignore\n\n    # The state class to use for the app. PRIVATE.\n    state: Optional[Type[BaseState]] = None\n\n    # Class to manage many client states.\n    _state_manager: Optional[StateManager] = None\n\n    # Mapping from a route to event handlers to trigger when the page loads. PRIVATE.\n    load_events: Dict[str, List[Union[EventHandler, EventSpec]]] = {}\n\n    # Admin dashboard to view and manage the database. PRIVATE.\n    admin_dash: Optional[AdminDash] = None\n\n    # The async server name space. PRIVATE.\n    event_namespace: Optional[EventNamespace] = None\n\n    # Background tasks that are currently running. PRIVATE.\n    background_tasks: Set[asyncio.Task] = set()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the app.\n\n        Args:\n            **kwargs: Kwargs to initialize the app with.\n\n        Raises:\n            ValueError: If the event namespace is not provided in the config.\n                        Also, if there are multiple client subclasses of rx.BaseState(Subclasses of rx.BaseState should consist\n                        of the DefaultState and the client app state).\n        \"\"\"\n        if \"connect_error_component\" in kwargs:\n            raise ValueError(\n                \"`connect_error_component` is deprecated, use `overlay_component` instead\"\n            )\n        super().__init__(**kwargs)\n        base_state_subclasses = BaseState.__subclasses__()\n\n        # Special case to allow test cases have multiple subclasses of rx.BaseState.\n        if not is_testing_env() and len(base_state_subclasses) > 1:\n            # Only one Base State class is allowed.\n            raise ValueError(\n                \"rx.BaseState cannot be subclassed multiple times. use rx.State instead\"\n            )\n\n        if \"breakpoints\" in self.style:\n            set_breakpoints(self.style.pop(\"breakpoints\"))\n\n        # Set up the API.\n        self.api = FastAPI(lifespan=self._run_lifespan_tasks)\n        self._add_cors()\n        self._add_default_endpoints()\n\n        for clz in App.__mro__:\n            if clz == App:\n                continue\n            if issubclass(clz, AppMixin):\n                clz._init_mixin(self)\n\n        self._setup_state()\n\n        # Set up the admin dash.\n        self._setup_admin_dash()\n\n        if sys.platform == \"win32\" and not is_prod_mode():\n            # Hack to fix Windows hot reload issue.\n            from reflex.utils.compat import windows_hot_reload_lifespan_hack\n\n            self.register_lifespan_task(windows_hot_reload_lifespan_hack)\n\n\n\n\n\n\n\n    def _setup_state(self) -> None:\n        \"\"\"Set up the state for the app.\n\n        Raises:\n            RuntimeError: If the socket server is invalid.\n        \"\"\"\n        if not self.state:\n            return\n\n        config = get_config()\n\n        # Set up the state manager.\n        self._state_manager = StateManager.create(state=self.state)\n\n        # Set up the Socket.IO AsyncServer.\n        if not self.sio:\n            self.sio = AsyncServer(\n                async_mode=\"asgi\",\n                cors_allowed_origins=(\n                    \"*\"\n                    if config.cors_allowed_origins == [\"*\"]\n                    else config.cors_allowed_origins\n                ),\n                cors_credentials=True,\n                max_http_buffer_size=constants.POLLING_MAX_HTTP_BUFFER_SIZE,\n                ping_interval=constants.Ping.INTERVAL,\n                ping_timeout=constants.Ping.TIMEOUT,\n            )\n        elif getattr(self.sio, \"async_mode\", \"\") != \"asgi\":\n            raise RuntimeError(\n                f\"Custom `sio` must use `async_mode='asgi'`, not '{self.sio.async_mode}'.\"\n            )\n\n        # Create the socket app. Note event endpoint constant replaces the default 'socket.io' path.\n        socket_app = ASGIApp(self.sio, socketio_path=\"\")\n        namespace = config.get_event_namespace()\n\n        # Create the event namespace and attach the main app. Not related to any paths.\n        self.event_namespace = EventNamespace(namespace, self)\n\n        # Register the event namespace with the socket.\n        self.sio.register_namespace(self.event_namespace)\n        # Mount the socket app with the API.\n        self.api.mount(str(constants.Endpoint.EVENT), socket_app)\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the app.\n\n        Returns:\n            The string representation of the app.\n        \"\"\"\n        return f\"<App state={self.state.__name__ if self.state else None}>\"\n\n    def __call__(self) -> FastAPI:\n        \"\"\"Run the backend api instance.\n\n        Returns:\n            The backend api.\n        \"\"\"\n        return self.api\n\n    def _add_default_endpoints(self):\n        \"\"\"Add default api endpoints (ping).\"\"\"\n        # To test the server.\n        self.api.get(str(constants.Endpoint.PING))(ping)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _add_cors(self):\n        \"\"\"Add CORS middleware to the app.\"\"\"\n        self.api.add_middleware(\n            cors.CORSMiddleware,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n            allow_origins=[\"*\"],\n        )\n\n    @property\n    def state_manager(self) -> StateManager:\n        \"\"\"Get the state manager.\n\n        Returns:\n            The initialized state manager.\n\n        Raises:\n            ValueError: if the state has not been initialized.\n        \"\"\"\n        if self._state_manager is None:\n            raise ValueError(\"The state manager has not been initialized.\")\n        return self._state_manager\n\n    @staticmethod\n    def _generate_component(component: Component | ComponentCallable) -> Component:\n        \"\"\"Generate a component from a callable.\n\n        Args:\n            component: The component function to call or Component to return as-is.\n\n        Returns:\n            The generated component.\n\n        Raises:\n            VarOperationTypeError: When an invalid component var related function is passed.\n            TypeError: When an invalid component function is passed.\n            exceptions.MatchTypeError: If the return types of match cases in rx.match are different.\n        \"\"\"\n        from reflex.utils.exceptions import VarOperationTypeError\n\n        try:\n            return component if isinstance(component, Component) else component()\n        except exceptions.MatchTypeError:\n            raise\n        except TypeError as e:\n            message = str(e)\n            if \"BaseVar\" in message or \"ComputedVar\" in message:\n                raise VarOperationTypeError(\n                    \"You may be trying to use an invalid Python function on a state var. \"\n                    \"When referencing a var inside your render code, only limited var operations are supported. \"\n                    \"See the var operation docs here: https://reflex.dev/docs/vars/var-operations/\"\n                ) from e\n            raise e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_load_events(self, route: str) -> list[EventHandler | EventSpec]:\n        \"\"\"Get the load events for a route.\n\n        Args:\n            route: The route to get the load events for.\n\n        Returns:\n            The load events for the route.\n        \"\"\"\n        route = route.lstrip(\"/\")\n        if route == \"\":\n            route = constants.PageNames.INDEX_ROUTE\n        return self.load_events.get(route, [])\n\n    def _check_routes_conflict(self, new_route: str):\n        \"\"\"Verify if there is any conflict between the new route and any existing route.\n\n        Based on conflicts that NextJS would throw if not intercepted.\n\n        Raises:\n            RouteValueError: exception showing which conflict exist with the route to be added\n\n        Args:\n            new_route: the route being newly added.\n        \"\"\"\n        from reflex.utils.exceptions import RouteValueError\n\n        if \"[\" not in new_route:\n            return\n\n        segments = (\n            constants.RouteRegex.SINGLE_SEGMENT,\n            constants.RouteRegex.DOUBLE_SEGMENT,\n            constants.RouteRegex.SINGLE_CATCHALL_SEGMENT,\n            constants.RouteRegex.DOUBLE_CATCHALL_SEGMENT,\n        )\n        for route in self.pages:\n            replaced_route = replace_brackets_with_keywords(route)\n            for rw, r, nr in zip(\n                replaced_route.split(\"/\"), route.split(\"/\"), new_route.split(\"/\")\n            ):\n                if rw in segments and r != nr:\n                    # If the slugs in the segments of both routes are not the same, then the route is invalid\n                    raise RouteValueError(\n                        f\"You cannot use different slug names for the same dynamic path in  {route} and {new_route} ('{r}' != '{nr}')\"\n                    )\n                elif rw not in segments and r != nr:\n                    # if the section being compared in both routes is not a dynamic segment(i.e not wrapped in brackets)\n                    # then we are guaranteed that the route is valid and there's no need checking the rest.\n                    # eg. /posts/[id]/info/[slug1] and /posts/[id]/info1/[slug1] is always going to be valid since\n                    # info1 will break away into its own tree.\n                    break\n\n    def add_custom_404_page(\n        self,\n        component: Component | ComponentCallable | None = None,\n        title: str = constants.Page404.TITLE,\n        image: str = constants.Page404.IMAGE,\n        description: str = constants.Page404.DESCRIPTION,\n        on_load: (\n            EventHandler | EventSpec | list[EventHandler | EventSpec] | None\n        ) = None,\n        meta: list[dict[str, str]] = constants.DefaultPage.META_LIST,\n    ):\n        \"\"\"Define a custom 404 page for any url having no match.\n\n        If there is no page defined on 'index' route, add the 404 page to it.\n        If there is no global catchall defined, add the 404 page with a catchall.\n\n        Args:\n            component: The component to display at the page.\n            title: The title of the page.\n            description: The description of the page.\n            image: The image to display on the page.\n            on_load: The event handler(s) that will be called each time the page load.\n            meta: The metadata of the page.\n        \"\"\"\n        if component is None:\n            component = Default404Page.create()\n        self.add_page(\n            component=wait_for_client_redirect(self._generate_component(component)),\n            route=constants.Page404.SLUG,\n            title=title or constants.Page404.TITLE,\n            image=image or constants.Page404.IMAGE,\n            description=description or constants.Page404.DESCRIPTION,\n            on_load=on_load,\n            meta=meta,\n        )\n\n    def _setup_admin_dash(self):\n        \"\"\"Setup the admin dash.\"\"\"\n        # Get the admin dash.\n        admin_dash = self.admin_dash\n\n        if admin_dash and admin_dash.models:\n            # Build the admin dashboard\n            admin = (\n                admin_dash.admin\n                if admin_dash.admin\n                else Admin(\n                    engine=Model.get_db_engine(),\n                    title=\"Reflex Admin Dashboard\",\n                    logo_url=\"https://reflex.dev/Reflex.svg\",\n                )\n            )\n\n            for model in admin_dash.models:\n                view = admin_dash.view_overrides.get(model, ModelView)\n                admin.add_view(view(model))\n\n            admin.mount_to(self.api)\n\n    def _get_frontend_packages(self, imports: Dict[str, set[ImportVar]]):\n        \"\"\"Gets the frontend packages to be installed and filters out the unnecessary ones.\n\n        Args:\n            imports: A dictionary containing the imports used in the current page.\n\n        Example:\n            >>> _get_frontend_packages({\"react\": \"16.14.0\", \"react-dom\": \"16.14.0\"})\n        \"\"\"\n        page_imports = {\n            i\n            for i, tags in imports.items()\n            if i not in constants.PackageJson.DEPENDENCIES\n            and i not in constants.PackageJson.DEV_DEPENDENCIES\n            and not any(i.startswith(prefix) for prefix in [\"/\", \".\", \"next/\"])\n            and i != \"\"\n            and any(tag.install for tag in tags)\n        }\n        frontend_packages = get_config().frontend_packages\n        _frontend_packages = []\n        for package in frontend_packages:\n            if package in (get_config().tailwind or {}).get(\"plugins\", []):  # type: ignore\n                console.warn(\n                    f\"Tailwind packages are inferred from 'plugins', remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            if package in page_imports:\n                console.warn(\n                    f\"React packages and their dependencies are inferred from Component.library and Component.lib_dependencies, remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            _frontend_packages.append(package)\n        page_imports.update(_frontend_packages)\n        prerequisites.install_frontend_packages(page_imports, get_config())\n\n    def _app_root(self, app_wrappers: dict[tuple[int, str], Component]) -> Component:\n        for component in tuple(app_wrappers.values()):\n            app_wrappers.update(component._get_all_app_wrap_components())\n        order = sorted(app_wrappers, key=lambda k: k[0], reverse=True)\n        root = parent = copy.deepcopy(app_wrappers[order[0]])\n        for key in order[1:]:\n            child = copy.deepcopy(app_wrappers[key])\n            parent.children.append(child)\n            parent = child\n        return root\n\n    def _should_compile(self) -> bool:\n        \"\"\"Check if the app should be compiled.\n\n        Returns:\n            Whether the app should be compiled.\n        \"\"\"\n        # Check the environment variable.\n        if should_skip_compile():\n            return False\n\n        nocompile = prerequisites.get_web_dir() / constants.NOCOMPILE_FILE\n\n        # Check the nocompile file.\n        if nocompile.exists():\n            # Delete the nocompile file\n            nocompile.unlink()\n            return False\n\n        # By default, compile the app.\n        return True\n\n    def _add_overlay_to_component(self, component: Component) -> Component:\n        if self.overlay_component is None:\n            return component\n\n        children = component.children\n        overlay_component = self._generate_component(self.overlay_component)\n\n        if children[0] == overlay_component:\n            return component\n\n        # recreate OverlayFragment with overlay_component as first child\n        component = OverlayFragment.create(overlay_component, *children)\n\n        return component\n\n\n\n\n\n\n\n\n    def _apply_decorated_pages(self):\n        \"\"\"Add @rx.page decorated pages to the app.\n\n        This has to be done in the MainThread for py38 and py39 compatibility, so the\n        decorated pages are added to the app before the app is compiled (in a thread)\n        to workaround REF-2172.\n\n        This can move back into `compile_` when py39 support is dropped.\n        \"\"\"\n        # Add the @rx.page decorated pages to collect on_load events.\n        for render, kwargs in DECORATED_PAGES[get_config().app_name]:\n            self.add_page(render, **kwargs)\n\n    def _validate_var_dependencies(\n        self, state: Optional[Type[BaseState]] = None\n    ) -> None:\n        \"\"\"Validate the dependencies of the vars in the app.\n\n        Args:\n            state: The state to validate the dependencies for.\n\n        Raises:\n            VarDependencyError: When a computed var has an invalid dependency.\n        \"\"\"\n        if not self.state:\n            return\n\n        if not state:\n            state = self.state\n\n        for var in state.computed_vars.values():\n            if not var._cache:\n                continue\n            deps = var._deps(objclass=state)\n            for dep in deps:\n                if dep not in state.vars and dep not in state.backend_vars:\n                    raise exceptions.VarDependencyError(\n                        f\"ComputedVar {var._var_name} on state {state.__name__} has an invalid dependency {dep}\"\n                    )\n\n        for substate in state.class_subclasses:\n            self._validate_var_dependencies(substate)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @contextlib.asynccontextmanager\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _process_background(\n        self, state: BaseState, event: Event\n    ) -> asyncio.Task | None:\n        \"\"\"Process an event in the background and emit updates as they arrive.\n\n        Args:\n            state: The state to process the event for.\n            event: The event to process.\n\n        Returns:\n            Task if the event was backgroundable, otherwise None\n        \"\"\"\n        substate, handler = state._get_event_handler(event)\n        if not handler.is_background:\n            return None\n\n        async def _coro():\n            \"\"\"Coroutine to process the event and emit updates inside an asyncio.Task.\n\n            Raises:\n                RuntimeError: If the app has not been initialized yet.\n            \"\"\"\n            if self.event_namespace is None:\n                raise RuntimeError(\"App has not been initialized yet.\")\n\n            # Process the event.\n            async for update in state._process_event(\n                handler=handler, state=substate, payload=event.payload\n            ):\n                # Postprocess the event.\n                update = await self._postprocess(state, event, update)\n\n                # Send the update to the client.\n                await self.event_namespace.emit_update(\n                    update=update,\n                    sid=state.router.session.session_id,\n                )\n\n        task = asyncio.create_task(_coro())\n        self.background_tasks.add(task)\n        # Clean up task from background_tasks set when complete.\n        task.add_done_callback(self.background_tasks.discard)\n        return task\n\n\nasync def process(\n    app: App, event: Event, sid: str, headers: Dict, client_ip: str\n) -> AsyncIterator[StateUpdate]:\n    \"\"\"Process an event.\n\n    Args:\n        app: The app to process the event for.\n        event: The event to process.\n        sid: The Socket.IO session id.\n        headers: The client headers.\n        client_ip: The client_ip.\n\n    Raises:\n        Exception: If a reflex specific error occurs during processing the event.\n\n    Yields:\n        The state updates after processing the event.\n    \"\"\"\n    from reflex.utils import telemetry\n\n    try:\n        # Add request data to the state.\n        router_data = event.router_data\n        router_data.update(\n            {\n                constants.RouteVar.QUERY: format.format_query_params(event.router_data),\n                constants.RouteVar.CLIENT_TOKEN: event.token,\n                constants.RouteVar.SESSION_ID: sid,\n                constants.RouteVar.HEADERS: headers,\n                constants.RouteVar.CLIENT_IP: client_ip,\n            }\n        )\n        # Get the state for the session exclusively.\n        async with app.state_manager.modify_state(event.substate_token) as state:\n            # re-assign only when the value is different\n            if state.router_data != router_data:\n                # assignment will recurse into substates and force recalculation of\n                # dependent ComputedVar (dynamic route variables)\n                state.router_data = router_data\n                state.router = RouterData(router_data)\n\n            # Preprocess the event.\n            update = await app._preprocess(state, event)\n\n            # If there was an update, yield it.\n            if update is not None:\n                yield update\n\n            # Only process the event if there is no update.\n            else:\n                if app._process_background(state, event) is not None:\n                    # `final=True` allows the frontend send more events immediately.\n                    yield StateUpdate(final=True)\n                    return\n\n                # Process the event synchronously.\n                async for update in state._process(event):\n                    # Postprocess the event.\n                    update = await app._postprocess(state, event, update)\n\n                    # Yield the\n```\n# Function Signature: def _enable_state(self) -> None:\n# Natural Language Annotations: \"\"\"Enable state for the app.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _enable_state(self) -> None:\n   ", "  ```\ndef _enable_state(self) -> None:\n   ", "  ```python \ndef _enable_state(self) -> None:"]}, "reflex-reflex/app.py-_add_optional_endpoints": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/app.py:\n```\n\"\"\"The main Reflex app.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport concurrent.futures\nimport contextlib\nimport copy\nimport functools\nimport io\nimport multiprocessing\nimport os\nimport platform\nimport sys\nfrom datetime import datetime\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n    get_args,\n    get_type_hints,\n)\n\nfrom fastapi import FastAPI, HTTPException, Request, UploadFile\nfrom fastapi.middleware import cors\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom rich.progress import MofNCompleteColumn, Progress, TimeElapsedColumn\nfrom socketio import ASGIApp, AsyncNamespace, AsyncServer\nfrom starlette_admin.contrib.sqla.admin import Admin\nfrom starlette_admin.contrib.sqla.view import ModelView\n\nfrom reflex import constants\nfrom reflex.admin import AdminDash\nfrom reflex.app_mixins import AppMixin, LifespanMixin, MiddlewareMixin\nfrom reflex.base import Base\nfrom reflex.compiler import compiler\nfrom reflex.compiler import utils as compiler_utils\nfrom reflex.compiler.compiler import ExecutorSafeFunctions\nfrom reflex.components.base.app_wrap import AppWrap\nfrom reflex.components.base.fragment import Fragment\nfrom reflex.components.component import (\n    Component,\n    ComponentStyle,\n    evaluate_style_namespaces,\n)\nfrom reflex.components.core.banner import connection_pulser, connection_toaster\nfrom reflex.components.core.breakpoints import set_breakpoints\nfrom reflex.components.core.client_side_routing import (\n    Default404Page,\n    wait_for_client_redirect,\n)\nfrom reflex.components.core.upload import Upload, get_upload_dir\nfrom reflex.components.radix import themes\nfrom reflex.config import get_config\nfrom reflex.event import Event, EventHandler, EventSpec\nfrom reflex.model import Model\nfrom reflex.page import (\n    DECORATED_PAGES,\n)\nfrom reflex.route import (\n    get_route_args,\n    replace_brackets_with_keywords,\n    verify_route_validity,\n)\nfrom reflex.state import (\n    BaseState,\n    RouterData,\n    State,\n    StateManager,\n    StateUpdate,\n    _substate_key,\n    code_uses_state_contexts,\n)\nfrom reflex.utils import codespaces, console, exceptions, format, prerequisites, types\nfrom reflex.utils.exec import is_prod_mode, is_testing_env, should_skip_compile\nfrom reflex.utils.imports import ImportVar\n\n# Define custom types.\nComponentCallable = Callable[[], Component]\nReducer = Callable[[Event], Coroutine[Any, Any, StateUpdate]]\n\n\ndef default_overlay_component() -> Component:\n    \"\"\"Default overlay_component attribute for App.\n\n    Returns:\n        The default overlay_component, which is a connection_modal.\n    \"\"\"\n    return Fragment.create(\n        connection_pulser(),\n        connection_toaster(),\n        *codespaces.codespaces_auto_redirect(),\n    )\n\n\nclass OverlayFragment(Fragment):\n    \"\"\"Alias for Fragment, used to wrap the overlay_component.\"\"\"\n\n    pass\n\n\nclass App(MiddlewareMixin, LifespanMixin, Base):\n    \"\"\"The main Reflex app that encapsulates the backend and frontend.\n\n    Every Reflex app needs an app defined in its main module.\n\n    ```python\n    # app.py\n    import reflex as rx\n\n    # Define state and pages\n    ...\n\n    app = rx.App(\n        # Set global level style.\n        style={...},\n        # Set the top level theme.\n        theme=rx.theme(accent_color=\"blue\"),\n    )\n    ```\n    \"\"\"\n\n    # The global [theme](https://reflex.dev/docs/styling/theming/#theme) for the entire app.\n    theme: Optional[Component] = themes.theme(accent_color=\"blue\")\n\n    # The [global style](https://reflex.dev/docs/styling/overview/#global-styles}) for the app.\n    style: ComponentStyle = {}\n\n    # A list of URLs to [stylesheets](https://reflex.dev/docs/styling/custom-stylesheets/) to include in the app.\n    stylesheets: List[str] = []\n\n    # A component that is present on every page (defaults to the Connection Error banner).\n    overlay_component: Optional[Union[Component, ComponentCallable]] = (\n        default_overlay_component\n    )\n\n    # Components to add to the head of every page.\n    head_components: List[Component] = []\n\n    # The Socket.IO AsyncServer instance.\n    sio: Optional[AsyncServer] = None\n\n    # The language to add to the html root tag of every page.\n    html_lang: Optional[str] = None\n\n    # Attributes to add to the html root tag of every page.\n    html_custom_attrs: Optional[Dict[str, str]] = None\n\n    # A map from a page route to the component to render. Users should use `add_page`. PRIVATE.\n    pages: Dict[str, Component] = {}\n\n    # The backend API object. PRIVATE.\n    api: FastAPI = None  # type: ignore\n\n    # The state class to use for the app. PRIVATE.\n    state: Optional[Type[BaseState]] = None\n\n    # Class to manage many client states.\n    _state_manager: Optional[StateManager] = None\n\n    # Mapping from a route to event handlers to trigger when the page loads. PRIVATE.\n    load_events: Dict[str, List[Union[EventHandler, EventSpec]]] = {}\n\n    # Admin dashboard to view and manage the database. PRIVATE.\n    admin_dash: Optional[AdminDash] = None\n\n    # The async server name space. PRIVATE.\n    event_namespace: Optional[EventNamespace] = None\n\n    # Background tasks that are currently running. PRIVATE.\n    background_tasks: Set[asyncio.Task] = set()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the app.\n\n        Args:\n            **kwargs: Kwargs to initialize the app with.\n\n        Raises:\n            ValueError: If the event namespace is not provided in the config.\n                        Also, if there are multiple client subclasses of rx.BaseState(Subclasses of rx.BaseState should consist\n                        of the DefaultState and the client app state).\n        \"\"\"\n        if \"connect_error_component\" in kwargs:\n            raise ValueError(\n                \"`connect_error_component` is deprecated, use `overlay_component` instead\"\n            )\n        super().__init__(**kwargs)\n        base_state_subclasses = BaseState.__subclasses__()\n\n        # Special case to allow test cases have multiple subclasses of rx.BaseState.\n        if not is_testing_env() and len(base_state_subclasses) > 1:\n            # Only one Base State class is allowed.\n            raise ValueError(\n                \"rx.BaseState cannot be subclassed multiple times. use rx.State instead\"\n            )\n\n        if \"breakpoints\" in self.style:\n            set_breakpoints(self.style.pop(\"breakpoints\"))\n\n        # Set up the API.\n        self.api = FastAPI(lifespan=self._run_lifespan_tasks)\n        self._add_cors()\n        self._add_default_endpoints()\n\n        for clz in App.__mro__:\n            if clz == App:\n                continue\n            if issubclass(clz, AppMixin):\n                clz._init_mixin(self)\n\n        self._setup_state()\n\n        # Set up the admin dash.\n        self._setup_admin_dash()\n\n        if sys.platform == \"win32\" and not is_prod_mode():\n            # Hack to fix Windows hot reload issue.\n            from reflex.utils.compat import windows_hot_reload_lifespan_hack\n\n            self.register_lifespan_task(windows_hot_reload_lifespan_hack)\n\n\n\n\n\n\n\n    def _setup_state(self) -> None:\n        \"\"\"Set up the state for the app.\n\n        Raises:\n            RuntimeError: If the socket server is invalid.\n        \"\"\"\n        if not self.state:\n            return\n\n        config = get_config()\n\n        # Set up the state manager.\n        self._state_manager = StateManager.create(state=self.state)\n\n        # Set up the Socket.IO AsyncServer.\n        if not self.sio:\n            self.sio = AsyncServer(\n                async_mode=\"asgi\",\n                cors_allowed_origins=(\n                    \"*\"\n                    if config.cors_allowed_origins == [\"*\"]\n                    else config.cors_allowed_origins\n                ),\n                cors_credentials=True,\n                max_http_buffer_size=constants.POLLING_MAX_HTTP_BUFFER_SIZE,\n                ping_interval=constants.Ping.INTERVAL,\n                ping_timeout=constants.Ping.TIMEOUT,\n            )\n        elif getattr(self.sio, \"async_mode\", \"\") != \"asgi\":\n            raise RuntimeError(\n                f\"Custom `sio` must use `async_mode='asgi'`, not '{self.sio.async_mode}'.\"\n            )\n\n        # Create the socket app. Note event endpoint constant replaces the default 'socket.io' path.\n        socket_app = ASGIApp(self.sio, socketio_path=\"\")\n        namespace = config.get_event_namespace()\n\n        # Create the event namespace and attach the main app. Not related to any paths.\n        self.event_namespace = EventNamespace(namespace, self)\n\n        # Register the event namespace with the socket.\n        self.sio.register_namespace(self.event_namespace)\n        # Mount the socket app with the API.\n        self.api.mount(str(constants.Endpoint.EVENT), socket_app)\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the app.\n\n        Returns:\n            The string representation of the app.\n        \"\"\"\n        return f\"<App state={self.state.__name__ if self.state else None}>\"\n\n    def __call__(self) -> FastAPI:\n        \"\"\"Run the backend api instance.\n\n        Returns:\n            The backend api.\n        \"\"\"\n        return self.api\n\n    def _add_default_endpoints(self):\n        \"\"\"Add default api endpoints (ping).\"\"\"\n        # To test the server.\n        self.api.get(str(constants.Endpoint.PING))(ping)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _add_cors(self):\n        \"\"\"Add CORS middleware to the app.\"\"\"\n        self.api.add_middleware(\n            cors.CORSMiddleware,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n            allow_origins=[\"*\"],\n        )\n\n    @property\n    def state_manager(self) -> StateManager:\n        \"\"\"Get the state manager.\n\n        Returns:\n            The initialized state manager.\n\n        Raises:\n            ValueError: if the state has not been initialized.\n        \"\"\"\n        if self._state_manager is None:\n            raise ValueError(\"The state manager has not been initialized.\")\n        return self._state_manager\n\n    @staticmethod\n    def _generate_component(component: Component | ComponentCallable) -> Component:\n        \"\"\"Generate a component from a callable.\n\n        Args:\n            component: The component function to call or Component to return as-is.\n\n        Returns:\n            The generated component.\n\n        Raises:\n            VarOperationTypeError: When an invalid component var related function is passed.\n            TypeError: When an invalid component function is passed.\n            exceptions.MatchTypeError: If the return types of match cases in rx.match are different.\n        \"\"\"\n        from reflex.utils.exceptions import VarOperationTypeError\n\n        try:\n            return component if isinstance(component, Component) else component()\n        except exceptions.MatchTypeError:\n            raise\n        except TypeError as e:\n            message = str(e)\n            if \"BaseVar\" in message or \"ComputedVar\" in message:\n                raise VarOperationTypeError(\n                    \"You may be trying to use an invalid Python function on a state var. \"\n                    \"When referencing a var inside your render code, only limited var operations are supported. \"\n                    \"See the var operation docs here: https://reflex.dev/docs/vars/var-operations/\"\n                ) from e\n            raise e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_load_events(self, route: str) -> list[EventHandler | EventSpec]:\n        \"\"\"Get the load events for a route.\n\n        Args:\n            route: The route to get the load events for.\n\n        Returns:\n            The load events for the route.\n        \"\"\"\n        route = route.lstrip(\"/\")\n        if route == \"\":\n            route = constants.PageNames.INDEX_ROUTE\n        return self.load_events.get(route, [])\n\n    def _check_routes_conflict(self, new_route: str):\n        \"\"\"Verify if there is any conflict between the new route and any existing route.\n\n        Based on conflicts that NextJS would throw if not intercepted.\n\n        Raises:\n            RouteValueError: exception showing which conflict exist with the route to be added\n\n        Args:\n            new_route: the route being newly added.\n        \"\"\"\n        from reflex.utils.exceptions import RouteValueError\n\n        if \"[\" not in new_route:\n            return\n\n        segments = (\n            constants.RouteRegex.SINGLE_SEGMENT,\n            constants.RouteRegex.DOUBLE_SEGMENT,\n            constants.RouteRegex.SINGLE_CATCHALL_SEGMENT,\n            constants.RouteRegex.DOUBLE_CATCHALL_SEGMENT,\n        )\n        for route in self.pages:\n            replaced_route = replace_brackets_with_keywords(route)\n            for rw, r, nr in zip(\n                replaced_route.split(\"/\"), route.split(\"/\"), new_route.split(\"/\")\n            ):\n                if rw in segments and r != nr:\n                    # If the slugs in the segments of both routes are not the same, then the route is invalid\n                    raise RouteValueError(\n                        f\"You cannot use different slug names for the same dynamic path in  {route} and {new_route} ('{r}' != '{nr}')\"\n                    )\n                elif rw not in segments and r != nr:\n                    # if the section being compared in both routes is not a dynamic segment(i.e not wrapped in brackets)\n                    # then we are guaranteed that the route is valid and there's no need checking the rest.\n                    # eg. /posts/[id]/info/[slug1] and /posts/[id]/info1/[slug1] is always going to be valid since\n                    # info1 will break away into its own tree.\n                    break\n\n    def add_custom_404_page(\n        self,\n        component: Component | ComponentCallable | None = None,\n        title: str = constants.Page404.TITLE,\n        image: str = constants.Page404.IMAGE,\n        description: str = constants.Page404.DESCRIPTION,\n        on_load: (\n            EventHandler | EventSpec | list[EventHandler | EventSpec] | None\n        ) = None,\n        meta: list[dict[str, str]] = constants.DefaultPage.META_LIST,\n    ):\n        \"\"\"Define a custom 404 page for any url having no match.\n\n        If there is no page defined on 'index' route, add the 404 page to it.\n        If there is no global catchall defined, add the 404 page with a catchall.\n\n        Args:\n            component: The component to display at the page.\n            title: The title of the page.\n            description: The description of the page.\n            image: The image to display on the page.\n            on_load: The event handler(s) that will be called each time the page load.\n            meta: The metadata of the page.\n        \"\"\"\n        if component is None:\n            component = Default404Page.create()\n        self.add_page(\n            component=wait_for_client_redirect(self._generate_component(component)),\n            route=constants.Page404.SLUG,\n            title=title or constants.Page404.TITLE,\n            image=image or constants.Page404.IMAGE,\n            description=description or constants.Page404.DESCRIPTION,\n            on_load=on_load,\n            meta=meta,\n        )\n\n    def _setup_admin_dash(self):\n        \"\"\"Setup the admin dash.\"\"\"\n        # Get the admin dash.\n        admin_dash = self.admin_dash\n\n        if admin_dash and admin_dash.models:\n            # Build the admin dashboard\n            admin = (\n                admin_dash.admin\n                if admin_dash.admin\n                else Admin(\n                    engine=Model.get_db_engine(),\n                    title=\"Reflex Admin Dashboard\",\n                    logo_url=\"https://reflex.dev/Reflex.svg\",\n                )\n            )\n\n            for model in admin_dash.models:\n                view = admin_dash.view_overrides.get(model, ModelView)\n                admin.add_view(view(model))\n\n            admin.mount_to(self.api)\n\n    def _get_frontend_packages(self, imports: Dict[str, set[ImportVar]]):\n        \"\"\"Gets the frontend packages to be installed and filters out the unnecessary ones.\n\n        Args:\n            imports: A dictionary containing the imports used in the current page.\n\n        Example:\n            >>> _get_frontend_packages({\"react\": \"16.14.0\", \"react-dom\": \"16.14.0\"})\n        \"\"\"\n        page_imports = {\n            i\n            for i, tags in imports.items()\n            if i not in constants.PackageJson.DEPENDENCIES\n            and i not in constants.PackageJson.DEV_DEPENDENCIES\n            and not any(i.startswith(prefix) for prefix in [\"/\", \".\", \"next/\"])\n            and i != \"\"\n            and any(tag.install for tag in tags)\n        }\n        frontend_packages = get_config().frontend_packages\n        _frontend_packages = []\n        for package in frontend_packages:\n            if package in (get_config().tailwind or {}).get(\"plugins\", []):  # type: ignore\n                console.warn(\n                    f\"Tailwind packages are inferred from 'plugins', remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            if package in page_imports:\n                console.warn(\n                    f\"React packages and their dependencies are inferred from Component.library and Component.lib_dependencies, remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            _frontend_packages.append(package)\n        page_imports.update(_frontend_packages)\n        prerequisites.install_frontend_packages(page_imports, get_config())\n\n    def _app_root(self, app_wrappers: dict[tuple[int, str], Component]) -> Component:\n        for component in tuple(app_wrappers.values()):\n            app_wrappers.update(component._get_all_app_wrap_components())\n        order = sorted(app_wrappers, key=lambda k: k[0], reverse=True)\n        root = parent = copy.deepcopy(app_wrappers[order[0]])\n        for key in order[1:]:\n            child = copy.deepcopy(app_wrappers[key])\n            parent.children.append(child)\n            parent = child\n        return root\n\n    def _should_compile(self) -> bool:\n        \"\"\"Check if the app should be compiled.\n\n        Returns:\n            Whether the app should be compiled.\n        \"\"\"\n        # Check the environment variable.\n        if should_skip_compile():\n            return False\n\n        nocompile = prerequisites.get_web_dir() / constants.NOCOMPILE_FILE\n\n        # Check the nocompile file.\n        if nocompile.exists():\n            # Delete the nocompile file\n            nocompile.unlink()\n            return False\n\n        # By default, compile the app.\n        return True\n\n    def _add_overlay_to_component(self, component: Component) -> Component:\n        if self.overlay_component is None:\n            return component\n\n        children = component.children\n        overlay_component = self._generate_component(self.overlay_component)\n\n        if children[0] == overlay_component:\n            return component\n\n        # recreate OverlayFragment with overlay_component as first child\n        component = OverlayFragment.create(overlay_component, *children)\n\n        return component\n\n\n\n\n\n\n\n\n    def _apply_decorated_pages(self):\n        \"\"\"Add @rx.page decorated pages to the app.\n\n        This has to be done in the MainThread for py38 and py39 compatibility, so the\n        decorated pages are added to the app before the app is compiled (in a thread)\n        to workaround REF-2172.\n\n        This can move back into `compile_` when py39 support is dropped.\n        \"\"\"\n        # Add the @rx.page decorated pages to collect on_load events.\n        for render, kwargs in DECORATED_PAGES[get_config().app_name]:\n            self.add_page(render, **kwargs)\n\n    def _validate_var_dependencies(\n        self, state: Optional[Type[BaseState]] = None\n    ) -> None:\n        \"\"\"Validate the dependencies of the vars in the app.\n\n        Args:\n            state: The state to validate the dependencies for.\n\n        Raises:\n            VarDependencyError: When a computed var has an invalid dependency.\n        \"\"\"\n        if not self.state:\n            return\n\n        if not state:\n            state = self.state\n\n        for var in state.computed_vars.values():\n            if not var._cache:\n                continue\n            deps = var._deps(objclass=state)\n            for dep in deps:\n                if dep not in state.vars and dep not in state.backend_vars:\n                    raise exceptions.VarDependencyError(\n                        f\"ComputedVar {var._var_name} on state {state.__name__} has an invalid dependency {dep}\"\n                    )\n\n        for substate in state.class_subclasses:\n            self._validate_var_dependencies(substate)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @contextlib.asynccontextmanager\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _process_background(\n        self, state: BaseState, event: Event\n    ) -> asyncio.Task | None:\n        \"\"\"Process an event in the background and emit updates as they arrive.\n\n        Args:\n            state: The state to process the event for.\n            event: The event to process.\n\n        Returns:\n            Task if the event was backgroundable, otherwise None\n        \"\"\"\n        substate, handler = state._get_event_handler(event)\n        if not handler.is_background:\n            return None\n\n        async def _coro():\n            \"\"\"Coroutine to process the event and emit updates inside an asyncio.Task.\n\n            Raises:\n                RuntimeError: If the app has not been initialized yet.\n            \"\"\"\n            if self.event_namespace is None:\n                raise RuntimeError(\"App has not been initialized yet.\")\n\n            # Process the event.\n            async for update in state._process_event(\n                handler=handler, state=substate, payload=event.payload\n            ):\n                # Postprocess the event.\n                update = await self._postprocess(state, event, update)\n\n                # Send the update to the client.\n                await self.event_namespace.emit_update(\n                    update=update,\n                    sid=state.router.session.session_id,\n                )\n\n        task = asyncio.create_task(_coro())\n        self.background_tasks.add(task)\n        # Clean up task from background_tasks set when complete.\n        task.add_done_callback(self.background_tasks.discard)\n        return task\n\n\nasync def process(\n    app: App, event: Event, sid: str, headers: Dict, client_ip: str\n) -> AsyncIterator[StateUpdate]:\n    \"\"\"Process an event.\n\n    Args:\n        app: The app to process the event for.\n        event: The event to process.\n        sid: The Socket.IO session id.\n        headers: The client headers.\n        client_ip: The client_ip.\n\n    Raises:\n        Exception: If a reflex specific error occurs during processing the event.\n\n    Yields:\n        The state updates after processing the event.\n    \"\"\"\n    from reflex.utils import telemetry\n\n    try:\n        # Add request data to the state.\n        router_data = event.router_data\n        router_data.update(\n            {\n                constants.RouteVar.QUERY: format.format_query_params(event.router_data),\n                constants.RouteVar.CLIENT_TOKEN: event.token,\n                constants.RouteVar.SESSION_ID: sid,\n                constants.RouteVar.HEADERS: headers,\n                constants.RouteVar.CLIENT_IP: client_ip,\n            }\n        )\n        # Get the state for the session exclusively.\n        async with app.state_manager.modify_state(event.substate_token) as state:\n            # re-assign only when the value is different\n            if state.router_data != router_data:\n                # assignment will recurse into substates and force recalculation of\n                # dependent ComputedVar (dynamic route variables)\n                state.router_data = router_data\n                state.router = RouterData(router_data)\n\n            # Preprocess the event.\n            update = await app._preprocess(state, event)\n\n            # If there was an update, yield it.\n            if update is not None:\n                yield update\n\n            # Only process the event if there is no update.\n            else:\n                if app._process_background(state, event) is not None:\n                    # `final=True` allows the frontend send more events immediately.\n                    yield StateUpdate(final=True)\n                    return\n\n                # Process the event synchronously.\n                async for update in state._process(event):\n                    # Postprocess the event.\n                    update = await app._postprocess(state, event, update)\n\n                    # Yield the\n```\n# Function Signature: def _add_optional_endpoints(self):\n# Natural Language Annotations: \"\"\"Add optional api endpoints (_upload).\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n# Reflex imports.\nfrom reflex.components.core", "  ```\nasync def _upload(self, file: UploadFile =", "  ```python\ndef _add_optional_endpoints(self):\n"]}, "reflex-reflex/app.py-add_page": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/app.py:\n```\n\"\"\"The main Reflex app.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport concurrent.futures\nimport contextlib\nimport copy\nimport functools\nimport io\nimport multiprocessing\nimport os\nimport platform\nimport sys\nfrom datetime import datetime\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n    get_args,\n    get_type_hints,\n)\n\nfrom fastapi import FastAPI, HTTPException, Request, UploadFile\nfrom fastapi.middleware import cors\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom rich.progress import MofNCompleteColumn, Progress, TimeElapsedColumn\nfrom socketio import ASGIApp, AsyncNamespace, AsyncServer\nfrom starlette_admin.contrib.sqla.admin import Admin\nfrom starlette_admin.contrib.sqla.view import ModelView\n\nfrom reflex import constants\nfrom reflex.admin import AdminDash\nfrom reflex.app_mixins import AppMixin, LifespanMixin, MiddlewareMixin\nfrom reflex.base import Base\nfrom reflex.compiler import compiler\nfrom reflex.compiler import utils as compiler_utils\nfrom reflex.compiler.compiler import ExecutorSafeFunctions\nfrom reflex.components.base.app_wrap import AppWrap\nfrom reflex.components.base.fragment import Fragment\nfrom reflex.components.component import (\n    Component,\n    ComponentStyle,\n    evaluate_style_namespaces,\n)\nfrom reflex.components.core.banner import connection_pulser, connection_toaster\nfrom reflex.components.core.breakpoints import set_breakpoints\nfrom reflex.components.core.client_side_routing import (\n    Default404Page,\n    wait_for_client_redirect,\n)\nfrom reflex.components.core.upload import Upload, get_upload_dir\nfrom reflex.components.radix import themes\nfrom reflex.config import get_config\nfrom reflex.event import Event, EventHandler, EventSpec\nfrom reflex.model import Model\nfrom reflex.page import (\n    DECORATED_PAGES,\n)\nfrom reflex.route import (\n    get_route_args,\n    replace_brackets_with_keywords,\n    verify_route_validity,\n)\nfrom reflex.state import (\n    BaseState,\n    RouterData,\n    State,\n    StateManager,\n    StateUpdate,\n    _substate_key,\n    code_uses_state_contexts,\n)\nfrom reflex.utils import codespaces, console, exceptions, format, prerequisites, types\nfrom reflex.utils.exec import is_prod_mode, is_testing_env, should_skip_compile\nfrom reflex.utils.imports import ImportVar\n\n# Define custom types.\nComponentCallable = Callable[[], Component]\nReducer = Callable[[Event], Coroutine[Any, Any, StateUpdate]]\n\n\ndef default_overlay_component() -> Component:\n    \"\"\"Default overlay_component attribute for App.\n\n    Returns:\n        The default overlay_component, which is a connection_modal.\n    \"\"\"\n    return Fragment.create(\n        connection_pulser(),\n        connection_toaster(),\n        *codespaces.codespaces_auto_redirect(),\n    )\n\n\nclass OverlayFragment(Fragment):\n    \"\"\"Alias for Fragment, used to wrap the overlay_component.\"\"\"\n\n    pass\n\n\nclass App(MiddlewareMixin, LifespanMixin, Base):\n    \"\"\"The main Reflex app that encapsulates the backend and frontend.\n\n    Every Reflex app needs an app defined in its main module.\n\n    ```python\n    # app.py\n    import reflex as rx\n\n    # Define state and pages\n    ...\n\n    app = rx.App(\n        # Set global level style.\n        style={...},\n        # Set the top level theme.\n        theme=rx.theme(accent_color=\"blue\"),\n    )\n    ```\n    \"\"\"\n\n    # The global [theme](https://reflex.dev/docs/styling/theming/#theme) for the entire app.\n    theme: Optional[Component] = themes.theme(accent_color=\"blue\")\n\n    # The [global style](https://reflex.dev/docs/styling/overview/#global-styles}) for the app.\n    style: ComponentStyle = {}\n\n    # A list of URLs to [stylesheets](https://reflex.dev/docs/styling/custom-stylesheets/) to include in the app.\n    stylesheets: List[str] = []\n\n    # A component that is present on every page (defaults to the Connection Error banner).\n    overlay_component: Optional[Union[Component, ComponentCallable]] = (\n        default_overlay_component\n    )\n\n    # Components to add to the head of every page.\n    head_components: List[Component] = []\n\n    # The Socket.IO AsyncServer instance.\n    sio: Optional[AsyncServer] = None\n\n    # The language to add to the html root tag of every page.\n    html_lang: Optional[str] = None\n\n    # Attributes to add to the html root tag of every page.\n    html_custom_attrs: Optional[Dict[str, str]] = None\n\n    # A map from a page route to the component to render. Users should use `add_page`. PRIVATE.\n    pages: Dict[str, Component] = {}\n\n    # The backend API object. PRIVATE.\n    api: FastAPI = None  # type: ignore\n\n    # The state class to use for the app. PRIVATE.\n    state: Optional[Type[BaseState]] = None\n\n    # Class to manage many client states.\n    _state_manager: Optional[StateManager] = None\n\n    # Mapping from a route to event handlers to trigger when the page loads. PRIVATE.\n    load_events: Dict[str, List[Union[EventHandler, EventSpec]]] = {}\n\n    # Admin dashboard to view and manage the database. PRIVATE.\n    admin_dash: Optional[AdminDash] = None\n\n    # The async server name space. PRIVATE.\n    event_namespace: Optional[EventNamespace] = None\n\n    # Background tasks that are currently running. PRIVATE.\n    background_tasks: Set[asyncio.Task] = set()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the app.\n\n        Args:\n            **kwargs: Kwargs to initialize the app with.\n\n        Raises:\n            ValueError: If the event namespace is not provided in the config.\n                        Also, if there are multiple client subclasses of rx.BaseState(Subclasses of rx.BaseState should consist\n                        of the DefaultState and the client app state).\n        \"\"\"\n        if \"connect_error_component\" in kwargs:\n            raise ValueError(\n                \"`connect_error_component` is deprecated, use `overlay_component` instead\"\n            )\n        super().__init__(**kwargs)\n        base_state_subclasses = BaseState.__subclasses__()\n\n        # Special case to allow test cases have multiple subclasses of rx.BaseState.\n        if not is_testing_env() and len(base_state_subclasses) > 1:\n            # Only one Base State class is allowed.\n            raise ValueError(\n                \"rx.BaseState cannot be subclassed multiple times. use rx.State instead\"\n            )\n\n        if \"breakpoints\" in self.style:\n            set_breakpoints(self.style.pop(\"breakpoints\"))\n\n        # Set up the API.\n        self.api = FastAPI(lifespan=self._run_lifespan_tasks)\n        self._add_cors()\n        self._add_default_endpoints()\n\n        for clz in App.__mro__:\n            if clz == App:\n                continue\n            if issubclass(clz, AppMixin):\n                clz._init_mixin(self)\n\n        self._setup_state()\n\n        # Set up the admin dash.\n        self._setup_admin_dash()\n\n        if sys.platform == \"win32\" and not is_prod_mode():\n            # Hack to fix Windows hot reload issue.\n            from reflex.utils.compat import windows_hot_reload_lifespan_hack\n\n            self.register_lifespan_task(windows_hot_reload_lifespan_hack)\n\n\n\n\n\n\n\n    def _setup_state(self) -> None:\n        \"\"\"Set up the state for the app.\n\n        Raises:\n            RuntimeError: If the socket server is invalid.\n        \"\"\"\n        if not self.state:\n            return\n\n        config = get_config()\n\n        # Set up the state manager.\n        self._state_manager = StateManager.create(state=self.state)\n\n        # Set up the Socket.IO AsyncServer.\n        if not self.sio:\n            self.sio = AsyncServer(\n                async_mode=\"asgi\",\n                cors_allowed_origins=(\n                    \"*\"\n                    if config.cors_allowed_origins == [\"*\"]\n                    else config.cors_allowed_origins\n                ),\n                cors_credentials=True,\n                max_http_buffer_size=constants.POLLING_MAX_HTTP_BUFFER_SIZE,\n                ping_interval=constants.Ping.INTERVAL,\n                ping_timeout=constants.Ping.TIMEOUT,\n            )\n        elif getattr(self.sio, \"async_mode\", \"\") != \"asgi\":\n            raise RuntimeError(\n                f\"Custom `sio` must use `async_mode='asgi'`, not '{self.sio.async_mode}'.\"\n            )\n\n        # Create the socket app. Note event endpoint constant replaces the default 'socket.io' path.\n        socket_app = ASGIApp(self.sio, socketio_path=\"\")\n        namespace = config.get_event_namespace()\n\n        # Create the event namespace and attach the main app. Not related to any paths.\n        self.event_namespace = EventNamespace(namespace, self)\n\n        # Register the event namespace with the socket.\n        self.sio.register_namespace(self.event_namespace)\n        # Mount the socket app with the API.\n        self.api.mount(str(constants.Endpoint.EVENT), socket_app)\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the app.\n\n        Returns:\n            The string representation of the app.\n        \"\"\"\n        return f\"<App state={self.state.__name__ if self.state else None}>\"\n\n    def __call__(self) -> FastAPI:\n        \"\"\"Run the backend api instance.\n\n        Returns:\n            The backend api.\n        \"\"\"\n        return self.api\n\n    def _add_default_endpoints(self):\n        \"\"\"Add default api endpoints (ping).\"\"\"\n        # To test the server.\n        self.api.get(str(constants.Endpoint.PING))(ping)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _add_cors(self):\n        \"\"\"Add CORS middleware to the app.\"\"\"\n        self.api.add_middleware(\n            cors.CORSMiddleware,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n            allow_origins=[\"*\"],\n        )\n\n    @property\n    def state_manager(self) -> StateManager:\n        \"\"\"Get the state manager.\n\n        Returns:\n            The initialized state manager.\n\n        Raises:\n            ValueError: if the state has not been initialized.\n        \"\"\"\n        if self._state_manager is None:\n            raise ValueError(\"The state manager has not been initialized.\")\n        return self._state_manager\n\n    @staticmethod\n    def _generate_component(component: Component | ComponentCallable) -> Component:\n        \"\"\"Generate a component from a callable.\n\n        Args:\n            component: The component function to call or Component to return as-is.\n\n        Returns:\n            The generated component.\n\n        Raises:\n            VarOperationTypeError: When an invalid component var related function is passed.\n            TypeError: When an invalid component function is passed.\n            exceptions.MatchTypeError: If the return types of match cases in rx.match are different.\n        \"\"\"\n        from reflex.utils.exceptions import VarOperationTypeError\n\n        try:\n            return component if isinstance(component, Component) else component()\n        except exceptions.MatchTypeError:\n            raise\n        except TypeError as e:\n            message = str(e)\n            if \"BaseVar\" in message or \"ComputedVar\" in message:\n                raise VarOperationTypeError(\n                    \"You may be trying to use an invalid Python function on a state var. \"\n                    \"When referencing a var inside your render code, only limited var operations are supported. \"\n                    \"See the var operation docs here: https://reflex.dev/docs/vars/var-operations/\"\n                ) from e\n            raise e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_load_events(self, route: str) -> list[EventHandler | EventSpec]:\n        \"\"\"Get the load events for a route.\n\n        Args:\n            route: The route to get the load events for.\n\n        Returns:\n            The load events for the route.\n        \"\"\"\n        route = route.lstrip(\"/\")\n        if route == \"\":\n            route = constants.PageNames.INDEX_ROUTE\n        return self.load_events.get(route, [])\n\n    def _check_routes_conflict(self, new_route: str):\n        \"\"\"Verify if there is any conflict between the new route and any existing route.\n\n        Based on conflicts that NextJS would throw if not intercepted.\n\n        Raises:\n            RouteValueError: exception showing which conflict exist with the route to be added\n\n        Args:\n            new_route: the route being newly added.\n        \"\"\"\n        from reflex.utils.exceptions import RouteValueError\n\n        if \"[\" not in new_route:\n            return\n\n        segments = (\n            constants.RouteRegex.SINGLE_SEGMENT,\n            constants.RouteRegex.DOUBLE_SEGMENT,\n            constants.RouteRegex.SINGLE_CATCHALL_SEGMENT,\n            constants.RouteRegex.DOUBLE_CATCHALL_SEGMENT,\n        )\n        for route in self.pages:\n            replaced_route = replace_brackets_with_keywords(route)\n            for rw, r, nr in zip(\n                replaced_route.split(\"/\"), route.split(\"/\"), new_route.split(\"/\")\n            ):\n                if rw in segments and r != nr:\n                    # If the slugs in the segments of both routes are not the same, then the route is invalid\n                    raise RouteValueError(\n                        f\"You cannot use different slug names for the same dynamic path in  {route} and {new_route} ('{r}' != '{nr}')\"\n                    )\n                elif rw not in segments and r != nr:\n                    # if the section being compared in both routes is not a dynamic segment(i.e not wrapped in brackets)\n                    # then we are guaranteed that the route is valid and there's no need checking the rest.\n                    # eg. /posts/[id]/info/[slug1] and /posts/[id]/info1/[slug1] is always going to be valid since\n                    # info1 will break away into its own tree.\n                    break\n\n    def add_custom_404_page(\n        self,\n        component: Component | ComponentCallable | None = None,\n        title: str = constants.Page404.TITLE,\n        image: str = constants.Page404.IMAGE,\n        description: str = constants.Page404.DESCRIPTION,\n        on_load: (\n            EventHandler | EventSpec | list[EventHandler | EventSpec] | None\n        ) = None,\n        meta: list[dict[str, str]] = constants.DefaultPage.META_LIST,\n    ):\n        \"\"\"Define a custom 404 page for any url having no match.\n\n        If there is no page defined on 'index' route, add the 404 page to it.\n        If there is no global catchall defined, add the 404 page with a catchall.\n\n        Args:\n            component: The component to display at the page.\n            title: The title of the page.\n            description: The description of the page.\n            image: The image to display on the page.\n            on_load: The event handler(s) that will be called each time the page load.\n            meta: The metadata of the page.\n        \"\"\"\n        if component is None:\n            component = Default404Page.create()\n        self.add_page(\n            component=wait_for_client_redirect(self._generate_component(component)),\n            route=constants.Page404.SLUG,\n            title=title or constants.Page404.TITLE,\n            image=image or constants.Page404.IMAGE,\n            description=description or constants.Page404.DESCRIPTION,\n            on_load=on_load,\n            meta=meta,\n        )\n\n    def _setup_admin_dash(self):\n        \"\"\"Setup the admin dash.\"\"\"\n        # Get the admin dash.\n        admin_dash = self.admin_dash\n\n        if admin_dash and admin_dash.models:\n            # Build the admin dashboard\n            admin = (\n                admin_dash.admin\n                if admin_dash.admin\n                else Admin(\n                    engine=Model.get_db_engine(),\n                    title=\"Reflex Admin Dashboard\",\n                    logo_url=\"https://reflex.dev/Reflex.svg\",\n                )\n            )\n\n            for model in admin_dash.models:\n                view = admin_dash.view_overrides.get(model, ModelView)\n                admin.add_view(view(model))\n\n            admin.mount_to(self.api)\n\n    def _get_frontend_packages(self, imports: Dict[str, set[ImportVar]]):\n        \"\"\"Gets the frontend packages to be installed and filters out the unnecessary ones.\n\n        Args:\n            imports: A dictionary containing the imports used in the current page.\n\n        Example:\n            >>> _get_frontend_packages({\"react\": \"16.14.0\", \"react-dom\": \"16.14.0\"})\n        \"\"\"\n        page_imports = {\n            i\n            for i, tags in imports.items()\n            if i not in constants.PackageJson.DEPENDENCIES\n            and i not in constants.PackageJson.DEV_DEPENDENCIES\n            and not any(i.startswith(prefix) for prefix in [\"/\", \".\", \"next/\"])\n            and i != \"\"\n            and any(tag.install for tag in tags)\n        }\n        frontend_packages = get_config().frontend_packages\n        _frontend_packages = []\n        for package in frontend_packages:\n            if package in (get_config().tailwind or {}).get(\"plugins\", []):  # type: ignore\n                console.warn(\n                    f\"Tailwind packages are inferred from 'plugins', remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            if package in page_imports:\n                console.warn(\n                    f\"React packages and their dependencies are inferred from Component.library and Component.lib_dependencies, remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            _frontend_packages.append(package)\n        page_imports.update(_frontend_packages)\n        prerequisites.install_frontend_packages(page_imports, get_config())\n\n    def _app_root(self, app_wrappers: dict[tuple[int, str], Component]) -> Component:\n        for component in tuple(app_wrappers.values()):\n            app_wrappers.update(component._get_all_app_wrap_components())\n        order = sorted(app_wrappers, key=lambda k: k[0], reverse=True)\n        root = parent = copy.deepcopy(app_wrappers[order[0]])\n        for key in order[1:]:\n            child = copy.deepcopy(app_wrappers[key])\n            parent.children.append(child)\n            parent = child\n        return root\n\n    def _should_compile(self) -> bool:\n        \"\"\"Check if the app should be compiled.\n\n        Returns:\n            Whether the app should be compiled.\n        \"\"\"\n        # Check the environment variable.\n        if should_skip_compile():\n            return False\n\n        nocompile = prerequisites.get_web_dir() / constants.NOCOMPILE_FILE\n\n        # Check the nocompile file.\n        if nocompile.exists():\n            # Delete the nocompile file\n            nocompile.unlink()\n            return False\n\n        # By default, compile the app.\n        return True\n\n    def _add_overlay_to_component(self, component: Component) -> Component:\n        if self.overlay_component is None:\n            return component\n\n        children = component.children\n        overlay_component = self._generate_component(self.overlay_component)\n\n        if children[0] == overlay_component:\n            return component\n\n        # recreate OverlayFragment with overlay_component as first child\n        component = OverlayFragment.create(overlay_component, *children)\n\n        return component\n\n\n\n\n\n\n\n\n    def _apply_decorated_pages(self):\n        \"\"\"Add @rx.page decorated pages to the app.\n\n        This has to be done in the MainThread for py38 and py39 compatibility, so the\n        decorated pages are added to the app before the app is compiled (in a thread)\n        to workaround REF-2172.\n\n        This can move back into `compile_` when py39 support is dropped.\n        \"\"\"\n        # Add the @rx.page decorated pages to collect on_load events.\n        for render, kwargs in DECORATED_PAGES[get_config().app_name]:\n            self.add_page(render, **kwargs)\n\n    def _validate_var_dependencies(\n        self, state: Optional[Type[BaseState]] = None\n    ) -> None:\n        \"\"\"Validate the dependencies of the vars in the app.\n\n        Args:\n            state: The state to validate the dependencies for.\n\n        Raises:\n            VarDependencyError: When a computed var has an invalid dependency.\n        \"\"\"\n        if not self.state:\n            return\n\n        if not state:\n            state = self.state\n\n        for var in state.computed_vars.values():\n            if not var._cache:\n                continue\n            deps = var._deps(objclass=state)\n            for dep in deps:\n                if dep not in state.vars and dep not in state.backend_vars:\n                    raise exceptions.VarDependencyError(\n                        f\"ComputedVar {var._var_name} on state {state.__name__} has an invalid dependency {dep}\"\n                    )\n\n        for substate in state.class_subclasses:\n            self._validate_var_dependencies(substate)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @contextlib.asynccontextmanager\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _process_background(\n        self, state: BaseState, event: Event\n    ) -> asyncio.Task | None:\n        \"\"\"Process an event in the background and emit updates as they arrive.\n\n        Args:\n            state: The state to process the event for.\n            event: The event to process.\n\n        Returns:\n            Task if the event was backgroundable, otherwise None\n        \"\"\"\n        substate, handler = state._get_event_handler(event)\n        if not handler.is_background:\n            return None\n\n        async def _coro():\n            \"\"\"Coroutine to process the event and emit updates inside an asyncio.Task.\n\n            Raises:\n                RuntimeError: If the app has not been initialized yet.\n            \"\"\"\n            if self.event_namespace is None:\n                raise RuntimeError(\"App has not been initialized yet.\")\n\n            # Process the event.\n            async for update in state._process_event(\n                handler=handler, state=substate, payload=event.payload\n            ):\n                # Postprocess the event.\n                update = await self._postprocess(state, event, update)\n\n                # Send the update to the client.\n                await self.event_namespace.emit_update(\n                    update=update,\n                    sid=state.router.session.session_id,\n                )\n\n        task = asyncio.create_task(_coro())\n        self.background_tasks.add(task)\n        # Clean up task from background_tasks set when complete.\n        task.add_done_callback(self.background_tasks.discard)\n        return task\n\n\nasync def process(\n    app: App, event: Event, sid: str, headers: Dict, client_ip: str\n) -> AsyncIterator[StateUpdate]:\n    \"\"\"Process an event.\n\n    Args:\n        app: The app to process the event for.\n        event: The event to process.\n        sid: The Socket.IO session id.\n        headers: The client headers.\n        client_ip: The client_ip.\n\n    Raises:\n        Exception: If a reflex specific error occurs during processing the event.\n\n    Yields:\n        The state updates after processing the event.\n    \"\"\"\n    from reflex.utils import telemetry\n\n    try:\n        # Add request data to the state.\n        router_data = event.router_data\n        router_data.update(\n            {\n                constants.RouteVar.QUERY: format.format_query_params(event.router_data),\n                constants.RouteVar.CLIENT_TOKEN: event.token,\n                constants.RouteVar.SESSION_ID: sid,\n                constants.RouteVar.HEADERS: headers,\n                constants.RouteVar.CLIENT_IP: client_ip,\n            }\n        )\n        # Get the state for the session exclusively.\n        async with app.state_manager.modify_state(event.substate_token) as state:\n            # re-assign only when the value is different\n            if state.router_data != router_data:\n                # assignment will recurse into substates and force recalculation of\n                # dependent ComputedVar (dynamic route variables)\n                state.router_data = router_data\n                state.router = RouterData(router_data)\n\n            # Preprocess the event.\n            update = await app._preprocess(state, event)\n\n            # If there was an update, yield it.\n            if update is not None:\n                yield update\n\n            # Only process the event if there is no update.\n            else:\n                if app._process_background(state, event) is not None:\n                    # `final=True` allows the frontend send more events immediately.\n                    yield StateUpdate(final=True)\n                    return\n\n                # Process the event synchronously.\n                async for update in state._process(event):\n                    # Postprocess the event.\n                    update = await app._postprocess(state, event, update)\n\n                    # Yield the\n```\n# Function Signature: def add_page(\n# Natural Language Annotations: \"\"\"Add a page to the app.\n\n        If the component is a callable, by default the route is the name of the\n        function. Otherwise, a route must be provided.\n\n        Args:\n            component: The component to display at the page.\n            route: The route to display the component at.\n            title: The title of the page.\n            description: The description of the page.\n            image: The image to display on the page.\n            on_load: The event handler(s) that will be called each time the page load.\n            meta: The metadata of the page.\n\n        Raises:\n            ValueError: When the specified route name already exists.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef add_page(self, component=None, route=", "  ```python\ndef add_page(self, component: Component | Fragment", "  ```\ndef add_page(\n    component: Union[Callable"]}, "reflex-reflex/app.py-_setup_overlay_component": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/app.py:\n```\n\"\"\"The main Reflex app.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport concurrent.futures\nimport contextlib\nimport copy\nimport functools\nimport io\nimport multiprocessing\nimport os\nimport platform\nimport sys\nfrom datetime import datetime\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n    get_args,\n    get_type_hints,\n)\n\nfrom fastapi import FastAPI, HTTPException, Request, UploadFile\nfrom fastapi.middleware import cors\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom rich.progress import MofNCompleteColumn, Progress, TimeElapsedColumn\nfrom socketio import ASGIApp, AsyncNamespace, AsyncServer\nfrom starlette_admin.contrib.sqla.admin import Admin\nfrom starlette_admin.contrib.sqla.view import ModelView\n\nfrom reflex import constants\nfrom reflex.admin import AdminDash\nfrom reflex.app_mixins import AppMixin, LifespanMixin, MiddlewareMixin\nfrom reflex.base import Base\nfrom reflex.compiler import compiler\nfrom reflex.compiler import utils as compiler_utils\nfrom reflex.compiler.compiler import ExecutorSafeFunctions\nfrom reflex.components.base.app_wrap import AppWrap\nfrom reflex.components.base.fragment import Fragment\nfrom reflex.components.component import (\n    Component,\n    ComponentStyle,\n    evaluate_style_namespaces,\n)\nfrom reflex.components.core.banner import connection_pulser, connection_toaster\nfrom reflex.components.core.breakpoints import set_breakpoints\nfrom reflex.components.core.client_side_routing import (\n    Default404Page,\n    wait_for_client_redirect,\n)\nfrom reflex.components.core.upload import Upload, get_upload_dir\nfrom reflex.components.radix import themes\nfrom reflex.config import get_config\nfrom reflex.event import Event, EventHandler, EventSpec\nfrom reflex.model import Model\nfrom reflex.page import (\n    DECORATED_PAGES,\n)\nfrom reflex.route import (\n    get_route_args,\n    replace_brackets_with_keywords,\n    verify_route_validity,\n)\nfrom reflex.state import (\n    BaseState,\n    RouterData,\n    State,\n    StateManager,\n    StateUpdate,\n    _substate_key,\n    code_uses_state_contexts,\n)\nfrom reflex.utils import codespaces, console, exceptions, format, prerequisites, types\nfrom reflex.utils.exec import is_prod_mode, is_testing_env, should_skip_compile\nfrom reflex.utils.imports import ImportVar\n\n# Define custom types.\nComponentCallable = Callable[[], Component]\nReducer = Callable[[Event], Coroutine[Any, Any, StateUpdate]]\n\n\ndef default_overlay_component() -> Component:\n    \"\"\"Default overlay_component attribute for App.\n\n    Returns:\n        The default overlay_component, which is a connection_modal.\n    \"\"\"\n    return Fragment.create(\n        connection_pulser(),\n        connection_toaster(),\n        *codespaces.codespaces_auto_redirect(),\n    )\n\n\nclass OverlayFragment(Fragment):\n    \"\"\"Alias for Fragment, used to wrap the overlay_component.\"\"\"\n\n    pass\n\n\nclass App(MiddlewareMixin, LifespanMixin, Base):\n    \"\"\"The main Reflex app that encapsulates the backend and frontend.\n\n    Every Reflex app needs an app defined in its main module.\n\n    ```python\n    # app.py\n    import reflex as rx\n\n    # Define state and pages\n    ...\n\n    app = rx.App(\n        # Set global level style.\n        style={...},\n        # Set the top level theme.\n        theme=rx.theme(accent_color=\"blue\"),\n    )\n    ```\n    \"\"\"\n\n    # The global [theme](https://reflex.dev/docs/styling/theming/#theme) for the entire app.\n    theme: Optional[Component] = themes.theme(accent_color=\"blue\")\n\n    # The [global style](https://reflex.dev/docs/styling/overview/#global-styles}) for the app.\n    style: ComponentStyle = {}\n\n    # A list of URLs to [stylesheets](https://reflex.dev/docs/styling/custom-stylesheets/) to include in the app.\n    stylesheets: List[str] = []\n\n    # A component that is present on every page (defaults to the Connection Error banner).\n    overlay_component: Optional[Union[Component, ComponentCallable]] = (\n        default_overlay_component\n    )\n\n    # Components to add to the head of every page.\n    head_components: List[Component] = []\n\n    # The Socket.IO AsyncServer instance.\n    sio: Optional[AsyncServer] = None\n\n    # The language to add to the html root tag of every page.\n    html_lang: Optional[str] = None\n\n    # Attributes to add to the html root tag of every page.\n    html_custom_attrs: Optional[Dict[str, str]] = None\n\n    # A map from a page route to the component to render. Users should use `add_page`. PRIVATE.\n    pages: Dict[str, Component] = {}\n\n    # The backend API object. PRIVATE.\n    api: FastAPI = None  # type: ignore\n\n    # The state class to use for the app. PRIVATE.\n    state: Optional[Type[BaseState]] = None\n\n    # Class to manage many client states.\n    _state_manager: Optional[StateManager] = None\n\n    # Mapping from a route to event handlers to trigger when the page loads. PRIVATE.\n    load_events: Dict[str, List[Union[EventHandler, EventSpec]]] = {}\n\n    # Admin dashboard to view and manage the database. PRIVATE.\n    admin_dash: Optional[AdminDash] = None\n\n    # The async server name space. PRIVATE.\n    event_namespace: Optional[EventNamespace] = None\n\n    # Background tasks that are currently running. PRIVATE.\n    background_tasks: Set[asyncio.Task] = set()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the app.\n\n        Args:\n            **kwargs: Kwargs to initialize the app with.\n\n        Raises:\n            ValueError: If the event namespace is not provided in the config.\n                        Also, if there are multiple client subclasses of rx.BaseState(Subclasses of rx.BaseState should consist\n                        of the DefaultState and the client app state).\n        \"\"\"\n        if \"connect_error_component\" in kwargs:\n            raise ValueError(\n                \"`connect_error_component` is deprecated, use `overlay_component` instead\"\n            )\n        super().__init__(**kwargs)\n        base_state_subclasses = BaseState.__subclasses__()\n\n        # Special case to allow test cases have multiple subclasses of rx.BaseState.\n        if not is_testing_env() and len(base_state_subclasses) > 1:\n            # Only one Base State class is allowed.\n            raise ValueError(\n                \"rx.BaseState cannot be subclassed multiple times. use rx.State instead\"\n            )\n\n        if \"breakpoints\" in self.style:\n            set_breakpoints(self.style.pop(\"breakpoints\"))\n\n        # Set up the API.\n        self.api = FastAPI(lifespan=self._run_lifespan_tasks)\n        self._add_cors()\n        self._add_default_endpoints()\n\n        for clz in App.__mro__:\n            if clz == App:\n                continue\n            if issubclass(clz, AppMixin):\n                clz._init_mixin(self)\n\n        self._setup_state()\n\n        # Set up the admin dash.\n        self._setup_admin_dash()\n\n        if sys.platform == \"win32\" and not is_prod_mode():\n            # Hack to fix Windows hot reload issue.\n            from reflex.utils.compat import windows_hot_reload_lifespan_hack\n\n            self.register_lifespan_task(windows_hot_reload_lifespan_hack)\n\n\n\n\n\n\n\n    def _setup_state(self) -> None:\n        \"\"\"Set up the state for the app.\n\n        Raises:\n            RuntimeError: If the socket server is invalid.\n        \"\"\"\n        if not self.state:\n            return\n\n        config = get_config()\n\n        # Set up the state manager.\n        self._state_manager = StateManager.create(state=self.state)\n\n        # Set up the Socket.IO AsyncServer.\n        if not self.sio:\n            self.sio = AsyncServer(\n                async_mode=\"asgi\",\n                cors_allowed_origins=(\n                    \"*\"\n                    if config.cors_allowed_origins == [\"*\"]\n                    else config.cors_allowed_origins\n                ),\n                cors_credentials=True,\n                max_http_buffer_size=constants.POLLING_MAX_HTTP_BUFFER_SIZE,\n                ping_interval=constants.Ping.INTERVAL,\n                ping_timeout=constants.Ping.TIMEOUT,\n            )\n        elif getattr(self.sio, \"async_mode\", \"\") != \"asgi\":\n            raise RuntimeError(\n                f\"Custom `sio` must use `async_mode='asgi'`, not '{self.sio.async_mode}'.\"\n            )\n\n        # Create the socket app. Note event endpoint constant replaces the default 'socket.io' path.\n        socket_app = ASGIApp(self.sio, socketio_path=\"\")\n        namespace = config.get_event_namespace()\n\n        # Create the event namespace and attach the main app. Not related to any paths.\n        self.event_namespace = EventNamespace(namespace, self)\n\n        # Register the event namespace with the socket.\n        self.sio.register_namespace(self.event_namespace)\n        # Mount the socket app with the API.\n        self.api.mount(str(constants.Endpoint.EVENT), socket_app)\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the app.\n\n        Returns:\n            The string representation of the app.\n        \"\"\"\n        return f\"<App state={self.state.__name__ if self.state else None}>\"\n\n    def __call__(self) -> FastAPI:\n        \"\"\"Run the backend api instance.\n\n        Returns:\n            The backend api.\n        \"\"\"\n        return self.api\n\n    def _add_default_endpoints(self):\n        \"\"\"Add default api endpoints (ping).\"\"\"\n        # To test the server.\n        self.api.get(str(constants.Endpoint.PING))(ping)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _add_cors(self):\n        \"\"\"Add CORS middleware to the app.\"\"\"\n        self.api.add_middleware(\n            cors.CORSMiddleware,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n            allow_origins=[\"*\"],\n        )\n\n    @property\n    def state_manager(self) -> StateManager:\n        \"\"\"Get the state manager.\n\n        Returns:\n            The initialized state manager.\n\n        Raises:\n            ValueError: if the state has not been initialized.\n        \"\"\"\n        if self._state_manager is None:\n            raise ValueError(\"The state manager has not been initialized.\")\n        return self._state_manager\n\n    @staticmethod\n    def _generate_component(component: Component | ComponentCallable) -> Component:\n        \"\"\"Generate a component from a callable.\n\n        Args:\n            component: The component function to call or Component to return as-is.\n\n        Returns:\n            The generated component.\n\n        Raises:\n            VarOperationTypeError: When an invalid component var related function is passed.\n            TypeError: When an invalid component function is passed.\n            exceptions.MatchTypeError: If the return types of match cases in rx.match are different.\n        \"\"\"\n        from reflex.utils.exceptions import VarOperationTypeError\n\n        try:\n            return component if isinstance(component, Component) else component()\n        except exceptions.MatchTypeError:\n            raise\n        except TypeError as e:\n            message = str(e)\n            if \"BaseVar\" in message or \"ComputedVar\" in message:\n                raise VarOperationTypeError(\n                    \"You may be trying to use an invalid Python function on a state var. \"\n                    \"When referencing a var inside your render code, only limited var operations are supported. \"\n                    \"See the var operation docs here: https://reflex.dev/docs/vars/var-operations/\"\n                ) from e\n            raise e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_load_events(self, route: str) -> list[EventHandler | EventSpec]:\n        \"\"\"Get the load events for a route.\n\n        Args:\n            route: The route to get the load events for.\n\n        Returns:\n            The load events for the route.\n        \"\"\"\n        route = route.lstrip(\"/\")\n        if route == \"\":\n            route = constants.PageNames.INDEX_ROUTE\n        return self.load_events.get(route, [])\n\n    def _check_routes_conflict(self, new_route: str):\n        \"\"\"Verify if there is any conflict between the new route and any existing route.\n\n        Based on conflicts that NextJS would throw if not intercepted.\n\n        Raises:\n            RouteValueError: exception showing which conflict exist with the route to be added\n\n        Args:\n            new_route: the route being newly added.\n        \"\"\"\n        from reflex.utils.exceptions import RouteValueError\n\n        if \"[\" not in new_route:\n            return\n\n        segments = (\n            constants.RouteRegex.SINGLE_SEGMENT,\n            constants.RouteRegex.DOUBLE_SEGMENT,\n            constants.RouteRegex.SINGLE_CATCHALL_SEGMENT,\n            constants.RouteRegex.DOUBLE_CATCHALL_SEGMENT,\n        )\n        for route in self.pages:\n            replaced_route = replace_brackets_with_keywords(route)\n            for rw, r, nr in zip(\n                replaced_route.split(\"/\"), route.split(\"/\"), new_route.split(\"/\")\n            ):\n                if rw in segments and r != nr:\n                    # If the slugs in the segments of both routes are not the same, then the route is invalid\n                    raise RouteValueError(\n                        f\"You cannot use different slug names for the same dynamic path in  {route} and {new_route} ('{r}' != '{nr}')\"\n                    )\n                elif rw not in segments and r != nr:\n                    # if the section being compared in both routes is not a dynamic segment(i.e not wrapped in brackets)\n                    # then we are guaranteed that the route is valid and there's no need checking the rest.\n                    # eg. /posts/[id]/info/[slug1] and /posts/[id]/info1/[slug1] is always going to be valid since\n                    # info1 will break away into its own tree.\n                    break\n\n    def add_custom_404_page(\n        self,\n        component: Component | ComponentCallable | None = None,\n        title: str = constants.Page404.TITLE,\n        image: str = constants.Page404.IMAGE,\n        description: str = constants.Page404.DESCRIPTION,\n        on_load: (\n            EventHandler | EventSpec | list[EventHandler | EventSpec] | None\n        ) = None,\n        meta: list[dict[str, str]] = constants.DefaultPage.META_LIST,\n    ):\n        \"\"\"Define a custom 404 page for any url having no match.\n\n        If there is no page defined on 'index' route, add the 404 page to it.\n        If there is no global catchall defined, add the 404 page with a catchall.\n\n        Args:\n            component: The component to display at the page.\n            title: The title of the page.\n            description: The description of the page.\n            image: The image to display on the page.\n            on_load: The event handler(s) that will be called each time the page load.\n            meta: The metadata of the page.\n        \"\"\"\n        if component is None:\n            component = Default404Page.create()\n        self.add_page(\n            component=wait_for_client_redirect(self._generate_component(component)),\n            route=constants.Page404.SLUG,\n            title=title or constants.Page404.TITLE,\n            image=image or constants.Page404.IMAGE,\n            description=description or constants.Page404.DESCRIPTION,\n            on_load=on_load,\n            meta=meta,\n        )\n\n    def _setup_admin_dash(self):\n        \"\"\"Setup the admin dash.\"\"\"\n        # Get the admin dash.\n        admin_dash = self.admin_dash\n\n        if admin_dash and admin_dash.models:\n            # Build the admin dashboard\n            admin = (\n                admin_dash.admin\n                if admin_dash.admin\n                else Admin(\n                    engine=Model.get_db_engine(),\n                    title=\"Reflex Admin Dashboard\",\n                    logo_url=\"https://reflex.dev/Reflex.svg\",\n                )\n            )\n\n            for model in admin_dash.models:\n                view = admin_dash.view_overrides.get(model, ModelView)\n                admin.add_view(view(model))\n\n            admin.mount_to(self.api)\n\n    def _get_frontend_packages(self, imports: Dict[str, set[ImportVar]]):\n        \"\"\"Gets the frontend packages to be installed and filters out the unnecessary ones.\n\n        Args:\n            imports: A dictionary containing the imports used in the current page.\n\n        Example:\n            >>> _get_frontend_packages({\"react\": \"16.14.0\", \"react-dom\": \"16.14.0\"})\n        \"\"\"\n        page_imports = {\n            i\n            for i, tags in imports.items()\n            if i not in constants.PackageJson.DEPENDENCIES\n            and i not in constants.PackageJson.DEV_DEPENDENCIES\n            and not any(i.startswith(prefix) for prefix in [\"/\", \".\", \"next/\"])\n            and i != \"\"\n            and any(tag.install for tag in tags)\n        }\n        frontend_packages = get_config().frontend_packages\n        _frontend_packages = []\n        for package in frontend_packages:\n            if package in (get_config().tailwind or {}).get(\"plugins\", []):  # type: ignore\n                console.warn(\n                    f\"Tailwind packages are inferred from 'plugins', remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            if package in page_imports:\n                console.warn(\n                    f\"React packages and their dependencies are inferred from Component.library and Component.lib_dependencies, remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            _frontend_packages.append(package)\n        page_imports.update(_frontend_packages)\n        prerequisites.install_frontend_packages(page_imports, get_config())\n\n    def _app_root(self, app_wrappers: dict[tuple[int, str], Component]) -> Component:\n        for component in tuple(app_wrappers.values()):\n            app_wrappers.update(component._get_all_app_wrap_components())\n        order = sorted(app_wrappers, key=lambda k: k[0], reverse=True)\n        root = parent = copy.deepcopy(app_wrappers[order[0]])\n        for key in order[1:]:\n            child = copy.deepcopy(app_wrappers[key])\n            parent.children.append(child)\n            parent = child\n        return root\n\n    def _should_compile(self) -> bool:\n        \"\"\"Check if the app should be compiled.\n\n        Returns:\n            Whether the app should be compiled.\n        \"\"\"\n        # Check the environment variable.\n        if should_skip_compile():\n            return False\n\n        nocompile = prerequisites.get_web_dir() / constants.NOCOMPILE_FILE\n\n        # Check the nocompile file.\n        if nocompile.exists():\n            # Delete the nocompile file\n            nocompile.unlink()\n            return False\n\n        # By default, compile the app.\n        return True\n\n    def _add_overlay_to_component(self, component: Component) -> Component:\n        if self.overlay_component is None:\n            return component\n\n        children = component.children\n        overlay_component = self._generate_component(self.overlay_component)\n\n        if children[0] == overlay_component:\n            return component\n\n        # recreate OverlayFragment with overlay_component as first child\n        component = OverlayFragment.create(overlay_component, *children)\n\n        return component\n\n\n\n\n\n\n\n\n    def _apply_decorated_pages(self):\n        \"\"\"Add @rx.page decorated pages to the app.\n\n        This has to be done in the MainThread for py38 and py39 compatibility, so the\n        decorated pages are added to the app before the app is compiled (in a thread)\n        to workaround REF-2172.\n\n        This can move back into `compile_` when py39 support is dropped.\n        \"\"\"\n        # Add the @rx.page decorated pages to collect on_load events.\n        for render, kwargs in DECORATED_PAGES[get_config().app_name]:\n            self.add_page(render, **kwargs)\n\n    def _validate_var_dependencies(\n        self, state: Optional[Type[BaseState]] = None\n    ) -> None:\n        \"\"\"Validate the dependencies of the vars in the app.\n\n        Args:\n            state: The state to validate the dependencies for.\n\n        Raises:\n            VarDependencyError: When a computed var has an invalid dependency.\n        \"\"\"\n        if not self.state:\n            return\n\n        if not state:\n            state = self.state\n\n        for var in state.computed_vars.values():\n            if not var._cache:\n                continue\n            deps = var._deps(objclass=state)\n            for dep in deps:\n                if dep not in state.vars and dep not in state.backend_vars:\n                    raise exceptions.VarDependencyError(\n                        f\"ComputedVar {var._var_name} on state {state.__name__} has an invalid dependency {dep}\"\n                    )\n\n        for substate in state.class_subclasses:\n            self._validate_var_dependencies(substate)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @contextlib.asynccontextmanager\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _process_background(\n        self, state: BaseState, event: Event\n    ) -> asyncio.Task | None:\n        \"\"\"Process an event in the background and emit updates as they arrive.\n\n        Args:\n            state: The state to process the event for.\n            event: The event to process.\n\n        Returns:\n            Task if the event was backgroundable, otherwise None\n        \"\"\"\n        substate, handler = state._get_event_handler(event)\n        if not handler.is_background:\n            return None\n\n        async def _coro():\n            \"\"\"Coroutine to process the event and emit updates inside an asyncio.Task.\n\n            Raises:\n                RuntimeError: If the app has not been initialized yet.\n            \"\"\"\n            if self.event_namespace is None:\n                raise RuntimeError(\"App has not been initialized yet.\")\n\n            # Process the event.\n            async for update in state._process_event(\n                handler=handler, state=substate, payload=event.payload\n            ):\n                # Postprocess the event.\n                update = await self._postprocess(state, event, update)\n\n                # Send the update to the client.\n                await self.event_namespace.emit_update(\n                    update=update,\n                    sid=state.router.session.session_id,\n                )\n\n        task = asyncio.create_task(_coro())\n        self.background_tasks.add(task)\n        # Clean up task from background_tasks set when complete.\n        task.add_done_callback(self.background_tasks.discard)\n        return task\n\n\nasync def process(\n    app: App, event: Event, sid: str, headers: Dict, client_ip: str\n) -> AsyncIterator[StateUpdate]:\n    \"\"\"Process an event.\n\n    Args:\n        app: The app to process the event for.\n        event: The event to process.\n        sid: The Socket.IO session id.\n        headers: The client headers.\n        client_ip: The client_ip.\n\n    Raises:\n        Exception: If a reflex specific error occurs during processing the event.\n\n    Yields:\n        The state updates after processing the event.\n    \"\"\"\n    from reflex.utils import telemetry\n\n    try:\n        # Add request data to the state.\n        router_data = event.router_data\n        router_data.update(\n            {\n                constants.RouteVar.QUERY: format.format_query_params(event.router_data),\n                constants.RouteVar.CLIENT_TOKEN: event.token,\n                constants.RouteVar.SESSION_ID: sid,\n                constants.RouteVar.HEADERS: headers,\n                constants.RouteVar.CLIENT_IP: client_ip,\n            }\n        )\n        # Get the state for the session exclusively.\n        async with app.state_manager.modify_state(event.substate_token) as state:\n            # re-assign only when the value is different\n            if state.router_data != router_data:\n                # assignment will recurse into substates and force recalculation of\n                # dependent ComputedVar (dynamic route variables)\n                state.router_data = router_data\n                state.router = RouterData(router_data)\n\n            # Preprocess the event.\n            update = await app._preprocess(state, event)\n\n            # If there was an update, yield it.\n            if update is not None:\n                yield update\n\n            # Only process the event if there is no update.\n            else:\n                if app._process_background(state, event) is not None:\n                    # `final=True` allows the frontend send more events immediately.\n                    yield StateUpdate(final=True)\n                    return\n\n                # Process the event synchronously.\n                async for update in state._process(event):\n                    # Postprocess the event.\n                    update = await app._postprocess(state, event, update)\n\n                    # Yield the\n```\n# Function Signature: def _setup_overlay_component(self):\n# Natural Language Annotations: \"\"\"If a State is not used and no overlay_component is specified, do not render the connection modal.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _setup_overlay_component(self):\n   ", "  ```\ndef _setup_overlay_component(self) -> None", "  ```\ndef _setup_overlay_component(self):\n   "]}, "reflex-reflex/app.py-_compile": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/app.py:\n```\n\"\"\"The main Reflex app.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport concurrent.futures\nimport contextlib\nimport copy\nimport functools\nimport io\nimport multiprocessing\nimport os\nimport platform\nimport sys\nfrom datetime import datetime\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n    get_args,\n    get_type_hints,\n)\n\nfrom fastapi import FastAPI, HTTPException, Request, UploadFile\nfrom fastapi.middleware import cors\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom rich.progress import MofNCompleteColumn, Progress, TimeElapsedColumn\nfrom socketio import ASGIApp, AsyncNamespace, AsyncServer\nfrom starlette_admin.contrib.sqla.admin import Admin\nfrom starlette_admin.contrib.sqla.view import ModelView\n\nfrom reflex import constants\nfrom reflex.admin import AdminDash\nfrom reflex.app_mixins import AppMixin, LifespanMixin, MiddlewareMixin\nfrom reflex.base import Base\nfrom reflex.compiler import compiler\nfrom reflex.compiler import utils as compiler_utils\nfrom reflex.compiler.compiler import ExecutorSafeFunctions\nfrom reflex.components.base.app_wrap import AppWrap\nfrom reflex.components.base.fragment import Fragment\nfrom reflex.components.component import (\n    Component,\n    ComponentStyle,\n    evaluate_style_namespaces,\n)\nfrom reflex.components.core.banner import connection_pulser, connection_toaster\nfrom reflex.components.core.breakpoints import set_breakpoints\nfrom reflex.components.core.client_side_routing import (\n    Default404Page,\n    wait_for_client_redirect,\n)\nfrom reflex.components.core.upload import Upload, get_upload_dir\nfrom reflex.components.radix import themes\nfrom reflex.config import get_config\nfrom reflex.event import Event, EventHandler, EventSpec\nfrom reflex.model import Model\nfrom reflex.page import (\n    DECORATED_PAGES,\n)\nfrom reflex.route import (\n    get_route_args,\n    replace_brackets_with_keywords,\n    verify_route_validity,\n)\nfrom reflex.state import (\n    BaseState,\n    RouterData,\n    State,\n    StateManager,\n    StateUpdate,\n    _substate_key,\n    code_uses_state_contexts,\n)\nfrom reflex.utils import codespaces, console, exceptions, format, prerequisites, types\nfrom reflex.utils.exec import is_prod_mode, is_testing_env, should_skip_compile\nfrom reflex.utils.imports import ImportVar\n\n# Define custom types.\nComponentCallable = Callable[[], Component]\nReducer = Callable[[Event], Coroutine[Any, Any, StateUpdate]]\n\n\ndef default_overlay_component() -> Component:\n    \"\"\"Default overlay_component attribute for App.\n\n    Returns:\n        The default overlay_component, which is a connection_modal.\n    \"\"\"\n    return Fragment.create(\n        connection_pulser(),\n        connection_toaster(),\n        *codespaces.codespaces_auto_redirect(),\n    )\n\n\nclass OverlayFragment(Fragment):\n    \"\"\"Alias for Fragment, used to wrap the overlay_component.\"\"\"\n\n    pass\n\n\nclass App(MiddlewareMixin, LifespanMixin, Base):\n    \"\"\"The main Reflex app that encapsulates the backend and frontend.\n\n    Every Reflex app needs an app defined in its main module.\n\n    ```python\n    # app.py\n    import reflex as rx\n\n    # Define state and pages\n    ...\n\n    app = rx.App(\n        # Set global level style.\n        style={...},\n        # Set the top level theme.\n        theme=rx.theme(accent_color=\"blue\"),\n    )\n    ```\n    \"\"\"\n\n    # The global [theme](https://reflex.dev/docs/styling/theming/#theme) for the entire app.\n    theme: Optional[Component] = themes.theme(accent_color=\"blue\")\n\n    # The [global style](https://reflex.dev/docs/styling/overview/#global-styles}) for the app.\n    style: ComponentStyle = {}\n\n    # A list of URLs to [stylesheets](https://reflex.dev/docs/styling/custom-stylesheets/) to include in the app.\n    stylesheets: List[str] = []\n\n    # A component that is present on every page (defaults to the Connection Error banner).\n    overlay_component: Optional[Union[Component, ComponentCallable]] = (\n        default_overlay_component\n    )\n\n    # Components to add to the head of every page.\n    head_components: List[Component] = []\n\n    # The Socket.IO AsyncServer instance.\n    sio: Optional[AsyncServer] = None\n\n    # The language to add to the html root tag of every page.\n    html_lang: Optional[str] = None\n\n    # Attributes to add to the html root tag of every page.\n    html_custom_attrs: Optional[Dict[str, str]] = None\n\n    # A map from a page route to the component to render. Users should use `add_page`. PRIVATE.\n    pages: Dict[str, Component] = {}\n\n    # The backend API object. PRIVATE.\n    api: FastAPI = None  # type: ignore\n\n    # The state class to use for the app. PRIVATE.\n    state: Optional[Type[BaseState]] = None\n\n    # Class to manage many client states.\n    _state_manager: Optional[StateManager] = None\n\n    # Mapping from a route to event handlers to trigger when the page loads. PRIVATE.\n    load_events: Dict[str, List[Union[EventHandler, EventSpec]]] = {}\n\n    # Admin dashboard to view and manage the database. PRIVATE.\n    admin_dash: Optional[AdminDash] = None\n\n    # The async server name space. PRIVATE.\n    event_namespace: Optional[EventNamespace] = None\n\n    # Background tasks that are currently running. PRIVATE.\n    background_tasks: Set[asyncio.Task] = set()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the app.\n\n        Args:\n            **kwargs: Kwargs to initialize the app with.\n\n        Raises:\n            ValueError: If the event namespace is not provided in the config.\n                        Also, if there are multiple client subclasses of rx.BaseState(Subclasses of rx.BaseState should consist\n                        of the DefaultState and the client app state).\n        \"\"\"\n        if \"connect_error_component\" in kwargs:\n            raise ValueError(\n                \"`connect_error_component` is deprecated, use `overlay_component` instead\"\n            )\n        super().__init__(**kwargs)\n        base_state_subclasses = BaseState.__subclasses__()\n\n        # Special case to allow test cases have multiple subclasses of rx.BaseState.\n        if not is_testing_env() and len(base_state_subclasses) > 1:\n            # Only one Base State class is allowed.\n            raise ValueError(\n                \"rx.BaseState cannot be subclassed multiple times. use rx.State instead\"\n            )\n\n        if \"breakpoints\" in self.style:\n            set_breakpoints(self.style.pop(\"breakpoints\"))\n\n        # Set up the API.\n        self.api = FastAPI(lifespan=self._run_lifespan_tasks)\n        self._add_cors()\n        self._add_default_endpoints()\n\n        for clz in App.__mro__:\n            if clz == App:\n                continue\n            if issubclass(clz, AppMixin):\n                clz._init_mixin(self)\n\n        self._setup_state()\n\n        # Set up the admin dash.\n        self._setup_admin_dash()\n\n        if sys.platform == \"win32\" and not is_prod_mode():\n            # Hack to fix Windows hot reload issue.\n            from reflex.utils.compat import windows_hot_reload_lifespan_hack\n\n            self.register_lifespan_task(windows_hot_reload_lifespan_hack)\n\n\n\n\n\n\n\n    def _setup_state(self) -> None:\n        \"\"\"Set up the state for the app.\n\n        Raises:\n            RuntimeError: If the socket server is invalid.\n        \"\"\"\n        if not self.state:\n            return\n\n        config = get_config()\n\n        # Set up the state manager.\n        self._state_manager = StateManager.create(state=self.state)\n\n        # Set up the Socket.IO AsyncServer.\n        if not self.sio:\n            self.sio = AsyncServer(\n                async_mode=\"asgi\",\n                cors_allowed_origins=(\n                    \"*\"\n                    if config.cors_allowed_origins == [\"*\"]\n                    else config.cors_allowed_origins\n                ),\n                cors_credentials=True,\n                max_http_buffer_size=constants.POLLING_MAX_HTTP_BUFFER_SIZE,\n                ping_interval=constants.Ping.INTERVAL,\n                ping_timeout=constants.Ping.TIMEOUT,\n            )\n        elif getattr(self.sio, \"async_mode\", \"\") != \"asgi\":\n            raise RuntimeError(\n                f\"Custom `sio` must use `async_mode='asgi'`, not '{self.sio.async_mode}'.\"\n            )\n\n        # Create the socket app. Note event endpoint constant replaces the default 'socket.io' path.\n        socket_app = ASGIApp(self.sio, socketio_path=\"\")\n        namespace = config.get_event_namespace()\n\n        # Create the event namespace and attach the main app. Not related to any paths.\n        self.event_namespace = EventNamespace(namespace, self)\n\n        # Register the event namespace with the socket.\n        self.sio.register_namespace(self.event_namespace)\n        # Mount the socket app with the API.\n        self.api.mount(str(constants.Endpoint.EVENT), socket_app)\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the app.\n\n        Returns:\n            The string representation of the app.\n        \"\"\"\n        return f\"<App state={self.state.__name__ if self.state else None}>\"\n\n    def __call__(self) -> FastAPI:\n        \"\"\"Run the backend api instance.\n\n        Returns:\n            The backend api.\n        \"\"\"\n        return self.api\n\n    def _add_default_endpoints(self):\n        \"\"\"Add default api endpoints (ping).\"\"\"\n        # To test the server.\n        self.api.get(str(constants.Endpoint.PING))(ping)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _add_cors(self):\n        \"\"\"Add CORS middleware to the app.\"\"\"\n        self.api.add_middleware(\n            cors.CORSMiddleware,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n            allow_origins=[\"*\"],\n        )\n\n    @property\n    def state_manager(self) -> StateManager:\n        \"\"\"Get the state manager.\n\n        Returns:\n            The initialized state manager.\n\n        Raises:\n            ValueError: if the state has not been initialized.\n        \"\"\"\n        if self._state_manager is None:\n            raise ValueError(\"The state manager has not been initialized.\")\n        return self._state_manager\n\n    @staticmethod\n    def _generate_component(component: Component | ComponentCallable) -> Component:\n        \"\"\"Generate a component from a callable.\n\n        Args:\n            component: The component function to call or Component to return as-is.\n\n        Returns:\n            The generated component.\n\n        Raises:\n            VarOperationTypeError: When an invalid component var related function is passed.\n            TypeError: When an invalid component function is passed.\n            exceptions.MatchTypeError: If the return types of match cases in rx.match are different.\n        \"\"\"\n        from reflex.utils.exceptions import VarOperationTypeError\n\n        try:\n            return component if isinstance(component, Component) else component()\n        except exceptions.MatchTypeError:\n            raise\n        except TypeError as e:\n            message = str(e)\n            if \"BaseVar\" in message or \"ComputedVar\" in message:\n                raise VarOperationTypeError(\n                    \"You may be trying to use an invalid Python function on a state var. \"\n                    \"When referencing a var inside your render code, only limited var operations are supported. \"\n                    \"See the var operation docs here: https://reflex.dev/docs/vars/var-operations/\"\n                ) from e\n            raise e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_load_events(self, route: str) -> list[EventHandler | EventSpec]:\n        \"\"\"Get the load events for a route.\n\n        Args:\n            route: The route to get the load events for.\n\n        Returns:\n            The load events for the route.\n        \"\"\"\n        route = route.lstrip(\"/\")\n        if route == \"\":\n            route = constants.PageNames.INDEX_ROUTE\n        return self.load_events.get(route, [])\n\n    def _check_routes_conflict(self, new_route: str):\n        \"\"\"Verify if there is any conflict between the new route and any existing route.\n\n        Based on conflicts that NextJS would throw if not intercepted.\n\n        Raises:\n            RouteValueError: exception showing which conflict exist with the route to be added\n\n        Args:\n            new_route: the route being newly added.\n        \"\"\"\n        from reflex.utils.exceptions import RouteValueError\n\n        if \"[\" not in new_route:\n            return\n\n        segments = (\n            constants.RouteRegex.SINGLE_SEGMENT,\n            constants.RouteRegex.DOUBLE_SEGMENT,\n            constants.RouteRegex.SINGLE_CATCHALL_SEGMENT,\n            constants.RouteRegex.DOUBLE_CATCHALL_SEGMENT,\n        )\n        for route in self.pages:\n            replaced_route = replace_brackets_with_keywords(route)\n            for rw, r, nr in zip(\n                replaced_route.split(\"/\"), route.split(\"/\"), new_route.split(\"/\")\n            ):\n                if rw in segments and r != nr:\n                    # If the slugs in the segments of both routes are not the same, then the route is invalid\n                    raise RouteValueError(\n                        f\"You cannot use different slug names for the same dynamic path in  {route} and {new_route} ('{r}' != '{nr}')\"\n                    )\n                elif rw not in segments and r != nr:\n                    # if the section being compared in both routes is not a dynamic segment(i.e not wrapped in brackets)\n                    # then we are guaranteed that the route is valid and there's no need checking the rest.\n                    # eg. /posts/[id]/info/[slug1] and /posts/[id]/info1/[slug1] is always going to be valid since\n                    # info1 will break away into its own tree.\n                    break\n\n    def add_custom_404_page(\n        self,\n        component: Component | ComponentCallable | None = None,\n        title: str = constants.Page404.TITLE,\n        image: str = constants.Page404.IMAGE,\n        description: str = constants.Page404.DESCRIPTION,\n        on_load: (\n            EventHandler | EventSpec | list[EventHandler | EventSpec] | None\n        ) = None,\n        meta: list[dict[str, str]] = constants.DefaultPage.META_LIST,\n    ):\n        \"\"\"Define a custom 404 page for any url having no match.\n\n        If there is no page defined on 'index' route, add the 404 page to it.\n        If there is no global catchall defined, add the 404 page with a catchall.\n\n        Args:\n            component: The component to display at the page.\n            title: The title of the page.\n            description: The description of the page.\n            image: The image to display on the page.\n            on_load: The event handler(s) that will be called each time the page load.\n            meta: The metadata of the page.\n        \"\"\"\n        if component is None:\n            component = Default404Page.create()\n        self.add_page(\n            component=wait_for_client_redirect(self._generate_component(component)),\n            route=constants.Page404.SLUG,\n            title=title or constants.Page404.TITLE,\n            image=image or constants.Page404.IMAGE,\n            description=description or constants.Page404.DESCRIPTION,\n            on_load=on_load,\n            meta=meta,\n        )\n\n    def _setup_admin_dash(self):\n        \"\"\"Setup the admin dash.\"\"\"\n        # Get the admin dash.\n        admin_dash = self.admin_dash\n\n        if admin_dash and admin_dash.models:\n            # Build the admin dashboard\n            admin = (\n                admin_dash.admin\n                if admin_dash.admin\n                else Admin(\n                    engine=Model.get_db_engine(),\n                    title=\"Reflex Admin Dashboard\",\n                    logo_url=\"https://reflex.dev/Reflex.svg\",\n                )\n            )\n\n            for model in admin_dash.models:\n                view = admin_dash.view_overrides.get(model, ModelView)\n                admin.add_view(view(model))\n\n            admin.mount_to(self.api)\n\n    def _get_frontend_packages(self, imports: Dict[str, set[ImportVar]]):\n        \"\"\"Gets the frontend packages to be installed and filters out the unnecessary ones.\n\n        Args:\n            imports: A dictionary containing the imports used in the current page.\n\n        Example:\n            >>> _get_frontend_packages({\"react\": \"16.14.0\", \"react-dom\": \"16.14.0\"})\n        \"\"\"\n        page_imports = {\n            i\n            for i, tags in imports.items()\n            if i not in constants.PackageJson.DEPENDENCIES\n            and i not in constants.PackageJson.DEV_DEPENDENCIES\n            and not any(i.startswith(prefix) for prefix in [\"/\", \".\", \"next/\"])\n            and i != \"\"\n            and any(tag.install for tag in tags)\n        }\n        frontend_packages = get_config().frontend_packages\n        _frontend_packages = []\n        for package in frontend_packages:\n            if package in (get_config().tailwind or {}).get(\"plugins\", []):  # type: ignore\n                console.warn(\n                    f\"Tailwind packages are inferred from 'plugins', remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            if package in page_imports:\n                console.warn(\n                    f\"React packages and their dependencies are inferred from Component.library and Component.lib_dependencies, remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            _frontend_packages.append(package)\n        page_imports.update(_frontend_packages)\n        prerequisites.install_frontend_packages(page_imports, get_config())\n\n    def _app_root(self, app_wrappers: dict[tuple[int, str], Component]) -> Component:\n        for component in tuple(app_wrappers.values()):\n            app_wrappers.update(component._get_all_app_wrap_components())\n        order = sorted(app_wrappers, key=lambda k: k[0], reverse=True)\n        root = parent = copy.deepcopy(app_wrappers[order[0]])\n        for key in order[1:]:\n            child = copy.deepcopy(app_wrappers[key])\n            parent.children.append(child)\n            parent = child\n        return root\n\n    def _should_compile(self) -> bool:\n        \"\"\"Check if the app should be compiled.\n\n        Returns:\n            Whether the app should be compiled.\n        \"\"\"\n        # Check the environment variable.\n        if should_skip_compile():\n            return False\n\n        nocompile = prerequisites.get_web_dir() / constants.NOCOMPILE_FILE\n\n        # Check the nocompile file.\n        if nocompile.exists():\n            # Delete the nocompile file\n            nocompile.unlink()\n            return False\n\n        # By default, compile the app.\n        return True\n\n    def _add_overlay_to_component(self, component: Component) -> Component:\n        if self.overlay_component is None:\n            return component\n\n        children = component.children\n        overlay_component = self._generate_component(self.overlay_component)\n\n        if children[0] == overlay_component:\n            return component\n\n        # recreate OverlayFragment with overlay_component as first child\n        component = OverlayFragment.create(overlay_component, *children)\n\n        return component\n\n\n\n\n\n\n\n\n    def _apply_decorated_pages(self):\n        \"\"\"Add @rx.page decorated pages to the app.\n\n        This has to be done in the MainThread for py38 and py39 compatibility, so the\n        decorated pages are added to the app before the app is compiled (in a thread)\n        to workaround REF-2172.\n\n        This can move back into `compile_` when py39 support is dropped.\n        \"\"\"\n        # Add the @rx.page decorated pages to collect on_load events.\n        for render, kwargs in DECORATED_PAGES[get_config().app_name]:\n            self.add_page(render, **kwargs)\n\n    def _validate_var_dependencies(\n        self, state: Optional[Type[BaseState]] = None\n    ) -> None:\n        \"\"\"Validate the dependencies of the vars in the app.\n\n        Args:\n            state: The state to validate the dependencies for.\n\n        Raises:\n            VarDependencyError: When a computed var has an invalid dependency.\n        \"\"\"\n        if not self.state:\n            return\n\n        if not state:\n            state = self.state\n\n        for var in state.computed_vars.values():\n            if not var._cache:\n                continue\n            deps = var._deps(objclass=state)\n            for dep in deps:\n                if dep not in state.vars and dep not in state.backend_vars:\n                    raise exceptions.VarDependencyError(\n                        f\"ComputedVar {var._var_name} on state {state.__name__} has an invalid dependency {dep}\"\n                    )\n\n        for substate in state.class_subclasses:\n            self._validate_var_dependencies(substate)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @contextlib.asynccontextmanager\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _process_background(\n        self, state: BaseState, event: Event\n    ) -> asyncio.Task | None:\n        \"\"\"Process an event in the background and emit updates as they arrive.\n\n        Args:\n            state: The state to process the event for.\n            event: The event to process.\n\n        Returns:\n            Task if the event was backgroundable, otherwise None\n        \"\"\"\n        substate, handler = state._get_event_handler(event)\n        if not handler.is_background:\n            return None\n\n        async def _coro():\n            \"\"\"Coroutine to process the event and emit updates inside an asyncio.Task.\n\n            Raises:\n                RuntimeError: If the app has not been initialized yet.\n            \"\"\"\n            if self.event_namespace is None:\n                raise RuntimeError(\"App has not been initialized yet.\")\n\n            # Process the event.\n            async for update in state._process_event(\n                handler=handler, state=substate, payload=event.payload\n            ):\n                # Postprocess the event.\n                update = await self._postprocess(state, event, update)\n\n                # Send the update to the client.\n                await self.event_namespace.emit_update(\n                    update=update,\n                    sid=state.router.session.session_id,\n                )\n\n        task = asyncio.create_task(_coro())\n        self.background_tasks.add(task)\n        # Clean up task from background_tasks set when complete.\n        task.add_done_callback(self.background_tasks.discard)\n        return task\n\n\nasync def process(\n    app: App, event: Event, sid: str, headers: Dict, client_ip: str\n) -> AsyncIterator[StateUpdate]:\n    \"\"\"Process an event.\n\n    Args:\n        app: The app to process the event for.\n        event: The event to process.\n        sid: The Socket.IO session id.\n        headers: The client headers.\n        client_ip: The client_ip.\n\n    Raises:\n        Exception: If a reflex specific error occurs during processing the event.\n\n    Yields:\n        The state updates after processing the event.\n    \"\"\"\n    from reflex.utils import telemetry\n\n    try:\n        # Add request data to the state.\n        router_data = event.router_data\n        router_data.update(\n            {\n                constants.RouteVar.QUERY: format.format_query_params(event.router_data),\n                constants.RouteVar.CLIENT_TOKEN: event.token,\n                constants.RouteVar.SESSION_ID: sid,\n                constants.RouteVar.HEADERS: headers,\n                constants.RouteVar.CLIENT_IP: client_ip,\n            }\n        )\n        # Get the state for the session exclusively.\n        async with app.state_manager.modify_state(event.substate_token) as state:\n            # re-assign only when the value is different\n            if state.router_data != router_data:\n                # assignment will recurse into substates and force recalculation of\n                # dependent ComputedVar (dynamic route variables)\n                state.router_data = router_data\n                state.router = RouterData(router_data)\n\n            # Preprocess the event.\n            update = await app._preprocess(state, event)\n\n            # If there was an update, yield it.\n            if update is not None:\n                yield update\n\n            # Only process the event if there is no update.\n            else:\n                if app._process_background(state, event) is not None:\n                    # `final=True` allows the frontend send more events immediately.\n                    yield StateUpdate(final=True)\n                    return\n\n                # Process the event synchronously.\n                async for update in state._process(event):\n                    # Postprocess the event.\n                    update = await app._postprocess(state, event, update)\n\n                    # Yield the\n```\n# Function Signature: def _compile(self, export: bool = False):\n# Natural Language Annotations: \"\"\"Compile the app and output it to the pages folder.\n\n        Args:\n            export: Whether to compile the app for export.\n\n        Raises:\n            ReflexRuntimeError: When any page uses state, but no rx.State subclass is defined.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```python\nimport asyncio\nimport concurrent.futures\n", "  ```\ndef _compile(self, export: bool = False):\n", "  ```python\ndef _compile(self, export=False):\n   "]}, "reflex-reflex/app.py-modify_state": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/app.py:\n```\n\"\"\"The main Reflex app.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport concurrent.futures\nimport contextlib\nimport copy\nimport functools\nimport io\nimport multiprocessing\nimport os\nimport platform\nimport sys\nfrom datetime import datetime\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n    get_args,\n    get_type_hints,\n)\n\nfrom fastapi import FastAPI, HTTPException, Request, UploadFile\nfrom fastapi.middleware import cors\nfrom fastapi.responses import StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom rich.progress import MofNCompleteColumn, Progress, TimeElapsedColumn\nfrom socketio import ASGIApp, AsyncNamespace, AsyncServer\nfrom starlette_admin.contrib.sqla.admin import Admin\nfrom starlette_admin.contrib.sqla.view import ModelView\n\nfrom reflex import constants\nfrom reflex.admin import AdminDash\nfrom reflex.app_mixins import AppMixin, LifespanMixin, MiddlewareMixin\nfrom reflex.base import Base\nfrom reflex.compiler import compiler\nfrom reflex.compiler import utils as compiler_utils\nfrom reflex.compiler.compiler import ExecutorSafeFunctions\nfrom reflex.components.base.app_wrap import AppWrap\nfrom reflex.components.base.fragment import Fragment\nfrom reflex.components.component import (\n    Component,\n    ComponentStyle,\n    evaluate_style_namespaces,\n)\nfrom reflex.components.core.banner import connection_pulser, connection_toaster\nfrom reflex.components.core.breakpoints import set_breakpoints\nfrom reflex.components.core.client_side_routing import (\n    Default404Page,\n    wait_for_client_redirect,\n)\nfrom reflex.components.core.upload import Upload, get_upload_dir\nfrom reflex.components.radix import themes\nfrom reflex.config import get_config\nfrom reflex.event import Event, EventHandler, EventSpec\nfrom reflex.model import Model\nfrom reflex.page import (\n    DECORATED_PAGES,\n)\nfrom reflex.route import (\n    get_route_args,\n    replace_brackets_with_keywords,\n    verify_route_validity,\n)\nfrom reflex.state import (\n    BaseState,\n    RouterData,\n    State,\n    StateManager,\n    StateUpdate,\n    _substate_key,\n    code_uses_state_contexts,\n)\nfrom reflex.utils import codespaces, console, exceptions, format, prerequisites, types\nfrom reflex.utils.exec import is_prod_mode, is_testing_env, should_skip_compile\nfrom reflex.utils.imports import ImportVar\n\n# Define custom types.\nComponentCallable = Callable[[], Component]\nReducer = Callable[[Event], Coroutine[Any, Any, StateUpdate]]\n\n\ndef default_overlay_component() -> Component:\n    \"\"\"Default overlay_component attribute for App.\n\n    Returns:\n        The default overlay_component, which is a connection_modal.\n    \"\"\"\n    return Fragment.create(\n        connection_pulser(),\n        connection_toaster(),\n        *codespaces.codespaces_auto_redirect(),\n    )\n\n\nclass OverlayFragment(Fragment):\n    \"\"\"Alias for Fragment, used to wrap the overlay_component.\"\"\"\n\n    pass\n\n\nclass App(MiddlewareMixin, LifespanMixin, Base):\n    \"\"\"The main Reflex app that encapsulates the backend and frontend.\n\n    Every Reflex app needs an app defined in its main module.\n\n    ```python\n    # app.py\n    import reflex as rx\n\n    # Define state and pages\n    ...\n\n    app = rx.App(\n        # Set global level style.\n        style={...},\n        # Set the top level theme.\n        theme=rx.theme(accent_color=\"blue\"),\n    )\n    ```\n    \"\"\"\n\n    # The global [theme](https://reflex.dev/docs/styling/theming/#theme) for the entire app.\n    theme: Optional[Component] = themes.theme(accent_color=\"blue\")\n\n    # The [global style](https://reflex.dev/docs/styling/overview/#global-styles}) for the app.\n    style: ComponentStyle = {}\n\n    # A list of URLs to [stylesheets](https://reflex.dev/docs/styling/custom-stylesheets/) to include in the app.\n    stylesheets: List[str] = []\n\n    # A component that is present on every page (defaults to the Connection Error banner).\n    overlay_component: Optional[Union[Component, ComponentCallable]] = (\n        default_overlay_component\n    )\n\n    # Components to add to the head of every page.\n    head_components: List[Component] = []\n\n    # The Socket.IO AsyncServer instance.\n    sio: Optional[AsyncServer] = None\n\n    # The language to add to the html root tag of every page.\n    html_lang: Optional[str] = None\n\n    # Attributes to add to the html root tag of every page.\n    html_custom_attrs: Optional[Dict[str, str]] = None\n\n    # A map from a page route to the component to render. Users should use `add_page`. PRIVATE.\n    pages: Dict[str, Component] = {}\n\n    # The backend API object. PRIVATE.\n    api: FastAPI = None  # type: ignore\n\n    # The state class to use for the app. PRIVATE.\n    state: Optional[Type[BaseState]] = None\n\n    # Class to manage many client states.\n    _state_manager: Optional[StateManager] = None\n\n    # Mapping from a route to event handlers to trigger when the page loads. PRIVATE.\n    load_events: Dict[str, List[Union[EventHandler, EventSpec]]] = {}\n\n    # Admin dashboard to view and manage the database. PRIVATE.\n    admin_dash: Optional[AdminDash] = None\n\n    # The async server name space. PRIVATE.\n    event_namespace: Optional[EventNamespace] = None\n\n    # Background tasks that are currently running. PRIVATE.\n    background_tasks: Set[asyncio.Task] = set()\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize the app.\n\n        Args:\n            **kwargs: Kwargs to initialize the app with.\n\n        Raises:\n            ValueError: If the event namespace is not provided in the config.\n                        Also, if there are multiple client subclasses of rx.BaseState(Subclasses of rx.BaseState should consist\n                        of the DefaultState and the client app state).\n        \"\"\"\n        if \"connect_error_component\" in kwargs:\n            raise ValueError(\n                \"`connect_error_component` is deprecated, use `overlay_component` instead\"\n            )\n        super().__init__(**kwargs)\n        base_state_subclasses = BaseState.__subclasses__()\n\n        # Special case to allow test cases have multiple subclasses of rx.BaseState.\n        if not is_testing_env() and len(base_state_subclasses) > 1:\n            # Only one Base State class is allowed.\n            raise ValueError(\n                \"rx.BaseState cannot be subclassed multiple times. use rx.State instead\"\n            )\n\n        if \"breakpoints\" in self.style:\n            set_breakpoints(self.style.pop(\"breakpoints\"))\n\n        # Set up the API.\n        self.api = FastAPI(lifespan=self._run_lifespan_tasks)\n        self._add_cors()\n        self._add_default_endpoints()\n\n        for clz in App.__mro__:\n            if clz == App:\n                continue\n            if issubclass(clz, AppMixin):\n                clz._init_mixin(self)\n\n        self._setup_state()\n\n        # Set up the admin dash.\n        self._setup_admin_dash()\n\n        if sys.platform == \"win32\" and not is_prod_mode():\n            # Hack to fix Windows hot reload issue.\n            from reflex.utils.compat import windows_hot_reload_lifespan_hack\n\n            self.register_lifespan_task(windows_hot_reload_lifespan_hack)\n\n\n\n\n\n\n\n    def _setup_state(self) -> None:\n        \"\"\"Set up the state for the app.\n\n        Raises:\n            RuntimeError: If the socket server is invalid.\n        \"\"\"\n        if not self.state:\n            return\n\n        config = get_config()\n\n        # Set up the state manager.\n        self._state_manager = StateManager.create(state=self.state)\n\n        # Set up the Socket.IO AsyncServer.\n        if not self.sio:\n            self.sio = AsyncServer(\n                async_mode=\"asgi\",\n                cors_allowed_origins=(\n                    \"*\"\n                    if config.cors_allowed_origins == [\"*\"]\n                    else config.cors_allowed_origins\n                ),\n                cors_credentials=True,\n                max_http_buffer_size=constants.POLLING_MAX_HTTP_BUFFER_SIZE,\n                ping_interval=constants.Ping.INTERVAL,\n                ping_timeout=constants.Ping.TIMEOUT,\n            )\n        elif getattr(self.sio, \"async_mode\", \"\") != \"asgi\":\n            raise RuntimeError(\n                f\"Custom `sio` must use `async_mode='asgi'`, not '{self.sio.async_mode}'.\"\n            )\n\n        # Create the socket app. Note event endpoint constant replaces the default 'socket.io' path.\n        socket_app = ASGIApp(self.sio, socketio_path=\"\")\n        namespace = config.get_event_namespace()\n\n        # Create the event namespace and attach the main app. Not related to any paths.\n        self.event_namespace = EventNamespace(namespace, self)\n\n        # Register the event namespace with the socket.\n        self.sio.register_namespace(self.event_namespace)\n        # Mount the socket app with the API.\n        self.api.mount(str(constants.Endpoint.EVENT), socket_app)\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the app.\n\n        Returns:\n            The string representation of the app.\n        \"\"\"\n        return f\"<App state={self.state.__name__ if self.state else None}>\"\n\n    def __call__(self) -> FastAPI:\n        \"\"\"Run the backend api instance.\n\n        Returns:\n            The backend api.\n        \"\"\"\n        return self.api\n\n    def _add_default_endpoints(self):\n        \"\"\"Add default api endpoints (ping).\"\"\"\n        # To test the server.\n        self.api.get(str(constants.Endpoint.PING))(ping)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _add_cors(self):\n        \"\"\"Add CORS middleware to the app.\"\"\"\n        self.api.add_middleware(\n            cors.CORSMiddleware,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n            allow_origins=[\"*\"],\n        )\n\n    @property\n    def state_manager(self) -> StateManager:\n        \"\"\"Get the state manager.\n\n        Returns:\n            The initialized state manager.\n\n        Raises:\n            ValueError: if the state has not been initialized.\n        \"\"\"\n        if self._state_manager is None:\n            raise ValueError(\"The state manager has not been initialized.\")\n        return self._state_manager\n\n    @staticmethod\n    def _generate_component(component: Component | ComponentCallable) -> Component:\n        \"\"\"Generate a component from a callable.\n\n        Args:\n            component: The component function to call or Component to return as-is.\n\n        Returns:\n            The generated component.\n\n        Raises:\n            VarOperationTypeError: When an invalid component var related function is passed.\n            TypeError: When an invalid component function is passed.\n            exceptions.MatchTypeError: If the return types of match cases in rx.match are different.\n        \"\"\"\n        from reflex.utils.exceptions import VarOperationTypeError\n\n        try:\n            return component if isinstance(component, Component) else component()\n        except exceptions.MatchTypeError:\n            raise\n        except TypeError as e:\n            message = str(e)\n            if \"BaseVar\" in message or \"ComputedVar\" in message:\n                raise VarOperationTypeError(\n                    \"You may be trying to use an invalid Python function on a state var. \"\n                    \"When referencing a var inside your render code, only limited var operations are supported. \"\n                    \"See the var operation docs here: https://reflex.dev/docs/vars/var-operations/\"\n                ) from e\n            raise e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_load_events(self, route: str) -> list[EventHandler | EventSpec]:\n        \"\"\"Get the load events for a route.\n\n        Args:\n            route: The route to get the load events for.\n\n        Returns:\n            The load events for the route.\n        \"\"\"\n        route = route.lstrip(\"/\")\n        if route == \"\":\n            route = constants.PageNames.INDEX_ROUTE\n        return self.load_events.get(route, [])\n\n    def _check_routes_conflict(self, new_route: str):\n        \"\"\"Verify if there is any conflict between the new route and any existing route.\n\n        Based on conflicts that NextJS would throw if not intercepted.\n\n        Raises:\n            RouteValueError: exception showing which conflict exist with the route to be added\n\n        Args:\n            new_route: the route being newly added.\n        \"\"\"\n        from reflex.utils.exceptions import RouteValueError\n\n        if \"[\" not in new_route:\n            return\n\n        segments = (\n            constants.RouteRegex.SINGLE_SEGMENT,\n            constants.RouteRegex.DOUBLE_SEGMENT,\n            constants.RouteRegex.SINGLE_CATCHALL_SEGMENT,\n            constants.RouteRegex.DOUBLE_CATCHALL_SEGMENT,\n        )\n        for route in self.pages:\n            replaced_route = replace_brackets_with_keywords(route)\n            for rw, r, nr in zip(\n                replaced_route.split(\"/\"), route.split(\"/\"), new_route.split(\"/\")\n            ):\n                if rw in segments and r != nr:\n                    # If the slugs in the segments of both routes are not the same, then the route is invalid\n                    raise RouteValueError(\n                        f\"You cannot use different slug names for the same dynamic path in  {route} and {new_route} ('{r}' != '{nr}')\"\n                    )\n                elif rw not in segments and r != nr:\n                    # if the section being compared in both routes is not a dynamic segment(i.e not wrapped in brackets)\n                    # then we are guaranteed that the route is valid and there's no need checking the rest.\n                    # eg. /posts/[id]/info/[slug1] and /posts/[id]/info1/[slug1] is always going to be valid since\n                    # info1 will break away into its own tree.\n                    break\n\n    def add_custom_404_page(\n        self,\n        component: Component | ComponentCallable | None = None,\n        title: str = constants.Page404.TITLE,\n        image: str = constants.Page404.IMAGE,\n        description: str = constants.Page404.DESCRIPTION,\n        on_load: (\n            EventHandler | EventSpec | list[EventHandler | EventSpec] | None\n        ) = None,\n        meta: list[dict[str, str]] = constants.DefaultPage.META_LIST,\n    ):\n        \"\"\"Define a custom 404 page for any url having no match.\n\n        If there is no page defined on 'index' route, add the 404 page to it.\n        If there is no global catchall defined, add the 404 page with a catchall.\n\n        Args:\n            component: The component to display at the page.\n            title: The title of the page.\n            description: The description of the page.\n            image: The image to display on the page.\n            on_load: The event handler(s) that will be called each time the page load.\n            meta: The metadata of the page.\n        \"\"\"\n        if component is None:\n            component = Default404Page.create()\n        self.add_page(\n            component=wait_for_client_redirect(self._generate_component(component)),\n            route=constants.Page404.SLUG,\n            title=title or constants.Page404.TITLE,\n            image=image or constants.Page404.IMAGE,\n            description=description or constants.Page404.DESCRIPTION,\n            on_load=on_load,\n            meta=meta,\n        )\n\n    def _setup_admin_dash(self):\n        \"\"\"Setup the admin dash.\"\"\"\n        # Get the admin dash.\n        admin_dash = self.admin_dash\n\n        if admin_dash and admin_dash.models:\n            # Build the admin dashboard\n            admin = (\n                admin_dash.admin\n                if admin_dash.admin\n                else Admin(\n                    engine=Model.get_db_engine(),\n                    title=\"Reflex Admin Dashboard\",\n                    logo_url=\"https://reflex.dev/Reflex.svg\",\n                )\n            )\n\n            for model in admin_dash.models:\n                view = admin_dash.view_overrides.get(model, ModelView)\n                admin.add_view(view(model))\n\n            admin.mount_to(self.api)\n\n    def _get_frontend_packages(self, imports: Dict[str, set[ImportVar]]):\n        \"\"\"Gets the frontend packages to be installed and filters out the unnecessary ones.\n\n        Args:\n            imports: A dictionary containing the imports used in the current page.\n\n        Example:\n            >>> _get_frontend_packages({\"react\": \"16.14.0\", \"react-dom\": \"16.14.0\"})\n        \"\"\"\n        page_imports = {\n            i\n            for i, tags in imports.items()\n            if i not in constants.PackageJson.DEPENDENCIES\n            and i not in constants.PackageJson.DEV_DEPENDENCIES\n            and not any(i.startswith(prefix) for prefix in [\"/\", \".\", \"next/\"])\n            and i != \"\"\n            and any(tag.install for tag in tags)\n        }\n        frontend_packages = get_config().frontend_packages\n        _frontend_packages = []\n        for package in frontend_packages:\n            if package in (get_config().tailwind or {}).get(\"plugins\", []):  # type: ignore\n                console.warn(\n                    f\"Tailwind packages are inferred from 'plugins', remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            if package in page_imports:\n                console.warn(\n                    f\"React packages and their dependencies are inferred from Component.library and Component.lib_dependencies, remove `{package}` from `frontend_packages`\"\n                )\n                continue\n            _frontend_packages.append(package)\n        page_imports.update(_frontend_packages)\n        prerequisites.install_frontend_packages(page_imports, get_config())\n\n    def _app_root(self, app_wrappers: dict[tuple[int, str], Component]) -> Component:\n        for component in tuple(app_wrappers.values()):\n            app_wrappers.update(component._get_all_app_wrap_components())\n        order = sorted(app_wrappers, key=lambda k: k[0], reverse=True)\n        root = parent = copy.deepcopy(app_wrappers[order[0]])\n        for key in order[1:]:\n            child = copy.deepcopy(app_wrappers[key])\n            parent.children.append(child)\n            parent = child\n        return root\n\n    def _should_compile(self) -> bool:\n        \"\"\"Check if the app should be compiled.\n\n        Returns:\n            Whether the app should be compiled.\n        \"\"\"\n        # Check the environment variable.\n        if should_skip_compile():\n            return False\n\n        nocompile = prerequisites.get_web_dir() / constants.NOCOMPILE_FILE\n\n        # Check the nocompile file.\n        if nocompile.exists():\n            # Delete the nocompile file\n            nocompile.unlink()\n            return False\n\n        # By default, compile the app.\n        return True\n\n    def _add_overlay_to_component(self, component: Component) -> Component:\n        if self.overlay_component is None:\n            return component\n\n        children = component.children\n        overlay_component = self._generate_component(self.overlay_component)\n\n        if children[0] == overlay_component:\n            return component\n\n        # recreate OverlayFragment with overlay_component as first child\n        component = OverlayFragment.create(overlay_component, *children)\n\n        return component\n\n\n\n\n\n\n\n\n    def _apply_decorated_pages(self):\n        \"\"\"Add @rx.page decorated pages to the app.\n\n        This has to be done in the MainThread for py38 and py39 compatibility, so the\n        decorated pages are added to the app before the app is compiled (in a thread)\n        to workaround REF-2172.\n\n        This can move back into `compile_` when py39 support is dropped.\n        \"\"\"\n        # Add the @rx.page decorated pages to collect on_load events.\n        for render, kwargs in DECORATED_PAGES[get_config().app_name]:\n            self.add_page(render, **kwargs)\n\n    def _validate_var_dependencies(\n        self, state: Optional[Type[BaseState]] = None\n    ) -> None:\n        \"\"\"Validate the dependencies of the vars in the app.\n\n        Args:\n            state: The state to validate the dependencies for.\n\n        Raises:\n            VarDependencyError: When a computed var has an invalid dependency.\n        \"\"\"\n        if not self.state:\n            return\n\n        if not state:\n            state = self.state\n\n        for var in state.computed_vars.values():\n            if not var._cache:\n                continue\n            deps = var._deps(objclass=state)\n            for dep in deps:\n                if dep not in state.vars and dep not in state.backend_vars:\n                    raise exceptions.VarDependencyError(\n                        f\"ComputedVar {var._var_name} on state {state.__name__} has an invalid dependency {dep}\"\n                    )\n\n        for substate in state.class_subclasses:\n            self._validate_var_dependencies(substate)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @contextlib.asynccontextmanager\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _process_background(\n        self, state: BaseState, event: Event\n    ) -> asyncio.Task | None:\n        \"\"\"Process an event in the background and emit updates as they arrive.\n\n        Args:\n            state: The state to process the event for.\n            event: The event to process.\n\n        Returns:\n            Task if the event was backgroundable, otherwise None\n        \"\"\"\n        substate, handler = state._get_event_handler(event)\n        if not handler.is_background:\n            return None\n\n        async def _coro():\n            \"\"\"Coroutine to process the event and emit updates inside an asyncio.Task.\n\n            Raises:\n                RuntimeError: If the app has not been initialized yet.\n            \"\"\"\n            if self.event_namespace is None:\n                raise RuntimeError(\"App has not been initialized yet.\")\n\n            # Process the event.\n            async for update in state._process_event(\n                handler=handler, state=substate, payload=event.payload\n            ):\n                # Postprocess the event.\n                update = await self._postprocess(state, event, update)\n\n                # Send the update to the client.\n                await self.event_namespace.emit_update(\n                    update=update,\n                    sid=state.router.session.session_id,\n                )\n\n        task = asyncio.create_task(_coro())\n        self.background_tasks.add(task)\n        # Clean up task from background_tasks set when complete.\n        task.add_done_callback(self.background_tasks.discard)\n        return task\n\n\nasync def process(\n    app: App, event: Event, sid: str, headers: Dict, client_ip: str\n) -> AsyncIterator[StateUpdate]:\n    \"\"\"Process an event.\n\n    Args:\n        app: The app to process the event for.\n        event: The event to process.\n        sid: The Socket.IO session id.\n        headers: The client headers.\n        client_ip: The client_ip.\n\n    Raises:\n        Exception: If a reflex specific error occurs during processing the event.\n\n    Yields:\n        The state updates after processing the event.\n    \"\"\"\n    from reflex.utils import telemetry\n\n    try:\n        # Add request data to the state.\n        router_data = event.router_data\n        router_data.update(\n            {\n                constants.RouteVar.QUERY: format.format_query_params(event.router_data),\n                constants.RouteVar.CLIENT_TOKEN: event.token,\n                constants.RouteVar.SESSION_ID: sid,\n                constants.RouteVar.HEADERS: headers,\n                constants.RouteVar.CLIENT_IP: client_ip,\n            }\n        )\n        # Get the state for the session exclusively.\n        async with app.state_manager.modify_state(event.substate_token) as state:\n            # re-assign only when the value is different\n            if state.router_data != router_data:\n                # assignment will recurse into substates and force recalculation of\n                # dependent ComputedVar (dynamic route variables)\n                state.router_data = router_data\n                state.router = RouterData(router_data)\n\n            # Preprocess the event.\n            update = await app._preprocess(state, event)\n\n            # If there was an update, yield it.\n            if update is not None:\n                yield update\n\n            # Only process the event if there is no update.\n            else:\n                if app._process_background(state, event) is not None:\n                    # `final=True` allows the frontend send more events immediately.\n                    yield StateUpdate(final=True)\n                    return\n\n                # Process the event synchronously.\n                async for update in state._process(event):\n                    # Postprocess the event.\n                    update = await app._postprocess(state, event, update)\n\n                    # Yield the\n```\n# Function Signature: async def modify_state(self, token: str) -> AsyncIterator[BaseState]:\n# Natural Language Annotations: \"\"\"Modify the state out of band.\n\n        Args:\n            token: The token to modify the state for.\n\n        Yields:\n            The state to modify.\n\n        Raises:\n            RuntimeError: If the app has not been initialized yet.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def modify_state(self, token: str) ->", "  ```\nasync def modify_state(self, token: str) ->", "  ```\nasync def modify_state(self, token: str) ->"]}, "reflex-reflex/vars.py-create": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/vars.py:\n```\n\"\"\"Define a state var.\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport dataclasses\nimport datetime\nimport dis\nimport functools\nimport inspect\nimport json\nimport random\nimport re\nimport string\nimport sys\nfrom types import CodeType, FunctionType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    _GenericAlias,  # type: ignore\n    cast,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.utils import console, imports, serializers, types\nfrom reflex.utils.exceptions import (\n    VarAttributeError,\n    VarDependencyError,\n    VarTypeError,\n    VarValueError,\n)\n\n# This module used to export ImportVar itself, so we still import it for export here\nfrom reflex.utils.imports import (\n    ImportDict,\n    ImportVar,\n    ParsedImportDict,\n    parse_imports,\n)\nfrom reflex.utils.types import override\n\nif TYPE_CHECKING:\n    from reflex.state import BaseState\n\n\n# Set of unique variable names.\nUSED_VARIABLES = set()\n\n# Supported operators for all types.\nALL_OPS = [\"==\", \"!=\", \"!==\", \"===\", \"&&\", \"||\"]\n# Delimiters used between function args or operands.\nDELIMITERS = [\",\"]\n# Mapping of valid operations for different type combinations.\nOPERATION_MAPPING = {\n    (int, int): {\n        \"+\",\n        \"-\",\n        \"/\",\n        \"//\",\n        \"*\",\n        \"%\",\n        \"**\",\n        \">\",\n        \"<\",\n        \"<=\",\n        \">=\",\n        \"|\",\n        \"&\",\n    },\n    (int, str): {\"*\"},\n    (int, list): {\"*\"},\n    (str, str): {\"+\", \">\", \"<\", \"<=\", \">=\"},\n    (float, float): {\"+\", \"-\", \"/\", \"//\", \"*\", \"%\", \"**\", \">\", \"<\", \"<=\", \">=\"},\n    (float, int): {\"+\", \"-\", \"/\", \"//\", \"*\", \"%\", \"**\", \">\", \"<\", \"<=\", \">=\"},\n    (list, list): {\"+\", \">\", \"<\", \"<=\", \">=\"},\n}\n\n# These names were changed in reflex 0.3.0\nREPLACED_NAMES = {\n    \"full_name\": \"_var_full_name\",\n    \"name\": \"_var_name\",\n    \"state\": \"_var_data.state\",\n    \"type_\": \"_var_type\",\n    \"is_local\": \"_var_is_local\",\n    \"is_string\": \"_var_is_string\",\n    \"set_state\": \"_var_set_state\",\n    \"deps\": \"_deps\",\n}\n\nPYTHON_JS_TYPE_MAP = {\n    (int, float): \"number\",\n    (str,): \"string\",\n    (bool,): \"boolean\",\n    (list, tuple): \"Array\",\n    (dict,): \"Object\",\n    (None,): \"null\",\n}\n\n\ndef get_unique_variable_name() -> str:\n    \"\"\"Get a unique variable name.\n\n    Returns:\n        The unique variable name.\n    \"\"\"\n    name = \"\".join([random.choice(string.ascii_lowercase) for _ in range(8)])\n    if name not in USED_VARIABLES:\n        USED_VARIABLES.add(name)\n        return name\n    return get_unique_variable_name()\n\n\nclass VarData(Base):\n    \"\"\"Metadata associated with a Var.\"\"\"\n\n    # The name of the enclosing state.\n    state: str = \"\"\n\n    # Imports needed to render this var\n    imports: ParsedImportDict = {}\n\n    # Hooks that need to be present in the component to render this var\n    hooks: Dict[str, None] = {}\n\n    # Positions of interpolated strings. This is used by the decoder to figure\n    # out where the interpolations are and only escape the non-interpolated\n    # segments.\n    interpolations: List[Tuple[int, int]] = []\n\n    def __init__(\n        self, imports: Union[ImportDict, ParsedImportDict] | None = None, **kwargs: Any\n    ):\n        \"\"\"Initialize the var data.\n\n        Args:\n            imports: The imports needed to render this var.\n            **kwargs: The var data fields.\n        \"\"\"\n        if imports:\n            kwargs[\"imports\"] = parse_imports(imports)\n        super().__init__(**kwargs)\n\n    @classmethod\n    def merge(cls, *others: VarData | None) -> VarData | None:\n        \"\"\"Merge multiple var data objects.\n\n        Args:\n            *others: The var data objects to merge.\n\n        Returns:\n            The merged var data object.\n        \"\"\"\n        state = \"\"\n        _imports = {}\n        hooks = {}\n        interpolations = []\n        for var_data in others:\n            if var_data is None:\n                continue\n            state = state or var_data.state\n            _imports = imports.merge_imports(_imports, var_data.imports)\n            hooks.update(var_data.hooks)\n            interpolations += var_data.interpolations\n\n        return (\n            cls(\n                state=state,\n                imports=_imports,\n                hooks=hooks,\n                interpolations=interpolations,\n            )\n            or None\n        )\n\n    def __bool__(self) -> bool:\n        \"\"\"Check if the var data is non-empty.\n\n        Returns:\n            True if any field is set to a non-default value.\n        \"\"\"\n        return bool(self.state or self.imports or self.hooks or self.interpolations)\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\"Check if two var data objects are equal.\n\n        Args:\n            other: The other var data object to compare.\n\n        Returns:\n            True if all fields are equal and collapsed imports are equal.\n        \"\"\"\n        if not isinstance(other, VarData):\n            return False\n\n        # Don't compare interpolations - that's added in by the decoder, and\n        # not part of the vardata itself.\n        return (\n            self.state == other.state\n            and self.hooks.keys() == other.hooks.keys()\n            and imports.collapse_imports(self.imports)\n            == imports.collapse_imports(other.imports)\n        )\n\n    def dict(self) -> dict:\n        \"\"\"Convert the var data to a dictionary.\n\n        Returns:\n            The var data dictionary.\n        \"\"\"\n        return {\n            \"state\": self.state,\n            \"interpolations\": list(self.interpolations),\n            \"imports\": {\n                lib: [import_var.dict() for import_var in import_vars]\n                for lib, import_vars in self.imports.items()\n            },\n            \"hooks\": self.hooks,\n        }\n\n\ndef _encode_var(value: Var) -> str:\n    \"\"\"Encode the state name into a formatted var.\n\n    Args:\n        value: The value to encode the state name into.\n\n    Returns:\n        The encoded var.\n    \"\"\"\n    if value._var_data:\n        from reflex.utils.serializers import serialize\n\n        final_value = str(value)\n        data = value._var_data.dict()\n        data[\"string_length\"] = len(final_value)\n        data_json = value._var_data.__config__.json_dumps(data, default=serialize)\n\n        return (\n            f\"{constants.REFLEX_VAR_OPENING_TAG}{data_json}{constants.REFLEX_VAR_CLOSING_TAG}\"\n            + final_value\n        )\n\n    return str(value)\n\n\n# Compile regex for finding reflex var tags.\n_decode_var_pattern_re = (\n    rf\"{constants.REFLEX_VAR_OPENING_TAG}(.*?){constants.REFLEX_VAR_CLOSING_TAG}\"\n)\n_decode_var_pattern = re.compile(_decode_var_pattern_re, flags=re.DOTALL)\n\n\ndef _decode_var(value: str) -> tuple[VarData | None, str]:\n    \"\"\"Decode the state name from a formatted var.\n\n    Args:\n        value: The value to extract the state name from.\n\n    Returns:\n        The extracted state name and the value without the state name.\n    \"\"\"\n    var_datas = []\n    if isinstance(value, str):\n        # fast path if there is no encoded VarData\n        if constants.REFLEX_VAR_OPENING_TAG not in value:\n            return None, value\n\n        offset = 0\n\n        # Initialize some methods for reading json.\n        var_data_config = VarData().__config__\n\n        def json_loads(s):\n            try:\n                return var_data_config.json_loads(s)\n            except json.decoder.JSONDecodeError:\n                return var_data_config.json_loads(var_data_config.json_loads(f'\"{s}\"'))\n\n        # Find all tags.\n        while m := _decode_var_pattern.search(value):\n            start, end = m.span()\n            value = value[:start] + value[end:]\n\n            # Read the JSON, pull out the string length, parse the rest as VarData.\n            data = json_loads(m.group(1))\n            string_length = data.pop(\"string_length\", None)\n            var_data = VarData.parse_obj(data)\n\n            # Use string length to compute positions of interpolations.\n            if string_length is not None:\n                realstart = start + offset\n                var_data.interpolations = [(realstart, realstart + string_length)]\n\n            var_datas.append(var_data)\n            offset += end - start\n\n    return VarData.merge(*var_datas) if var_datas else None, value\n\n\ndef _extract_var_data(value: Iterable) -> list[VarData | None]:\n    \"\"\"Extract the var imports and hooks from an iterable containing a Var.\n\n    Args:\n        value: The iterable to extract the VarData from\n\n    Returns:\n        The extracted VarDatas.\n    \"\"\"\n    from reflex.style import Style\n\n    var_datas = []\n    with contextlib.suppress(TypeError):\n        for sub in value:\n            if isinstance(sub, Var):\n                var_datas.append(sub._var_data)\n            elif not isinstance(sub, str):\n                # Recurse into dict values.\n                if hasattr(sub, \"values\") and callable(sub.values):\n                    var_datas.extend(_extract_var_data(sub.values()))\n                # Recurse into iterable values (or dict keys).\n                var_datas.extend(_extract_var_data(sub))\n\n    # Style objects should already have _var_data.\n    if isinstance(value, Style):\n        var_datas.append(value._var_data)\n    else:\n        # Recurse when value is a dict itself.\n        values = getattr(value, \"values\", None)\n        if callable(values):\n            var_datas.extend(_extract_var_data(values()))\n    return var_datas\n\n\nclass Var:\n    \"\"\"An abstract var.\"\"\"\n\n    # The name of the var.\n    _var_name: str\n\n    # The type of the var.\n    _var_type: Type\n\n    # Whether this is a local javascript variable.\n    _var_is_local: bool\n\n    # Whether the var is a string literal.\n    _var_is_string: bool\n\n    # _var_full_name should be prefixed with _var_state\n    _var_full_name_needs_state_prefix: bool\n\n    # Extra metadata associated with the Var\n    _var_data: Optional[VarData]\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def __class_getitem__(cls, type_: str) -> _GenericAlias:\n        \"\"\"Get a typed var.\n\n        Args:\n            type_: The type of the var.\n\n        Returns:\n            The var class item.\n        \"\"\"\n        return _GenericAlias(cls, type_)\n\n    def __post_init__(self) -> None:\n        \"\"\"Post-initialize the var.\"\"\"\n        # Decode any inline Var markup and apply it to the instance\n        _var_data, _var_name = _decode_var(self._var_name)\n        if _var_data:\n            self._var_name = _var_name\n            self._var_data = VarData.merge(self._var_data, _var_data)\n\n    def _replace(self, merge_var_data=None, **kwargs: Any) -> BaseVar:\n        \"\"\"Make a copy of this Var with updated fields.\n\n        Args:\n            merge_var_data: VarData to merge into the existing VarData.\n            **kwargs: Var fields to update.\n\n        Returns:\n            A new BaseVar with the updated fields overwriting the corresponding fields in this Var.\n        \"\"\"\n        field_values = dict(\n            _var_name=kwargs.pop(\"_var_name\", self._var_name),\n            _var_type=kwargs.pop(\"_var_type\", self._var_type),\n            _var_is_local=kwargs.pop(\"_var_is_local\", self._var_is_local),\n            _var_is_string=kwargs.pop(\"_var_is_string\", self._var_is_string),\n            _var_full_name_needs_state_prefix=kwargs.pop(\n                \"_var_full_name_needs_state_prefix\",\n                self._var_full_name_needs_state_prefix,\n            ),\n            _var_data=VarData.merge(\n                kwargs.get(\"_var_data\", self._var_data), merge_var_data\n            ),\n        )\n        return BaseVar(**field_values)\n\n    def _decode(self) -> Any:\n        \"\"\"Decode Var as a python value.\n\n        Note that Var with state set cannot be decoded python-side and will be\n        returned as full_name.\n\n        Returns:\n            The decoded value or the Var name.\n        \"\"\"\n        if self._var_is_string:\n            return self._var_name\n        try:\n            return json.loads(self._var_name)\n        except ValueError:\n            return self._var_name\n\n    def equals(self, other: Var) -> bool:\n        \"\"\"Check if two vars are equal.\n\n        Args:\n            other: The other var to compare.\n\n        Returns:\n            Whether the vars are equal.\n        \"\"\"\n        return (\n            self._var_name == other._var_name\n            and self._var_type == other._var_type\n            and self._var_is_local == other._var_is_local\n            and self._var_full_name_needs_state_prefix\n            == other._var_full_name_needs_state_prefix\n            and self._var_data == other._var_data\n        )\n\n    def _merge(self, other) -> Var:\n        \"\"\"Merge two or more dicts.\n\n        Args:\n            other: The other var to merge.\n\n        Returns:\n            The merged var.\n        \"\"\"\n        if other is None:\n            return self._replace()\n        if not isinstance(other, Var):\n            other = Var.create(other, _var_is_string=False)\n        return self._replace(\n            _var_name=f\"{{...{self._var_name}, ...{other._var_name}}}\"  # type: ignore\n        )\n\n    def to_string(self, json: bool = True) -> Var:\n        \"\"\"Convert a var to a string.\n\n        Args:\n            json: Whether to convert to a JSON string.\n\n        Returns:\n            The stringified var.\n        \"\"\"\n        fn = \"JSON.stringify\" if json else \"String\"\n        return self.operation(fn=fn, type_=str)\n\n    def to_int(self) -> Var:\n        \"\"\"Convert a var to an int.\n\n        Returns:\n            The parseInt var.\n        \"\"\"\n        return self.operation(fn=\"parseInt\", type_=int)\n\n    def __hash__(self) -> int:\n        \"\"\"Define a hash function for a var.\n\n        Returns:\n            The hash of the var.\n        \"\"\"\n        return hash((self._var_name, str(self._var_type)))\n\n    def __str__(self) -> str:\n        \"\"\"Wrap the var so it can be used in templates.\n\n        Returns:\n            The wrapped var, i.e. {state.var}.\n        \"\"\"\n        from reflex.utils import format\n\n        out = (\n            self._var_full_name\n            if self._var_is_local\n            else format.wrap(self._var_full_name, \"{\")\n        )\n        if self._var_is_string:\n            out = format.format_string(out)\n        return out\n\n    def __bool__(self) -> bool:\n        \"\"\"Raise exception if using Var in a boolean context.\n\n        Raises:\n            VarTypeError: when attempting to bool-ify the Var.\n        \"\"\"\n        raise VarTypeError(\n            f\"Cannot convert Var {self._var_full_name!r} to bool for use with `if`, `and`, `or`, and `not`. \"\n            \"Instead use `rx.cond` and bitwise operators `&` (and), `|` (or), `~` (invert).\"\n        )\n\n    def __iter__(self) -> Any:\n        \"\"\"Raise exception if using Var in an iterable context.\n\n        Raises:\n            VarTypeError: when attempting to iterate over the Var.\n        \"\"\"\n        raise VarTypeError(\n            f\"Cannot iterate over Var {self._var_full_name!r}. Instead use `rx.foreach`.\"\n        )\n\n    def __format__(self, format_spec: str) -> str:\n        \"\"\"Format the var into a Javascript equivalent to an f-string.\n\n        Args:\n            format_spec: The format specifier (Ignored for now).\n\n        Returns:\n            The formatted var.\n        \"\"\"\n        # Encode the _var_data into the formatted output for tracking purposes.\n        str_self = _encode_var(self)\n        if self._var_is_local:\n            return str_self\n        return f\"${str_self}\"\n\n    def __getitem__(self, i: Any) -> Var:\n        \"\"\"Index into a var.\n\n        Args:\n            i: The index to index into.\n\n        Returns:\n            The indexed var.\n\n        Raises:\n            VarTypeError: If the var is not indexable.\n        \"\"\"\n        from reflex.utils import format\n\n        # Indexing is only supported for strings, lists, tuples, dicts, and dataframes.\n        if not (\n            types._issubclass(self._var_type, Union[List, Dict, Tuple, str])\n            or types.is_dataframe(self._var_type)\n        ):\n            if self._var_type == Any:\n                raise VarTypeError(\n                    \"Could not index into var of type Any. (If you are trying to index into a state var, \"\n                    \"add the correct type annotation to the var.)\"\n                )\n            raise VarTypeError(\n                f\"Var {self._var_name} of type {self._var_type} does not support indexing.\"\n            )\n\n        # The type of the indexed var.\n        type_ = Any\n\n        # Convert any vars to local vars.\n        if isinstance(i, Var):\n            i = i._replace(_var_is_local=True)\n\n        # Handle list/tuple/str indexing.\n        if types._issubclass(self._var_type, Union[List, Tuple, str]):\n            # List/Tuple/String indices must be ints, slices, or vars.\n            if (\n                not isinstance(i, types.get_args(Union[int, slice, Var]))\n                or isinstance(i, Var)\n                and not i._var_type == int\n            ):\n                raise VarTypeError(\"Index must be an integer or an integer var.\")\n\n            # Handle slices first.\n            if isinstance(i, slice):\n                # Get the start and stop indices.\n                start = i.start or 0\n                stop = i.stop or \"undefined\"\n\n                # Use the slice function.\n                return self._replace(\n                    _var_name=f\"{self._var_name}.slice({start}, {stop})\",\n                    _var_is_string=False,\n                )\n\n            # Get the type of the indexed var.\n            if types.is_generic_alias(self._var_type):\n                index = i if not isinstance(i, Var) else 0\n                type_ = types.get_args(self._var_type)\n                type_ = type_[index % len(type_)] if type_ else Any\n            elif types._issubclass(self._var_type, str):\n                type_ = str\n\n            # Use `at` to support negative indices.\n            return self._replace(\n                _var_name=f\"{self._var_name}.at({i})\",\n                _var_type=type_,\n                _var_is_string=False,\n            )\n\n        # Dictionary / dataframe indexing.\n        # Tuples are currently not supported as indexes.\n        if (\n            (\n                types._issubclass(self._var_type, Dict)\n                or types.is_dataframe(self._var_type)\n            )\n            and not isinstance(i, types.get_args(Union[int, str, float, Var]))\n        ) or (\n            isinstance(i, Var)\n            and not types._issubclass(\n                i._var_type, types.get_args(Union[int, str, float])\n            )\n        ):\n            raise VarTypeError(\n                \"Index must be one of the following types: int, str, int or str Var\"\n            )\n        # Get the type of the indexed var.\n        if isinstance(i, str):\n            i = format.wrap(i, '\"')\n        type_ = (\n            types.get_args(self._var_type)[1]\n            if types.is_generic_alias(self._var_type)\n            else Any\n        )\n\n        # Use normal indexing here.\n        return self._replace(\n            _var_name=f\"{self._var_name}[{i}]\",\n            _var_type=type_,\n            _var_is_string=False,\n        )\n\n    def __getattribute__(self, name: str) -> Any:\n        \"\"\"Get a var attribute.\n\n        Args:\n            name: The name of the attribute.\n\n        Returns:\n            The var attribute.\n\n        Raises:\n            VarAttributeError: If the attribute cannot be found, or if __getattr__ fallback should be used.\n        \"\"\"\n        try:\n            var_attribute = super().__getattribute__(name)\n            if (\n                not name.startswith(\"_\")\n                and name not in Var.__dict__\n                and name not in BaseVar.__dict__\n            ):\n                # Check if the attribute should be accessed through the Var instead of\n                # accessing one of the Var operations\n                type_ = types.get_attribute_access_type(\n                    super().__getattribute__(\"_var_type\"), name\n                )\n                if type_ is not None:\n                    raise VarAttributeError(\n                        f\"{name} is being accessed through the Var.\"\n                    )\n            # Return the attribute as-is.\n            return var_attribute\n        except VarAttributeError:\n            raise  # fall back to __getattr__ anyway\n\n    def __getattr__(self, name: str) -> Var:\n        \"\"\"Get a var attribute.\n\n        Args:\n            name: The name of the attribute.\n\n        Returns:\n            The var attribute.\n\n        Raises:\n            VarAttributeError: If the var is wrongly annotated or can't find attribute.\n            VarTypeError: If an annotation to the var isn't provided.\n        \"\"\"\n        # Check if the attribute is one of the class fields.\n        if not name.startswith(\"_\"):\n            if self._var_type == Any:\n                raise VarTypeError(\n                    f\"You must provide an annotation for the state var `{self._var_full_name}`. Annotation cannot be `{self._var_type}`\"\n                ) from None\n            is_optional = types.is_optional(self._var_type)\n            type_ = types.get_attribute_access_type(self._var_type, name)\n\n            if type_ is not None:\n                return self._replace(\n                    _var_name=f\"{self._var_name}{'?' if is_optional else ''}.{name}\",\n                    _var_type=type_,\n                    _var_is_string=False,\n                )\n\n            if name in REPLACED_NAMES:\n                raise VarAttributeError(\n                    f\"Field {name!r} was renamed to {REPLACED_NAMES[name]!r}\"\n                )\n\n            raise VarAttributeError(\n                f\"The State var `{self._var_full_name}` has no attribute '{name}' or may have been annotated \"\n                f\"wrongly.\"\n            )\n\n        raise VarAttributeError(\n            f\"The State var has no attribute '{name}' or may have been annotated wrongly.\",\n        )\n\n    def operation(\n        self,\n        op: str = \"\",\n        other: Var | None = None,\n        type_: Type | None = None,\n        flip: bool = False,\n        fn: str | None = None,\n        invoke_fn: bool = False,\n    ) -> Var:\n        \"\"\"Perform an operation on a var.\n\n        Args:\n            op: The operation to perform.\n            other: The other var to perform the operation on.\n            type_: The type of the operation result.\n            flip: Whether to flip the order of the operation.\n            fn: A function to apply to\n```\n# Function Signature: def create(\n# Natural Language Annotations: \"\"\"Create a var from a value.\n\n        Args:\n            value: The value to create the var from.\n            _var_is_local: Whether the var is local.\n            _var_is_string: Whether the var is a string literal.\n            _var_data: Additional hooks and imports associated with the Var.\n\n        Returns:\n            The var.\n\n        Raises:\n            VarTypeError: If the value is JSON-unserializable.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef create(\n    value: Var,\n    _var", "  ```\nresult = self.operation(op=op, other=other", "  ```\ndef create(value: Any, _var_is_local"]}, "reflex-reflex/vars.py-create_safe": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/vars.py:\n```\n\"\"\"Define a state var.\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport dataclasses\nimport datetime\nimport dis\nimport functools\nimport inspect\nimport json\nimport random\nimport re\nimport string\nimport sys\nfrom types import CodeType, FunctionType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    _GenericAlias,  # type: ignore\n    cast,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.utils import console, imports, serializers, types\nfrom reflex.utils.exceptions import (\n    VarAttributeError,\n    VarDependencyError,\n    VarTypeError,\n    VarValueError,\n)\n\n# This module used to export ImportVar itself, so we still import it for export here\nfrom reflex.utils.imports import (\n    ImportDict,\n    ImportVar,\n    ParsedImportDict,\n    parse_imports,\n)\nfrom reflex.utils.types import override\n\nif TYPE_CHECKING:\n    from reflex.state import BaseState\n\n\n# Set of unique variable names.\nUSED_VARIABLES = set()\n\n# Supported operators for all types.\nALL_OPS = [\"==\", \"!=\", \"!==\", \"===\", \"&&\", \"||\"]\n# Delimiters used between function args or operands.\nDELIMITERS = [\",\"]\n# Mapping of valid operations for different type combinations.\nOPERATION_MAPPING = {\n    (int, int): {\n        \"+\",\n        \"-\",\n        \"/\",\n        \"//\",\n        \"*\",\n        \"%\",\n        \"**\",\n        \">\",\n        \"<\",\n        \"<=\",\n        \">=\",\n        \"|\",\n        \"&\",\n    },\n    (int, str): {\"*\"},\n    (int, list): {\"*\"},\n    (str, str): {\"+\", \">\", \"<\", \"<=\", \">=\"},\n    (float, float): {\"+\", \"-\", \"/\", \"//\", \"*\", \"%\", \"**\", \">\", \"<\", \"<=\", \">=\"},\n    (float, int): {\"+\", \"-\", \"/\", \"//\", \"*\", \"%\", \"**\", \">\", \"<\", \"<=\", \">=\"},\n    (list, list): {\"+\", \">\", \"<\", \"<=\", \">=\"},\n}\n\n# These names were changed in reflex 0.3.0\nREPLACED_NAMES = {\n    \"full_name\": \"_var_full_name\",\n    \"name\": \"_var_name\",\n    \"state\": \"_var_data.state\",\n    \"type_\": \"_var_type\",\n    \"is_local\": \"_var_is_local\",\n    \"is_string\": \"_var_is_string\",\n    \"set_state\": \"_var_set_state\",\n    \"deps\": \"_deps\",\n}\n\nPYTHON_JS_TYPE_MAP = {\n    (int, float): \"number\",\n    (str,): \"string\",\n    (bool,): \"boolean\",\n    (list, tuple): \"Array\",\n    (dict,): \"Object\",\n    (None,): \"null\",\n}\n\n\ndef get_unique_variable_name() -> str:\n    \"\"\"Get a unique variable name.\n\n    Returns:\n        The unique variable name.\n    \"\"\"\n    name = \"\".join([random.choice(string.ascii_lowercase) for _ in range(8)])\n    if name not in USED_VARIABLES:\n        USED_VARIABLES.add(name)\n        return name\n    return get_unique_variable_name()\n\n\nclass VarData(Base):\n    \"\"\"Metadata associated with a Var.\"\"\"\n\n    # The name of the enclosing state.\n    state: str = \"\"\n\n    # Imports needed to render this var\n    imports: ParsedImportDict = {}\n\n    # Hooks that need to be present in the component to render this var\n    hooks: Dict[str, None] = {}\n\n    # Positions of interpolated strings. This is used by the decoder to figure\n    # out where the interpolations are and only escape the non-interpolated\n    # segments.\n    interpolations: List[Tuple[int, int]] = []\n\n    def __init__(\n        self, imports: Union[ImportDict, ParsedImportDict] | None = None, **kwargs: Any\n    ):\n        \"\"\"Initialize the var data.\n\n        Args:\n            imports: The imports needed to render this var.\n            **kwargs: The var data fields.\n        \"\"\"\n        if imports:\n            kwargs[\"imports\"] = parse_imports(imports)\n        super().__init__(**kwargs)\n\n    @classmethod\n    def merge(cls, *others: VarData | None) -> VarData | None:\n        \"\"\"Merge multiple var data objects.\n\n        Args:\n            *others: The var data objects to merge.\n\n        Returns:\n            The merged var data object.\n        \"\"\"\n        state = \"\"\n        _imports = {}\n        hooks = {}\n        interpolations = []\n        for var_data in others:\n            if var_data is None:\n                continue\n            state = state or var_data.state\n            _imports = imports.merge_imports(_imports, var_data.imports)\n            hooks.update(var_data.hooks)\n            interpolations += var_data.interpolations\n\n        return (\n            cls(\n                state=state,\n                imports=_imports,\n                hooks=hooks,\n                interpolations=interpolations,\n            )\n            or None\n        )\n\n    def __bool__(self) -> bool:\n        \"\"\"Check if the var data is non-empty.\n\n        Returns:\n            True if any field is set to a non-default value.\n        \"\"\"\n        return bool(self.state or self.imports or self.hooks or self.interpolations)\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\"Check if two var data objects are equal.\n\n        Args:\n            other: The other var data object to compare.\n\n        Returns:\n            True if all fields are equal and collapsed imports are equal.\n        \"\"\"\n        if not isinstance(other, VarData):\n            return False\n\n        # Don't compare interpolations - that's added in by the decoder, and\n        # not part of the vardata itself.\n        return (\n            self.state == other.state\n            and self.hooks.keys() == other.hooks.keys()\n            and imports.collapse_imports(self.imports)\n            == imports.collapse_imports(other.imports)\n        )\n\n    def dict(self) -> dict:\n        \"\"\"Convert the var data to a dictionary.\n\n        Returns:\n            The var data dictionary.\n        \"\"\"\n        return {\n            \"state\": self.state,\n            \"interpolations\": list(self.interpolations),\n            \"imports\": {\n                lib: [import_var.dict() for import_var in import_vars]\n                for lib, import_vars in self.imports.items()\n            },\n            \"hooks\": self.hooks,\n        }\n\n\ndef _encode_var(value: Var) -> str:\n    \"\"\"Encode the state name into a formatted var.\n\n    Args:\n        value: The value to encode the state name into.\n\n    Returns:\n        The encoded var.\n    \"\"\"\n    if value._var_data:\n        from reflex.utils.serializers import serialize\n\n        final_value = str(value)\n        data = value._var_data.dict()\n        data[\"string_length\"] = len(final_value)\n        data_json = value._var_data.__config__.json_dumps(data, default=serialize)\n\n        return (\n            f\"{constants.REFLEX_VAR_OPENING_TAG}{data_json}{constants.REFLEX_VAR_CLOSING_TAG}\"\n            + final_value\n        )\n\n    return str(value)\n\n\n# Compile regex for finding reflex var tags.\n_decode_var_pattern_re = (\n    rf\"{constants.REFLEX_VAR_OPENING_TAG}(.*?){constants.REFLEX_VAR_CLOSING_TAG}\"\n)\n_decode_var_pattern = re.compile(_decode_var_pattern_re, flags=re.DOTALL)\n\n\ndef _decode_var(value: str) -> tuple[VarData | None, str]:\n    \"\"\"Decode the state name from a formatted var.\n\n    Args:\n        value: The value to extract the state name from.\n\n    Returns:\n        The extracted state name and the value without the state name.\n    \"\"\"\n    var_datas = []\n    if isinstance(value, str):\n        # fast path if there is no encoded VarData\n        if constants.REFLEX_VAR_OPENING_TAG not in value:\n            return None, value\n\n        offset = 0\n\n        # Initialize some methods for reading json.\n        var_data_config = VarData().__config__\n\n        def json_loads(s):\n            try:\n                return var_data_config.json_loads(s)\n            except json.decoder.JSONDecodeError:\n                return var_data_config.json_loads(var_data_config.json_loads(f'\"{s}\"'))\n\n        # Find all tags.\n        while m := _decode_var_pattern.search(value):\n            start, end = m.span()\n            value = value[:start] + value[end:]\n\n            # Read the JSON, pull out the string length, parse the rest as VarData.\n            data = json_loads(m.group(1))\n            string_length = data.pop(\"string_length\", None)\n            var_data = VarData.parse_obj(data)\n\n            # Use string length to compute positions of interpolations.\n            if string_length is not None:\n                realstart = start + offset\n                var_data.interpolations = [(realstart, realstart + string_length)]\n\n            var_datas.append(var_data)\n            offset += end - start\n\n    return VarData.merge(*var_datas) if var_datas else None, value\n\n\ndef _extract_var_data(value: Iterable) -> list[VarData | None]:\n    \"\"\"Extract the var imports and hooks from an iterable containing a Var.\n\n    Args:\n        value: The iterable to extract the VarData from\n\n    Returns:\n        The extracted VarDatas.\n    \"\"\"\n    from reflex.style import Style\n\n    var_datas = []\n    with contextlib.suppress(TypeError):\n        for sub in value:\n            if isinstance(sub, Var):\n                var_datas.append(sub._var_data)\n            elif not isinstance(sub, str):\n                # Recurse into dict values.\n                if hasattr(sub, \"values\") and callable(sub.values):\n                    var_datas.extend(_extract_var_data(sub.values()))\n                # Recurse into iterable values (or dict keys).\n                var_datas.extend(_extract_var_data(sub))\n\n    # Style objects should already have _var_data.\n    if isinstance(value, Style):\n        var_datas.append(value._var_data)\n    else:\n        # Recurse when value is a dict itself.\n        values = getattr(value, \"values\", None)\n        if callable(values):\n            var_datas.extend(_extract_var_data(values()))\n    return var_datas\n\n\nclass Var:\n    \"\"\"An abstract var.\"\"\"\n\n    # The name of the var.\n    _var_name: str\n\n    # The type of the var.\n    _var_type: Type\n\n    # Whether this is a local javascript variable.\n    _var_is_local: bool\n\n    # Whether the var is a string literal.\n    _var_is_string: bool\n\n    # _var_full_name should be prefixed with _var_state\n    _var_full_name_needs_state_prefix: bool\n\n    # Extra metadata associated with the Var\n    _var_data: Optional[VarData]\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def __class_getitem__(cls, type_: str) -> _GenericAlias:\n        \"\"\"Get a typed var.\n\n        Args:\n            type_: The type of the var.\n\n        Returns:\n            The var class item.\n        \"\"\"\n        return _GenericAlias(cls, type_)\n\n    def __post_init__(self) -> None:\n        \"\"\"Post-initialize the var.\"\"\"\n        # Decode any inline Var markup and apply it to the instance\n        _var_data, _var_name = _decode_var(self._var_name)\n        if _var_data:\n            self._var_name = _var_name\n            self._var_data = VarData.merge(self._var_data, _var_data)\n\n    def _replace(self, merge_var_data=None, **kwargs: Any) -> BaseVar:\n        \"\"\"Make a copy of this Var with updated fields.\n\n        Args:\n            merge_var_data: VarData to merge into the existing VarData.\n            **kwargs: Var fields to update.\n\n        Returns:\n            A new BaseVar with the updated fields overwriting the corresponding fields in this Var.\n        \"\"\"\n        field_values = dict(\n            _var_name=kwargs.pop(\"_var_name\", self._var_name),\n            _var_type=kwargs.pop(\"_var_type\", self._var_type),\n            _var_is_local=kwargs.pop(\"_var_is_local\", self._var_is_local),\n            _var_is_string=kwargs.pop(\"_var_is_string\", self._var_is_string),\n            _var_full_name_needs_state_prefix=kwargs.pop(\n                \"_var_full_name_needs_state_prefix\",\n                self._var_full_name_needs_state_prefix,\n            ),\n            _var_data=VarData.merge(\n                kwargs.get(\"_var_data\", self._var_data), merge_var_data\n            ),\n        )\n        return BaseVar(**field_values)\n\n    def _decode(self) -> Any:\n        \"\"\"Decode Var as a python value.\n\n        Note that Var with state set cannot be decoded python-side and will be\n        returned as full_name.\n\n        Returns:\n            The decoded value or the Var name.\n        \"\"\"\n        if self._var_is_string:\n            return self._var_name\n        try:\n            return json.loads(self._var_name)\n        except ValueError:\n            return self._var_name\n\n    def equals(self, other: Var) -> bool:\n        \"\"\"Check if two vars are equal.\n\n        Args:\n            other: The other var to compare.\n\n        Returns:\n            Whether the vars are equal.\n        \"\"\"\n        return (\n            self._var_name == other._var_name\n            and self._var_type == other._var_type\n            and self._var_is_local == other._var_is_local\n            and self._var_full_name_needs_state_prefix\n            == other._var_full_name_needs_state_prefix\n            and self._var_data == other._var_data\n        )\n\n    def _merge(self, other) -> Var:\n        \"\"\"Merge two or more dicts.\n\n        Args:\n            other: The other var to merge.\n\n        Returns:\n            The merged var.\n        \"\"\"\n        if other is None:\n            return self._replace()\n        if not isinstance(other, Var):\n            other = Var.create(other, _var_is_string=False)\n        return self._replace(\n            _var_name=f\"{{...{self._var_name}, ...{other._var_name}}}\"  # type: ignore\n        )\n\n    def to_string(self, json: bool = True) -> Var:\n        \"\"\"Convert a var to a string.\n\n        Args:\n            json: Whether to convert to a JSON string.\n\n        Returns:\n            The stringified var.\n        \"\"\"\n        fn = \"JSON.stringify\" if json else \"String\"\n        return self.operation(fn=fn, type_=str)\n\n    def to_int(self) -> Var:\n        \"\"\"Convert a var to an int.\n\n        Returns:\n            The parseInt var.\n        \"\"\"\n        return self.operation(fn=\"parseInt\", type_=int)\n\n    def __hash__(self) -> int:\n        \"\"\"Define a hash function for a var.\n\n        Returns:\n            The hash of the var.\n        \"\"\"\n        return hash((self._var_name, str(self._var_type)))\n\n    def __str__(self) -> str:\n        \"\"\"Wrap the var so it can be used in templates.\n\n        Returns:\n            The wrapped var, i.e. {state.var}.\n        \"\"\"\n        from reflex.utils import format\n\n        out = (\n            self._var_full_name\n            if self._var_is_local\n            else format.wrap(self._var_full_name, \"{\")\n        )\n        if self._var_is_string:\n            out = format.format_string(out)\n        return out\n\n    def __bool__(self) -> bool:\n        \"\"\"Raise exception if using Var in a boolean context.\n\n        Raises:\n            VarTypeError: when attempting to bool-ify the Var.\n        \"\"\"\n        raise VarTypeError(\n            f\"Cannot convert Var {self._var_full_name!r} to bool for use with `if`, `and`, `or`, and `not`. \"\n            \"Instead use `rx.cond` and bitwise operators `&` (and), `|` (or), `~` (invert).\"\n        )\n\n    def __iter__(self) -> Any:\n        \"\"\"Raise exception if using Var in an iterable context.\n\n        Raises:\n            VarTypeError: when attempting to iterate over the Var.\n        \"\"\"\n        raise VarTypeError(\n            f\"Cannot iterate over Var {self._var_full_name!r}. Instead use `rx.foreach`.\"\n        )\n\n    def __format__(self, format_spec: str) -> str:\n        \"\"\"Format the var into a Javascript equivalent to an f-string.\n\n        Args:\n            format_spec: The format specifier (Ignored for now).\n\n        Returns:\n            The formatted var.\n        \"\"\"\n        # Encode the _var_data into the formatted output for tracking purposes.\n        str_self = _encode_var(self)\n        if self._var_is_local:\n            return str_self\n        return f\"${str_self}\"\n\n    def __getitem__(self, i: Any) -> Var:\n        \"\"\"Index into a var.\n\n        Args:\n            i: The index to index into.\n\n        Returns:\n            The indexed var.\n\n        Raises:\n            VarTypeError: If the var is not indexable.\n        \"\"\"\n        from reflex.utils import format\n\n        # Indexing is only supported for strings, lists, tuples, dicts, and dataframes.\n        if not (\n            types._issubclass(self._var_type, Union[List, Dict, Tuple, str])\n            or types.is_dataframe(self._var_type)\n        ):\n            if self._var_type == Any:\n                raise VarTypeError(\n                    \"Could not index into var of type Any. (If you are trying to index into a state var, \"\n                    \"add the correct type annotation to the var.)\"\n                )\n            raise VarTypeError(\n                f\"Var {self._var_name} of type {self._var_type} does not support indexing.\"\n            )\n\n        # The type of the indexed var.\n        type_ = Any\n\n        # Convert any vars to local vars.\n        if isinstance(i, Var):\n            i = i._replace(_var_is_local=True)\n\n        # Handle list/tuple/str indexing.\n        if types._issubclass(self._var_type, Union[List, Tuple, str]):\n            # List/Tuple/String indices must be ints, slices, or vars.\n            if (\n                not isinstance(i, types.get_args(Union[int, slice, Var]))\n                or isinstance(i, Var)\n                and not i._var_type == int\n            ):\n                raise VarTypeError(\"Index must be an integer or an integer var.\")\n\n            # Handle slices first.\n            if isinstance(i, slice):\n                # Get the start and stop indices.\n                start = i.start or 0\n                stop = i.stop or \"undefined\"\n\n                # Use the slice function.\n                return self._replace(\n                    _var_name=f\"{self._var_name}.slice({start}, {stop})\",\n                    _var_is_string=False,\n                )\n\n            # Get the type of the indexed var.\n            if types.is_generic_alias(self._var_type):\n                index = i if not isinstance(i, Var) else 0\n                type_ = types.get_args(self._var_type)\n                type_ = type_[index % len(type_)] if type_ else Any\n            elif types._issubclass(self._var_type, str):\n                type_ = str\n\n            # Use `at` to support negative indices.\n            return self._replace(\n                _var_name=f\"{self._var_name}.at({i})\",\n                _var_type=type_,\n                _var_is_string=False,\n            )\n\n        # Dictionary / dataframe indexing.\n        # Tuples are currently not supported as indexes.\n        if (\n            (\n                types._issubclass(self._var_type, Dict)\n                or types.is_dataframe(self._var_type)\n            )\n            and not isinstance(i, types.get_args(Union[int, str, float, Var]))\n        ) or (\n            isinstance(i, Var)\n            and not types._issubclass(\n                i._var_type, types.get_args(Union[int, str, float])\n            )\n        ):\n            raise VarTypeError(\n                \"Index must be one of the following types: int, str, int or str Var\"\n            )\n        # Get the type of the indexed var.\n        if isinstance(i, str):\n            i = format.wrap(i, '\"')\n        type_ = (\n            types.get_args(self._var_type)[1]\n            if types.is_generic_alias(self._var_type)\n            else Any\n        )\n\n        # Use normal indexing here.\n        return self._replace(\n            _var_name=f\"{self._var_name}[{i}]\",\n            _var_type=type_,\n            _var_is_string=False,\n        )\n\n    def __getattribute__(self, name: str) -> Any:\n        \"\"\"Get a var attribute.\n\n        Args:\n            name: The name of the attribute.\n\n        Returns:\n            The var attribute.\n\n        Raises:\n            VarAttributeError: If the attribute cannot be found, or if __getattr__ fallback should be used.\n        \"\"\"\n        try:\n            var_attribute = super().__getattribute__(name)\n            if (\n                not name.startswith(\"_\")\n                and name not in Var.__dict__\n                and name not in BaseVar.__dict__\n            ):\n                # Check if the attribute should be accessed through the Var instead of\n                # accessing one of the Var operations\n                type_ = types.get_attribute_access_type(\n                    super().__getattribute__(\"_var_type\"), name\n                )\n                if type_ is not None:\n                    raise VarAttributeError(\n                        f\"{name} is being accessed through the Var.\"\n                    )\n            # Return the attribute as-is.\n            return var_attribute\n        except VarAttributeError:\n            raise  # fall back to __getattr__ anyway\n\n    def __getattr__(self, name: str) -> Var:\n        \"\"\"Get a var attribute.\n\n        Args:\n            name: The name of the attribute.\n\n        Returns:\n            The var attribute.\n\n        Raises:\n            VarAttributeError: If the var is wrongly annotated or can't find attribute.\n            VarTypeError: If an annotation to the var isn't provided.\n        \"\"\"\n        # Check if the attribute is one of the class fields.\n        if not name.startswith(\"_\"):\n            if self._var_type == Any:\n                raise VarTypeError(\n                    f\"You must provide an annotation for the state var `{self._var_full_name}`. Annotation cannot be `{self._var_type}`\"\n                ) from None\n            is_optional = types.is_optional(self._var_type)\n            type_ = types.get_attribute_access_type(self._var_type, name)\n\n            if type_ is not None:\n                return self._replace(\n                    _var_name=f\"{self._var_name}{'?' if is_optional else ''}.{name}\",\n                    _var_type=type_,\n                    _var_is_string=False,\n                )\n\n            if name in REPLACED_NAMES:\n                raise VarAttributeError(\n                    f\"Field {name!r} was renamed to {REPLACED_NAMES[name]!r}\"\n                )\n\n            raise VarAttributeError(\n                f\"The State var `{self._var_full_name}` has no attribute '{name}' or may have been annotated \"\n                f\"wrongly.\"\n            )\n\n        raise VarAttributeError(\n            f\"The State var has no attribute '{name}' or may have been annotated wrongly.\",\n        )\n\n    def operation(\n        self,\n        op: str = \"\",\n        other: Var | None = None,\n        type_: Type | None = None,\n        flip: bool = False,\n        fn: str | None = None,\n        invoke_fn: bool = False,\n    ) -> Var:\n        \"\"\"Perform an operation on a var.\n\n        Args:\n            op: The operation to perform.\n            other: The other var to perform the operation on.\n            type_: The type of the operation result.\n            flip: Whether to flip the order of the operation.\n            fn: A function to apply to\n```\n# Function Signature: def create_safe(\n# Natural Language Annotations: \"\"\"Create a var from a value, asserting that it is not None.\n\n        Args:\n            value: The value to create the var from.\n            _var_is_local: Whether the var is local.\n            _var_is_string: Whether the var is a string literal.\n            _var_data: Additional hooks and imports associated with the Var.\n\n        Returns:\n            The var.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\")))\n```\nvar_is_string = _var_", "  ```\nfrom typing import Optional\n\n\ndef create_safe(\n", "  ```\ndef create_safe(value: Any, _var_is"]}, "reflex-reflex/vars.py-foreach": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/vars.py:\n```\n\"\"\"Define a state var.\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport dataclasses\nimport datetime\nimport dis\nimport functools\nimport inspect\nimport json\nimport random\nimport re\nimport string\nimport sys\nfrom types import CodeType, FunctionType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    _GenericAlias,  # type: ignore\n    cast,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.utils import console, imports, serializers, types\nfrom reflex.utils.exceptions import (\n    VarAttributeError,\n    VarDependencyError,\n    VarTypeError,\n    VarValueError,\n)\n\n# This module used to export ImportVar itself, so we still import it for export here\nfrom reflex.utils.imports import (\n    ImportDict,\n    ImportVar,\n    ParsedImportDict,\n    parse_imports,\n)\nfrom reflex.utils.types import override\n\nif TYPE_CHECKING:\n    from reflex.state import BaseState\n\n\n# Set of unique variable names.\nUSED_VARIABLES = set()\n\n# Supported operators for all types.\nALL_OPS = [\"==\", \"!=\", \"!==\", \"===\", \"&&\", \"||\"]\n# Delimiters used between function args or operands.\nDELIMITERS = [\",\"]\n# Mapping of valid operations for different type combinations.\nOPERATION_MAPPING = {\n    (int, int): {\n        \"+\",\n        \"-\",\n        \"/\",\n        \"//\",\n        \"*\",\n        \"%\",\n        \"**\",\n        \">\",\n        \"<\",\n        \"<=\",\n        \">=\",\n        \"|\",\n        \"&\",\n    },\n    (int, str): {\"*\"},\n    (int, list): {\"*\"},\n    (str, str): {\"+\", \">\", \"<\", \"<=\", \">=\"},\n    (float, float): {\"+\", \"-\", \"/\", \"//\", \"*\", \"%\", \"**\", \">\", \"<\", \"<=\", \">=\"},\n    (float, int): {\"+\", \"-\", \"/\", \"//\", \"*\", \"%\", \"**\", \">\", \"<\", \"<=\", \">=\"},\n    (list, list): {\"+\", \">\", \"<\", \"<=\", \">=\"},\n}\n\n# These names were changed in reflex 0.3.0\nREPLACED_NAMES = {\n    \"full_name\": \"_var_full_name\",\n    \"name\": \"_var_name\",\n    \"state\": \"_var_data.state\",\n    \"type_\": \"_var_type\",\n    \"is_local\": \"_var_is_local\",\n    \"is_string\": \"_var_is_string\",\n    \"set_state\": \"_var_set_state\",\n    \"deps\": \"_deps\",\n}\n\nPYTHON_JS_TYPE_MAP = {\n    (int, float): \"number\",\n    (str,): \"string\",\n    (bool,): \"boolean\",\n    (list, tuple): \"Array\",\n    (dict,): \"Object\",\n    (None,): \"null\",\n}\n\n\ndef get_unique_variable_name() -> str:\n    \"\"\"Get a unique variable name.\n\n    Returns:\n        The unique variable name.\n    \"\"\"\n    name = \"\".join([random.choice(string.ascii_lowercase) for _ in range(8)])\n    if name not in USED_VARIABLES:\n        USED_VARIABLES.add(name)\n        return name\n    return get_unique_variable_name()\n\n\nclass VarData(Base):\n    \"\"\"Metadata associated with a Var.\"\"\"\n\n    # The name of the enclosing state.\n    state: str = \"\"\n\n    # Imports needed to render this var\n    imports: ParsedImportDict = {}\n\n    # Hooks that need to be present in the component to render this var\n    hooks: Dict[str, None] = {}\n\n    # Positions of interpolated strings. This is used by the decoder to figure\n    # out where the interpolations are and only escape the non-interpolated\n    # segments.\n    interpolations: List[Tuple[int, int]] = []\n\n    def __init__(\n        self, imports: Union[ImportDict, ParsedImportDict] | None = None, **kwargs: Any\n    ):\n        \"\"\"Initialize the var data.\n\n        Args:\n            imports: The imports needed to render this var.\n            **kwargs: The var data fields.\n        \"\"\"\n        if imports:\n            kwargs[\"imports\"] = parse_imports(imports)\n        super().__init__(**kwargs)\n\n    @classmethod\n    def merge(cls, *others: VarData | None) -> VarData | None:\n        \"\"\"Merge multiple var data objects.\n\n        Args:\n            *others: The var data objects to merge.\n\n        Returns:\n            The merged var data object.\n        \"\"\"\n        state = \"\"\n        _imports = {}\n        hooks = {}\n        interpolations = []\n        for var_data in others:\n            if var_data is None:\n                continue\n            state = state or var_data.state\n            _imports = imports.merge_imports(_imports, var_data.imports)\n            hooks.update(var_data.hooks)\n            interpolations += var_data.interpolations\n\n        return (\n            cls(\n                state=state,\n                imports=_imports,\n                hooks=hooks,\n                interpolations=interpolations,\n            )\n            or None\n        )\n\n    def __bool__(self) -> bool:\n        \"\"\"Check if the var data is non-empty.\n\n        Returns:\n            True if any field is set to a non-default value.\n        \"\"\"\n        return bool(self.state or self.imports or self.hooks or self.interpolations)\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\"Check if two var data objects are equal.\n\n        Args:\n            other: The other var data object to compare.\n\n        Returns:\n            True if all fields are equal and collapsed imports are equal.\n        \"\"\"\n        if not isinstance(other, VarData):\n            return False\n\n        # Don't compare interpolations - that's added in by the decoder, and\n        # not part of the vardata itself.\n        return (\n            self.state == other.state\n            and self.hooks.keys() == other.hooks.keys()\n            and imports.collapse_imports(self.imports)\n            == imports.collapse_imports(other.imports)\n        )\n\n    def dict(self) -> dict:\n        \"\"\"Convert the var data to a dictionary.\n\n        Returns:\n            The var data dictionary.\n        \"\"\"\n        return {\n            \"state\": self.state,\n            \"interpolations\": list(self.interpolations),\n            \"imports\": {\n                lib: [import_var.dict() for import_var in import_vars]\n                for lib, import_vars in self.imports.items()\n            },\n            \"hooks\": self.hooks,\n        }\n\n\ndef _encode_var(value: Var) -> str:\n    \"\"\"Encode the state name into a formatted var.\n\n    Args:\n        value: The value to encode the state name into.\n\n    Returns:\n        The encoded var.\n    \"\"\"\n    if value._var_data:\n        from reflex.utils.serializers import serialize\n\n        final_value = str(value)\n        data = value._var_data.dict()\n        data[\"string_length\"] = len(final_value)\n        data_json = value._var_data.__config__.json_dumps(data, default=serialize)\n\n        return (\n            f\"{constants.REFLEX_VAR_OPENING_TAG}{data_json}{constants.REFLEX_VAR_CLOSING_TAG}\"\n            + final_value\n        )\n\n    return str(value)\n\n\n# Compile regex for finding reflex var tags.\n_decode_var_pattern_re = (\n    rf\"{constants.REFLEX_VAR_OPENING_TAG}(.*?){constants.REFLEX_VAR_CLOSING_TAG}\"\n)\n_decode_var_pattern = re.compile(_decode_var_pattern_re, flags=re.DOTALL)\n\n\ndef _decode_var(value: str) -> tuple[VarData | None, str]:\n    \"\"\"Decode the state name from a formatted var.\n\n    Args:\n        value: The value to extract the state name from.\n\n    Returns:\n        The extracted state name and the value without the state name.\n    \"\"\"\n    var_datas = []\n    if isinstance(value, str):\n        # fast path if there is no encoded VarData\n        if constants.REFLEX_VAR_OPENING_TAG not in value:\n            return None, value\n\n        offset = 0\n\n        # Initialize some methods for reading json.\n        var_data_config = VarData().__config__\n\n        def json_loads(s):\n            try:\n                return var_data_config.json_loads(s)\n            except json.decoder.JSONDecodeError:\n                return var_data_config.json_loads(var_data_config.json_loads(f'\"{s}\"'))\n\n        # Find all tags.\n        while m := _decode_var_pattern.search(value):\n            start, end = m.span()\n            value = value[:start] + value[end:]\n\n            # Read the JSON, pull out the string length, parse the rest as VarData.\n            data = json_loads(m.group(1))\n            string_length = data.pop(\"string_length\", None)\n            var_data = VarData.parse_obj(data)\n\n            # Use string length to compute positions of interpolations.\n            if string_length is not None:\n                realstart = start + offset\n                var_data.interpolations = [(realstart, realstart + string_length)]\n\n            var_datas.append(var_data)\n            offset += end - start\n\n    return VarData.merge(*var_datas) if var_datas else None, value\n\n\ndef _extract_var_data(value: Iterable) -> list[VarData | None]:\n    \"\"\"Extract the var imports and hooks from an iterable containing a Var.\n\n    Args:\n        value: The iterable to extract the VarData from\n\n    Returns:\n        The extracted VarDatas.\n    \"\"\"\n    from reflex.style import Style\n\n    var_datas = []\n    with contextlib.suppress(TypeError):\n        for sub in value:\n            if isinstance(sub, Var):\n                var_datas.append(sub._var_data)\n            elif not isinstance(sub, str):\n                # Recurse into dict values.\n                if hasattr(sub, \"values\") and callable(sub.values):\n                    var_datas.extend(_extract_var_data(sub.values()))\n                # Recurse into iterable values (or dict keys).\n                var_datas.extend(_extract_var_data(sub))\n\n    # Style objects should already have _var_data.\n    if isinstance(value, Style):\n        var_datas.append(value._var_data)\n    else:\n        # Recurse when value is a dict itself.\n        values = getattr(value, \"values\", None)\n        if callable(values):\n            var_datas.extend(_extract_var_data(values()))\n    return var_datas\n\n\nclass Var:\n    \"\"\"An abstract var.\"\"\"\n\n    # The name of the var.\n    _var_name: str\n\n    # The type of the var.\n    _var_type: Type\n\n    # Whether this is a local javascript variable.\n    _var_is_local: bool\n\n    # Whether the var is a string literal.\n    _var_is_string: bool\n\n    # _var_full_name should be prefixed with _var_state\n    _var_full_name_needs_state_prefix: bool\n\n    # Extra metadata associated with the Var\n    _var_data: Optional[VarData]\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def __class_getitem__(cls, type_: str) -> _GenericAlias:\n        \"\"\"Get a typed var.\n\n        Args:\n            type_: The type of the var.\n\n        Returns:\n            The var class item.\n        \"\"\"\n        return _GenericAlias(cls, type_)\n\n    def __post_init__(self) -> None:\n        \"\"\"Post-initialize the var.\"\"\"\n        # Decode any inline Var markup and apply it to the instance\n        _var_data, _var_name = _decode_var(self._var_name)\n        if _var_data:\n            self._var_name = _var_name\n            self._var_data = VarData.merge(self._var_data, _var_data)\n\n    def _replace(self, merge_var_data=None, **kwargs: Any) -> BaseVar:\n        \"\"\"Make a copy of this Var with updated fields.\n\n        Args:\n            merge_var_data: VarData to merge into the existing VarData.\n            **kwargs: Var fields to update.\n\n        Returns:\n            A new BaseVar with the updated fields overwriting the corresponding fields in this Var.\n        \"\"\"\n        field_values = dict(\n            _var_name=kwargs.pop(\"_var_name\", self._var_name),\n            _var_type=kwargs.pop(\"_var_type\", self._var_type),\n            _var_is_local=kwargs.pop(\"_var_is_local\", self._var_is_local),\n            _var_is_string=kwargs.pop(\"_var_is_string\", self._var_is_string),\n            _var_full_name_needs_state_prefix=kwargs.pop(\n                \"_var_full_name_needs_state_prefix\",\n                self._var_full_name_needs_state_prefix,\n            ),\n            _var_data=VarData.merge(\n                kwargs.get(\"_var_data\", self._var_data), merge_var_data\n            ),\n        )\n        return BaseVar(**field_values)\n\n    def _decode(self) -> Any:\n        \"\"\"Decode Var as a python value.\n\n        Note that Var with state set cannot be decoded python-side and will be\n        returned as full_name.\n\n        Returns:\n            The decoded value or the Var name.\n        \"\"\"\n        if self._var_is_string:\n            return self._var_name\n        try:\n            return json.loads(self._var_name)\n        except ValueError:\n            return self._var_name\n\n    def equals(self, other: Var) -> bool:\n        \"\"\"Check if two vars are equal.\n\n        Args:\n            other: The other var to compare.\n\n        Returns:\n            Whether the vars are equal.\n        \"\"\"\n        return (\n            self._var_name == other._var_name\n            and self._var_type == other._var_type\n            and self._var_is_local == other._var_is_local\n            and self._var_full_name_needs_state_prefix\n            == other._var_full_name_needs_state_prefix\n            and self._var_data == other._var_data\n        )\n\n    def _merge(self, other) -> Var:\n        \"\"\"Merge two or more dicts.\n\n        Args:\n            other: The other var to merge.\n\n        Returns:\n            The merged var.\n        \"\"\"\n        if other is None:\n            return self._replace()\n        if not isinstance(other, Var):\n            other = Var.create(other, _var_is_string=False)\n        return self._replace(\n            _var_name=f\"{{...{self._var_name}, ...{other._var_name}}}\"  # type: ignore\n        )\n\n    def to_string(self, json: bool = True) -> Var:\n        \"\"\"Convert a var to a string.\n\n        Args:\n            json: Whether to convert to a JSON string.\n\n        Returns:\n            The stringified var.\n        \"\"\"\n        fn = \"JSON.stringify\" if json else \"String\"\n        return self.operation(fn=fn, type_=str)\n\n    def to_int(self) -> Var:\n        \"\"\"Convert a var to an int.\n\n        Returns:\n            The parseInt var.\n        \"\"\"\n        return self.operation(fn=\"parseInt\", type_=int)\n\n    def __hash__(self) -> int:\n        \"\"\"Define a hash function for a var.\n\n        Returns:\n            The hash of the var.\n        \"\"\"\n        return hash((self._var_name, str(self._var_type)))\n\n    def __str__(self) -> str:\n        \"\"\"Wrap the var so it can be used in templates.\n\n        Returns:\n            The wrapped var, i.e. {state.var}.\n        \"\"\"\n        from reflex.utils import format\n\n        out = (\n            self._var_full_name\n            if self._var_is_local\n            else format.wrap(self._var_full_name, \"{\")\n        )\n        if self._var_is_string:\n            out = format.format_string(out)\n        return out\n\n    def __bool__(self) -> bool:\n        \"\"\"Raise exception if using Var in a boolean context.\n\n        Raises:\n            VarTypeError: when attempting to bool-ify the Var.\n        \"\"\"\n        raise VarTypeError(\n            f\"Cannot convert Var {self._var_full_name!r} to bool for use with `if`, `and`, `or`, and `not`. \"\n            \"Instead use `rx.cond` and bitwise operators `&` (and), `|` (or), `~` (invert).\"\n        )\n\n    def __iter__(self) -> Any:\n        \"\"\"Raise exception if using Var in an iterable context.\n\n        Raises:\n            VarTypeError: when attempting to iterate over the Var.\n        \"\"\"\n        raise VarTypeError(\n            f\"Cannot iterate over Var {self._var_full_name!r}. Instead use `rx.foreach`.\"\n        )\n\n    def __format__(self, format_spec: str) -> str:\n        \"\"\"Format the var into a Javascript equivalent to an f-string.\n\n        Args:\n            format_spec: The format specifier (Ignored for now).\n\n        Returns:\n            The formatted var.\n        \"\"\"\n        # Encode the _var_data into the formatted output for tracking purposes.\n        str_self = _encode_var(self)\n        if self._var_is_local:\n            return str_self\n        return f\"${str_self}\"\n\n    def __getitem__(self, i: Any) -> Var:\n        \"\"\"Index into a var.\n\n        Args:\n            i: The index to index into.\n\n        Returns:\n            The indexed var.\n\n        Raises:\n            VarTypeError: If the var is not indexable.\n        \"\"\"\n        from reflex.utils import format\n\n        # Indexing is only supported for strings, lists, tuples, dicts, and dataframes.\n        if not (\n            types._issubclass(self._var_type, Union[List, Dict, Tuple, str])\n            or types.is_dataframe(self._var_type)\n        ):\n            if self._var_type == Any:\n                raise VarTypeError(\n                    \"Could not index into var of type Any. (If you are trying to index into a state var, \"\n                    \"add the correct type annotation to the var.)\"\n                )\n            raise VarTypeError(\n                f\"Var {self._var_name} of type {self._var_type} does not support indexing.\"\n            )\n\n        # The type of the indexed var.\n        type_ = Any\n\n        # Convert any vars to local vars.\n        if isinstance(i, Var):\n            i = i._replace(_var_is_local=True)\n\n        # Handle list/tuple/str indexing.\n        if types._issubclass(self._var_type, Union[List, Tuple, str]):\n            # List/Tuple/String indices must be ints, slices, or vars.\n            if (\n                not isinstance(i, types.get_args(Union[int, slice, Var]))\n                or isinstance(i, Var)\n                and not i._var_type == int\n            ):\n                raise VarTypeError(\"Index must be an integer or an integer var.\")\n\n            # Handle slices first.\n            if isinstance(i, slice):\n                # Get the start and stop indices.\n                start = i.start or 0\n                stop = i.stop or \"undefined\"\n\n                # Use the slice function.\n                return self._replace(\n                    _var_name=f\"{self._var_name}.slice({start}, {stop})\",\n                    _var_is_string=False,\n                )\n\n            # Get the type of the indexed var.\n            if types.is_generic_alias(self._var_type):\n                index = i if not isinstance(i, Var) else 0\n                type_ = types.get_args(self._var_type)\n                type_ = type_[index % len(type_)] if type_ else Any\n            elif types._issubclass(self._var_type, str):\n                type_ = str\n\n            # Use `at` to support negative indices.\n            return self._replace(\n                _var_name=f\"{self._var_name}.at({i})\",\n                _var_type=type_,\n                _var_is_string=False,\n            )\n\n        # Dictionary / dataframe indexing.\n        # Tuples are currently not supported as indexes.\n        if (\n            (\n                types._issubclass(self._var_type, Dict)\n                or types.is_dataframe(self._var_type)\n            )\n            and not isinstance(i, types.get_args(Union[int, str, float, Var]))\n        ) or (\n            isinstance(i, Var)\n            and not types._issubclass(\n                i._var_type, types.get_args(Union[int, str, float])\n            )\n        ):\n            raise VarTypeError(\n                \"Index must be one of the following types: int, str, int or str Var\"\n            )\n        # Get the type of the indexed var.\n        if isinstance(i, str):\n            i = format.wrap(i, '\"')\n        type_ = (\n            types.get_args(self._var_type)[1]\n            if types.is_generic_alias(self._var_type)\n            else Any\n        )\n\n        # Use normal indexing here.\n        return self._replace(\n            _var_name=f\"{self._var_name}[{i}]\",\n            _var_type=type_,\n            _var_is_string=False,\n        )\n\n    def __getattribute__(self, name: str) -> Any:\n        \"\"\"Get a var attribute.\n\n        Args:\n            name: The name of the attribute.\n\n        Returns:\n            The var attribute.\n\n        Raises:\n            VarAttributeError: If the attribute cannot be found, or if __getattr__ fallback should be used.\n        \"\"\"\n        try:\n            var_attribute = super().__getattribute__(name)\n            if (\n                not name.startswith(\"_\")\n                and name not in Var.__dict__\n                and name not in BaseVar.__dict__\n            ):\n                # Check if the attribute should be accessed through the Var instead of\n                # accessing one of the Var operations\n                type_ = types.get_attribute_access_type(\n                    super().__getattribute__(\"_var_type\"), name\n                )\n                if type_ is not None:\n                    raise VarAttributeError(\n                        f\"{name} is being accessed through the Var.\"\n                    )\n            # Return the attribute as-is.\n            return var_attribute\n        except VarAttributeError:\n            raise  # fall back to __getattr__ anyway\n\n    def __getattr__(self, name: str) -> Var:\n        \"\"\"Get a var attribute.\n\n        Args:\n            name: The name of the attribute.\n\n        Returns:\n            The var attribute.\n\n        Raises:\n            VarAttributeError: If the var is wrongly annotated or can't find attribute.\n            VarTypeError: If an annotation to the var isn't provided.\n        \"\"\"\n        # Check if the attribute is one of the class fields.\n        if not name.startswith(\"_\"):\n            if self._var_type == Any:\n                raise VarTypeError(\n                    f\"You must provide an annotation for the state var `{self._var_full_name}`. Annotation cannot be `{self._var_type}`\"\n                ) from None\n            is_optional = types.is_optional(self._var_type)\n            type_ = types.get_attribute_access_type(self._var_type, name)\n\n            if type_ is not None:\n                return self._replace(\n                    _var_name=f\"{self._var_name}{'?' if is_optional else ''}.{name}\",\n                    _var_type=type_,\n                    _var_is_string=False,\n                )\n\n            if name in REPLACED_NAMES:\n                raise VarAttributeError(\n                    f\"Field {name!r} was renamed to {REPLACED_NAMES[name]!r}\"\n                )\n\n            raise VarAttributeError(\n                f\"The State var `{self._var_full_name}` has no attribute '{name}' or may have been annotated \"\n                f\"wrongly.\"\n            )\n\n        raise VarAttributeError(\n            f\"The State var has no attribute '{name}' or may have been annotated wrongly.\",\n        )\n\n    def operation(\n        self,\n        op: str = \"\",\n        other: Var | None = None,\n        type_: Type | None = None,\n        flip: bool = False,\n        fn: str | None = None,\n        invoke_fn: bool = False,\n    ) -> Var:\n        \"\"\"Perform an operation on a var.\n\n        Args:\n            op: The operation to perform.\n            other: The other var to perform the operation on.\n            type_: The type of the operation result.\n            flip: Whether to flip the order of the operation.\n            fn: A function to apply to\n```\n# Function Signature: def foreach(self, fn: Callable) -> Var:\n# Natural Language Annotations: \"\"\"Return a list of components. after doing a foreach on this var.\n\n        Args:\n            fn: The function to call on each component.\n\n        Returns:\n            A var representing foreach operation.\n\n        Raises:\n            VarTypeError: If the var is not a list.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef foreach(fn: Callable):\n    if isinstance", "  This function creates a new instance of the `Var class` with the specified", "  ```\ndef foreach(self, fn: Callable) -> Var:"]}, "reflex-reflex/vars.py-get_default_value": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/vars.py:\n```\n\"\"\"Define a state var.\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport dataclasses\nimport datetime\nimport dis\nimport functools\nimport inspect\nimport json\nimport random\nimport re\nimport string\nimport sys\nfrom types import CodeType, FunctionType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    _GenericAlias,  # type: ignore\n    cast,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.utils import console, imports, serializers, types\nfrom reflex.utils.exceptions import (\n    VarAttributeError,\n    VarDependencyError,\n    VarTypeError,\n    VarValueError,\n)\n\n# This module used to export ImportVar itself, so we still import it for export here\nfrom reflex.utils.imports import (\n    ImportDict,\n    ImportVar,\n    ParsedImportDict,\n    parse_imports,\n)\nfrom reflex.utils.types import override\n\nif TYPE_CHECKING:\n    from reflex.state import BaseState\n\n\n# Set of unique variable names.\nUSED_VARIABLES = set()\n\n# Supported operators for all types.\nALL_OPS = [\"==\", \"!=\", \"!==\", \"===\", \"&&\", \"||\"]\n# Delimiters used between function args or operands.\nDELIMITERS = [\",\"]\n# Mapping of valid operations for different type combinations.\nOPERATION_MAPPING = {\n    (int, int): {\n        \"+\",\n        \"-\",\n        \"/\",\n        \"//\",\n        \"*\",\n        \"%\",\n        \"**\",\n        \">\",\n        \"<\",\n        \"<=\",\n        \">=\",\n        \"|\",\n        \"&\",\n    },\n    (int, str): {\"*\"},\n    (int, list): {\"*\"},\n    (str, str): {\"+\", \">\", \"<\", \"<=\", \">=\"},\n    (float, float): {\"+\", \"-\", \"/\", \"//\", \"*\", \"%\", \"**\", \">\", \"<\", \"<=\", \">=\"},\n    (float, int): {\"+\", \"-\", \"/\", \"//\", \"*\", \"%\", \"**\", \">\", \"<\", \"<=\", \">=\"},\n    (list, list): {\"+\", \">\", \"<\", \"<=\", \">=\"},\n}\n\n# These names were changed in reflex 0.3.0\nREPLACED_NAMES = {\n    \"full_name\": \"_var_full_name\",\n    \"name\": \"_var_name\",\n    \"state\": \"_var_data.state\",\n    \"type_\": \"_var_type\",\n    \"is_local\": \"_var_is_local\",\n    \"is_string\": \"_var_is_string\",\n    \"set_state\": \"_var_set_state\",\n    \"deps\": \"_deps\",\n}\n\nPYTHON_JS_TYPE_MAP = {\n    (int, float): \"number\",\n    (str,): \"string\",\n    (bool,): \"boolean\",\n    (list, tuple): \"Array\",\n    (dict,): \"Object\",\n    (None,): \"null\",\n}\n\n\ndef get_unique_variable_name() -> str:\n    \"\"\"Get a unique variable name.\n\n    Returns:\n        The unique variable name.\n    \"\"\"\n    name = \"\".join([random.choice(string.ascii_lowercase) for _ in range(8)])\n    if name not in USED_VARIABLES:\n        USED_VARIABLES.add(name)\n        return name\n    return get_unique_variable_name()\n\n\nclass VarData(Base):\n    \"\"\"Metadata associated with a Var.\"\"\"\n\n    # The name of the enclosing state.\n    state: str = \"\"\n\n    # Imports needed to render this var\n    imports: ParsedImportDict = {}\n\n    # Hooks that need to be present in the component to render this var\n    hooks: Dict[str, None] = {}\n\n    # Positions of interpolated strings. This is used by the decoder to figure\n    # out where the interpolations are and only escape the non-interpolated\n    # segments.\n    interpolations: List[Tuple[int, int]] = []\n\n    def __init__(\n        self, imports: Union[ImportDict, ParsedImportDict] | None = None, **kwargs: Any\n    ):\n        \"\"\"Initialize the var data.\n\n        Args:\n            imports: The imports needed to render this var.\n            **kwargs: The var data fields.\n        \"\"\"\n        if imports:\n            kwargs[\"imports\"] = parse_imports(imports)\n        super().__init__(**kwargs)\n\n    @classmethod\n    def merge(cls, *others: VarData | None) -> VarData | None:\n        \"\"\"Merge multiple var data objects.\n\n        Args:\n            *others: The var data objects to merge.\n\n        Returns:\n            The merged var data object.\n        \"\"\"\n        state = \"\"\n        _imports = {}\n        hooks = {}\n        interpolations = []\n        for var_data in others:\n            if var_data is None:\n                continue\n            state = state or var_data.state\n            _imports = imports.merge_imports(_imports, var_data.imports)\n            hooks.update(var_data.hooks)\n            interpolations += var_data.interpolations\n\n        return (\n            cls(\n                state=state,\n                imports=_imports,\n                hooks=hooks,\n                interpolations=interpolations,\n            )\n            or None\n        )\n\n    def __bool__(self) -> bool:\n        \"\"\"Check if the var data is non-empty.\n\n        Returns:\n            True if any field is set to a non-default value.\n        \"\"\"\n        return bool(self.state or self.imports or self.hooks or self.interpolations)\n\n    def __eq__(self, other: Any) -> bool:\n        \"\"\"Check if two var data objects are equal.\n\n        Args:\n            other: The other var data object to compare.\n\n        Returns:\n            True if all fields are equal and collapsed imports are equal.\n        \"\"\"\n        if not isinstance(other, VarData):\n            return False\n\n        # Don't compare interpolations - that's added in by the decoder, and\n        # not part of the vardata itself.\n        return (\n            self.state == other.state\n            and self.hooks.keys() == other.hooks.keys()\n            and imports.collapse_imports(self.imports)\n            == imports.collapse_imports(other.imports)\n        )\n\n    def dict(self) -> dict:\n        \"\"\"Convert the var data to a dictionary.\n\n        Returns:\n            The var data dictionary.\n        \"\"\"\n        return {\n            \"state\": self.state,\n            \"interpolations\": list(self.interpolations),\n            \"imports\": {\n                lib: [import_var.dict() for import_var in import_vars]\n                for lib, import_vars in self.imports.items()\n            },\n            \"hooks\": self.hooks,\n        }\n\n\ndef _encode_var(value: Var) -> str:\n    \"\"\"Encode the state name into a formatted var.\n\n    Args:\n        value: The value to encode the state name into.\n\n    Returns:\n        The encoded var.\n    \"\"\"\n    if value._var_data:\n        from reflex.utils.serializers import serialize\n\n        final_value = str(value)\n        data = value._var_data.dict()\n        data[\"string_length\"] = len(final_value)\n        data_json = value._var_data.__config__.json_dumps(data, default=serialize)\n\n        return (\n            f\"{constants.REFLEX_VAR_OPENING_TAG}{data_json}{constants.REFLEX_VAR_CLOSING_TAG}\"\n            + final_value\n        )\n\n    return str(value)\n\n\n# Compile regex for finding reflex var tags.\n_decode_var_pattern_re = (\n    rf\"{constants.REFLEX_VAR_OPENING_TAG}(.*?){constants.REFLEX_VAR_CLOSING_TAG}\"\n)\n_decode_var_pattern = re.compile(_decode_var_pattern_re, flags=re.DOTALL)\n\n\ndef _decode_var(value: str) -> tuple[VarData | None, str]:\n    \"\"\"Decode the state name from a formatted var.\n\n    Args:\n        value: The value to extract the state name from.\n\n    Returns:\n        The extracted state name and the value without the state name.\n    \"\"\"\n    var_datas = []\n    if isinstance(value, str):\n        # fast path if there is no encoded VarData\n        if constants.REFLEX_VAR_OPENING_TAG not in value:\n            return None, value\n\n        offset = 0\n\n        # Initialize some methods for reading json.\n        var_data_config = VarData().__config__\n\n        def json_loads(s):\n            try:\n                return var_data_config.json_loads(s)\n            except json.decoder.JSONDecodeError:\n                return var_data_config.json_loads(var_data_config.json_loads(f'\"{s}\"'))\n\n        # Find all tags.\n        while m := _decode_var_pattern.search(value):\n            start, end = m.span()\n            value = value[:start] + value[end:]\n\n            # Read the JSON, pull out the string length, parse the rest as VarData.\n            data = json_loads(m.group(1))\n            string_length = data.pop(\"string_length\", None)\n            var_data = VarData.parse_obj(data)\n\n            # Use string length to compute positions of interpolations.\n            if string_length is not None:\n                realstart = start + offset\n                var_data.interpolations = [(realstart, realstart + string_length)]\n\n            var_datas.append(var_data)\n            offset += end - start\n\n    return VarData.merge(*var_datas) if var_datas else None, value\n\n\ndef _extract_var_data(value: Iterable) -> list[VarData | None]:\n    \"\"\"Extract the var imports and hooks from an iterable containing a Var.\n\n    Args:\n        value: The iterable to extract the VarData from\n\n    Returns:\n        The extracted VarDatas.\n    \"\"\"\n    from reflex.style import Style\n\n    var_datas = []\n    with contextlib.suppress(TypeError):\n        for sub in value:\n            if isinstance(sub, Var):\n                var_datas.append(sub._var_data)\n            elif not isinstance(sub, str):\n                # Recurse into dict values.\n                if hasattr(sub, \"values\") and callable(sub.values):\n                    var_datas.extend(_extract_var_data(sub.values()))\n                # Recurse into iterable values (or dict keys).\n                var_datas.extend(_extract_var_data(sub))\n\n    # Style objects should already have _var_data.\n    if isinstance(value, Style):\n        var_datas.append(value._var_data)\n    else:\n        # Recurse when value is a dict itself.\n        values = getattr(value, \"values\", None)\n        if callable(values):\n            var_datas.extend(_extract_var_data(values()))\n    return var_datas\n\n\nclass Var:\n    \"\"\"An abstract var.\"\"\"\n\n    # The name of the var.\n    _var_name: str\n\n    # The type of the var.\n    _var_type: Type\n\n    # Whether this is a local javascript variable.\n    _var_is_local: bool\n\n    # Whether the var is a string literal.\n    _var_is_string: bool\n\n    # _var_full_name should be prefixed with _var_state\n    _var_full_name_needs_state_prefix: bool\n\n    # Extra metadata associated with the Var\n    _var_data: Optional[VarData]\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def __class_getitem__(cls, type_: str) -> _GenericAlias:\n        \"\"\"Get a typed var.\n\n        Args:\n            type_: The type of the var.\n\n        Returns:\n            The var class item.\n        \"\"\"\n        return _GenericAlias(cls, type_)\n\n    def __post_init__(self) -> None:\n        \"\"\"Post-initialize the var.\"\"\"\n        # Decode any inline Var markup and apply it to the instance\n        _var_data, _var_name = _decode_var(self._var_name)\n        if _var_data:\n            self._var_name = _var_name\n            self._var_data = VarData.merge(self._var_data, _var_data)\n\n    def _replace(self, merge_var_data=None, **kwargs: Any) -> BaseVar:\n        \"\"\"Make a copy of this Var with updated fields.\n\n        Args:\n            merge_var_data: VarData to merge into the existing VarData.\n            **kwargs: Var fields to update.\n\n        Returns:\n            A new BaseVar with the updated fields overwriting the corresponding fields in this Var.\n        \"\"\"\n        field_values = dict(\n            _var_name=kwargs.pop(\"_var_name\", self._var_name),\n            _var_type=kwargs.pop(\"_var_type\", self._var_type),\n            _var_is_local=kwargs.pop(\"_var_is_local\", self._var_is_local),\n            _var_is_string=kwargs.pop(\"_var_is_string\", self._var_is_string),\n            _var_full_name_needs_state_prefix=kwargs.pop(\n                \"_var_full_name_needs_state_prefix\",\n                self._var_full_name_needs_state_prefix,\n            ),\n            _var_data=VarData.merge(\n                kwargs.get(\"_var_data\", self._var_data), merge_var_data\n            ),\n        )\n        return BaseVar(**field_values)\n\n    def _decode(self) -> Any:\n        \"\"\"Decode Var as a python value.\n\n        Note that Var with state set cannot be decoded python-side and will be\n        returned as full_name.\n\n        Returns:\n            The decoded value or the Var name.\n        \"\"\"\n        if self._var_is_string:\n            return self._var_name\n        try:\n            return json.loads(self._var_name)\n        except ValueError:\n            return self._var_name\n\n    def equals(self, other: Var) -> bool:\n        \"\"\"Check if two vars are equal.\n\n        Args:\n            other: The other var to compare.\n\n        Returns:\n            Whether the vars are equal.\n        \"\"\"\n        return (\n            self._var_name == other._var_name\n            and self._var_type == other._var_type\n            and self._var_is_local == other._var_is_local\n            and self._var_full_name_needs_state_prefix\n            == other._var_full_name_needs_state_prefix\n            and self._var_data == other._var_data\n        )\n\n    def _merge(self, other) -> Var:\n        \"\"\"Merge two or more dicts.\n\n        Args:\n            other: The other var to merge.\n\n        Returns:\n            The merged var.\n        \"\"\"\n        if other is None:\n            return self._replace()\n        if not isinstance(other, Var):\n            other = Var.create(other, _var_is_string=False)\n        return self._replace(\n            _var_name=f\"{{...{self._var_name}, ...{other._var_name}}}\"  # type: ignore\n        )\n\n    def to_string(self, json: bool = True) -> Var:\n        \"\"\"Convert a var to a string.\n\n        Args:\n            json: Whether to convert to a JSON string.\n\n        Returns:\n            The stringified var.\n        \"\"\"\n        fn = \"JSON.stringify\" if json else \"String\"\n        return self.operation(fn=fn, type_=str)\n\n    def to_int(self) -> Var:\n        \"\"\"Convert a var to an int.\n\n        Returns:\n            The parseInt var.\n        \"\"\"\n        return self.operation(fn=\"parseInt\", type_=int)\n\n    def __hash__(self) -> int:\n        \"\"\"Define a hash function for a var.\n\n        Returns:\n            The hash of the var.\n        \"\"\"\n        return hash((self._var_name, str(self._var_type)))\n\n    def __str__(self) -> str:\n        \"\"\"Wrap the var so it can be used in templates.\n\n        Returns:\n            The wrapped var, i.e. {state.var}.\n        \"\"\"\n        from reflex.utils import format\n\n        out = (\n            self._var_full_name\n            if self._var_is_local\n            else format.wrap(self._var_full_name, \"{\")\n        )\n        if self._var_is_string:\n            out = format.format_string(out)\n        return out\n\n    def __bool__(self) -> bool:\n        \"\"\"Raise exception if using Var in a boolean context.\n\n        Raises:\n            VarTypeError: when attempting to bool-ify the Var.\n        \"\"\"\n        raise VarTypeError(\n            f\"Cannot convert Var {self._var_full_name!r} to bool for use with `if`, `and`, `or`, and `not`. \"\n            \"Instead use `rx.cond` and bitwise operators `&` (and), `|` (or), `~` (invert).\"\n        )\n\n    def __iter__(self) -> Any:\n        \"\"\"Raise exception if using Var in an iterable context.\n\n        Raises:\n            VarTypeError: when attempting to iterate over the Var.\n        \"\"\"\n        raise VarTypeError(\n            f\"Cannot iterate over Var {self._var_full_name!r}. Instead use `rx.foreach`.\"\n        )\n\n    def __format__(self, format_spec: str) -> str:\n        \"\"\"Format the var into a Javascript equivalent to an f-string.\n\n        Args:\n            format_spec: The format specifier (Ignored for now).\n\n        Returns:\n            The formatted var.\n        \"\"\"\n        # Encode the _var_data into the formatted output for tracking purposes.\n        str_self = _encode_var(self)\n        if self._var_is_local:\n            return str_self\n        return f\"${str_self}\"\n\n    def __getitem__(self, i: Any) -> Var:\n        \"\"\"Index into a var.\n\n        Args:\n            i: The index to index into.\n\n        Returns:\n            The indexed var.\n\n        Raises:\n            VarTypeError: If the var is not indexable.\n        \"\"\"\n        from reflex.utils import format\n\n        # Indexing is only supported for strings, lists, tuples, dicts, and dataframes.\n        if not (\n            types._issubclass(self._var_type, Union[List, Dict, Tuple, str])\n            or types.is_dataframe(self._var_type)\n        ):\n            if self._var_type == Any:\n                raise VarTypeError(\n                    \"Could not index into var of type Any. (If you are trying to index into a state var, \"\n                    \"add the correct type annotation to the var.)\"\n                )\n            raise VarTypeError(\n                f\"Var {self._var_name} of type {self._var_type} does not support indexing.\"\n            )\n\n        # The type of the indexed var.\n        type_ = Any\n\n        # Convert any vars to local vars.\n        if isinstance(i, Var):\n            i = i._replace(_var_is_local=True)\n\n        # Handle list/tuple/str indexing.\n        if types._issubclass(self._var_type, Union[List, Tuple, str]):\n            # List/Tuple/String indices must be ints, slices, or vars.\n            if (\n                not isinstance(i, types.get_args(Union[int, slice, Var]))\n                or isinstance(i, Var)\n                and not i._var_type == int\n            ):\n                raise VarTypeError(\"Index must be an integer or an integer var.\")\n\n            # Handle slices first.\n            if isinstance(i, slice):\n                # Get the start and stop indices.\n                start = i.start or 0\n                stop = i.stop or \"undefined\"\n\n                # Use the slice function.\n                return self._replace(\n                    _var_name=f\"{self._var_name}.slice({start}, {stop})\",\n                    _var_is_string=False,\n                )\n\n            # Get the type of the indexed var.\n            if types.is_generic_alias(self._var_type):\n                index = i if not isinstance(i, Var) else 0\n                type_ = types.get_args(self._var_type)\n                type_ = type_[index % len(type_)] if type_ else Any\n            elif types._issubclass(self._var_type, str):\n                type_ = str\n\n            # Use `at` to support negative indices.\n            return self._replace(\n                _var_name=f\"{self._var_name}.at({i})\",\n                _var_type=type_,\n                _var_is_string=False,\n            )\n\n        # Dictionary / dataframe indexing.\n        # Tuples are currently not supported as indexes.\n        if (\n            (\n                types._issubclass(self._var_type, Dict)\n                or types.is_dataframe(self._var_type)\n            )\n            and not isinstance(i, types.get_args(Union[int, str, float, Var]))\n        ) or (\n            isinstance(i, Var)\n            and not types._issubclass(\n                i._var_type, types.get_args(Union[int, str, float])\n            )\n        ):\n            raise VarTypeError(\n                \"Index must be one of the following types: int, str, int or str Var\"\n            )\n        # Get the type of the indexed var.\n        if isinstance(i, str):\n            i = format.wrap(i, '\"')\n        type_ = (\n            types.get_args(self._var_type)[1]\n            if types.is_generic_alias(self._var_type)\n            else Any\n        )\n\n        # Use normal indexing here.\n        return self._replace(\n            _var_name=f\"{self._var_name}[{i}]\",\n            _var_type=type_,\n            _var_is_string=False,\n        )\n\n    def __getattribute__(self, name: str) -> Any:\n        \"\"\"Get a var attribute.\n\n        Args:\n            name: The name of the attribute.\n\n        Returns:\n            The var attribute.\n\n        Raises:\n            VarAttributeError: If the attribute cannot be found, or if __getattr__ fallback should be used.\n        \"\"\"\n        try:\n            var_attribute = super().__getattribute__(name)\n            if (\n                not name.startswith(\"_\")\n                and name not in Var.__dict__\n                and name not in BaseVar.__dict__\n            ):\n                # Check if the attribute should be accessed through the Var instead of\n                # accessing one of the Var operations\n                type_ = types.get_attribute_access_type(\n                    super().__getattribute__(\"_var_type\"), name\n                )\n                if type_ is not None:\n                    raise VarAttributeError(\n                        f\"{name} is being accessed through the Var.\"\n                    )\n            # Return the attribute as-is.\n            return var_attribute\n        except VarAttributeError:\n            raise  # fall back to __getattr__ anyway\n\n    def __getattr__(self, name: str) -> Var:\n        \"\"\"Get a var attribute.\n\n        Args:\n            name: The name of the attribute.\n\n        Returns:\n            The var attribute.\n\n        Raises:\n            VarAttributeError: If the var is wrongly annotated or can't find attribute.\n            VarTypeError: If an annotation to the var isn't provided.\n        \"\"\"\n        # Check if the attribute is one of the class fields.\n        if not name.startswith(\"_\"):\n            if self._var_type == Any:\n                raise VarTypeError(\n                    f\"You must provide an annotation for the state var `{self._var_full_name}`. Annotation cannot be `{self._var_type}`\"\n                ) from None\n            is_optional = types.is_optional(self._var_type)\n            type_ = types.get_attribute_access_type(self._var_type, name)\n\n            if type_ is not None:\n                return self._replace(\n                    _var_name=f\"{self._var_name}{'?' if is_optional else ''}.{name}\",\n                    _var_type=type_,\n                    _var_is_string=False,\n                )\n\n            if name in REPLACED_NAMES:\n                raise VarAttributeError(\n                    f\"Field {name!r} was renamed to {REPLACED_NAMES[name]!r}\"\n                )\n\n            raise VarAttributeError(\n                f\"The State var `{self._var_full_name}` has no attribute '{name}' or may have been annotated \"\n                f\"wrongly.\"\n            )\n\n        raise VarAttributeError(\n            f\"The State var has no attribute '{name}' or may have been annotated wrongly.\",\n        )\n\n    def operation(\n        self,\n        op: str = \"\",\n        other: Var | None = None,\n        type_: Type | None = None,\n        flip: bool = False,\n        fn: str | None = None,\n        invoke_fn: bool = False,\n    ) -> Var:\n        \"\"\"Perform an operation on a var.\n\n        Args:\n            op: The operation to perform.\n            other: The other var to perform the operation on.\n            type_: The type of the operation result.\n            flip: Whether to flip the order of the operation.\n            fn: A function to apply to\n```\n# Function Signature: def get_default_value(self) -> Any:\n# Natural Language Annotations: \"\"\"Get the default value of the var.\n\n        Returns:\n            The default value of the var.\n\n        Raises:\n            ImportError: If the var is a dataframe and pandas is not installed.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_default_value(self) -> Any:\n", "  ```\ndef get_default_value(self) -> Any:\n", "  ```\ndef get_default_value(self) -> Any:\n"]}, "reflex-reflex/state.py-get_skip_vars": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/state.py:\n```\n\"\"\"Define the reflex state specification.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport copy\nimport functools\nimport inspect\nimport os\nimport traceback\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom types import FunctionType, MethodType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Callable,\n    ClassVar,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Type,\n)\n\nimport dill\n\ntry:\n    import pydantic.v1 as pydantic\nexcept ModuleNotFoundError:\n    import pydantic\n\nimport wrapt\nfrom redis.asyncio import Redis\nfrom redis.exceptions import ResponseError\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.event import (\n    BACKGROUND_TASK_MARKER,\n    Event,\n    EventHandler,\n    EventSpec,\n    fix_events,\n    window_alert,\n)\nfrom reflex.utils import console, format, prerequisites, types\nfrom reflex.utils.exceptions import ImmutableStateError, LockExpiredError\nfrom reflex.utils.exec import is_testing_env\nfrom reflex.utils.serializers import SerializedType, serialize, serializer\nfrom reflex.vars import BaseVar, ComputedVar, Var, computed_var\n\nif TYPE_CHECKING:\n    from reflex.components.component import Component\n\n\nDelta = Dict[str, Any]\nvar = computed_var\n\n\n# If the state is this large, it's considered a performance issue.\nTOO_LARGE_SERIALIZED_STATE = 100 * 1024  # 100kb\n\n\nclass HeaderData(Base):\n    \"\"\"An object containing headers data.\"\"\"\n\n    host: str = \"\"\n    origin: str = \"\"\n    upgrade: str = \"\"\n    connection: str = \"\"\n    pragma: str = \"\"\n    cache_control: str = \"\"\n    user_agent: str = \"\"\n    sec_websocket_version: str = \"\"\n    sec_websocket_key: str = \"\"\n    sec_websocket_extensions: str = \"\"\n    accept_encoding: str = \"\"\n    accept_language: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the HeaderData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            for k, v in router_data.get(constants.RouteVar.HEADERS, {}).items():\n                setattr(self, format.to_snake_case(k), v)\n\n\nclass PageData(Base):\n    \"\"\"An object containing page data.\"\"\"\n\n    host: str = \"\"  # repeated with self.headers.origin (remove or keep the duplicate?)\n    path: str = \"\"\n    raw_path: str = \"\"\n    full_path: str = \"\"\n    full_raw_path: str = \"\"\n    params: dict = {}\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the PageData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.host = router_data.get(constants.RouteVar.HEADERS, {}).get(\"origin\")\n            self.path = router_data.get(constants.RouteVar.PATH, \"\")\n            self.raw_path = router_data.get(constants.RouteVar.ORIGIN, \"\")\n            self.full_path = f\"{self.host}{self.path}\"\n            self.full_raw_path = f\"{self.host}{self.raw_path}\"\n            self.params = router_data.get(constants.RouteVar.QUERY, {})\n\n\nclass SessionData(Base):\n    \"\"\"An object containing session data.\"\"\"\n\n    client_token: str = \"\"\n    client_ip: str = \"\"\n    session_id: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the SessionData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.client_token = router_data.get(constants.RouteVar.CLIENT_TOKEN, \"\")\n            self.client_ip = router_data.get(constants.RouteVar.CLIENT_IP, \"\")\n            self.session_id = router_data.get(constants.RouteVar.SESSION_ID, \"\")\n\n\nclass RouterData(Base):\n    \"\"\"An object containing RouterData.\"\"\"\n\n    session: SessionData = SessionData()\n    headers: HeaderData = HeaderData()\n    page: PageData = PageData()\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initialize the RouterData object.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        self.session = SessionData(router_data)\n        self.headers = HeaderData(router_data)\n        self.page = PageData(router_data)\n\n\ndef _no_chain_background_task(\n    state_cls: Type[\"BaseState\"], name: str, fn: Callable\n) -> Callable:\n    \"\"\"Protect against directly chaining a background task from another event handler.\n\n    Args:\n        state_cls: The state class that the event handler is in.\n        name: The name of the background task.\n        fn: The background task coroutine function / generator.\n\n    Returns:\n        A compatible coroutine function / generator that raises a runtime error.\n\n    Raises:\n        TypeError: If the background task is not async.\n    \"\"\"\n    call = f\"{state_cls.__name__}.{name}\"\n    message = (\n        f\"Cannot directly call background task {name!r}, use \"\n        f\"`yield {call}` or `return {call}` instead.\"\n    )\n    if inspect.iscoroutinefunction(fn):\n\n        async def _no_chain_background_task_co(*args, **kwargs):\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_co\n    if inspect.isasyncgenfunction(fn):\n\n        async def _no_chain_background_task_gen(*args, **kwargs):\n            yield\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_gen\n\n    raise TypeError(f\"{fn} is marked as a background task, but is not async.\")\n\n\ndef _substate_key(\n    token: str,\n    state_cls_or_name: BaseState | Type[BaseState] | str | list[str],\n) -> str:\n    \"\"\"Get the substate key.\n\n    Args:\n        token: The token of the state.\n        state_cls_or_name: The state class/instance or name or sequence of name parts.\n\n    Returns:\n        The substate key.\n    \"\"\"\n    if isinstance(state_cls_or_name, BaseState) or (\n        isinstance(state_cls_or_name, type) and issubclass(state_cls_or_name, BaseState)\n    ):\n        state_cls_or_name = state_cls_or_name.get_full_name()\n    elif isinstance(state_cls_or_name, (list, tuple)):\n        state_cls_or_name = \".\".join(state_cls_or_name)\n    return f\"{token}_{state_cls_or_name}\"\n\n\ndef _split_substate_key(substate_key: str) -> tuple[str, str]:\n    \"\"\"Split the substate key into token and state name.\n\n    Args:\n        substate_key: The substate key.\n\n    Returns:\n        Tuple of token and state name.\n    \"\"\"\n    token, _, state_name = substate_key.partition(\"_\")\n    return token, state_name\n\n\nclass EventHandlerSetVar(EventHandler):\n    \"\"\"A special event handler to wrap setvar functionality.\"\"\"\n\n    state_cls: Type[BaseState]\n\n    def __init__(self, state_cls: Type[BaseState]):\n        \"\"\"Initialize the EventHandlerSetVar.\n\n        Args:\n            state_cls: The state class that vars will be set on.\n        \"\"\"\n        super().__init__(\n            fn=type(self).setvar,\n            state_full_name=state_cls.get_full_name(),\n            state_cls=state_cls,  # type: ignore\n        )\n\n    def setvar(self, var_name: str, value: Any):\n        \"\"\"Set the state variable to the value of the event.\n\n        Note: `self` here will be an instance of the state, not EventHandlerSetVar.\n\n        Args:\n            var_name: The name of the variable to set.\n            value: The value to set the variable to.\n        \"\"\"\n        getattr(self, constants.SETTER_PREFIX + var_name)(value)\n\n    def __call__(self, *args: Any) -> EventSpec:\n        \"\"\"Performs pre-checks and munging on the provided args that will become an EventSpec.\n\n        Args:\n            *args: The event args.\n\n        Returns:\n            The (partial) EventSpec that will be used to create the event to setvar.\n\n        Raises:\n            AttributeError: If the given Var name does not exist on the state.\n            EventHandlerValueError: If the given Var name is not a str\n        \"\"\"\n        from reflex.utils.exceptions import EventHandlerValueError\n\n        if args:\n            if not isinstance(args[0], str):\n                raise EventHandlerValueError(\n                    f\"Var name must be passed as a string, got {args[0]!r}\"\n                )\n            # Check that the requested Var setter exists on the State at compile time.\n            if getattr(self.state_cls, constants.SETTER_PREFIX + args[0], None) is None:\n                raise AttributeError(\n                    f\"Variable `{args[0]}` cannot be set on `{self.state_cls.get_full_name()}`\"\n                )\n        return super().__call__(*args)\n\n\nclass BaseState(Base, ABC, extra=pydantic.Extra.allow):\n    \"\"\"The state of the app.\"\"\"\n\n    # A map from the var name to the var.\n    vars: ClassVar[Dict[str, Var]] = {}\n\n    # The base vars of the class.\n    base_vars: ClassVar[Dict[str, BaseVar]] = {}\n\n    # The computed vars of the class.\n    computed_vars: ClassVar[Dict[str, ComputedVar]] = {}\n\n    # Vars inherited by the parent state.\n    inherited_vars: ClassVar[Dict[str, Var]] = {}\n\n    # Backend base vars that are never sent to the client.\n    backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # Backend base vars inherited\n    inherited_backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # The event handlers.\n    event_handlers: ClassVar[Dict[str, EventHandler]] = {}\n\n    # A set of subclassses of this class.\n    class_subclasses: ClassVar[Set[Type[BaseState]]] = set()\n\n    # Mapping of var name to set of computed variables that depend on it\n    _computed_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Mapping of var name to set of substates that depend on it\n    _substate_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Set of vars which always need to be recomputed\n    _always_dirty_computed_vars: ClassVar[Set[str]] = set()\n\n    # Set of substates which always need to be recomputed\n    _always_dirty_substates: ClassVar[Set[str]] = set()\n\n    # The parent state.\n    parent_state: Optional[BaseState] = None\n\n    # The substates of the state.\n    substates: Dict[str, BaseState] = {}\n\n    # The set of dirty vars.\n    dirty_vars: Set[str] = set()\n\n    # The set of dirty substates.\n    dirty_substates: Set[str] = set()\n\n    # The routing path that triggered the state\n    router_data: Dict[str, Any] = {}\n\n    # Per-instance copy of backend base variable values\n    _backend_vars: Dict[str, Any] = {}\n\n    # The router data for the current page\n    router: RouterData = RouterData()\n\n    # Whether the state has ever been touched since instantiation.\n    _was_touched: bool = False\n\n    # Whether this state class is a mixin and should not be instantiated.\n    _mixin: ClassVar[bool] = False\n\n    # A special event handler for setting base vars.\n    setvar: ClassVar[EventHandler]\n\n    def __init__(\n        self,\n        *args,\n        parent_state: BaseState | None = None,\n        init_substates: bool = True,\n        _reflex_internal_init: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize the state.\n\n        DO NOT INSTANTIATE STATE CLASSES DIRECTLY! Use StateManager.get_state() instead.\n\n        Args:\n            *args: The args to pass to the Pydantic init method.\n            parent_state: The parent state.\n            init_substates: Whether to initialize the substates in this instance.\n            _reflex_internal_init: A flag to indicate that the state is being initialized by the framework.\n            **kwargs: The kwargs to pass to the Pydantic init method.\n\n        Raises:\n            ReflexRuntimeError: If the state is instantiated directly by end user.\n        \"\"\"\n        from reflex.utils.exceptions import ReflexRuntimeError\n\n        if not _reflex_internal_init and not is_testing_env():\n            raise ReflexRuntimeError(\n                \"State classes should not be instantiated directly in a Reflex app. \"\n                \"See https://reflex.dev/docs/state/ for further information.\"\n            )\n        kwargs[\"parent_state\"] = parent_state\n        super().__init__(*args, **kwargs)\n\n        # Setup the substates (for memory state manager only).\n        if init_substates:\n            for substate in self.get_substates():\n                self.substates[substate.get_name()] = substate(\n                    parent_state=self,\n                    _reflex_internal_init=True,\n                )\n\n        # Create a fresh copy of the backend variables for this instance\n        self._backend_vars = copy.deepcopy(\n            {name: item for name, item in self.backend_vars.items()}\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the state.\n\n        Returns:\n            The string representation of the state.\n        \"\"\"\n        return f\"{self.__class__.__name__}({self.dict()})\"\n\n    @classmethod\n    def _get_computed_vars(cls) -> list[ComputedVar]:\n        \"\"\"Helper function to get all computed vars of a instance.\n\n        Returns:\n            A list of computed vars.\n        \"\"\"\n        return [\n            v\n            for mixin in cls._mixins() + [cls]\n            for v in mixin.__dict__.values()\n            if isinstance(v, ComputedVar)\n        ]\n\n    @classmethod\n    def __init_subclass__(cls, mixin: bool = False, **kwargs):\n        \"\"\"Do some magic for the subclass initialization.\n\n        Args:\n            mixin: Whether the subclass is a mixin and should not be initialized.\n            **kwargs: The kwargs to pass to the pydantic init_subclass method.\n\n        Raises:\n            StateValueError: If a substate class shadows another.\n        \"\"\"\n        from reflex.utils.exceptions import StateValueError\n\n        super().__init_subclass__(**kwargs)\n\n        cls._mixin = mixin\n        if mixin:\n            return\n\n        # Event handlers should not shadow builtin state methods.\n        cls._check_overridden_methods()\n        # Computed vars should not shadow builtin state props.\n        cls._check_overriden_basevars()\n\n        # Reset subclass tracking for this class.\n        cls.class_subclasses = set()\n\n        # Reset dirty substate tracking for this class.\n        cls._always_dirty_substates = set()\n\n        # Get the parent vars.\n        parent_state = cls.get_parent_state()\n        if parent_state is not None:\n            cls.inherited_vars = parent_state.vars\n            cls.inherited_backend_vars = parent_state.backend_vars\n\n            # Check if another substate class with the same name has already been defined.\n            if cls.__name__ in set(c.__name__ for c in parent_state.class_subclasses):\n                if is_testing_env():\n                    # Clear existing subclass with same name when app is reloaded via\n                    # utils.prerequisites.get_app(reload=True)\n                    parent_state.class_subclasses = set(\n                        c\n                        for c in parent_state.class_subclasses\n                        if c.__name__ != cls.__name__\n                    )\n                else:\n                    # During normal operation, subclasses cannot have the same name, even if they are\n                    # defined in different modules.\n                    raise StateValueError(\n                        f\"The substate class '{cls.__name__}' has been defined multiple times. \"\n                        \"Shadowing substate classes is not allowed.\"\n                    )\n            # Track this new subclass in the parent state's subclasses set.\n            parent_state.class_subclasses.add(cls)\n\n        # Get computed vars.\n        computed_vars = cls._get_computed_vars()\n\n        new_backend_vars = {\n            name: value\n            for name, value in cls.__dict__.items()\n            if types.is_backend_base_variable(name, cls)\n        }\n\n        cls.backend_vars = {\n            **cls.inherited_backend_vars,\n            **new_backend_vars,\n        }\n\n        # Set the base and computed vars.\n        cls.base_vars = {\n            f.name: BaseVar(_var_name=f.name, _var_type=f.outer_type_)._var_set_state(\n                cls\n            )\n            for f in cls.get_fields().values()\n            if f.name not in cls.get_skip_vars()\n        }\n        cls.computed_vars = {v._var_name: v._var_set_state(cls) for v in computed_vars}\n        cls.vars = {\n            **cls.inherited_vars,\n            **cls.base_vars,\n            **cls.computed_vars,\n        }\n        cls.event_handlers = {}\n\n        # Setup the base vars at the class level.\n        for prop in cls.base_vars.values():\n            cls._init_var(prop)\n\n        # Set up the event handlers.\n        events = {\n            name: fn\n            for name, fn in cls.__dict__.items()\n            if cls._item_is_event_handler(name, fn)\n        }\n\n        for mixin in cls._mixins():\n            for name, value in mixin.__dict__.items():\n                if isinstance(value, ComputedVar):\n                    fget = cls._copy_fn(value.fget)\n                    newcv = value._replace(fget=fget)\n                    # cleanup refs to mixin cls in var_data\n                    newcv._var_data = None\n                    newcv._var_set_state(cls)\n                    setattr(cls, name, newcv)\n                    cls.computed_vars[newcv._var_name] = newcv\n                    cls.vars[newcv._var_name] = newcv\n                    continue\n                if types.is_backend_base_variable(name, mixin):\n                    cls.backend_vars[name] = copy.deepcopy(value)\n                    continue\n                if events.get(name) is not None:\n                    continue\n                if not cls._item_is_event_handler(name, value):\n                    continue\n                if parent_state is not None and parent_state.event_handlers.get(name):\n                    continue\n                value = cls._copy_fn(value)\n                value.__qualname__ = f\"{cls.__name__}.{name}\"\n                events[name] = value\n\n        # Create the setvar event handler for this state\n        cls._create_setvar()\n\n        for name, fn in events.items():\n            handler = cls._create_event_handler(fn)\n            cls.event_handlers[name] = handler\n            setattr(cls, name, handler)\n\n        cls._init_var_dependency_dicts()\n\n    @staticmethod\n    def _copy_fn(fn: Callable) -> Callable:\n        \"\"\"Copy a function. Used to copy ComputedVars and EventHandlers from mixins.\n\n        Args:\n            fn: The function to copy.\n\n        Returns:\n            The copied function.\n        \"\"\"\n        newfn = FunctionType(\n            fn.__code__,\n            fn.__globals__,\n            name=fn.__name__,\n            argdefs=fn.__defaults__,\n            closure=fn.__closure__,\n        )\n        newfn.__annotations__ = fn.__annotations__\n        if mark := getattr(fn, BACKGROUND_TASK_MARKER, None):\n            setattr(newfn, BACKGROUND_TASK_MARKER, mark)\n        return newfn\n\n    @staticmethod\n    def _item_is_event_handler(name: str, value: Any) -> bool:\n        \"\"\"Check if the item is an event handler.\n\n        Args:\n            name: The name of the item.\n            value: The value of the item.\n\n        Returns:\n            Whether the item is an event handler.\n        \"\"\"\n        return (\n            not name.startswith(\"_\")\n            and isinstance(value, Callable)\n            and not isinstance(value, EventHandler)\n            and hasattr(value, \"__code__\")\n        )\n\n    @classmethod\n    def _mixins(cls) -> List[Type]:\n        \"\"\"Get the mixin classes of the state.\n\n        Returns:\n            The mixin classes of the state.\n        \"\"\"\n        return [\n            mixin\n            for mixin in cls.__mro__\n            if (\n                mixin not in [pydantic.BaseModel, Base, cls]\n                and issubclass(mixin, BaseState)\n                and mixin._mixin is True\n            )\n        ]\n\n    @classmethod\n    def _init_var_dependency_dicts(cls):\n        \"\"\"Initialize the var dependency tracking dicts.\n\n        Allows the state to know which vars each ComputedVar depends on and\n        whether a ComputedVar depends on a var in its parent state.\n\n        Additional updates tracking dicts for vars and substates that always\n        need to be recomputed.\n        \"\"\"\n        # Initialize per-class var dependency tracking.\n        cls._computed_var_dependencies = defaultdict(set)\n        cls._substate_var_dependencies = defaultdict(set)\n\n        inherited_vars = set(cls.inherited_vars).union(\n            set(cls.inherited_backend_vars),\n        )\n        for cvar_name, cvar in cls.computed_vars.items():\n            # Add the dependencies.\n            for var in cvar._deps(objclass=cls):\n                cls._computed_var_dependencies[var].add(cvar_name)\n                if var in inherited_vars:\n                    # track that this substate depends on its parent for this var\n                    state_name = cls.get_name()\n                    parent_state = cls.get_parent_state()\n                    while parent_state is not None and var in {\n                        **parent_state.vars,\n                        **parent_state.backend_vars,\n                    }:\n                        parent_state._substate_var_dependencies[var].add(state_name)\n                        state_name, parent_state = (\n                            parent_state.get_name(),\n                            parent_state.get_parent_state(),\n                        )\n\n        # ComputedVar with cache=False always need to be recomputed\n        cls._always_dirty_computed_vars = set(\n            cvar_name\n            for cvar_name, cvar in cls.computed_vars.items()\n            if not cvar._cache\n        )\n\n        # Any substate containing a ComputedVar with cache=False always needs to be recomputed\n        if cls._always_dirty_computed_vars:\n            # Tell parent classes that this substate has always dirty computed vars\n            state_name = cls.get_name()\n            parent_state = cls.get_parent_state()\n            while parent_state is not None:\n                parent_state._always_dirty_substates.add(state_name)\n                state_name, parent_state = (\n                    parent_state.get_name(),\n                    parent_state.get_parent_state(),\n                )\n\n    @classmethod\n    def _check_overridden_methods(cls):\n        \"\"\"Check for shadow methods and raise error if any.\n\n        Raises:\n            NameError: When an event handler shadows an inbuilt state method.\n        \"\"\"\n        overridden_methods = set()\n        state_base_functions = cls._get_base_functions()\n        for name, method in inspect.getmembers(cls, inspect.isfunction):\n            # Check if the method is overridden and not a dunder method\n            if (\n                not name.startswith(\"__\")\n                and method.__name__ in state_base_functions\n                and state_base_functions[method.__name__] != method\n            ):\n                overridden_methods.add(method.__name__)\n\n        for method_name in overridden_methods:\n            raise NameError(\n                f\"The event handler name `{method_name}` shadows a builtin State method; use a different name instead\"\n            )\n\n    @classmethod\n    def _check_overriden_basevars(cls):\n        \"\"\"Check for shadow base vars and raise error if any.\n\n        Raises:\n            NameError: When a computed var shadows a base var.\n        \"\"\"\n        for computed_var_ in cls._get_computed_vars():\n            if computed_var_._var_name in cls.__annotations__:\n                raise NameError(\n                    f\"The computed var name `{computed_var_._var_name}` shadows a base var in {cls.__module__}.{cls.__\n```\n# Function Signature: def get_skip_vars(cls) -> set[str]:\n# Natural Language Annotations: \"\"\"Get the vars to skip when serializing.\n\n        Returns:\n            The vars to skip when serializing.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_skip_vars(cls) -> set[str", "  ```\ndef get_skip_vars(cls) -> set[str", "  ```\ndef get_skip_vars(cls) -> set[str"]}, "reflex-reflex/state.py-add_var": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/state.py:\n```\n\"\"\"Define the reflex state specification.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport copy\nimport functools\nimport inspect\nimport os\nimport traceback\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom types import FunctionType, MethodType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Callable,\n    ClassVar,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Type,\n)\n\nimport dill\n\ntry:\n    import pydantic.v1 as pydantic\nexcept ModuleNotFoundError:\n    import pydantic\n\nimport wrapt\nfrom redis.asyncio import Redis\nfrom redis.exceptions import ResponseError\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.event import (\n    BACKGROUND_TASK_MARKER,\n    Event,\n    EventHandler,\n    EventSpec,\n    fix_events,\n    window_alert,\n)\nfrom reflex.utils import console, format, prerequisites, types\nfrom reflex.utils.exceptions import ImmutableStateError, LockExpiredError\nfrom reflex.utils.exec import is_testing_env\nfrom reflex.utils.serializers import SerializedType, serialize, serializer\nfrom reflex.vars import BaseVar, ComputedVar, Var, computed_var\n\nif TYPE_CHECKING:\n    from reflex.components.component import Component\n\n\nDelta = Dict[str, Any]\nvar = computed_var\n\n\n# If the state is this large, it's considered a performance issue.\nTOO_LARGE_SERIALIZED_STATE = 100 * 1024  # 100kb\n\n\nclass HeaderData(Base):\n    \"\"\"An object containing headers data.\"\"\"\n\n    host: str = \"\"\n    origin: str = \"\"\n    upgrade: str = \"\"\n    connection: str = \"\"\n    pragma: str = \"\"\n    cache_control: str = \"\"\n    user_agent: str = \"\"\n    sec_websocket_version: str = \"\"\n    sec_websocket_key: str = \"\"\n    sec_websocket_extensions: str = \"\"\n    accept_encoding: str = \"\"\n    accept_language: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the HeaderData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            for k, v in router_data.get(constants.RouteVar.HEADERS, {}).items():\n                setattr(self, format.to_snake_case(k), v)\n\n\nclass PageData(Base):\n    \"\"\"An object containing page data.\"\"\"\n\n    host: str = \"\"  # repeated with self.headers.origin (remove or keep the duplicate?)\n    path: str = \"\"\n    raw_path: str = \"\"\n    full_path: str = \"\"\n    full_raw_path: str = \"\"\n    params: dict = {}\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the PageData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.host = router_data.get(constants.RouteVar.HEADERS, {}).get(\"origin\")\n            self.path = router_data.get(constants.RouteVar.PATH, \"\")\n            self.raw_path = router_data.get(constants.RouteVar.ORIGIN, \"\")\n            self.full_path = f\"{self.host}{self.path}\"\n            self.full_raw_path = f\"{self.host}{self.raw_path}\"\n            self.params = router_data.get(constants.RouteVar.QUERY, {})\n\n\nclass SessionData(Base):\n    \"\"\"An object containing session data.\"\"\"\n\n    client_token: str = \"\"\n    client_ip: str = \"\"\n    session_id: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the SessionData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.client_token = router_data.get(constants.RouteVar.CLIENT_TOKEN, \"\")\n            self.client_ip = router_data.get(constants.RouteVar.CLIENT_IP, \"\")\n            self.session_id = router_data.get(constants.RouteVar.SESSION_ID, \"\")\n\n\nclass RouterData(Base):\n    \"\"\"An object containing RouterData.\"\"\"\n\n    session: SessionData = SessionData()\n    headers: HeaderData = HeaderData()\n    page: PageData = PageData()\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initialize the RouterData object.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        self.session = SessionData(router_data)\n        self.headers = HeaderData(router_data)\n        self.page = PageData(router_data)\n\n\ndef _no_chain_background_task(\n    state_cls: Type[\"BaseState\"], name: str, fn: Callable\n) -> Callable:\n    \"\"\"Protect against directly chaining a background task from another event handler.\n\n    Args:\n        state_cls: The state class that the event handler is in.\n        name: The name of the background task.\n        fn: The background task coroutine function / generator.\n\n    Returns:\n        A compatible coroutine function / generator that raises a runtime error.\n\n    Raises:\n        TypeError: If the background task is not async.\n    \"\"\"\n    call = f\"{state_cls.__name__}.{name}\"\n    message = (\n        f\"Cannot directly call background task {name!r}, use \"\n        f\"`yield {call}` or `return {call}` instead.\"\n    )\n    if inspect.iscoroutinefunction(fn):\n\n        async def _no_chain_background_task_co(*args, **kwargs):\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_co\n    if inspect.isasyncgenfunction(fn):\n\n        async def _no_chain_background_task_gen(*args, **kwargs):\n            yield\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_gen\n\n    raise TypeError(f\"{fn} is marked as a background task, but is not async.\")\n\n\ndef _substate_key(\n    token: str,\n    state_cls_or_name: BaseState | Type[BaseState] | str | list[str],\n) -> str:\n    \"\"\"Get the substate key.\n\n    Args:\n        token: The token of the state.\n        state_cls_or_name: The state class/instance or name or sequence of name parts.\n\n    Returns:\n        The substate key.\n    \"\"\"\n    if isinstance(state_cls_or_name, BaseState) or (\n        isinstance(state_cls_or_name, type) and issubclass(state_cls_or_name, BaseState)\n    ):\n        state_cls_or_name = state_cls_or_name.get_full_name()\n    elif isinstance(state_cls_or_name, (list, tuple)):\n        state_cls_or_name = \".\".join(state_cls_or_name)\n    return f\"{token}_{state_cls_or_name}\"\n\n\ndef _split_substate_key(substate_key: str) -> tuple[str, str]:\n    \"\"\"Split the substate key into token and state name.\n\n    Args:\n        substate_key: The substate key.\n\n    Returns:\n        Tuple of token and state name.\n    \"\"\"\n    token, _, state_name = substate_key.partition(\"_\")\n    return token, state_name\n\n\nclass EventHandlerSetVar(EventHandler):\n    \"\"\"A special event handler to wrap setvar functionality.\"\"\"\n\n    state_cls: Type[BaseState]\n\n    def __init__(self, state_cls: Type[BaseState]):\n        \"\"\"Initialize the EventHandlerSetVar.\n\n        Args:\n            state_cls: The state class that vars will be set on.\n        \"\"\"\n        super().__init__(\n            fn=type(self).setvar,\n            state_full_name=state_cls.get_full_name(),\n            state_cls=state_cls,  # type: ignore\n        )\n\n    def setvar(self, var_name: str, value: Any):\n        \"\"\"Set the state variable to the value of the event.\n\n        Note: `self` here will be an instance of the state, not EventHandlerSetVar.\n\n        Args:\n            var_name: The name of the variable to set.\n            value: The value to set the variable to.\n        \"\"\"\n        getattr(self, constants.SETTER_PREFIX + var_name)(value)\n\n    def __call__(self, *args: Any) -> EventSpec:\n        \"\"\"Performs pre-checks and munging on the provided args that will become an EventSpec.\n\n        Args:\n            *args: The event args.\n\n        Returns:\n            The (partial) EventSpec that will be used to create the event to setvar.\n\n        Raises:\n            AttributeError: If the given Var name does not exist on the state.\n            EventHandlerValueError: If the given Var name is not a str\n        \"\"\"\n        from reflex.utils.exceptions import EventHandlerValueError\n\n        if args:\n            if not isinstance(args[0], str):\n                raise EventHandlerValueError(\n                    f\"Var name must be passed as a string, got {args[0]!r}\"\n                )\n            # Check that the requested Var setter exists on the State at compile time.\n            if getattr(self.state_cls, constants.SETTER_PREFIX + args[0], None) is None:\n                raise AttributeError(\n                    f\"Variable `{args[0]}` cannot be set on `{self.state_cls.get_full_name()}`\"\n                )\n        return super().__call__(*args)\n\n\nclass BaseState(Base, ABC, extra=pydantic.Extra.allow):\n    \"\"\"The state of the app.\"\"\"\n\n    # A map from the var name to the var.\n    vars: ClassVar[Dict[str, Var]] = {}\n\n    # The base vars of the class.\n    base_vars: ClassVar[Dict[str, BaseVar]] = {}\n\n    # The computed vars of the class.\n    computed_vars: ClassVar[Dict[str, ComputedVar]] = {}\n\n    # Vars inherited by the parent state.\n    inherited_vars: ClassVar[Dict[str, Var]] = {}\n\n    # Backend base vars that are never sent to the client.\n    backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # Backend base vars inherited\n    inherited_backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # The event handlers.\n    event_handlers: ClassVar[Dict[str, EventHandler]] = {}\n\n    # A set of subclassses of this class.\n    class_subclasses: ClassVar[Set[Type[BaseState]]] = set()\n\n    # Mapping of var name to set of computed variables that depend on it\n    _computed_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Mapping of var name to set of substates that depend on it\n    _substate_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Set of vars which always need to be recomputed\n    _always_dirty_computed_vars: ClassVar[Set[str]] = set()\n\n    # Set of substates which always need to be recomputed\n    _always_dirty_substates: ClassVar[Set[str]] = set()\n\n    # The parent state.\n    parent_state: Optional[BaseState] = None\n\n    # The substates of the state.\n    substates: Dict[str, BaseState] = {}\n\n    # The set of dirty vars.\n    dirty_vars: Set[str] = set()\n\n    # The set of dirty substates.\n    dirty_substates: Set[str] = set()\n\n    # The routing path that triggered the state\n    router_data: Dict[str, Any] = {}\n\n    # Per-instance copy of backend base variable values\n    _backend_vars: Dict[str, Any] = {}\n\n    # The router data for the current page\n    router: RouterData = RouterData()\n\n    # Whether the state has ever been touched since instantiation.\n    _was_touched: bool = False\n\n    # Whether this state class is a mixin and should not be instantiated.\n    _mixin: ClassVar[bool] = False\n\n    # A special event handler for setting base vars.\n    setvar: ClassVar[EventHandler]\n\n    def __init__(\n        self,\n        *args,\n        parent_state: BaseState | None = None,\n        init_substates: bool = True,\n        _reflex_internal_init: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize the state.\n\n        DO NOT INSTANTIATE STATE CLASSES DIRECTLY! Use StateManager.get_state() instead.\n\n        Args:\n            *args: The args to pass to the Pydantic init method.\n            parent_state: The parent state.\n            init_substates: Whether to initialize the substates in this instance.\n            _reflex_internal_init: A flag to indicate that the state is being initialized by the framework.\n            **kwargs: The kwargs to pass to the Pydantic init method.\n\n        Raises:\n            ReflexRuntimeError: If the state is instantiated directly by end user.\n        \"\"\"\n        from reflex.utils.exceptions import ReflexRuntimeError\n\n        if not _reflex_internal_init and not is_testing_env():\n            raise ReflexRuntimeError(\n                \"State classes should not be instantiated directly in a Reflex app. \"\n                \"See https://reflex.dev/docs/state/ for further information.\"\n            )\n        kwargs[\"parent_state\"] = parent_state\n        super().__init__(*args, **kwargs)\n\n        # Setup the substates (for memory state manager only).\n        if init_substates:\n            for substate in self.get_substates():\n                self.substates[substate.get_name()] = substate(\n                    parent_state=self,\n                    _reflex_internal_init=True,\n                )\n\n        # Create a fresh copy of the backend variables for this instance\n        self._backend_vars = copy.deepcopy(\n            {name: item for name, item in self.backend_vars.items()}\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the state.\n\n        Returns:\n            The string representation of the state.\n        \"\"\"\n        return f\"{self.__class__.__name__}({self.dict()})\"\n\n    @classmethod\n    def _get_computed_vars(cls) -> list[ComputedVar]:\n        \"\"\"Helper function to get all computed vars of a instance.\n\n        Returns:\n            A list of computed vars.\n        \"\"\"\n        return [\n            v\n            for mixin in cls._mixins() + [cls]\n            for v in mixin.__dict__.values()\n            if isinstance(v, ComputedVar)\n        ]\n\n    @classmethod\n    def __init_subclass__(cls, mixin: bool = False, **kwargs):\n        \"\"\"Do some magic for the subclass initialization.\n\n        Args:\n            mixin: Whether the subclass is a mixin and should not be initialized.\n            **kwargs: The kwargs to pass to the pydantic init_subclass method.\n\n        Raises:\n            StateValueError: If a substate class shadows another.\n        \"\"\"\n        from reflex.utils.exceptions import StateValueError\n\n        super().__init_subclass__(**kwargs)\n\n        cls._mixin = mixin\n        if mixin:\n            return\n\n        # Event handlers should not shadow builtin state methods.\n        cls._check_overridden_methods()\n        # Computed vars should not shadow builtin state props.\n        cls._check_overriden_basevars()\n\n        # Reset subclass tracking for this class.\n        cls.class_subclasses = set()\n\n        # Reset dirty substate tracking for this class.\n        cls._always_dirty_substates = set()\n\n        # Get the parent vars.\n        parent_state = cls.get_parent_state()\n        if parent_state is not None:\n            cls.inherited_vars = parent_state.vars\n            cls.inherited_backend_vars = parent_state.backend_vars\n\n            # Check if another substate class with the same name has already been defined.\n            if cls.__name__ in set(c.__name__ for c in parent_state.class_subclasses):\n                if is_testing_env():\n                    # Clear existing subclass with same name when app is reloaded via\n                    # utils.prerequisites.get_app(reload=True)\n                    parent_state.class_subclasses = set(\n                        c\n                        for c in parent_state.class_subclasses\n                        if c.__name__ != cls.__name__\n                    )\n                else:\n                    # During normal operation, subclasses cannot have the same name, even if they are\n                    # defined in different modules.\n                    raise StateValueError(\n                        f\"The substate class '{cls.__name__}' has been defined multiple times. \"\n                        \"Shadowing substate classes is not allowed.\"\n                    )\n            # Track this new subclass in the parent state's subclasses set.\n            parent_state.class_subclasses.add(cls)\n\n        # Get computed vars.\n        computed_vars = cls._get_computed_vars()\n\n        new_backend_vars = {\n            name: value\n            for name, value in cls.__dict__.items()\n            if types.is_backend_base_variable(name, cls)\n        }\n\n        cls.backend_vars = {\n            **cls.inherited_backend_vars,\n            **new_backend_vars,\n        }\n\n        # Set the base and computed vars.\n        cls.base_vars = {\n            f.name: BaseVar(_var_name=f.name, _var_type=f.outer_type_)._var_set_state(\n                cls\n            )\n            for f in cls.get_fields().values()\n            if f.name not in cls.get_skip_vars()\n        }\n        cls.computed_vars = {v._var_name: v._var_set_state(cls) for v in computed_vars}\n        cls.vars = {\n            **cls.inherited_vars,\n            **cls.base_vars,\n            **cls.computed_vars,\n        }\n        cls.event_handlers = {}\n\n        # Setup the base vars at the class level.\n        for prop in cls.base_vars.values():\n            cls._init_var(prop)\n\n        # Set up the event handlers.\n        events = {\n            name: fn\n            for name, fn in cls.__dict__.items()\n            if cls._item_is_event_handler(name, fn)\n        }\n\n        for mixin in cls._mixins():\n            for name, value in mixin.__dict__.items():\n                if isinstance(value, ComputedVar):\n                    fget = cls._copy_fn(value.fget)\n                    newcv = value._replace(fget=fget)\n                    # cleanup refs to mixin cls in var_data\n                    newcv._var_data = None\n                    newcv._var_set_state(cls)\n                    setattr(cls, name, newcv)\n                    cls.computed_vars[newcv._var_name] = newcv\n                    cls.vars[newcv._var_name] = newcv\n                    continue\n                if types.is_backend_base_variable(name, mixin):\n                    cls.backend_vars[name] = copy.deepcopy(value)\n                    continue\n                if events.get(name) is not None:\n                    continue\n                if not cls._item_is_event_handler(name, value):\n                    continue\n                if parent_state is not None and parent_state.event_handlers.get(name):\n                    continue\n                value = cls._copy_fn(value)\n                value.__qualname__ = f\"{cls.__name__}.{name}\"\n                events[name] = value\n\n        # Create the setvar event handler for this state\n        cls._create_setvar()\n\n        for name, fn in events.items():\n            handler = cls._create_event_handler(fn)\n            cls.event_handlers[name] = handler\n            setattr(cls, name, handler)\n\n        cls._init_var_dependency_dicts()\n\n    @staticmethod\n    def _copy_fn(fn: Callable) -> Callable:\n        \"\"\"Copy a function. Used to copy ComputedVars and EventHandlers from mixins.\n\n        Args:\n            fn: The function to copy.\n\n        Returns:\n            The copied function.\n        \"\"\"\n        newfn = FunctionType(\n            fn.__code__,\n            fn.__globals__,\n            name=fn.__name__,\n            argdefs=fn.__defaults__,\n            closure=fn.__closure__,\n        )\n        newfn.__annotations__ = fn.__annotations__\n        if mark := getattr(fn, BACKGROUND_TASK_MARKER, None):\n            setattr(newfn, BACKGROUND_TASK_MARKER, mark)\n        return newfn\n\n    @staticmethod\n    def _item_is_event_handler(name: str, value: Any) -> bool:\n        \"\"\"Check if the item is an event handler.\n\n        Args:\n            name: The name of the item.\n            value: The value of the item.\n\n        Returns:\n            Whether the item is an event handler.\n        \"\"\"\n        return (\n            not name.startswith(\"_\")\n            and isinstance(value, Callable)\n            and not isinstance(value, EventHandler)\n            and hasattr(value, \"__code__\")\n        )\n\n    @classmethod\n    def _mixins(cls) -> List[Type]:\n        \"\"\"Get the mixin classes of the state.\n\n        Returns:\n            The mixin classes of the state.\n        \"\"\"\n        return [\n            mixin\n            for mixin in cls.__mro__\n            if (\n                mixin not in [pydantic.BaseModel, Base, cls]\n                and issubclass(mixin, BaseState)\n                and mixin._mixin is True\n            )\n        ]\n\n    @classmethod\n    def _init_var_dependency_dicts(cls):\n        \"\"\"Initialize the var dependency tracking dicts.\n\n        Allows the state to know which vars each ComputedVar depends on and\n        whether a ComputedVar depends on a var in its parent state.\n\n        Additional updates tracking dicts for vars and substates that always\n        need to be recomputed.\n        \"\"\"\n        # Initialize per-class var dependency tracking.\n        cls._computed_var_dependencies = defaultdict(set)\n        cls._substate_var_dependencies = defaultdict(set)\n\n        inherited_vars = set(cls.inherited_vars).union(\n            set(cls.inherited_backend_vars),\n        )\n        for cvar_name, cvar in cls.computed_vars.items():\n            # Add the dependencies.\n            for var in cvar._deps(objclass=cls):\n                cls._computed_var_dependencies[var].add(cvar_name)\n                if var in inherited_vars:\n                    # track that this substate depends on its parent for this var\n                    state_name = cls.get_name()\n                    parent_state = cls.get_parent_state()\n                    while parent_state is not None and var in {\n                        **parent_state.vars,\n                        **parent_state.backend_vars,\n                    }:\n                        parent_state._substate_var_dependencies[var].add(state_name)\n                        state_name, parent_state = (\n                            parent_state.get_name(),\n                            parent_state.get_parent_state(),\n                        )\n\n        # ComputedVar with cache=False always need to be recomputed\n        cls._always_dirty_computed_vars = set(\n            cvar_name\n            for cvar_name, cvar in cls.computed_vars.items()\n            if not cvar._cache\n        )\n\n        # Any substate containing a ComputedVar with cache=False always needs to be recomputed\n        if cls._always_dirty_computed_vars:\n            # Tell parent classes that this substate has always dirty computed vars\n            state_name = cls.get_name()\n            parent_state = cls.get_parent_state()\n            while parent_state is not None:\n                parent_state._always_dirty_substates.add(state_name)\n                state_name, parent_state = (\n                    parent_state.get_name(),\n                    parent_state.get_parent_state(),\n                )\n\n    @classmethod\n    def _check_overridden_methods(cls):\n        \"\"\"Check for shadow methods and raise error if any.\n\n        Raises:\n            NameError: When an event handler shadows an inbuilt state method.\n        \"\"\"\n        overridden_methods = set()\n        state_base_functions = cls._get_base_functions()\n        for name, method in inspect.getmembers(cls, inspect.isfunction):\n            # Check if the method is overridden and not a dunder method\n            if (\n                not name.startswith(\"__\")\n                and method.__name__ in state_base_functions\n                and state_base_functions[method.__name__] != method\n            ):\n                overridden_methods.add(method.__name__)\n\n        for method_name in overridden_methods:\n            raise NameError(\n                f\"The event handler name `{method_name}` shadows a builtin State method; use a different name instead\"\n            )\n\n    @classmethod\n    def _check_overriden_basevars(cls):\n        \"\"\"Check for shadow base vars and raise error if any.\n\n        Raises:\n            NameError: When a computed var shadows a base var.\n        \"\"\"\n        for computed_var_ in cls._get_computed_vars():\n            if computed_var_._var_name in cls.__annotations__:\n                raise NameError(\n                    f\"The computed var name `{computed_var_._var_name}` shadows a base var in {cls.__module__}.{cls.__\n```\n# Function Signature: def add_var(cls, name: str, type_: Any, default_value: Any = None):\n# Natural Language Annotations: \"\"\"Add dynamically a variable to the State.\n\n        The variable added this way can be used in the same way as a variable\n        defined statically in the model.\n\n        Args:\n            name: The name of the variable\n            type_: The type of the variable\n            default_value: The default value of the variable\n\n        Raises:\n            NameError: if a variable of this name already exists\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef add_var(cls, name: str, type_", "  ```\ndef add_var(cls, name: str, type_", "  ```\n@classmethod\ndef add_var(cls, name:"]}, "reflex-reflex/state.py-reset": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/state.py:\n```\n\"\"\"Define the reflex state specification.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport copy\nimport functools\nimport inspect\nimport os\nimport traceback\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom types import FunctionType, MethodType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Callable,\n    ClassVar,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Type,\n)\n\nimport dill\n\ntry:\n    import pydantic.v1 as pydantic\nexcept ModuleNotFoundError:\n    import pydantic\n\nimport wrapt\nfrom redis.asyncio import Redis\nfrom redis.exceptions import ResponseError\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.event import (\n    BACKGROUND_TASK_MARKER,\n    Event,\n    EventHandler,\n    EventSpec,\n    fix_events,\n    window_alert,\n)\nfrom reflex.utils import console, format, prerequisites, types\nfrom reflex.utils.exceptions import ImmutableStateError, LockExpiredError\nfrom reflex.utils.exec import is_testing_env\nfrom reflex.utils.serializers import SerializedType, serialize, serializer\nfrom reflex.vars import BaseVar, ComputedVar, Var, computed_var\n\nif TYPE_CHECKING:\n    from reflex.components.component import Component\n\n\nDelta = Dict[str, Any]\nvar = computed_var\n\n\n# If the state is this large, it's considered a performance issue.\nTOO_LARGE_SERIALIZED_STATE = 100 * 1024  # 100kb\n\n\nclass HeaderData(Base):\n    \"\"\"An object containing headers data.\"\"\"\n\n    host: str = \"\"\n    origin: str = \"\"\n    upgrade: str = \"\"\n    connection: str = \"\"\n    pragma: str = \"\"\n    cache_control: str = \"\"\n    user_agent: str = \"\"\n    sec_websocket_version: str = \"\"\n    sec_websocket_key: str = \"\"\n    sec_websocket_extensions: str = \"\"\n    accept_encoding: str = \"\"\n    accept_language: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the HeaderData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            for k, v in router_data.get(constants.RouteVar.HEADERS, {}).items():\n                setattr(self, format.to_snake_case(k), v)\n\n\nclass PageData(Base):\n    \"\"\"An object containing page data.\"\"\"\n\n    host: str = \"\"  # repeated with self.headers.origin (remove or keep the duplicate?)\n    path: str = \"\"\n    raw_path: str = \"\"\n    full_path: str = \"\"\n    full_raw_path: str = \"\"\n    params: dict = {}\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the PageData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.host = router_data.get(constants.RouteVar.HEADERS, {}).get(\"origin\")\n            self.path = router_data.get(constants.RouteVar.PATH, \"\")\n            self.raw_path = router_data.get(constants.RouteVar.ORIGIN, \"\")\n            self.full_path = f\"{self.host}{self.path}\"\n            self.full_raw_path = f\"{self.host}{self.raw_path}\"\n            self.params = router_data.get(constants.RouteVar.QUERY, {})\n\n\nclass SessionData(Base):\n    \"\"\"An object containing session data.\"\"\"\n\n    client_token: str = \"\"\n    client_ip: str = \"\"\n    session_id: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the SessionData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.client_token = router_data.get(constants.RouteVar.CLIENT_TOKEN, \"\")\n            self.client_ip = router_data.get(constants.RouteVar.CLIENT_IP, \"\")\n            self.session_id = router_data.get(constants.RouteVar.SESSION_ID, \"\")\n\n\nclass RouterData(Base):\n    \"\"\"An object containing RouterData.\"\"\"\n\n    session: SessionData = SessionData()\n    headers: HeaderData = HeaderData()\n    page: PageData = PageData()\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initialize the RouterData object.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        self.session = SessionData(router_data)\n        self.headers = HeaderData(router_data)\n        self.page = PageData(router_data)\n\n\ndef _no_chain_background_task(\n    state_cls: Type[\"BaseState\"], name: str, fn: Callable\n) -> Callable:\n    \"\"\"Protect against directly chaining a background task from another event handler.\n\n    Args:\n        state_cls: The state class that the event handler is in.\n        name: The name of the background task.\n        fn: The background task coroutine function / generator.\n\n    Returns:\n        A compatible coroutine function / generator that raises a runtime error.\n\n    Raises:\n        TypeError: If the background task is not async.\n    \"\"\"\n    call = f\"{state_cls.__name__}.{name}\"\n    message = (\n        f\"Cannot directly call background task {name!r}, use \"\n        f\"`yield {call}` or `return {call}` instead.\"\n    )\n    if inspect.iscoroutinefunction(fn):\n\n        async def _no_chain_background_task_co(*args, **kwargs):\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_co\n    if inspect.isasyncgenfunction(fn):\n\n        async def _no_chain_background_task_gen(*args, **kwargs):\n            yield\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_gen\n\n    raise TypeError(f\"{fn} is marked as a background task, but is not async.\")\n\n\ndef _substate_key(\n    token: str,\n    state_cls_or_name: BaseState | Type[BaseState] | str | list[str],\n) -> str:\n    \"\"\"Get the substate key.\n\n    Args:\n        token: The token of the state.\n        state_cls_or_name: The state class/instance or name or sequence of name parts.\n\n    Returns:\n        The substate key.\n    \"\"\"\n    if isinstance(state_cls_or_name, BaseState) or (\n        isinstance(state_cls_or_name, type) and issubclass(state_cls_or_name, BaseState)\n    ):\n        state_cls_or_name = state_cls_or_name.get_full_name()\n    elif isinstance(state_cls_or_name, (list, tuple)):\n        state_cls_or_name = \".\".join(state_cls_or_name)\n    return f\"{token}_{state_cls_or_name}\"\n\n\ndef _split_substate_key(substate_key: str) -> tuple[str, str]:\n    \"\"\"Split the substate key into token and state name.\n\n    Args:\n        substate_key: The substate key.\n\n    Returns:\n        Tuple of token and state name.\n    \"\"\"\n    token, _, state_name = substate_key.partition(\"_\")\n    return token, state_name\n\n\nclass EventHandlerSetVar(EventHandler):\n    \"\"\"A special event handler to wrap setvar functionality.\"\"\"\n\n    state_cls: Type[BaseState]\n\n    def __init__(self, state_cls: Type[BaseState]):\n        \"\"\"Initialize the EventHandlerSetVar.\n\n        Args:\n            state_cls: The state class that vars will be set on.\n        \"\"\"\n        super().__init__(\n            fn=type(self).setvar,\n            state_full_name=state_cls.get_full_name(),\n            state_cls=state_cls,  # type: ignore\n        )\n\n    def setvar(self, var_name: str, value: Any):\n        \"\"\"Set the state variable to the value of the event.\n\n        Note: `self` here will be an instance of the state, not EventHandlerSetVar.\n\n        Args:\n            var_name: The name of the variable to set.\n            value: The value to set the variable to.\n        \"\"\"\n        getattr(self, constants.SETTER_PREFIX + var_name)(value)\n\n    def __call__(self, *args: Any) -> EventSpec:\n        \"\"\"Performs pre-checks and munging on the provided args that will become an EventSpec.\n\n        Args:\n            *args: The event args.\n\n        Returns:\n            The (partial) EventSpec that will be used to create the event to setvar.\n\n        Raises:\n            AttributeError: If the given Var name does not exist on the state.\n            EventHandlerValueError: If the given Var name is not a str\n        \"\"\"\n        from reflex.utils.exceptions import EventHandlerValueError\n\n        if args:\n            if not isinstance(args[0], str):\n                raise EventHandlerValueError(\n                    f\"Var name must be passed as a string, got {args[0]!r}\"\n                )\n            # Check that the requested Var setter exists on the State at compile time.\n            if getattr(self.state_cls, constants.SETTER_PREFIX + args[0], None) is None:\n                raise AttributeError(\n                    f\"Variable `{args[0]}` cannot be set on `{self.state_cls.get_full_name()}`\"\n                )\n        return super().__call__(*args)\n\n\nclass BaseState(Base, ABC, extra=pydantic.Extra.allow):\n    \"\"\"The state of the app.\"\"\"\n\n    # A map from the var name to the var.\n    vars: ClassVar[Dict[str, Var]] = {}\n\n    # The base vars of the class.\n    base_vars: ClassVar[Dict[str, BaseVar]] = {}\n\n    # The computed vars of the class.\n    computed_vars: ClassVar[Dict[str, ComputedVar]] = {}\n\n    # Vars inherited by the parent state.\n    inherited_vars: ClassVar[Dict[str, Var]] = {}\n\n    # Backend base vars that are never sent to the client.\n    backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # Backend base vars inherited\n    inherited_backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # The event handlers.\n    event_handlers: ClassVar[Dict[str, EventHandler]] = {}\n\n    # A set of subclassses of this class.\n    class_subclasses: ClassVar[Set[Type[BaseState]]] = set()\n\n    # Mapping of var name to set of computed variables that depend on it\n    _computed_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Mapping of var name to set of substates that depend on it\n    _substate_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Set of vars which always need to be recomputed\n    _always_dirty_computed_vars: ClassVar[Set[str]] = set()\n\n    # Set of substates which always need to be recomputed\n    _always_dirty_substates: ClassVar[Set[str]] = set()\n\n    # The parent state.\n    parent_state: Optional[BaseState] = None\n\n    # The substates of the state.\n    substates: Dict[str, BaseState] = {}\n\n    # The set of dirty vars.\n    dirty_vars: Set[str] = set()\n\n    # The set of dirty substates.\n    dirty_substates: Set[str] = set()\n\n    # The routing path that triggered the state\n    router_data: Dict[str, Any] = {}\n\n    # Per-instance copy of backend base variable values\n    _backend_vars: Dict[str, Any] = {}\n\n    # The router data for the current page\n    router: RouterData = RouterData()\n\n    # Whether the state has ever been touched since instantiation.\n    _was_touched: bool = False\n\n    # Whether this state class is a mixin and should not be instantiated.\n    _mixin: ClassVar[bool] = False\n\n    # A special event handler for setting base vars.\n    setvar: ClassVar[EventHandler]\n\n    def __init__(\n        self,\n        *args,\n        parent_state: BaseState | None = None,\n        init_substates: bool = True,\n        _reflex_internal_init: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize the state.\n\n        DO NOT INSTANTIATE STATE CLASSES DIRECTLY! Use StateManager.get_state() instead.\n\n        Args:\n            *args: The args to pass to the Pydantic init method.\n            parent_state: The parent state.\n            init_substates: Whether to initialize the substates in this instance.\n            _reflex_internal_init: A flag to indicate that the state is being initialized by the framework.\n            **kwargs: The kwargs to pass to the Pydantic init method.\n\n        Raises:\n            ReflexRuntimeError: If the state is instantiated directly by end user.\n        \"\"\"\n        from reflex.utils.exceptions import ReflexRuntimeError\n\n        if not _reflex_internal_init and not is_testing_env():\n            raise ReflexRuntimeError(\n                \"State classes should not be instantiated directly in a Reflex app. \"\n                \"See https://reflex.dev/docs/state/ for further information.\"\n            )\n        kwargs[\"parent_state\"] = parent_state\n        super().__init__(*args, **kwargs)\n\n        # Setup the substates (for memory state manager only).\n        if init_substates:\n            for substate in self.get_substates():\n                self.substates[substate.get_name()] = substate(\n                    parent_state=self,\n                    _reflex_internal_init=True,\n                )\n\n        # Create a fresh copy of the backend variables for this instance\n        self._backend_vars = copy.deepcopy(\n            {name: item for name, item in self.backend_vars.items()}\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the state.\n\n        Returns:\n            The string representation of the state.\n        \"\"\"\n        return f\"{self.__class__.__name__}({self.dict()})\"\n\n    @classmethod\n    def _get_computed_vars(cls) -> list[ComputedVar]:\n        \"\"\"Helper function to get all computed vars of a instance.\n\n        Returns:\n            A list of computed vars.\n        \"\"\"\n        return [\n            v\n            for mixin in cls._mixins() + [cls]\n            for v in mixin.__dict__.values()\n            if isinstance(v, ComputedVar)\n        ]\n\n    @classmethod\n    def __init_subclass__(cls, mixin: bool = False, **kwargs):\n        \"\"\"Do some magic for the subclass initialization.\n\n        Args:\n            mixin: Whether the subclass is a mixin and should not be initialized.\n            **kwargs: The kwargs to pass to the pydantic init_subclass method.\n\n        Raises:\n            StateValueError: If a substate class shadows another.\n        \"\"\"\n        from reflex.utils.exceptions import StateValueError\n\n        super().__init_subclass__(**kwargs)\n\n        cls._mixin = mixin\n        if mixin:\n            return\n\n        # Event handlers should not shadow builtin state methods.\n        cls._check_overridden_methods()\n        # Computed vars should not shadow builtin state props.\n        cls._check_overriden_basevars()\n\n        # Reset subclass tracking for this class.\n        cls.class_subclasses = set()\n\n        # Reset dirty substate tracking for this class.\n        cls._always_dirty_substates = set()\n\n        # Get the parent vars.\n        parent_state = cls.get_parent_state()\n        if parent_state is not None:\n            cls.inherited_vars = parent_state.vars\n            cls.inherited_backend_vars = parent_state.backend_vars\n\n            # Check if another substate class with the same name has already been defined.\n            if cls.__name__ in set(c.__name__ for c in parent_state.class_subclasses):\n                if is_testing_env():\n                    # Clear existing subclass with same name when app is reloaded via\n                    # utils.prerequisites.get_app(reload=True)\n                    parent_state.class_subclasses = set(\n                        c\n                        for c in parent_state.class_subclasses\n                        if c.__name__ != cls.__name__\n                    )\n                else:\n                    # During normal operation, subclasses cannot have the same name, even if they are\n                    # defined in different modules.\n                    raise StateValueError(\n                        f\"The substate class '{cls.__name__}' has been defined multiple times. \"\n                        \"Shadowing substate classes is not allowed.\"\n                    )\n            # Track this new subclass in the parent state's subclasses set.\n            parent_state.class_subclasses.add(cls)\n\n        # Get computed vars.\n        computed_vars = cls._get_computed_vars()\n\n        new_backend_vars = {\n            name: value\n            for name, value in cls.__dict__.items()\n            if types.is_backend_base_variable(name, cls)\n        }\n\n        cls.backend_vars = {\n            **cls.inherited_backend_vars,\n            **new_backend_vars,\n        }\n\n        # Set the base and computed vars.\n        cls.base_vars = {\n            f.name: BaseVar(_var_name=f.name, _var_type=f.outer_type_)._var_set_state(\n                cls\n            )\n            for f in cls.get_fields().values()\n            if f.name not in cls.get_skip_vars()\n        }\n        cls.computed_vars = {v._var_name: v._var_set_state(cls) for v in computed_vars}\n        cls.vars = {\n            **cls.inherited_vars,\n            **cls.base_vars,\n            **cls.computed_vars,\n        }\n        cls.event_handlers = {}\n\n        # Setup the base vars at the class level.\n        for prop in cls.base_vars.values():\n            cls._init_var(prop)\n\n        # Set up the event handlers.\n        events = {\n            name: fn\n            for name, fn in cls.__dict__.items()\n            if cls._item_is_event_handler(name, fn)\n        }\n\n        for mixin in cls._mixins():\n            for name, value in mixin.__dict__.items():\n                if isinstance(value, ComputedVar):\n                    fget = cls._copy_fn(value.fget)\n                    newcv = value._replace(fget=fget)\n                    # cleanup refs to mixin cls in var_data\n                    newcv._var_data = None\n                    newcv._var_set_state(cls)\n                    setattr(cls, name, newcv)\n                    cls.computed_vars[newcv._var_name] = newcv\n                    cls.vars[newcv._var_name] = newcv\n                    continue\n                if types.is_backend_base_variable(name, mixin):\n                    cls.backend_vars[name] = copy.deepcopy(value)\n                    continue\n                if events.get(name) is not None:\n                    continue\n                if not cls._item_is_event_handler(name, value):\n                    continue\n                if parent_state is not None and parent_state.event_handlers.get(name):\n                    continue\n                value = cls._copy_fn(value)\n                value.__qualname__ = f\"{cls.__name__}.{name}\"\n                events[name] = value\n\n        # Create the setvar event handler for this state\n        cls._create_setvar()\n\n        for name, fn in events.items():\n            handler = cls._create_event_handler(fn)\n            cls.event_handlers[name] = handler\n            setattr(cls, name, handler)\n\n        cls._init_var_dependency_dicts()\n\n    @staticmethod\n    def _copy_fn(fn: Callable) -> Callable:\n        \"\"\"Copy a function. Used to copy ComputedVars and EventHandlers from mixins.\n\n        Args:\n            fn: The function to copy.\n\n        Returns:\n            The copied function.\n        \"\"\"\n        newfn = FunctionType(\n            fn.__code__,\n            fn.__globals__,\n            name=fn.__name__,\n            argdefs=fn.__defaults__,\n            closure=fn.__closure__,\n        )\n        newfn.__annotations__ = fn.__annotations__\n        if mark := getattr(fn, BACKGROUND_TASK_MARKER, None):\n            setattr(newfn, BACKGROUND_TASK_MARKER, mark)\n        return newfn\n\n    @staticmethod\n    def _item_is_event_handler(name: str, value: Any) -> bool:\n        \"\"\"Check if the item is an event handler.\n\n        Args:\n            name: The name of the item.\n            value: The value of the item.\n\n        Returns:\n            Whether the item is an event handler.\n        \"\"\"\n        return (\n            not name.startswith(\"_\")\n            and isinstance(value, Callable)\n            and not isinstance(value, EventHandler)\n            and hasattr(value, \"__code__\")\n        )\n\n    @classmethod\n    def _mixins(cls) -> List[Type]:\n        \"\"\"Get the mixin classes of the state.\n\n        Returns:\n            The mixin classes of the state.\n        \"\"\"\n        return [\n            mixin\n            for mixin in cls.__mro__\n            if (\n                mixin not in [pydantic.BaseModel, Base, cls]\n                and issubclass(mixin, BaseState)\n                and mixin._mixin is True\n            )\n        ]\n\n    @classmethod\n    def _init_var_dependency_dicts(cls):\n        \"\"\"Initialize the var dependency tracking dicts.\n\n        Allows the state to know which vars each ComputedVar depends on and\n        whether a ComputedVar depends on a var in its parent state.\n\n        Additional updates tracking dicts for vars and substates that always\n        need to be recomputed.\n        \"\"\"\n        # Initialize per-class var dependency tracking.\n        cls._computed_var_dependencies = defaultdict(set)\n        cls._substate_var_dependencies = defaultdict(set)\n\n        inherited_vars = set(cls.inherited_vars).union(\n            set(cls.inherited_backend_vars),\n        )\n        for cvar_name, cvar in cls.computed_vars.items():\n            # Add the dependencies.\n            for var in cvar._deps(objclass=cls):\n                cls._computed_var_dependencies[var].add(cvar_name)\n                if var in inherited_vars:\n                    # track that this substate depends on its parent for this var\n                    state_name = cls.get_name()\n                    parent_state = cls.get_parent_state()\n                    while parent_state is not None and var in {\n                        **parent_state.vars,\n                        **parent_state.backend_vars,\n                    }:\n                        parent_state._substate_var_dependencies[var].add(state_name)\n                        state_name, parent_state = (\n                            parent_state.get_name(),\n                            parent_state.get_parent_state(),\n                        )\n\n        # ComputedVar with cache=False always need to be recomputed\n        cls._always_dirty_computed_vars = set(\n            cvar_name\n            for cvar_name, cvar in cls.computed_vars.items()\n            if not cvar._cache\n        )\n\n        # Any substate containing a ComputedVar with cache=False always needs to be recomputed\n        if cls._always_dirty_computed_vars:\n            # Tell parent classes that this substate has always dirty computed vars\n            state_name = cls.get_name()\n            parent_state = cls.get_parent_state()\n            while parent_state is not None:\n                parent_state._always_dirty_substates.add(state_name)\n                state_name, parent_state = (\n                    parent_state.get_name(),\n                    parent_state.get_parent_state(),\n                )\n\n    @classmethod\n    def _check_overridden_methods(cls):\n        \"\"\"Check for shadow methods and raise error if any.\n\n        Raises:\n            NameError: When an event handler shadows an inbuilt state method.\n        \"\"\"\n        overridden_methods = set()\n        state_base_functions = cls._get_base_functions()\n        for name, method in inspect.getmembers(cls, inspect.isfunction):\n            # Check if the method is overridden and not a dunder method\n            if (\n                not name.startswith(\"__\")\n                and method.__name__ in state_base_functions\n                and state_base_functions[method.__name__] != method\n            ):\n                overridden_methods.add(method.__name__)\n\n        for method_name in overridden_methods:\n            raise NameError(\n                f\"The event handler name `{method_name}` shadows a builtin State method; use a different name instead\"\n            )\n\n    @classmethod\n    def _check_overriden_basevars(cls):\n        \"\"\"Check for shadow base vars and raise error if any.\n\n        Raises:\n            NameError: When a computed var shadows a base var.\n        \"\"\"\n        for computed_var_ in cls._get_computed_vars():\n            if computed_var_._var_name in cls.__annotations__:\n                raise NameError(\n                    f\"The computed var name `{computed_var_._var_name}` shadows a base var in {cls.__module__}.{cls.__\n```\n# Function Signature: def reset(self):\n# Natural Language Annotations: \"\"\"Reset all the base vars to their default values.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef reset(self) -> None:\n    # Reset", "  ```\ndef reset(self):\n    \"\"\"Reset all the base vars", "  ```\ndef reset(self):\n    \"\"\"Reset all the base vars"]}, "reflex-reflex/state.py-get_state": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/state.py:\n```\n\"\"\"Define the reflex state specification.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport copy\nimport functools\nimport inspect\nimport os\nimport traceback\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom types import FunctionType, MethodType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Callable,\n    ClassVar,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Type,\n)\n\nimport dill\n\ntry:\n    import pydantic.v1 as pydantic\nexcept ModuleNotFoundError:\n    import pydantic\n\nimport wrapt\nfrom redis.asyncio import Redis\nfrom redis.exceptions import ResponseError\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.event import (\n    BACKGROUND_TASK_MARKER,\n    Event,\n    EventHandler,\n    EventSpec,\n    fix_events,\n    window_alert,\n)\nfrom reflex.utils import console, format, prerequisites, types\nfrom reflex.utils.exceptions import ImmutableStateError, LockExpiredError\nfrom reflex.utils.exec import is_testing_env\nfrom reflex.utils.serializers import SerializedType, serialize, serializer\nfrom reflex.vars import BaseVar, ComputedVar, Var, computed_var\n\nif TYPE_CHECKING:\n    from reflex.components.component import Component\n\n\nDelta = Dict[str, Any]\nvar = computed_var\n\n\n# If the state is this large, it's considered a performance issue.\nTOO_LARGE_SERIALIZED_STATE = 100 * 1024  # 100kb\n\n\nclass HeaderData(Base):\n    \"\"\"An object containing headers data.\"\"\"\n\n    host: str = \"\"\n    origin: str = \"\"\n    upgrade: str = \"\"\n    connection: str = \"\"\n    pragma: str = \"\"\n    cache_control: str = \"\"\n    user_agent: str = \"\"\n    sec_websocket_version: str = \"\"\n    sec_websocket_key: str = \"\"\n    sec_websocket_extensions: str = \"\"\n    accept_encoding: str = \"\"\n    accept_language: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the HeaderData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            for k, v in router_data.get(constants.RouteVar.HEADERS, {}).items():\n                setattr(self, format.to_snake_case(k), v)\n\n\nclass PageData(Base):\n    \"\"\"An object containing page data.\"\"\"\n\n    host: str = \"\"  # repeated with self.headers.origin (remove or keep the duplicate?)\n    path: str = \"\"\n    raw_path: str = \"\"\n    full_path: str = \"\"\n    full_raw_path: str = \"\"\n    params: dict = {}\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the PageData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.host = router_data.get(constants.RouteVar.HEADERS, {}).get(\"origin\")\n            self.path = router_data.get(constants.RouteVar.PATH, \"\")\n            self.raw_path = router_data.get(constants.RouteVar.ORIGIN, \"\")\n            self.full_path = f\"{self.host}{self.path}\"\n            self.full_raw_path = f\"{self.host}{self.raw_path}\"\n            self.params = router_data.get(constants.RouteVar.QUERY, {})\n\n\nclass SessionData(Base):\n    \"\"\"An object containing session data.\"\"\"\n\n    client_token: str = \"\"\n    client_ip: str = \"\"\n    session_id: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the SessionData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.client_token = router_data.get(constants.RouteVar.CLIENT_TOKEN, \"\")\n            self.client_ip = router_data.get(constants.RouteVar.CLIENT_IP, \"\")\n            self.session_id = router_data.get(constants.RouteVar.SESSION_ID, \"\")\n\n\nclass RouterData(Base):\n    \"\"\"An object containing RouterData.\"\"\"\n\n    session: SessionData = SessionData()\n    headers: HeaderData = HeaderData()\n    page: PageData = PageData()\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initialize the RouterData object.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        self.session = SessionData(router_data)\n        self.headers = HeaderData(router_data)\n        self.page = PageData(router_data)\n\n\ndef _no_chain_background_task(\n    state_cls: Type[\"BaseState\"], name: str, fn: Callable\n) -> Callable:\n    \"\"\"Protect against directly chaining a background task from another event handler.\n\n    Args:\n        state_cls: The state class that the event handler is in.\n        name: The name of the background task.\n        fn: The background task coroutine function / generator.\n\n    Returns:\n        A compatible coroutine function / generator that raises a runtime error.\n\n    Raises:\n        TypeError: If the background task is not async.\n    \"\"\"\n    call = f\"{state_cls.__name__}.{name}\"\n    message = (\n        f\"Cannot directly call background task {name!r}, use \"\n        f\"`yield {call}` or `return {call}` instead.\"\n    )\n    if inspect.iscoroutinefunction(fn):\n\n        async def _no_chain_background_task_co(*args, **kwargs):\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_co\n    if inspect.isasyncgenfunction(fn):\n\n        async def _no_chain_background_task_gen(*args, **kwargs):\n            yield\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_gen\n\n    raise TypeError(f\"{fn} is marked as a background task, but is not async.\")\n\n\ndef _substate_key(\n    token: str,\n    state_cls_or_name: BaseState | Type[BaseState] | str | list[str],\n) -> str:\n    \"\"\"Get the substate key.\n\n    Args:\n        token: The token of the state.\n        state_cls_or_name: The state class/instance or name or sequence of name parts.\n\n    Returns:\n        The substate key.\n    \"\"\"\n    if isinstance(state_cls_or_name, BaseState) or (\n        isinstance(state_cls_or_name, type) and issubclass(state_cls_or_name, BaseState)\n    ):\n        state_cls_or_name = state_cls_or_name.get_full_name()\n    elif isinstance(state_cls_or_name, (list, tuple)):\n        state_cls_or_name = \".\".join(state_cls_or_name)\n    return f\"{token}_{state_cls_or_name}\"\n\n\ndef _split_substate_key(substate_key: str) -> tuple[str, str]:\n    \"\"\"Split the substate key into token and state name.\n\n    Args:\n        substate_key: The substate key.\n\n    Returns:\n        Tuple of token and state name.\n    \"\"\"\n    token, _, state_name = substate_key.partition(\"_\")\n    return token, state_name\n\n\nclass EventHandlerSetVar(EventHandler):\n    \"\"\"A special event handler to wrap setvar functionality.\"\"\"\n\n    state_cls: Type[BaseState]\n\n    def __init__(self, state_cls: Type[BaseState]):\n        \"\"\"Initialize the EventHandlerSetVar.\n\n        Args:\n            state_cls: The state class that vars will be set on.\n        \"\"\"\n        super().__init__(\n            fn=type(self).setvar,\n            state_full_name=state_cls.get_full_name(),\n            state_cls=state_cls,  # type: ignore\n        )\n\n    def setvar(self, var_name: str, value: Any):\n        \"\"\"Set the state variable to the value of the event.\n\n        Note: `self` here will be an instance of the state, not EventHandlerSetVar.\n\n        Args:\n            var_name: The name of the variable to set.\n            value: The value to set the variable to.\n        \"\"\"\n        getattr(self, constants.SETTER_PREFIX + var_name)(value)\n\n    def __call__(self, *args: Any) -> EventSpec:\n        \"\"\"Performs pre-checks and munging on the provided args that will become an EventSpec.\n\n        Args:\n            *args: The event args.\n\n        Returns:\n            The (partial) EventSpec that will be used to create the event to setvar.\n\n        Raises:\n            AttributeError: If the given Var name does not exist on the state.\n            EventHandlerValueError: If the given Var name is not a str\n        \"\"\"\n        from reflex.utils.exceptions import EventHandlerValueError\n\n        if args:\n            if not isinstance(args[0], str):\n                raise EventHandlerValueError(\n                    f\"Var name must be passed as a string, got {args[0]!r}\"\n                )\n            # Check that the requested Var setter exists on the State at compile time.\n            if getattr(self.state_cls, constants.SETTER_PREFIX + args[0], None) is None:\n                raise AttributeError(\n                    f\"Variable `{args[0]}` cannot be set on `{self.state_cls.get_full_name()}`\"\n                )\n        return super().__call__(*args)\n\n\nclass BaseState(Base, ABC, extra=pydantic.Extra.allow):\n    \"\"\"The state of the app.\"\"\"\n\n    # A map from the var name to the var.\n    vars: ClassVar[Dict[str, Var]] = {}\n\n    # The base vars of the class.\n    base_vars: ClassVar[Dict[str, BaseVar]] = {}\n\n    # The computed vars of the class.\n    computed_vars: ClassVar[Dict[str, ComputedVar]] = {}\n\n    # Vars inherited by the parent state.\n    inherited_vars: ClassVar[Dict[str, Var]] = {}\n\n    # Backend base vars that are never sent to the client.\n    backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # Backend base vars inherited\n    inherited_backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # The event handlers.\n    event_handlers: ClassVar[Dict[str, EventHandler]] = {}\n\n    # A set of subclassses of this class.\n    class_subclasses: ClassVar[Set[Type[BaseState]]] = set()\n\n    # Mapping of var name to set of computed variables that depend on it\n    _computed_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Mapping of var name to set of substates that depend on it\n    _substate_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Set of vars which always need to be recomputed\n    _always_dirty_computed_vars: ClassVar[Set[str]] = set()\n\n    # Set of substates which always need to be recomputed\n    _always_dirty_substates: ClassVar[Set[str]] = set()\n\n    # The parent state.\n    parent_state: Optional[BaseState] = None\n\n    # The substates of the state.\n    substates: Dict[str, BaseState] = {}\n\n    # The set of dirty vars.\n    dirty_vars: Set[str] = set()\n\n    # The set of dirty substates.\n    dirty_substates: Set[str] = set()\n\n    # The routing path that triggered the state\n    router_data: Dict[str, Any] = {}\n\n    # Per-instance copy of backend base variable values\n    _backend_vars: Dict[str, Any] = {}\n\n    # The router data for the current page\n    router: RouterData = RouterData()\n\n    # Whether the state has ever been touched since instantiation.\n    _was_touched: bool = False\n\n    # Whether this state class is a mixin and should not be instantiated.\n    _mixin: ClassVar[bool] = False\n\n    # A special event handler for setting base vars.\n    setvar: ClassVar[EventHandler]\n\n    def __init__(\n        self,\n        *args,\n        parent_state: BaseState | None = None,\n        init_substates: bool = True,\n        _reflex_internal_init: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize the state.\n\n        DO NOT INSTANTIATE STATE CLASSES DIRECTLY! Use StateManager.get_state() instead.\n\n        Args:\n            *args: The args to pass to the Pydantic init method.\n            parent_state: The parent state.\n            init_substates: Whether to initialize the substates in this instance.\n            _reflex_internal_init: A flag to indicate that the state is being initialized by the framework.\n            **kwargs: The kwargs to pass to the Pydantic init method.\n\n        Raises:\n            ReflexRuntimeError: If the state is instantiated directly by end user.\n        \"\"\"\n        from reflex.utils.exceptions import ReflexRuntimeError\n\n        if not _reflex_internal_init and not is_testing_env():\n            raise ReflexRuntimeError(\n                \"State classes should not be instantiated directly in a Reflex app. \"\n                \"See https://reflex.dev/docs/state/ for further information.\"\n            )\n        kwargs[\"parent_state\"] = parent_state\n        super().__init__(*args, **kwargs)\n\n        # Setup the substates (for memory state manager only).\n        if init_substates:\n            for substate in self.get_substates():\n                self.substates[substate.get_name()] = substate(\n                    parent_state=self,\n                    _reflex_internal_init=True,\n                )\n\n        # Create a fresh copy of the backend variables for this instance\n        self._backend_vars = copy.deepcopy(\n            {name: item for name, item in self.backend_vars.items()}\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the state.\n\n        Returns:\n            The string representation of the state.\n        \"\"\"\n        return f\"{self.__class__.__name__}({self.dict()})\"\n\n    @classmethod\n    def _get_computed_vars(cls) -> list[ComputedVar]:\n        \"\"\"Helper function to get all computed vars of a instance.\n\n        Returns:\n            A list of computed vars.\n        \"\"\"\n        return [\n            v\n            for mixin in cls._mixins() + [cls]\n            for v in mixin.__dict__.values()\n            if isinstance(v, ComputedVar)\n        ]\n\n    @classmethod\n    def __init_subclass__(cls, mixin: bool = False, **kwargs):\n        \"\"\"Do some magic for the subclass initialization.\n\n        Args:\n            mixin: Whether the subclass is a mixin and should not be initialized.\n            **kwargs: The kwargs to pass to the pydantic init_subclass method.\n\n        Raises:\n            StateValueError: If a substate class shadows another.\n        \"\"\"\n        from reflex.utils.exceptions import StateValueError\n\n        super().__init_subclass__(**kwargs)\n\n        cls._mixin = mixin\n        if mixin:\n            return\n\n        # Event handlers should not shadow builtin state methods.\n        cls._check_overridden_methods()\n        # Computed vars should not shadow builtin state props.\n        cls._check_overriden_basevars()\n\n        # Reset subclass tracking for this class.\n        cls.class_subclasses = set()\n\n        # Reset dirty substate tracking for this class.\n        cls._always_dirty_substates = set()\n\n        # Get the parent vars.\n        parent_state = cls.get_parent_state()\n        if parent_state is not None:\n            cls.inherited_vars = parent_state.vars\n            cls.inherited_backend_vars = parent_state.backend_vars\n\n            # Check if another substate class with the same name has already been defined.\n            if cls.__name__ in set(c.__name__ for c in parent_state.class_subclasses):\n                if is_testing_env():\n                    # Clear existing subclass with same name when app is reloaded via\n                    # utils.prerequisites.get_app(reload=True)\n                    parent_state.class_subclasses = set(\n                        c\n                        for c in parent_state.class_subclasses\n                        if c.__name__ != cls.__name__\n                    )\n                else:\n                    # During normal operation, subclasses cannot have the same name, even if they are\n                    # defined in different modules.\n                    raise StateValueError(\n                        f\"The substate class '{cls.__name__}' has been defined multiple times. \"\n                        \"Shadowing substate classes is not allowed.\"\n                    )\n            # Track this new subclass in the parent state's subclasses set.\n            parent_state.class_subclasses.add(cls)\n\n        # Get computed vars.\n        computed_vars = cls._get_computed_vars()\n\n        new_backend_vars = {\n            name: value\n            for name, value in cls.__dict__.items()\n            if types.is_backend_base_variable(name, cls)\n        }\n\n        cls.backend_vars = {\n            **cls.inherited_backend_vars,\n            **new_backend_vars,\n        }\n\n        # Set the base and computed vars.\n        cls.base_vars = {\n            f.name: BaseVar(_var_name=f.name, _var_type=f.outer_type_)._var_set_state(\n                cls\n            )\n            for f in cls.get_fields().values()\n            if f.name not in cls.get_skip_vars()\n        }\n        cls.computed_vars = {v._var_name: v._var_set_state(cls) for v in computed_vars}\n        cls.vars = {\n            **cls.inherited_vars,\n            **cls.base_vars,\n            **cls.computed_vars,\n        }\n        cls.event_handlers = {}\n\n        # Setup the base vars at the class level.\n        for prop in cls.base_vars.values():\n            cls._init_var(prop)\n\n        # Set up the event handlers.\n        events = {\n            name: fn\n            for name, fn in cls.__dict__.items()\n            if cls._item_is_event_handler(name, fn)\n        }\n\n        for mixin in cls._mixins():\n            for name, value in mixin.__dict__.items():\n                if isinstance(value, ComputedVar):\n                    fget = cls._copy_fn(value.fget)\n                    newcv = value._replace(fget=fget)\n                    # cleanup refs to mixin cls in var_data\n                    newcv._var_data = None\n                    newcv._var_set_state(cls)\n                    setattr(cls, name, newcv)\n                    cls.computed_vars[newcv._var_name] = newcv\n                    cls.vars[newcv._var_name] = newcv\n                    continue\n                if types.is_backend_base_variable(name, mixin):\n                    cls.backend_vars[name] = copy.deepcopy(value)\n                    continue\n                if events.get(name) is not None:\n                    continue\n                if not cls._item_is_event_handler(name, value):\n                    continue\n                if parent_state is not None and parent_state.event_handlers.get(name):\n                    continue\n                value = cls._copy_fn(value)\n                value.__qualname__ = f\"{cls.__name__}.{name}\"\n                events[name] = value\n\n        # Create the setvar event handler for this state\n        cls._create_setvar()\n\n        for name, fn in events.items():\n            handler = cls._create_event_handler(fn)\n            cls.event_handlers[name] = handler\n            setattr(cls, name, handler)\n\n        cls._init_var_dependency_dicts()\n\n    @staticmethod\n    def _copy_fn(fn: Callable) -> Callable:\n        \"\"\"Copy a function. Used to copy ComputedVars and EventHandlers from mixins.\n\n        Args:\n            fn: The function to copy.\n\n        Returns:\n            The copied function.\n        \"\"\"\n        newfn = FunctionType(\n            fn.__code__,\n            fn.__globals__,\n            name=fn.__name__,\n            argdefs=fn.__defaults__,\n            closure=fn.__closure__,\n        )\n        newfn.__annotations__ = fn.__annotations__\n        if mark := getattr(fn, BACKGROUND_TASK_MARKER, None):\n            setattr(newfn, BACKGROUND_TASK_MARKER, mark)\n        return newfn\n\n    @staticmethod\n    def _item_is_event_handler(name: str, value: Any) -> bool:\n        \"\"\"Check if the item is an event handler.\n\n        Args:\n            name: The name of the item.\n            value: The value of the item.\n\n        Returns:\n            Whether the item is an event handler.\n        \"\"\"\n        return (\n            not name.startswith(\"_\")\n            and isinstance(value, Callable)\n            and not isinstance(value, EventHandler)\n            and hasattr(value, \"__code__\")\n        )\n\n    @classmethod\n    def _mixins(cls) -> List[Type]:\n        \"\"\"Get the mixin classes of the state.\n\n        Returns:\n            The mixin classes of the state.\n        \"\"\"\n        return [\n            mixin\n            for mixin in cls.__mro__\n            if (\n                mixin not in [pydantic.BaseModel, Base, cls]\n                and issubclass(mixin, BaseState)\n                and mixin._mixin is True\n            )\n        ]\n\n    @classmethod\n    def _init_var_dependency_dicts(cls):\n        \"\"\"Initialize the var dependency tracking dicts.\n\n        Allows the state to know which vars each ComputedVar depends on and\n        whether a ComputedVar depends on a var in its parent state.\n\n        Additional updates tracking dicts for vars and substates that always\n        need to be recomputed.\n        \"\"\"\n        # Initialize per-class var dependency tracking.\n        cls._computed_var_dependencies = defaultdict(set)\n        cls._substate_var_dependencies = defaultdict(set)\n\n        inherited_vars = set(cls.inherited_vars).union(\n            set(cls.inherited_backend_vars),\n        )\n        for cvar_name, cvar in cls.computed_vars.items():\n            # Add the dependencies.\n            for var in cvar._deps(objclass=cls):\n                cls._computed_var_dependencies[var].add(cvar_name)\n                if var in inherited_vars:\n                    # track that this substate depends on its parent for this var\n                    state_name = cls.get_name()\n                    parent_state = cls.get_parent_state()\n                    while parent_state is not None and var in {\n                        **parent_state.vars,\n                        **parent_state.backend_vars,\n                    }:\n                        parent_state._substate_var_dependencies[var].add(state_name)\n                        state_name, parent_state = (\n                            parent_state.get_name(),\n                            parent_state.get_parent_state(),\n                        )\n\n        # ComputedVar with cache=False always need to be recomputed\n        cls._always_dirty_computed_vars = set(\n            cvar_name\n            for cvar_name, cvar in cls.computed_vars.items()\n            if not cvar._cache\n        )\n\n        # Any substate containing a ComputedVar with cache=False always needs to be recomputed\n        if cls._always_dirty_computed_vars:\n            # Tell parent classes that this substate has always dirty computed vars\n            state_name = cls.get_name()\n            parent_state = cls.get_parent_state()\n            while parent_state is not None:\n                parent_state._always_dirty_substates.add(state_name)\n                state_name, parent_state = (\n                    parent_state.get_name(),\n                    parent_state.get_parent_state(),\n                )\n\n    @classmethod\n    def _check_overridden_methods(cls):\n        \"\"\"Check for shadow methods and raise error if any.\n\n        Raises:\n            NameError: When an event handler shadows an inbuilt state method.\n        \"\"\"\n        overridden_methods = set()\n        state_base_functions = cls._get_base_functions()\n        for name, method in inspect.getmembers(cls, inspect.isfunction):\n            # Check if the method is overridden and not a dunder method\n            if (\n                not name.startswith(\"__\")\n                and method.__name__ in state_base_functions\n                and state_base_functions[method.__name__] != method\n            ):\n                overridden_methods.add(method.__name__)\n\n        for method_name in overridden_methods:\n            raise NameError(\n                f\"The event handler name `{method_name}` shadows a builtin State method; use a different name instead\"\n            )\n\n    @classmethod\n    def _check_overriden_basevars(cls):\n        \"\"\"Check for shadow base vars and raise error if any.\n\n        Raises:\n            NameError: When a computed var shadows a base var.\n        \"\"\"\n        for computed_var_ in cls._get_computed_vars():\n            if computed_var_._var_name in cls.__annotations__:\n                raise NameError(\n                    f\"The computed var name `{computed_var_._var_name}` shadows a base var in {cls.__module__}.{cls.__\n```\n# Function Signature: async def get_state(self, state_cls: Type[BaseState]) -> BaseState:\n# Natural Language Annotations: \"\"\"Get an instance of the state associated with this token.\n\n        Allows for arbitrary access to sibling states from within an event handler.\n\n        Args:\n            state_cls: The class of the state.\n\n        Returns:\n            The instance of state_cls associated with this state's client_token.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def get_state(self, state_cls: Type", "  ```\nasync def get_state(self, state_cls: Type", "  ```\nasync def get_state(self, state_cls: Type"]}, "reflex-reflex/state.py-_process": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/state.py:\n```\n\"\"\"Define the reflex state specification.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport copy\nimport functools\nimport inspect\nimport os\nimport traceback\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom types import FunctionType, MethodType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Callable,\n    ClassVar,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Type,\n)\n\nimport dill\n\ntry:\n    import pydantic.v1 as pydantic\nexcept ModuleNotFoundError:\n    import pydantic\n\nimport wrapt\nfrom redis.asyncio import Redis\nfrom redis.exceptions import ResponseError\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.event import (\n    BACKGROUND_TASK_MARKER,\n    Event,\n    EventHandler,\n    EventSpec,\n    fix_events,\n    window_alert,\n)\nfrom reflex.utils import console, format, prerequisites, types\nfrom reflex.utils.exceptions import ImmutableStateError, LockExpiredError\nfrom reflex.utils.exec import is_testing_env\nfrom reflex.utils.serializers import SerializedType, serialize, serializer\nfrom reflex.vars import BaseVar, ComputedVar, Var, computed_var\n\nif TYPE_CHECKING:\n    from reflex.components.component import Component\n\n\nDelta = Dict[str, Any]\nvar = computed_var\n\n\n# If the state is this large, it's considered a performance issue.\nTOO_LARGE_SERIALIZED_STATE = 100 * 1024  # 100kb\n\n\nclass HeaderData(Base):\n    \"\"\"An object containing headers data.\"\"\"\n\n    host: str = \"\"\n    origin: str = \"\"\n    upgrade: str = \"\"\n    connection: str = \"\"\n    pragma: str = \"\"\n    cache_control: str = \"\"\n    user_agent: str = \"\"\n    sec_websocket_version: str = \"\"\n    sec_websocket_key: str = \"\"\n    sec_websocket_extensions: str = \"\"\n    accept_encoding: str = \"\"\n    accept_language: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the HeaderData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            for k, v in router_data.get(constants.RouteVar.HEADERS, {}).items():\n                setattr(self, format.to_snake_case(k), v)\n\n\nclass PageData(Base):\n    \"\"\"An object containing page data.\"\"\"\n\n    host: str = \"\"  # repeated with self.headers.origin (remove or keep the duplicate?)\n    path: str = \"\"\n    raw_path: str = \"\"\n    full_path: str = \"\"\n    full_raw_path: str = \"\"\n    params: dict = {}\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the PageData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.host = router_data.get(constants.RouteVar.HEADERS, {}).get(\"origin\")\n            self.path = router_data.get(constants.RouteVar.PATH, \"\")\n            self.raw_path = router_data.get(constants.RouteVar.ORIGIN, \"\")\n            self.full_path = f\"{self.host}{self.path}\"\n            self.full_raw_path = f\"{self.host}{self.raw_path}\"\n            self.params = router_data.get(constants.RouteVar.QUERY, {})\n\n\nclass SessionData(Base):\n    \"\"\"An object containing session data.\"\"\"\n\n    client_token: str = \"\"\n    client_ip: str = \"\"\n    session_id: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the SessionData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.client_token = router_data.get(constants.RouteVar.CLIENT_TOKEN, \"\")\n            self.client_ip = router_data.get(constants.RouteVar.CLIENT_IP, \"\")\n            self.session_id = router_data.get(constants.RouteVar.SESSION_ID, \"\")\n\n\nclass RouterData(Base):\n    \"\"\"An object containing RouterData.\"\"\"\n\n    session: SessionData = SessionData()\n    headers: HeaderData = HeaderData()\n    page: PageData = PageData()\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initialize the RouterData object.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        self.session = SessionData(router_data)\n        self.headers = HeaderData(router_data)\n        self.page = PageData(router_data)\n\n\ndef _no_chain_background_task(\n    state_cls: Type[\"BaseState\"], name: str, fn: Callable\n) -> Callable:\n    \"\"\"Protect against directly chaining a background task from another event handler.\n\n    Args:\n        state_cls: The state class that the event handler is in.\n        name: The name of the background task.\n        fn: The background task coroutine function / generator.\n\n    Returns:\n        A compatible coroutine function / generator that raises a runtime error.\n\n    Raises:\n        TypeError: If the background task is not async.\n    \"\"\"\n    call = f\"{state_cls.__name__}.{name}\"\n    message = (\n        f\"Cannot directly call background task {name!r}, use \"\n        f\"`yield {call}` or `return {call}` instead.\"\n    )\n    if inspect.iscoroutinefunction(fn):\n\n        async def _no_chain_background_task_co(*args, **kwargs):\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_co\n    if inspect.isasyncgenfunction(fn):\n\n        async def _no_chain_background_task_gen(*args, **kwargs):\n            yield\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_gen\n\n    raise TypeError(f\"{fn} is marked as a background task, but is not async.\")\n\n\ndef _substate_key(\n    token: str,\n    state_cls_or_name: BaseState | Type[BaseState] | str | list[str],\n) -> str:\n    \"\"\"Get the substate key.\n\n    Args:\n        token: The token of the state.\n        state_cls_or_name: The state class/instance or name or sequence of name parts.\n\n    Returns:\n        The substate key.\n    \"\"\"\n    if isinstance(state_cls_or_name, BaseState) or (\n        isinstance(state_cls_or_name, type) and issubclass(state_cls_or_name, BaseState)\n    ):\n        state_cls_or_name = state_cls_or_name.get_full_name()\n    elif isinstance(state_cls_or_name, (list, tuple)):\n        state_cls_or_name = \".\".join(state_cls_or_name)\n    return f\"{token}_{state_cls_or_name}\"\n\n\ndef _split_substate_key(substate_key: str) -> tuple[str, str]:\n    \"\"\"Split the substate key into token and state name.\n\n    Args:\n        substate_key: The substate key.\n\n    Returns:\n        Tuple of token and state name.\n    \"\"\"\n    token, _, state_name = substate_key.partition(\"_\")\n    return token, state_name\n\n\nclass EventHandlerSetVar(EventHandler):\n    \"\"\"A special event handler to wrap setvar functionality.\"\"\"\n\n    state_cls: Type[BaseState]\n\n    def __init__(self, state_cls: Type[BaseState]):\n        \"\"\"Initialize the EventHandlerSetVar.\n\n        Args:\n            state_cls: The state class that vars will be set on.\n        \"\"\"\n        super().__init__(\n            fn=type(self).setvar,\n            state_full_name=state_cls.get_full_name(),\n            state_cls=state_cls,  # type: ignore\n        )\n\n    def setvar(self, var_name: str, value: Any):\n        \"\"\"Set the state variable to the value of the event.\n\n        Note: `self` here will be an instance of the state, not EventHandlerSetVar.\n\n        Args:\n            var_name: The name of the variable to set.\n            value: The value to set the variable to.\n        \"\"\"\n        getattr(self, constants.SETTER_PREFIX + var_name)(value)\n\n    def __call__(self, *args: Any) -> EventSpec:\n        \"\"\"Performs pre-checks and munging on the provided args that will become an EventSpec.\n\n        Args:\n            *args: The event args.\n\n        Returns:\n            The (partial) EventSpec that will be used to create the event to setvar.\n\n        Raises:\n            AttributeError: If the given Var name does not exist on the state.\n            EventHandlerValueError: If the given Var name is not a str\n        \"\"\"\n        from reflex.utils.exceptions import EventHandlerValueError\n\n        if args:\n            if not isinstance(args[0], str):\n                raise EventHandlerValueError(\n                    f\"Var name must be passed as a string, got {args[0]!r}\"\n                )\n            # Check that the requested Var setter exists on the State at compile time.\n            if getattr(self.state_cls, constants.SETTER_PREFIX + args[0], None) is None:\n                raise AttributeError(\n                    f\"Variable `{args[0]}` cannot be set on `{self.state_cls.get_full_name()}`\"\n                )\n        return super().__call__(*args)\n\n\nclass BaseState(Base, ABC, extra=pydantic.Extra.allow):\n    \"\"\"The state of the app.\"\"\"\n\n    # A map from the var name to the var.\n    vars: ClassVar[Dict[str, Var]] = {}\n\n    # The base vars of the class.\n    base_vars: ClassVar[Dict[str, BaseVar]] = {}\n\n    # The computed vars of the class.\n    computed_vars: ClassVar[Dict[str, ComputedVar]] = {}\n\n    # Vars inherited by the parent state.\n    inherited_vars: ClassVar[Dict[str, Var]] = {}\n\n    # Backend base vars that are never sent to the client.\n    backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # Backend base vars inherited\n    inherited_backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # The event handlers.\n    event_handlers: ClassVar[Dict[str, EventHandler]] = {}\n\n    # A set of subclassses of this class.\n    class_subclasses: ClassVar[Set[Type[BaseState]]] = set()\n\n    # Mapping of var name to set of computed variables that depend on it\n    _computed_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Mapping of var name to set of substates that depend on it\n    _substate_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Set of vars which always need to be recomputed\n    _always_dirty_computed_vars: ClassVar[Set[str]] = set()\n\n    # Set of substates which always need to be recomputed\n    _always_dirty_substates: ClassVar[Set[str]] = set()\n\n    # The parent state.\n    parent_state: Optional[BaseState] = None\n\n    # The substates of the state.\n    substates: Dict[str, BaseState] = {}\n\n    # The set of dirty vars.\n    dirty_vars: Set[str] = set()\n\n    # The set of dirty substates.\n    dirty_substates: Set[str] = set()\n\n    # The routing path that triggered the state\n    router_data: Dict[str, Any] = {}\n\n    # Per-instance copy of backend base variable values\n    _backend_vars: Dict[str, Any] = {}\n\n    # The router data for the current page\n    router: RouterData = RouterData()\n\n    # Whether the state has ever been touched since instantiation.\n    _was_touched: bool = False\n\n    # Whether this state class is a mixin and should not be instantiated.\n    _mixin: ClassVar[bool] = False\n\n    # A special event handler for setting base vars.\n    setvar: ClassVar[EventHandler]\n\n    def __init__(\n        self,\n        *args,\n        parent_state: BaseState | None = None,\n        init_substates: bool = True,\n        _reflex_internal_init: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize the state.\n\n        DO NOT INSTANTIATE STATE CLASSES DIRECTLY! Use StateManager.get_state() instead.\n\n        Args:\n            *args: The args to pass to the Pydantic init method.\n            parent_state: The parent state.\n            init_substates: Whether to initialize the substates in this instance.\n            _reflex_internal_init: A flag to indicate that the state is being initialized by the framework.\n            **kwargs: The kwargs to pass to the Pydantic init method.\n\n        Raises:\n            ReflexRuntimeError: If the state is instantiated directly by end user.\n        \"\"\"\n        from reflex.utils.exceptions import ReflexRuntimeError\n\n        if not _reflex_internal_init and not is_testing_env():\n            raise ReflexRuntimeError(\n                \"State classes should not be instantiated directly in a Reflex app. \"\n                \"See https://reflex.dev/docs/state/ for further information.\"\n            )\n        kwargs[\"parent_state\"] = parent_state\n        super().__init__(*args, **kwargs)\n\n        # Setup the substates (for memory state manager only).\n        if init_substates:\n            for substate in self.get_substates():\n                self.substates[substate.get_name()] = substate(\n                    parent_state=self,\n                    _reflex_internal_init=True,\n                )\n\n        # Create a fresh copy of the backend variables for this instance\n        self._backend_vars = copy.deepcopy(\n            {name: item for name, item in self.backend_vars.items()}\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the state.\n\n        Returns:\n            The string representation of the state.\n        \"\"\"\n        return f\"{self.__class__.__name__}({self.dict()})\"\n\n    @classmethod\n    def _get_computed_vars(cls) -> list[ComputedVar]:\n        \"\"\"Helper function to get all computed vars of a instance.\n\n        Returns:\n            A list of computed vars.\n        \"\"\"\n        return [\n            v\n            for mixin in cls._mixins() + [cls]\n            for v in mixin.__dict__.values()\n            if isinstance(v, ComputedVar)\n        ]\n\n    @classmethod\n    def __init_subclass__(cls, mixin: bool = False, **kwargs):\n        \"\"\"Do some magic for the subclass initialization.\n\n        Args:\n            mixin: Whether the subclass is a mixin and should not be initialized.\n            **kwargs: The kwargs to pass to the pydantic init_subclass method.\n\n        Raises:\n            StateValueError: If a substate class shadows another.\n        \"\"\"\n        from reflex.utils.exceptions import StateValueError\n\n        super().__init_subclass__(**kwargs)\n\n        cls._mixin = mixin\n        if mixin:\n            return\n\n        # Event handlers should not shadow builtin state methods.\n        cls._check_overridden_methods()\n        # Computed vars should not shadow builtin state props.\n        cls._check_overriden_basevars()\n\n        # Reset subclass tracking for this class.\n        cls.class_subclasses = set()\n\n        # Reset dirty substate tracking for this class.\n        cls._always_dirty_substates = set()\n\n        # Get the parent vars.\n        parent_state = cls.get_parent_state()\n        if parent_state is not None:\n            cls.inherited_vars = parent_state.vars\n            cls.inherited_backend_vars = parent_state.backend_vars\n\n            # Check if another substate class with the same name has already been defined.\n            if cls.__name__ in set(c.__name__ for c in parent_state.class_subclasses):\n                if is_testing_env():\n                    # Clear existing subclass with same name when app is reloaded via\n                    # utils.prerequisites.get_app(reload=True)\n                    parent_state.class_subclasses = set(\n                        c\n                        for c in parent_state.class_subclasses\n                        if c.__name__ != cls.__name__\n                    )\n                else:\n                    # During normal operation, subclasses cannot have the same name, even if they are\n                    # defined in different modules.\n                    raise StateValueError(\n                        f\"The substate class '{cls.__name__}' has been defined multiple times. \"\n                        \"Shadowing substate classes is not allowed.\"\n                    )\n            # Track this new subclass in the parent state's subclasses set.\n            parent_state.class_subclasses.add(cls)\n\n        # Get computed vars.\n        computed_vars = cls._get_computed_vars()\n\n        new_backend_vars = {\n            name: value\n            for name, value in cls.__dict__.items()\n            if types.is_backend_base_variable(name, cls)\n        }\n\n        cls.backend_vars = {\n            **cls.inherited_backend_vars,\n            **new_backend_vars,\n        }\n\n        # Set the base and computed vars.\n        cls.base_vars = {\n            f.name: BaseVar(_var_name=f.name, _var_type=f.outer_type_)._var_set_state(\n                cls\n            )\n            for f in cls.get_fields().values()\n            if f.name not in cls.get_skip_vars()\n        }\n        cls.computed_vars = {v._var_name: v._var_set_state(cls) for v in computed_vars}\n        cls.vars = {\n            **cls.inherited_vars,\n            **cls.base_vars,\n            **cls.computed_vars,\n        }\n        cls.event_handlers = {}\n\n        # Setup the base vars at the class level.\n        for prop in cls.base_vars.values():\n            cls._init_var(prop)\n\n        # Set up the event handlers.\n        events = {\n            name: fn\n            for name, fn in cls.__dict__.items()\n            if cls._item_is_event_handler(name, fn)\n        }\n\n        for mixin in cls._mixins():\n            for name, value in mixin.__dict__.items():\n                if isinstance(value, ComputedVar):\n                    fget = cls._copy_fn(value.fget)\n                    newcv = value._replace(fget=fget)\n                    # cleanup refs to mixin cls in var_data\n                    newcv._var_data = None\n                    newcv._var_set_state(cls)\n                    setattr(cls, name, newcv)\n                    cls.computed_vars[newcv._var_name] = newcv\n                    cls.vars[newcv._var_name] = newcv\n                    continue\n                if types.is_backend_base_variable(name, mixin):\n                    cls.backend_vars[name] = copy.deepcopy(value)\n                    continue\n                if events.get(name) is not None:\n                    continue\n                if not cls._item_is_event_handler(name, value):\n                    continue\n                if parent_state is not None and parent_state.event_handlers.get(name):\n                    continue\n                value = cls._copy_fn(value)\n                value.__qualname__ = f\"{cls.__name__}.{name}\"\n                events[name] = value\n\n        # Create the setvar event handler for this state\n        cls._create_setvar()\n\n        for name, fn in events.items():\n            handler = cls._create_event_handler(fn)\n            cls.event_handlers[name] = handler\n            setattr(cls, name, handler)\n\n        cls._init_var_dependency_dicts()\n\n    @staticmethod\n    def _copy_fn(fn: Callable) -> Callable:\n        \"\"\"Copy a function. Used to copy ComputedVars and EventHandlers from mixins.\n\n        Args:\n            fn: The function to copy.\n\n        Returns:\n            The copied function.\n        \"\"\"\n        newfn = FunctionType(\n            fn.__code__,\n            fn.__globals__,\n            name=fn.__name__,\n            argdefs=fn.__defaults__,\n            closure=fn.__closure__,\n        )\n        newfn.__annotations__ = fn.__annotations__\n        if mark := getattr(fn, BACKGROUND_TASK_MARKER, None):\n            setattr(newfn, BACKGROUND_TASK_MARKER, mark)\n        return newfn\n\n    @staticmethod\n    def _item_is_event_handler(name: str, value: Any) -> bool:\n        \"\"\"Check if the item is an event handler.\n\n        Args:\n            name: The name of the item.\n            value: The value of the item.\n\n        Returns:\n            Whether the item is an event handler.\n        \"\"\"\n        return (\n            not name.startswith(\"_\")\n            and isinstance(value, Callable)\n            and not isinstance(value, EventHandler)\n            and hasattr(value, \"__code__\")\n        )\n\n    @classmethod\n    def _mixins(cls) -> List[Type]:\n        \"\"\"Get the mixin classes of the state.\n\n        Returns:\n            The mixin classes of the state.\n        \"\"\"\n        return [\n            mixin\n            for mixin in cls.__mro__\n            if (\n                mixin not in [pydantic.BaseModel, Base, cls]\n                and issubclass(mixin, BaseState)\n                and mixin._mixin is True\n            )\n        ]\n\n    @classmethod\n    def _init_var_dependency_dicts(cls):\n        \"\"\"Initialize the var dependency tracking dicts.\n\n        Allows the state to know which vars each ComputedVar depends on and\n        whether a ComputedVar depends on a var in its parent state.\n\n        Additional updates tracking dicts for vars and substates that always\n        need to be recomputed.\n        \"\"\"\n        # Initialize per-class var dependency tracking.\n        cls._computed_var_dependencies = defaultdict(set)\n        cls._substate_var_dependencies = defaultdict(set)\n\n        inherited_vars = set(cls.inherited_vars).union(\n            set(cls.inherited_backend_vars),\n        )\n        for cvar_name, cvar in cls.computed_vars.items():\n            # Add the dependencies.\n            for var in cvar._deps(objclass=cls):\n                cls._computed_var_dependencies[var].add(cvar_name)\n                if var in inherited_vars:\n                    # track that this substate depends on its parent for this var\n                    state_name = cls.get_name()\n                    parent_state = cls.get_parent_state()\n                    while parent_state is not None and var in {\n                        **parent_state.vars,\n                        **parent_state.backend_vars,\n                    }:\n                        parent_state._substate_var_dependencies[var].add(state_name)\n                        state_name, parent_state = (\n                            parent_state.get_name(),\n                            parent_state.get_parent_state(),\n                        )\n\n        # ComputedVar with cache=False always need to be recomputed\n        cls._always_dirty_computed_vars = set(\n            cvar_name\n            for cvar_name, cvar in cls.computed_vars.items()\n            if not cvar._cache\n        )\n\n        # Any substate containing a ComputedVar with cache=False always needs to be recomputed\n        if cls._always_dirty_computed_vars:\n            # Tell parent classes that this substate has always dirty computed vars\n            state_name = cls.get_name()\n            parent_state = cls.get_parent_state()\n            while parent_state is not None:\n                parent_state._always_dirty_substates.add(state_name)\n                state_name, parent_state = (\n                    parent_state.get_name(),\n                    parent_state.get_parent_state(),\n                )\n\n    @classmethod\n    def _check_overridden_methods(cls):\n        \"\"\"Check for shadow methods and raise error if any.\n\n        Raises:\n            NameError: When an event handler shadows an inbuilt state method.\n        \"\"\"\n        overridden_methods = set()\n        state_base_functions = cls._get_base_functions()\n        for name, method in inspect.getmembers(cls, inspect.isfunction):\n            # Check if the method is overridden and not a dunder method\n            if (\n                not name.startswith(\"__\")\n                and method.__name__ in state_base_functions\n                and state_base_functions[method.__name__] != method\n            ):\n                overridden_methods.add(method.__name__)\n\n        for method_name in overridden_methods:\n            raise NameError(\n                f\"The event handler name `{method_name}` shadows a builtin State method; use a different name instead\"\n            )\n\n    @classmethod\n    def _check_overriden_basevars(cls):\n        \"\"\"Check for shadow base vars and raise error if any.\n\n        Raises:\n            NameError: When a computed var shadows a base var.\n        \"\"\"\n        for computed_var_ in cls._get_computed_vars():\n            if computed_var_._var_name in cls.__annotations__:\n                raise NameError(\n                    f\"The computed var name `{computed_var_._var_name}` shadows a base var in {cls.__module__}.{cls.__\n```\n# Function Signature: async def _process(self, event: Event) -> AsyncIterator[StateUpdate]:\n# Natural Language Annotations: \"\"\"Obtain event info and process event.\n\n        Args:\n            event: The event to process.\n\n        Yields:\n            The state update after processing the event.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def _process(self, event: Event) -> Async", "  ```\nasync def _process(self, event: Event) -> Async", "  ```\nasync def _process(self, event: Event) -> Async"]}, "reflex-reflex/state.py-_dirty_computed_vars": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/state.py:\n```\n\"\"\"Define the reflex state specification.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport copy\nimport functools\nimport inspect\nimport os\nimport traceback\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom types import FunctionType, MethodType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Callable,\n    ClassVar,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Type,\n)\n\nimport dill\n\ntry:\n    import pydantic.v1 as pydantic\nexcept ModuleNotFoundError:\n    import pydantic\n\nimport wrapt\nfrom redis.asyncio import Redis\nfrom redis.exceptions import ResponseError\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.event import (\n    BACKGROUND_TASK_MARKER,\n    Event,\n    EventHandler,\n    EventSpec,\n    fix_events,\n    window_alert,\n)\nfrom reflex.utils import console, format, prerequisites, types\nfrom reflex.utils.exceptions import ImmutableStateError, LockExpiredError\nfrom reflex.utils.exec import is_testing_env\nfrom reflex.utils.serializers import SerializedType, serialize, serializer\nfrom reflex.vars import BaseVar, ComputedVar, Var, computed_var\n\nif TYPE_CHECKING:\n    from reflex.components.component import Component\n\n\nDelta = Dict[str, Any]\nvar = computed_var\n\n\n# If the state is this large, it's considered a performance issue.\nTOO_LARGE_SERIALIZED_STATE = 100 * 1024  # 100kb\n\n\nclass HeaderData(Base):\n    \"\"\"An object containing headers data.\"\"\"\n\n    host: str = \"\"\n    origin: str = \"\"\n    upgrade: str = \"\"\n    connection: str = \"\"\n    pragma: str = \"\"\n    cache_control: str = \"\"\n    user_agent: str = \"\"\n    sec_websocket_version: str = \"\"\n    sec_websocket_key: str = \"\"\n    sec_websocket_extensions: str = \"\"\n    accept_encoding: str = \"\"\n    accept_language: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the HeaderData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            for k, v in router_data.get(constants.RouteVar.HEADERS, {}).items():\n                setattr(self, format.to_snake_case(k), v)\n\n\nclass PageData(Base):\n    \"\"\"An object containing page data.\"\"\"\n\n    host: str = \"\"  # repeated with self.headers.origin (remove or keep the duplicate?)\n    path: str = \"\"\n    raw_path: str = \"\"\n    full_path: str = \"\"\n    full_raw_path: str = \"\"\n    params: dict = {}\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the PageData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.host = router_data.get(constants.RouteVar.HEADERS, {}).get(\"origin\")\n            self.path = router_data.get(constants.RouteVar.PATH, \"\")\n            self.raw_path = router_data.get(constants.RouteVar.ORIGIN, \"\")\n            self.full_path = f\"{self.host}{self.path}\"\n            self.full_raw_path = f\"{self.host}{self.raw_path}\"\n            self.params = router_data.get(constants.RouteVar.QUERY, {})\n\n\nclass SessionData(Base):\n    \"\"\"An object containing session data.\"\"\"\n\n    client_token: str = \"\"\n    client_ip: str = \"\"\n    session_id: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the SessionData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.client_token = router_data.get(constants.RouteVar.CLIENT_TOKEN, \"\")\n            self.client_ip = router_data.get(constants.RouteVar.CLIENT_IP, \"\")\n            self.session_id = router_data.get(constants.RouteVar.SESSION_ID, \"\")\n\n\nclass RouterData(Base):\n    \"\"\"An object containing RouterData.\"\"\"\n\n    session: SessionData = SessionData()\n    headers: HeaderData = HeaderData()\n    page: PageData = PageData()\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initialize the RouterData object.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        self.session = SessionData(router_data)\n        self.headers = HeaderData(router_data)\n        self.page = PageData(router_data)\n\n\ndef _no_chain_background_task(\n    state_cls: Type[\"BaseState\"], name: str, fn: Callable\n) -> Callable:\n    \"\"\"Protect against directly chaining a background task from another event handler.\n\n    Args:\n        state_cls: The state class that the event handler is in.\n        name: The name of the background task.\n        fn: The background task coroutine function / generator.\n\n    Returns:\n        A compatible coroutine function / generator that raises a runtime error.\n\n    Raises:\n        TypeError: If the background task is not async.\n    \"\"\"\n    call = f\"{state_cls.__name__}.{name}\"\n    message = (\n        f\"Cannot directly call background task {name!r}, use \"\n        f\"`yield {call}` or `return {call}` instead.\"\n    )\n    if inspect.iscoroutinefunction(fn):\n\n        async def _no_chain_background_task_co(*args, **kwargs):\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_co\n    if inspect.isasyncgenfunction(fn):\n\n        async def _no_chain_background_task_gen(*args, **kwargs):\n            yield\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_gen\n\n    raise TypeError(f\"{fn} is marked as a background task, but is not async.\")\n\n\ndef _substate_key(\n    token: str,\n    state_cls_or_name: BaseState | Type[BaseState] | str | list[str],\n) -> str:\n    \"\"\"Get the substate key.\n\n    Args:\n        token: The token of the state.\n        state_cls_or_name: The state class/instance or name or sequence of name parts.\n\n    Returns:\n        The substate key.\n    \"\"\"\n    if isinstance(state_cls_or_name, BaseState) or (\n        isinstance(state_cls_or_name, type) and issubclass(state_cls_or_name, BaseState)\n    ):\n        state_cls_or_name = state_cls_or_name.get_full_name()\n    elif isinstance(state_cls_or_name, (list, tuple)):\n        state_cls_or_name = \".\".join(state_cls_or_name)\n    return f\"{token}_{state_cls_or_name}\"\n\n\ndef _split_substate_key(substate_key: str) -> tuple[str, str]:\n    \"\"\"Split the substate key into token and state name.\n\n    Args:\n        substate_key: The substate key.\n\n    Returns:\n        Tuple of token and state name.\n    \"\"\"\n    token, _, state_name = substate_key.partition(\"_\")\n    return token, state_name\n\n\nclass EventHandlerSetVar(EventHandler):\n    \"\"\"A special event handler to wrap setvar functionality.\"\"\"\n\n    state_cls: Type[BaseState]\n\n    def __init__(self, state_cls: Type[BaseState]):\n        \"\"\"Initialize the EventHandlerSetVar.\n\n        Args:\n            state_cls: The state class that vars will be set on.\n        \"\"\"\n        super().__init__(\n            fn=type(self).setvar,\n            state_full_name=state_cls.get_full_name(),\n            state_cls=state_cls,  # type: ignore\n        )\n\n    def setvar(self, var_name: str, value: Any):\n        \"\"\"Set the state variable to the value of the event.\n\n        Note: `self` here will be an instance of the state, not EventHandlerSetVar.\n\n        Args:\n            var_name: The name of the variable to set.\n            value: The value to set the variable to.\n        \"\"\"\n        getattr(self, constants.SETTER_PREFIX + var_name)(value)\n\n    def __call__(self, *args: Any) -> EventSpec:\n        \"\"\"Performs pre-checks and munging on the provided args that will become an EventSpec.\n\n        Args:\n            *args: The event args.\n\n        Returns:\n            The (partial) EventSpec that will be used to create the event to setvar.\n\n        Raises:\n            AttributeError: If the given Var name does not exist on the state.\n            EventHandlerValueError: If the given Var name is not a str\n        \"\"\"\n        from reflex.utils.exceptions import EventHandlerValueError\n\n        if args:\n            if not isinstance(args[0], str):\n                raise EventHandlerValueError(\n                    f\"Var name must be passed as a string, got {args[0]!r}\"\n                )\n            # Check that the requested Var setter exists on the State at compile time.\n            if getattr(self.state_cls, constants.SETTER_PREFIX + args[0], None) is None:\n                raise AttributeError(\n                    f\"Variable `{args[0]}` cannot be set on `{self.state_cls.get_full_name()}`\"\n                )\n        return super().__call__(*args)\n\n\nclass BaseState(Base, ABC, extra=pydantic.Extra.allow):\n    \"\"\"The state of the app.\"\"\"\n\n    # A map from the var name to the var.\n    vars: ClassVar[Dict[str, Var]] = {}\n\n    # The base vars of the class.\n    base_vars: ClassVar[Dict[str, BaseVar]] = {}\n\n    # The computed vars of the class.\n    computed_vars: ClassVar[Dict[str, ComputedVar]] = {}\n\n    # Vars inherited by the parent state.\n    inherited_vars: ClassVar[Dict[str, Var]] = {}\n\n    # Backend base vars that are never sent to the client.\n    backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # Backend base vars inherited\n    inherited_backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # The event handlers.\n    event_handlers: ClassVar[Dict[str, EventHandler]] = {}\n\n    # A set of subclassses of this class.\n    class_subclasses: ClassVar[Set[Type[BaseState]]] = set()\n\n    # Mapping of var name to set of computed variables that depend on it\n    _computed_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Mapping of var name to set of substates that depend on it\n    _substate_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Set of vars which always need to be recomputed\n    _always_dirty_computed_vars: ClassVar[Set[str]] = set()\n\n    # Set of substates which always need to be recomputed\n    _always_dirty_substates: ClassVar[Set[str]] = set()\n\n    # The parent state.\n    parent_state: Optional[BaseState] = None\n\n    # The substates of the state.\n    substates: Dict[str, BaseState] = {}\n\n    # The set of dirty vars.\n    dirty_vars: Set[str] = set()\n\n    # The set of dirty substates.\n    dirty_substates: Set[str] = set()\n\n    # The routing path that triggered the state\n    router_data: Dict[str, Any] = {}\n\n    # Per-instance copy of backend base variable values\n    _backend_vars: Dict[str, Any] = {}\n\n    # The router data for the current page\n    router: RouterData = RouterData()\n\n    # Whether the state has ever been touched since instantiation.\n    _was_touched: bool = False\n\n    # Whether this state class is a mixin and should not be instantiated.\n    _mixin: ClassVar[bool] = False\n\n    # A special event handler for setting base vars.\n    setvar: ClassVar[EventHandler]\n\n    def __init__(\n        self,\n        *args,\n        parent_state: BaseState | None = None,\n        init_substates: bool = True,\n        _reflex_internal_init: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize the state.\n\n        DO NOT INSTANTIATE STATE CLASSES DIRECTLY! Use StateManager.get_state() instead.\n\n        Args:\n            *args: The args to pass to the Pydantic init method.\n            parent_state: The parent state.\n            init_substates: Whether to initialize the substates in this instance.\n            _reflex_internal_init: A flag to indicate that the state is being initialized by the framework.\n            **kwargs: The kwargs to pass to the Pydantic init method.\n\n        Raises:\n            ReflexRuntimeError: If the state is instantiated directly by end user.\n        \"\"\"\n        from reflex.utils.exceptions import ReflexRuntimeError\n\n        if not _reflex_internal_init and not is_testing_env():\n            raise ReflexRuntimeError(\n                \"State classes should not be instantiated directly in a Reflex app. \"\n                \"See https://reflex.dev/docs/state/ for further information.\"\n            )\n        kwargs[\"parent_state\"] = parent_state\n        super().__init__(*args, **kwargs)\n\n        # Setup the substates (for memory state manager only).\n        if init_substates:\n            for substate in self.get_substates():\n                self.substates[substate.get_name()] = substate(\n                    parent_state=self,\n                    _reflex_internal_init=True,\n                )\n\n        # Create a fresh copy of the backend variables for this instance\n        self._backend_vars = copy.deepcopy(\n            {name: item for name, item in self.backend_vars.items()}\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the state.\n\n        Returns:\n            The string representation of the state.\n        \"\"\"\n        return f\"{self.__class__.__name__}({self.dict()})\"\n\n    @classmethod\n    def _get_computed_vars(cls) -> list[ComputedVar]:\n        \"\"\"Helper function to get all computed vars of a instance.\n\n        Returns:\n            A list of computed vars.\n        \"\"\"\n        return [\n            v\n            for mixin in cls._mixins() + [cls]\n            for v in mixin.__dict__.values()\n            if isinstance(v, ComputedVar)\n        ]\n\n    @classmethod\n    def __init_subclass__(cls, mixin: bool = False, **kwargs):\n        \"\"\"Do some magic for the subclass initialization.\n\n        Args:\n            mixin: Whether the subclass is a mixin and should not be initialized.\n            **kwargs: The kwargs to pass to the pydantic init_subclass method.\n\n        Raises:\n            StateValueError: If a substate class shadows another.\n        \"\"\"\n        from reflex.utils.exceptions import StateValueError\n\n        super().__init_subclass__(**kwargs)\n\n        cls._mixin = mixin\n        if mixin:\n            return\n\n        # Event handlers should not shadow builtin state methods.\n        cls._check_overridden_methods()\n        # Computed vars should not shadow builtin state props.\n        cls._check_overriden_basevars()\n\n        # Reset subclass tracking for this class.\n        cls.class_subclasses = set()\n\n        # Reset dirty substate tracking for this class.\n        cls._always_dirty_substates = set()\n\n        # Get the parent vars.\n        parent_state = cls.get_parent_state()\n        if parent_state is not None:\n            cls.inherited_vars = parent_state.vars\n            cls.inherited_backend_vars = parent_state.backend_vars\n\n            # Check if another substate class with the same name has already been defined.\n            if cls.__name__ in set(c.__name__ for c in parent_state.class_subclasses):\n                if is_testing_env():\n                    # Clear existing subclass with same name when app is reloaded via\n                    # utils.prerequisites.get_app(reload=True)\n                    parent_state.class_subclasses = set(\n                        c\n                        for c in parent_state.class_subclasses\n                        if c.__name__ != cls.__name__\n                    )\n                else:\n                    # During normal operation, subclasses cannot have the same name, even if they are\n                    # defined in different modules.\n                    raise StateValueError(\n                        f\"The substate class '{cls.__name__}' has been defined multiple times. \"\n                        \"Shadowing substate classes is not allowed.\"\n                    )\n            # Track this new subclass in the parent state's subclasses set.\n            parent_state.class_subclasses.add(cls)\n\n        # Get computed vars.\n        computed_vars = cls._get_computed_vars()\n\n        new_backend_vars = {\n            name: value\n            for name, value in cls.__dict__.items()\n            if types.is_backend_base_variable(name, cls)\n        }\n\n        cls.backend_vars = {\n            **cls.inherited_backend_vars,\n            **new_backend_vars,\n        }\n\n        # Set the base and computed vars.\n        cls.base_vars = {\n            f.name: BaseVar(_var_name=f.name, _var_type=f.outer_type_)._var_set_state(\n                cls\n            )\n            for f in cls.get_fields().values()\n            if f.name not in cls.get_skip_vars()\n        }\n        cls.computed_vars = {v._var_name: v._var_set_state(cls) for v in computed_vars}\n        cls.vars = {\n            **cls.inherited_vars,\n            **cls.base_vars,\n            **cls.computed_vars,\n        }\n        cls.event_handlers = {}\n\n        # Setup the base vars at the class level.\n        for prop in cls.base_vars.values():\n            cls._init_var(prop)\n\n        # Set up the event handlers.\n        events = {\n            name: fn\n            for name, fn in cls.__dict__.items()\n            if cls._item_is_event_handler(name, fn)\n        }\n\n        for mixin in cls._mixins():\n            for name, value in mixin.__dict__.items():\n                if isinstance(value, ComputedVar):\n                    fget = cls._copy_fn(value.fget)\n                    newcv = value._replace(fget=fget)\n                    # cleanup refs to mixin cls in var_data\n                    newcv._var_data = None\n                    newcv._var_set_state(cls)\n                    setattr(cls, name, newcv)\n                    cls.computed_vars[newcv._var_name] = newcv\n                    cls.vars[newcv._var_name] = newcv\n                    continue\n                if types.is_backend_base_variable(name, mixin):\n                    cls.backend_vars[name] = copy.deepcopy(value)\n                    continue\n                if events.get(name) is not None:\n                    continue\n                if not cls._item_is_event_handler(name, value):\n                    continue\n                if parent_state is not None and parent_state.event_handlers.get(name):\n                    continue\n                value = cls._copy_fn(value)\n                value.__qualname__ = f\"{cls.__name__}.{name}\"\n                events[name] = value\n\n        # Create the setvar event handler for this state\n        cls._create_setvar()\n\n        for name, fn in events.items():\n            handler = cls._create_event_handler(fn)\n            cls.event_handlers[name] = handler\n            setattr(cls, name, handler)\n\n        cls._init_var_dependency_dicts()\n\n    @staticmethod\n    def _copy_fn(fn: Callable) -> Callable:\n        \"\"\"Copy a function. Used to copy ComputedVars and EventHandlers from mixins.\n\n        Args:\n            fn: The function to copy.\n\n        Returns:\n            The copied function.\n        \"\"\"\n        newfn = FunctionType(\n            fn.__code__,\n            fn.__globals__,\n            name=fn.__name__,\n            argdefs=fn.__defaults__,\n            closure=fn.__closure__,\n        )\n        newfn.__annotations__ = fn.__annotations__\n        if mark := getattr(fn, BACKGROUND_TASK_MARKER, None):\n            setattr(newfn, BACKGROUND_TASK_MARKER, mark)\n        return newfn\n\n    @staticmethod\n    def _item_is_event_handler(name: str, value: Any) -> bool:\n        \"\"\"Check if the item is an event handler.\n\n        Args:\n            name: The name of the item.\n            value: The value of the item.\n\n        Returns:\n            Whether the item is an event handler.\n        \"\"\"\n        return (\n            not name.startswith(\"_\")\n            and isinstance(value, Callable)\n            and not isinstance(value, EventHandler)\n            and hasattr(value, \"__code__\")\n        )\n\n    @classmethod\n    def _mixins(cls) -> List[Type]:\n        \"\"\"Get the mixin classes of the state.\n\n        Returns:\n            The mixin classes of the state.\n        \"\"\"\n        return [\n            mixin\n            for mixin in cls.__mro__\n            if (\n                mixin not in [pydantic.BaseModel, Base, cls]\n                and issubclass(mixin, BaseState)\n                and mixin._mixin is True\n            )\n        ]\n\n    @classmethod\n    def _init_var_dependency_dicts(cls):\n        \"\"\"Initialize the var dependency tracking dicts.\n\n        Allows the state to know which vars each ComputedVar depends on and\n        whether a ComputedVar depends on a var in its parent state.\n\n        Additional updates tracking dicts for vars and substates that always\n        need to be recomputed.\n        \"\"\"\n        # Initialize per-class var dependency tracking.\n        cls._computed_var_dependencies = defaultdict(set)\n        cls._substate_var_dependencies = defaultdict(set)\n\n        inherited_vars = set(cls.inherited_vars).union(\n            set(cls.inherited_backend_vars),\n        )\n        for cvar_name, cvar in cls.computed_vars.items():\n            # Add the dependencies.\n            for var in cvar._deps(objclass=cls):\n                cls._computed_var_dependencies[var].add(cvar_name)\n                if var in inherited_vars:\n                    # track that this substate depends on its parent for this var\n                    state_name = cls.get_name()\n                    parent_state = cls.get_parent_state()\n                    while parent_state is not None and var in {\n                        **parent_state.vars,\n                        **parent_state.backend_vars,\n                    }:\n                        parent_state._substate_var_dependencies[var].add(state_name)\n                        state_name, parent_state = (\n                            parent_state.get_name(),\n                            parent_state.get_parent_state(),\n                        )\n\n        # ComputedVar with cache=False always need to be recomputed\n        cls._always_dirty_computed_vars = set(\n            cvar_name\n            for cvar_name, cvar in cls.computed_vars.items()\n            if not cvar._cache\n        )\n\n        # Any substate containing a ComputedVar with cache=False always needs to be recomputed\n        if cls._always_dirty_computed_vars:\n            # Tell parent classes that this substate has always dirty computed vars\n            state_name = cls.get_name()\n            parent_state = cls.get_parent_state()\n            while parent_state is not None:\n                parent_state._always_dirty_substates.add(state_name)\n                state_name, parent_state = (\n                    parent_state.get_name(),\n                    parent_state.get_parent_state(),\n                )\n\n    @classmethod\n    def _check_overridden_methods(cls):\n        \"\"\"Check for shadow methods and raise error if any.\n\n        Raises:\n            NameError: When an event handler shadows an inbuilt state method.\n        \"\"\"\n        overridden_methods = set()\n        state_base_functions = cls._get_base_functions()\n        for name, method in inspect.getmembers(cls, inspect.isfunction):\n            # Check if the method is overridden and not a dunder method\n            if (\n                not name.startswith(\"__\")\n                and method.__name__ in state_base_functions\n                and state_base_functions[method.__name__] != method\n            ):\n                overridden_methods.add(method.__name__)\n\n        for method_name in overridden_methods:\n            raise NameError(\n                f\"The event handler name `{method_name}` shadows a builtin State method; use a different name instead\"\n            )\n\n    @classmethod\n    def _check_overriden_basevars(cls):\n        \"\"\"Check for shadow base vars and raise error if any.\n\n        Raises:\n            NameError: When a computed var shadows a base var.\n        \"\"\"\n        for computed_var_ in cls._get_computed_vars():\n            if computed_var_._var_name in cls.__annotations__:\n                raise NameError(\n                    f\"The computed var name `{computed_var_._var_name}` shadows a base var in {cls.__module__}.{cls.__\n```\n# Function Signature: def _dirty_computed_vars(\n# Natural Language Annotations: \"\"\"Determine ComputedVars that need to be recalculated based on the given vars.\n\n        Args:\n            from_vars: find ComputedVar that depend on this set of vars. If unspecified, will use the dirty_vars.\n            include_backend: whether to include backend vars in the calculation.\n\n        Returns:\n            Set of computed vars to include in the delta.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _dirty_computed_vars(\n    self", "  ```\n# -- generated code --\ndef _dirty_computed", "  ```\ndef _dirty_computed_vars(self, from"]}, "reflex-reflex/state.py-_potentially_dirty_substates": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/state.py:\n```\n\"\"\"Define the reflex state specification.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport copy\nimport functools\nimport inspect\nimport os\nimport traceback\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom types import FunctionType, MethodType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Callable,\n    ClassVar,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Type,\n)\n\nimport dill\n\ntry:\n    import pydantic.v1 as pydantic\nexcept ModuleNotFoundError:\n    import pydantic\n\nimport wrapt\nfrom redis.asyncio import Redis\nfrom redis.exceptions import ResponseError\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.event import (\n    BACKGROUND_TASK_MARKER,\n    Event,\n    EventHandler,\n    EventSpec,\n    fix_events,\n    window_alert,\n)\nfrom reflex.utils import console, format, prerequisites, types\nfrom reflex.utils.exceptions import ImmutableStateError, LockExpiredError\nfrom reflex.utils.exec import is_testing_env\nfrom reflex.utils.serializers import SerializedType, serialize, serializer\nfrom reflex.vars import BaseVar, ComputedVar, Var, computed_var\n\nif TYPE_CHECKING:\n    from reflex.components.component import Component\n\n\nDelta = Dict[str, Any]\nvar = computed_var\n\n\n# If the state is this large, it's considered a performance issue.\nTOO_LARGE_SERIALIZED_STATE = 100 * 1024  # 100kb\n\n\nclass HeaderData(Base):\n    \"\"\"An object containing headers data.\"\"\"\n\n    host: str = \"\"\n    origin: str = \"\"\n    upgrade: str = \"\"\n    connection: str = \"\"\n    pragma: str = \"\"\n    cache_control: str = \"\"\n    user_agent: str = \"\"\n    sec_websocket_version: str = \"\"\n    sec_websocket_key: str = \"\"\n    sec_websocket_extensions: str = \"\"\n    accept_encoding: str = \"\"\n    accept_language: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the HeaderData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            for k, v in router_data.get(constants.RouteVar.HEADERS, {}).items():\n                setattr(self, format.to_snake_case(k), v)\n\n\nclass PageData(Base):\n    \"\"\"An object containing page data.\"\"\"\n\n    host: str = \"\"  # repeated with self.headers.origin (remove or keep the duplicate?)\n    path: str = \"\"\n    raw_path: str = \"\"\n    full_path: str = \"\"\n    full_raw_path: str = \"\"\n    params: dict = {}\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the PageData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.host = router_data.get(constants.RouteVar.HEADERS, {}).get(\"origin\")\n            self.path = router_data.get(constants.RouteVar.PATH, \"\")\n            self.raw_path = router_data.get(constants.RouteVar.ORIGIN, \"\")\n            self.full_path = f\"{self.host}{self.path}\"\n            self.full_raw_path = f\"{self.host}{self.raw_path}\"\n            self.params = router_data.get(constants.RouteVar.QUERY, {})\n\n\nclass SessionData(Base):\n    \"\"\"An object containing session data.\"\"\"\n\n    client_token: str = \"\"\n    client_ip: str = \"\"\n    session_id: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the SessionData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.client_token = router_data.get(constants.RouteVar.CLIENT_TOKEN, \"\")\n            self.client_ip = router_data.get(constants.RouteVar.CLIENT_IP, \"\")\n            self.session_id = router_data.get(constants.RouteVar.SESSION_ID, \"\")\n\n\nclass RouterData(Base):\n    \"\"\"An object containing RouterData.\"\"\"\n\n    session: SessionData = SessionData()\n    headers: HeaderData = HeaderData()\n    page: PageData = PageData()\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initialize the RouterData object.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        self.session = SessionData(router_data)\n        self.headers = HeaderData(router_data)\n        self.page = PageData(router_data)\n\n\ndef _no_chain_background_task(\n    state_cls: Type[\"BaseState\"], name: str, fn: Callable\n) -> Callable:\n    \"\"\"Protect against directly chaining a background task from another event handler.\n\n    Args:\n        state_cls: The state class that the event handler is in.\n        name: The name of the background task.\n        fn: The background task coroutine function / generator.\n\n    Returns:\n        A compatible coroutine function / generator that raises a runtime error.\n\n    Raises:\n        TypeError: If the background task is not async.\n    \"\"\"\n    call = f\"{state_cls.__name__}.{name}\"\n    message = (\n        f\"Cannot directly call background task {name!r}, use \"\n        f\"`yield {call}` or `return {call}` instead.\"\n    )\n    if inspect.iscoroutinefunction(fn):\n\n        async def _no_chain_background_task_co(*args, **kwargs):\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_co\n    if inspect.isasyncgenfunction(fn):\n\n        async def _no_chain_background_task_gen(*args, **kwargs):\n            yield\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_gen\n\n    raise TypeError(f\"{fn} is marked as a background task, but is not async.\")\n\n\ndef _substate_key(\n    token: str,\n    state_cls_or_name: BaseState | Type[BaseState] | str | list[str],\n) -> str:\n    \"\"\"Get the substate key.\n\n    Args:\n        token: The token of the state.\n        state_cls_or_name: The state class/instance or name or sequence of name parts.\n\n    Returns:\n        The substate key.\n    \"\"\"\n    if isinstance(state_cls_or_name, BaseState) or (\n        isinstance(state_cls_or_name, type) and issubclass(state_cls_or_name, BaseState)\n    ):\n        state_cls_or_name = state_cls_or_name.get_full_name()\n    elif isinstance(state_cls_or_name, (list, tuple)):\n        state_cls_or_name = \".\".join(state_cls_or_name)\n    return f\"{token}_{state_cls_or_name}\"\n\n\ndef _split_substate_key(substate_key: str) -> tuple[str, str]:\n    \"\"\"Split the substate key into token and state name.\n\n    Args:\n        substate_key: The substate key.\n\n    Returns:\n        Tuple of token and state name.\n    \"\"\"\n    token, _, state_name = substate_key.partition(\"_\")\n    return token, state_name\n\n\nclass EventHandlerSetVar(EventHandler):\n    \"\"\"A special event handler to wrap setvar functionality.\"\"\"\n\n    state_cls: Type[BaseState]\n\n    def __init__(self, state_cls: Type[BaseState]):\n        \"\"\"Initialize the EventHandlerSetVar.\n\n        Args:\n            state_cls: The state class that vars will be set on.\n        \"\"\"\n        super().__init__(\n            fn=type(self).setvar,\n            state_full_name=state_cls.get_full_name(),\n            state_cls=state_cls,  # type: ignore\n        )\n\n    def setvar(self, var_name: str, value: Any):\n        \"\"\"Set the state variable to the value of the event.\n\n        Note: `self` here will be an instance of the state, not EventHandlerSetVar.\n\n        Args:\n            var_name: The name of the variable to set.\n            value: The value to set the variable to.\n        \"\"\"\n        getattr(self, constants.SETTER_PREFIX + var_name)(value)\n\n    def __call__(self, *args: Any) -> EventSpec:\n        \"\"\"Performs pre-checks and munging on the provided args that will become an EventSpec.\n\n        Args:\n            *args: The event args.\n\n        Returns:\n            The (partial) EventSpec that will be used to create the event to setvar.\n\n        Raises:\n            AttributeError: If the given Var name does not exist on the state.\n            EventHandlerValueError: If the given Var name is not a str\n        \"\"\"\n        from reflex.utils.exceptions import EventHandlerValueError\n\n        if args:\n            if not isinstance(args[0], str):\n                raise EventHandlerValueError(\n                    f\"Var name must be passed as a string, got {args[0]!r}\"\n                )\n            # Check that the requested Var setter exists on the State at compile time.\n            if getattr(self.state_cls, constants.SETTER_PREFIX + args[0], None) is None:\n                raise AttributeError(\n                    f\"Variable `{args[0]}` cannot be set on `{self.state_cls.get_full_name()}`\"\n                )\n        return super().__call__(*args)\n\n\nclass BaseState(Base, ABC, extra=pydantic.Extra.allow):\n    \"\"\"The state of the app.\"\"\"\n\n    # A map from the var name to the var.\n    vars: ClassVar[Dict[str, Var]] = {}\n\n    # The base vars of the class.\n    base_vars: ClassVar[Dict[str, BaseVar]] = {}\n\n    # The computed vars of the class.\n    computed_vars: ClassVar[Dict[str, ComputedVar]] = {}\n\n    # Vars inherited by the parent state.\n    inherited_vars: ClassVar[Dict[str, Var]] = {}\n\n    # Backend base vars that are never sent to the client.\n    backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # Backend base vars inherited\n    inherited_backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # The event handlers.\n    event_handlers: ClassVar[Dict[str, EventHandler]] = {}\n\n    # A set of subclassses of this class.\n    class_subclasses: ClassVar[Set[Type[BaseState]]] = set()\n\n    # Mapping of var name to set of computed variables that depend on it\n    _computed_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Mapping of var name to set of substates that depend on it\n    _substate_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Set of vars which always need to be recomputed\n    _always_dirty_computed_vars: ClassVar[Set[str]] = set()\n\n    # Set of substates which always need to be recomputed\n    _always_dirty_substates: ClassVar[Set[str]] = set()\n\n    # The parent state.\n    parent_state: Optional[BaseState] = None\n\n    # The substates of the state.\n    substates: Dict[str, BaseState] = {}\n\n    # The set of dirty vars.\n    dirty_vars: Set[str] = set()\n\n    # The set of dirty substates.\n    dirty_substates: Set[str] = set()\n\n    # The routing path that triggered the state\n    router_data: Dict[str, Any] = {}\n\n    # Per-instance copy of backend base variable values\n    _backend_vars: Dict[str, Any] = {}\n\n    # The router data for the current page\n    router: RouterData = RouterData()\n\n    # Whether the state has ever been touched since instantiation.\n    _was_touched: bool = False\n\n    # Whether this state class is a mixin and should not be instantiated.\n    _mixin: ClassVar[bool] = False\n\n    # A special event handler for setting base vars.\n    setvar: ClassVar[EventHandler]\n\n    def __init__(\n        self,\n        *args,\n        parent_state: BaseState | None = None,\n        init_substates: bool = True,\n        _reflex_internal_init: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize the state.\n\n        DO NOT INSTANTIATE STATE CLASSES DIRECTLY! Use StateManager.get_state() instead.\n\n        Args:\n            *args: The args to pass to the Pydantic init method.\n            parent_state: The parent state.\n            init_substates: Whether to initialize the substates in this instance.\n            _reflex_internal_init: A flag to indicate that the state is being initialized by the framework.\n            **kwargs: The kwargs to pass to the Pydantic init method.\n\n        Raises:\n            ReflexRuntimeError: If the state is instantiated directly by end user.\n        \"\"\"\n        from reflex.utils.exceptions import ReflexRuntimeError\n\n        if not _reflex_internal_init and not is_testing_env():\n            raise ReflexRuntimeError(\n                \"State classes should not be instantiated directly in a Reflex app. \"\n                \"See https://reflex.dev/docs/state/ for further information.\"\n            )\n        kwargs[\"parent_state\"] = parent_state\n        super().__init__(*args, **kwargs)\n\n        # Setup the substates (for memory state manager only).\n        if init_substates:\n            for substate in self.get_substates():\n                self.substates[substate.get_name()] = substate(\n                    parent_state=self,\n                    _reflex_internal_init=True,\n                )\n\n        # Create a fresh copy of the backend variables for this instance\n        self._backend_vars = copy.deepcopy(\n            {name: item for name, item in self.backend_vars.items()}\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the state.\n\n        Returns:\n            The string representation of the state.\n        \"\"\"\n        return f\"{self.__class__.__name__}({self.dict()})\"\n\n    @classmethod\n    def _get_computed_vars(cls) -> list[ComputedVar]:\n        \"\"\"Helper function to get all computed vars of a instance.\n\n        Returns:\n            A list of computed vars.\n        \"\"\"\n        return [\n            v\n            for mixin in cls._mixins() + [cls]\n            for v in mixin.__dict__.values()\n            if isinstance(v, ComputedVar)\n        ]\n\n    @classmethod\n    def __init_subclass__(cls, mixin: bool = False, **kwargs):\n        \"\"\"Do some magic for the subclass initialization.\n\n        Args:\n            mixin: Whether the subclass is a mixin and should not be initialized.\n            **kwargs: The kwargs to pass to the pydantic init_subclass method.\n\n        Raises:\n            StateValueError: If a substate class shadows another.\n        \"\"\"\n        from reflex.utils.exceptions import StateValueError\n\n        super().__init_subclass__(**kwargs)\n\n        cls._mixin = mixin\n        if mixin:\n            return\n\n        # Event handlers should not shadow builtin state methods.\n        cls._check_overridden_methods()\n        # Computed vars should not shadow builtin state props.\n        cls._check_overriden_basevars()\n\n        # Reset subclass tracking for this class.\n        cls.class_subclasses = set()\n\n        # Reset dirty substate tracking for this class.\n        cls._always_dirty_substates = set()\n\n        # Get the parent vars.\n        parent_state = cls.get_parent_state()\n        if parent_state is not None:\n            cls.inherited_vars = parent_state.vars\n            cls.inherited_backend_vars = parent_state.backend_vars\n\n            # Check if another substate class with the same name has already been defined.\n            if cls.__name__ in set(c.__name__ for c in parent_state.class_subclasses):\n                if is_testing_env():\n                    # Clear existing subclass with same name when app is reloaded via\n                    # utils.prerequisites.get_app(reload=True)\n                    parent_state.class_subclasses = set(\n                        c\n                        for c in parent_state.class_subclasses\n                        if c.__name__ != cls.__name__\n                    )\n                else:\n                    # During normal operation, subclasses cannot have the same name, even if they are\n                    # defined in different modules.\n                    raise StateValueError(\n                        f\"The substate class '{cls.__name__}' has been defined multiple times. \"\n                        \"Shadowing substate classes is not allowed.\"\n                    )\n            # Track this new subclass in the parent state's subclasses set.\n            parent_state.class_subclasses.add(cls)\n\n        # Get computed vars.\n        computed_vars = cls._get_computed_vars()\n\n        new_backend_vars = {\n            name: value\n            for name, value in cls.__dict__.items()\n            if types.is_backend_base_variable(name, cls)\n        }\n\n        cls.backend_vars = {\n            **cls.inherited_backend_vars,\n            **new_backend_vars,\n        }\n\n        # Set the base and computed vars.\n        cls.base_vars = {\n            f.name: BaseVar(_var_name=f.name, _var_type=f.outer_type_)._var_set_state(\n                cls\n            )\n            for f in cls.get_fields().values()\n            if f.name not in cls.get_skip_vars()\n        }\n        cls.computed_vars = {v._var_name: v._var_set_state(cls) for v in computed_vars}\n        cls.vars = {\n            **cls.inherited_vars,\n            **cls.base_vars,\n            **cls.computed_vars,\n        }\n        cls.event_handlers = {}\n\n        # Setup the base vars at the class level.\n        for prop in cls.base_vars.values():\n            cls._init_var(prop)\n\n        # Set up the event handlers.\n        events = {\n            name: fn\n            for name, fn in cls.__dict__.items()\n            if cls._item_is_event_handler(name, fn)\n        }\n\n        for mixin in cls._mixins():\n            for name, value in mixin.__dict__.items():\n                if isinstance(value, ComputedVar):\n                    fget = cls._copy_fn(value.fget)\n                    newcv = value._replace(fget=fget)\n                    # cleanup refs to mixin cls in var_data\n                    newcv._var_data = None\n                    newcv._var_set_state(cls)\n                    setattr(cls, name, newcv)\n                    cls.computed_vars[newcv._var_name] = newcv\n                    cls.vars[newcv._var_name] = newcv\n                    continue\n                if types.is_backend_base_variable(name, mixin):\n                    cls.backend_vars[name] = copy.deepcopy(value)\n                    continue\n                if events.get(name) is not None:\n                    continue\n                if not cls._item_is_event_handler(name, value):\n                    continue\n                if parent_state is not None and parent_state.event_handlers.get(name):\n                    continue\n                value = cls._copy_fn(value)\n                value.__qualname__ = f\"{cls.__name__}.{name}\"\n                events[name] = value\n\n        # Create the setvar event handler for this state\n        cls._create_setvar()\n\n        for name, fn in events.items():\n            handler = cls._create_event_handler(fn)\n            cls.event_handlers[name] = handler\n            setattr(cls, name, handler)\n\n        cls._init_var_dependency_dicts()\n\n    @staticmethod\n    def _copy_fn(fn: Callable) -> Callable:\n        \"\"\"Copy a function. Used to copy ComputedVars and EventHandlers from mixins.\n\n        Args:\n            fn: The function to copy.\n\n        Returns:\n            The copied function.\n        \"\"\"\n        newfn = FunctionType(\n            fn.__code__,\n            fn.__globals__,\n            name=fn.__name__,\n            argdefs=fn.__defaults__,\n            closure=fn.__closure__,\n        )\n        newfn.__annotations__ = fn.__annotations__\n        if mark := getattr(fn, BACKGROUND_TASK_MARKER, None):\n            setattr(newfn, BACKGROUND_TASK_MARKER, mark)\n        return newfn\n\n    @staticmethod\n    def _item_is_event_handler(name: str, value: Any) -> bool:\n        \"\"\"Check if the item is an event handler.\n\n        Args:\n            name: The name of the item.\n            value: The value of the item.\n\n        Returns:\n            Whether the item is an event handler.\n        \"\"\"\n        return (\n            not name.startswith(\"_\")\n            and isinstance(value, Callable)\n            and not isinstance(value, EventHandler)\n            and hasattr(value, \"__code__\")\n        )\n\n    @classmethod\n    def _mixins(cls) -> List[Type]:\n        \"\"\"Get the mixin classes of the state.\n\n        Returns:\n            The mixin classes of the state.\n        \"\"\"\n        return [\n            mixin\n            for mixin in cls.__mro__\n            if (\n                mixin not in [pydantic.BaseModel, Base, cls]\n                and issubclass(mixin, BaseState)\n                and mixin._mixin is True\n            )\n        ]\n\n    @classmethod\n    def _init_var_dependency_dicts(cls):\n        \"\"\"Initialize the var dependency tracking dicts.\n\n        Allows the state to know which vars each ComputedVar depends on and\n        whether a ComputedVar depends on a var in its parent state.\n\n        Additional updates tracking dicts for vars and substates that always\n        need to be recomputed.\n        \"\"\"\n        # Initialize per-class var dependency tracking.\n        cls._computed_var_dependencies = defaultdict(set)\n        cls._substate_var_dependencies = defaultdict(set)\n\n        inherited_vars = set(cls.inherited_vars).union(\n            set(cls.inherited_backend_vars),\n        )\n        for cvar_name, cvar in cls.computed_vars.items():\n            # Add the dependencies.\n            for var in cvar._deps(objclass=cls):\n                cls._computed_var_dependencies[var].add(cvar_name)\n                if var in inherited_vars:\n                    # track that this substate depends on its parent for this var\n                    state_name = cls.get_name()\n                    parent_state = cls.get_parent_state()\n                    while parent_state is not None and var in {\n                        **parent_state.vars,\n                        **parent_state.backend_vars,\n                    }:\n                        parent_state._substate_var_dependencies[var].add(state_name)\n                        state_name, parent_state = (\n                            parent_state.get_name(),\n                            parent_state.get_parent_state(),\n                        )\n\n        # ComputedVar with cache=False always need to be recomputed\n        cls._always_dirty_computed_vars = set(\n            cvar_name\n            for cvar_name, cvar in cls.computed_vars.items()\n            if not cvar._cache\n        )\n\n        # Any substate containing a ComputedVar with cache=False always needs to be recomputed\n        if cls._always_dirty_computed_vars:\n            # Tell parent classes that this substate has always dirty computed vars\n            state_name = cls.get_name()\n            parent_state = cls.get_parent_state()\n            while parent_state is not None:\n                parent_state._always_dirty_substates.add(state_name)\n                state_name, parent_state = (\n                    parent_state.get_name(),\n                    parent_state.get_parent_state(),\n                )\n\n    @classmethod\n    def _check_overridden_methods(cls):\n        \"\"\"Check for shadow methods and raise error if any.\n\n        Raises:\n            NameError: When an event handler shadows an inbuilt state method.\n        \"\"\"\n        overridden_methods = set()\n        state_base_functions = cls._get_base_functions()\n        for name, method in inspect.getmembers(cls, inspect.isfunction):\n            # Check if the method is overridden and not a dunder method\n            if (\n                not name.startswith(\"__\")\n                and method.__name__ in state_base_functions\n                and state_base_functions[method.__name__] != method\n            ):\n                overridden_methods.add(method.__name__)\n\n        for method_name in overridden_methods:\n            raise NameError(\n                f\"The event handler name `{method_name}` shadows a builtin State method; use a different name instead\"\n            )\n\n    @classmethod\n    def _check_overriden_basevars(cls):\n        \"\"\"Check for shadow base vars and raise error if any.\n\n        Raises:\n            NameError: When a computed var shadows a base var.\n        \"\"\"\n        for computed_var_ in cls._get_computed_vars():\n            if computed_var_._var_name in cls.__annotations__:\n                raise NameError(\n                    f\"The computed var name `{computed_var_._var_name}` shadows a base var in {cls.__module__}.{cls.__\n```\n# Function Signature: def _potentially_dirty_substates(cls) -> set[Type[BaseState]]:\n# Natural Language Annotations: \"\"\"Determine substates which could be affected by dirty vars in this state.\n\n        Returns:\n            Set of State classes that may need to be fetched to recalc computed vars.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _potentially_dirty_substates(cls)", "  def _potentially_dirty_substates(cls) -> set", "  ```\ndef _potentially_dirty_substates(cls)"]}, "reflex-reflex/state.py-get_delta": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/state.py:\n```\n\"\"\"Define the reflex state specification.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport copy\nimport functools\nimport inspect\nimport os\nimport traceback\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom types import FunctionType, MethodType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Callable,\n    ClassVar,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Type,\n)\n\nimport dill\n\ntry:\n    import pydantic.v1 as pydantic\nexcept ModuleNotFoundError:\n    import pydantic\n\nimport wrapt\nfrom redis.asyncio import Redis\nfrom redis.exceptions import ResponseError\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.event import (\n    BACKGROUND_TASK_MARKER,\n    Event,\n    EventHandler,\n    EventSpec,\n    fix_events,\n    window_alert,\n)\nfrom reflex.utils import console, format, prerequisites, types\nfrom reflex.utils.exceptions import ImmutableStateError, LockExpiredError\nfrom reflex.utils.exec import is_testing_env\nfrom reflex.utils.serializers import SerializedType, serialize, serializer\nfrom reflex.vars import BaseVar, ComputedVar, Var, computed_var\n\nif TYPE_CHECKING:\n    from reflex.components.component import Component\n\n\nDelta = Dict[str, Any]\nvar = computed_var\n\n\n# If the state is this large, it's considered a performance issue.\nTOO_LARGE_SERIALIZED_STATE = 100 * 1024  # 100kb\n\n\nclass HeaderData(Base):\n    \"\"\"An object containing headers data.\"\"\"\n\n    host: str = \"\"\n    origin: str = \"\"\n    upgrade: str = \"\"\n    connection: str = \"\"\n    pragma: str = \"\"\n    cache_control: str = \"\"\n    user_agent: str = \"\"\n    sec_websocket_version: str = \"\"\n    sec_websocket_key: str = \"\"\n    sec_websocket_extensions: str = \"\"\n    accept_encoding: str = \"\"\n    accept_language: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the HeaderData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            for k, v in router_data.get(constants.RouteVar.HEADERS, {}).items():\n                setattr(self, format.to_snake_case(k), v)\n\n\nclass PageData(Base):\n    \"\"\"An object containing page data.\"\"\"\n\n    host: str = \"\"  # repeated with self.headers.origin (remove or keep the duplicate?)\n    path: str = \"\"\n    raw_path: str = \"\"\n    full_path: str = \"\"\n    full_raw_path: str = \"\"\n    params: dict = {}\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the PageData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.host = router_data.get(constants.RouteVar.HEADERS, {}).get(\"origin\")\n            self.path = router_data.get(constants.RouteVar.PATH, \"\")\n            self.raw_path = router_data.get(constants.RouteVar.ORIGIN, \"\")\n            self.full_path = f\"{self.host}{self.path}\"\n            self.full_raw_path = f\"{self.host}{self.raw_path}\"\n            self.params = router_data.get(constants.RouteVar.QUERY, {})\n\n\nclass SessionData(Base):\n    \"\"\"An object containing session data.\"\"\"\n\n    client_token: str = \"\"\n    client_ip: str = \"\"\n    session_id: str = \"\"\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initalize the SessionData object based on router_data.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        if router_data:\n            self.client_token = router_data.get(constants.RouteVar.CLIENT_TOKEN, \"\")\n            self.client_ip = router_data.get(constants.RouteVar.CLIENT_IP, \"\")\n            self.session_id = router_data.get(constants.RouteVar.SESSION_ID, \"\")\n\n\nclass RouterData(Base):\n    \"\"\"An object containing RouterData.\"\"\"\n\n    session: SessionData = SessionData()\n    headers: HeaderData = HeaderData()\n    page: PageData = PageData()\n\n    def __init__(self, router_data: Optional[dict] = None):\n        \"\"\"Initialize the RouterData object.\n\n        Args:\n            router_data: the router_data dict.\n        \"\"\"\n        super().__init__()\n        self.session = SessionData(router_data)\n        self.headers = HeaderData(router_data)\n        self.page = PageData(router_data)\n\n\ndef _no_chain_background_task(\n    state_cls: Type[\"BaseState\"], name: str, fn: Callable\n) -> Callable:\n    \"\"\"Protect against directly chaining a background task from another event handler.\n\n    Args:\n        state_cls: The state class that the event handler is in.\n        name: The name of the background task.\n        fn: The background task coroutine function / generator.\n\n    Returns:\n        A compatible coroutine function / generator that raises a runtime error.\n\n    Raises:\n        TypeError: If the background task is not async.\n    \"\"\"\n    call = f\"{state_cls.__name__}.{name}\"\n    message = (\n        f\"Cannot directly call background task {name!r}, use \"\n        f\"`yield {call}` or `return {call}` instead.\"\n    )\n    if inspect.iscoroutinefunction(fn):\n\n        async def _no_chain_background_task_co(*args, **kwargs):\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_co\n    if inspect.isasyncgenfunction(fn):\n\n        async def _no_chain_background_task_gen(*args, **kwargs):\n            yield\n            raise RuntimeError(message)\n\n        return _no_chain_background_task_gen\n\n    raise TypeError(f\"{fn} is marked as a background task, but is not async.\")\n\n\ndef _substate_key(\n    token: str,\n    state_cls_or_name: BaseState | Type[BaseState] | str | list[str],\n) -> str:\n    \"\"\"Get the substate key.\n\n    Args:\n        token: The token of the state.\n        state_cls_or_name: The state class/instance or name or sequence of name parts.\n\n    Returns:\n        The substate key.\n    \"\"\"\n    if isinstance(state_cls_or_name, BaseState) or (\n        isinstance(state_cls_or_name, type) and issubclass(state_cls_or_name, BaseState)\n    ):\n        state_cls_or_name = state_cls_or_name.get_full_name()\n    elif isinstance(state_cls_or_name, (list, tuple)):\n        state_cls_or_name = \".\".join(state_cls_or_name)\n    return f\"{token}_{state_cls_or_name}\"\n\n\ndef _split_substate_key(substate_key: str) -> tuple[str, str]:\n    \"\"\"Split the substate key into token and state name.\n\n    Args:\n        substate_key: The substate key.\n\n    Returns:\n        Tuple of token and state name.\n    \"\"\"\n    token, _, state_name = substate_key.partition(\"_\")\n    return token, state_name\n\n\nclass EventHandlerSetVar(EventHandler):\n    \"\"\"A special event handler to wrap setvar functionality.\"\"\"\n\n    state_cls: Type[BaseState]\n\n    def __init__(self, state_cls: Type[BaseState]):\n        \"\"\"Initialize the EventHandlerSetVar.\n\n        Args:\n            state_cls: The state class that vars will be set on.\n        \"\"\"\n        super().__init__(\n            fn=type(self).setvar,\n            state_full_name=state_cls.get_full_name(),\n            state_cls=state_cls,  # type: ignore\n        )\n\n    def setvar(self, var_name: str, value: Any):\n        \"\"\"Set the state variable to the value of the event.\n\n        Note: `self` here will be an instance of the state, not EventHandlerSetVar.\n\n        Args:\n            var_name: The name of the variable to set.\n            value: The value to set the variable to.\n        \"\"\"\n        getattr(self, constants.SETTER_PREFIX + var_name)(value)\n\n    def __call__(self, *args: Any) -> EventSpec:\n        \"\"\"Performs pre-checks and munging on the provided args that will become an EventSpec.\n\n        Args:\n            *args: The event args.\n\n        Returns:\n            The (partial) EventSpec that will be used to create the event to setvar.\n\n        Raises:\n            AttributeError: If the given Var name does not exist on the state.\n            EventHandlerValueError: If the given Var name is not a str\n        \"\"\"\n        from reflex.utils.exceptions import EventHandlerValueError\n\n        if args:\n            if not isinstance(args[0], str):\n                raise EventHandlerValueError(\n                    f\"Var name must be passed as a string, got {args[0]!r}\"\n                )\n            # Check that the requested Var setter exists on the State at compile time.\n            if getattr(self.state_cls, constants.SETTER_PREFIX + args[0], None) is None:\n                raise AttributeError(\n                    f\"Variable `{args[0]}` cannot be set on `{self.state_cls.get_full_name()}`\"\n                )\n        return super().__call__(*args)\n\n\nclass BaseState(Base, ABC, extra=pydantic.Extra.allow):\n    \"\"\"The state of the app.\"\"\"\n\n    # A map from the var name to the var.\n    vars: ClassVar[Dict[str, Var]] = {}\n\n    # The base vars of the class.\n    base_vars: ClassVar[Dict[str, BaseVar]] = {}\n\n    # The computed vars of the class.\n    computed_vars: ClassVar[Dict[str, ComputedVar]] = {}\n\n    # Vars inherited by the parent state.\n    inherited_vars: ClassVar[Dict[str, Var]] = {}\n\n    # Backend base vars that are never sent to the client.\n    backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # Backend base vars inherited\n    inherited_backend_vars: ClassVar[Dict[str, Any]] = {}\n\n    # The event handlers.\n    event_handlers: ClassVar[Dict[str, EventHandler]] = {}\n\n    # A set of subclassses of this class.\n    class_subclasses: ClassVar[Set[Type[BaseState]]] = set()\n\n    # Mapping of var name to set of computed variables that depend on it\n    _computed_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Mapping of var name to set of substates that depend on it\n    _substate_var_dependencies: ClassVar[Dict[str, Set[str]]] = {}\n\n    # Set of vars which always need to be recomputed\n    _always_dirty_computed_vars: ClassVar[Set[str]] = set()\n\n    # Set of substates which always need to be recomputed\n    _always_dirty_substates: ClassVar[Set[str]] = set()\n\n    # The parent state.\n    parent_state: Optional[BaseState] = None\n\n    # The substates of the state.\n    substates: Dict[str, BaseState] = {}\n\n    # The set of dirty vars.\n    dirty_vars: Set[str] = set()\n\n    # The set of dirty substates.\n    dirty_substates: Set[str] = set()\n\n    # The routing path that triggered the state\n    router_data: Dict[str, Any] = {}\n\n    # Per-instance copy of backend base variable values\n    _backend_vars: Dict[str, Any] = {}\n\n    # The router data for the current page\n    router: RouterData = RouterData()\n\n    # Whether the state has ever been touched since instantiation.\n    _was_touched: bool = False\n\n    # Whether this state class is a mixin and should not be instantiated.\n    _mixin: ClassVar[bool] = False\n\n    # A special event handler for setting base vars.\n    setvar: ClassVar[EventHandler]\n\n    def __init__(\n        self,\n        *args,\n        parent_state: BaseState | None = None,\n        init_substates: bool = True,\n        _reflex_internal_init: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Initialize the state.\n\n        DO NOT INSTANTIATE STATE CLASSES DIRECTLY! Use StateManager.get_state() instead.\n\n        Args:\n            *args: The args to pass to the Pydantic init method.\n            parent_state: The parent state.\n            init_substates: Whether to initialize the substates in this instance.\n            _reflex_internal_init: A flag to indicate that the state is being initialized by the framework.\n            **kwargs: The kwargs to pass to the Pydantic init method.\n\n        Raises:\n            ReflexRuntimeError: If the state is instantiated directly by end user.\n        \"\"\"\n        from reflex.utils.exceptions import ReflexRuntimeError\n\n        if not _reflex_internal_init and not is_testing_env():\n            raise ReflexRuntimeError(\n                \"State classes should not be instantiated directly in a Reflex app. \"\n                \"See https://reflex.dev/docs/state/ for further information.\"\n            )\n        kwargs[\"parent_state\"] = parent_state\n        super().__init__(*args, **kwargs)\n\n        # Setup the substates (for memory state manager only).\n        if init_substates:\n            for substate in self.get_substates():\n                self.substates[substate.get_name()] = substate(\n                    parent_state=self,\n                    _reflex_internal_init=True,\n                )\n\n        # Create a fresh copy of the backend variables for this instance\n        self._backend_vars = copy.deepcopy(\n            {name: item for name, item in self.backend_vars.items()}\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Get the string representation of the state.\n\n        Returns:\n            The string representation of the state.\n        \"\"\"\n        return f\"{self.__class__.__name__}({self.dict()})\"\n\n    @classmethod\n    def _get_computed_vars(cls) -> list[ComputedVar]:\n        \"\"\"Helper function to get all computed vars of a instance.\n\n        Returns:\n            A list of computed vars.\n        \"\"\"\n        return [\n            v\n            for mixin in cls._mixins() + [cls]\n            for v in mixin.__dict__.values()\n            if isinstance(v, ComputedVar)\n        ]\n\n    @classmethod\n    def __init_subclass__(cls, mixin: bool = False, **kwargs):\n        \"\"\"Do some magic for the subclass initialization.\n\n        Args:\n            mixin: Whether the subclass is a mixin and should not be initialized.\n            **kwargs: The kwargs to pass to the pydantic init_subclass method.\n\n        Raises:\n            StateValueError: If a substate class shadows another.\n        \"\"\"\n        from reflex.utils.exceptions import StateValueError\n\n        super().__init_subclass__(**kwargs)\n\n        cls._mixin = mixin\n        if mixin:\n            return\n\n        # Event handlers should not shadow builtin state methods.\n        cls._check_overridden_methods()\n        # Computed vars should not shadow builtin state props.\n        cls._check_overriden_basevars()\n\n        # Reset subclass tracking for this class.\n        cls.class_subclasses = set()\n\n        # Reset dirty substate tracking for this class.\n        cls._always_dirty_substates = set()\n\n        # Get the parent vars.\n        parent_state = cls.get_parent_state()\n        if parent_state is not None:\n            cls.inherited_vars = parent_state.vars\n            cls.inherited_backend_vars = parent_state.backend_vars\n\n            # Check if another substate class with the same name has already been defined.\n            if cls.__name__ in set(c.__name__ for c in parent_state.class_subclasses):\n                if is_testing_env():\n                    # Clear existing subclass with same name when app is reloaded via\n                    # utils.prerequisites.get_app(reload=True)\n                    parent_state.class_subclasses = set(\n                        c\n                        for c in parent_state.class_subclasses\n                        if c.__name__ != cls.__name__\n                    )\n                else:\n                    # During normal operation, subclasses cannot have the same name, even if they are\n                    # defined in different modules.\n                    raise StateValueError(\n                        f\"The substate class '{cls.__name__}' has been defined multiple times. \"\n                        \"Shadowing substate classes is not allowed.\"\n                    )\n            # Track this new subclass in the parent state's subclasses set.\n            parent_state.class_subclasses.add(cls)\n\n        # Get computed vars.\n        computed_vars = cls._get_computed_vars()\n\n        new_backend_vars = {\n            name: value\n            for name, value in cls.__dict__.items()\n            if types.is_backend_base_variable(name, cls)\n        }\n\n        cls.backend_vars = {\n            **cls.inherited_backend_vars,\n            **new_backend_vars,\n        }\n\n        # Set the base and computed vars.\n        cls.base_vars = {\n            f.name: BaseVar(_var_name=f.name, _var_type=f.outer_type_)._var_set_state(\n                cls\n            )\n            for f in cls.get_fields().values()\n            if f.name not in cls.get_skip_vars()\n        }\n        cls.computed_vars = {v._var_name: v._var_set_state(cls) for v in computed_vars}\n        cls.vars = {\n            **cls.inherited_vars,\n            **cls.base_vars,\n            **cls.computed_vars,\n        }\n        cls.event_handlers = {}\n\n        # Setup the base vars at the class level.\n        for prop in cls.base_vars.values():\n            cls._init_var(prop)\n\n        # Set up the event handlers.\n        events = {\n            name: fn\n            for name, fn in cls.__dict__.items()\n            if cls._item_is_event_handler(name, fn)\n        }\n\n        for mixin in cls._mixins():\n            for name, value in mixin.__dict__.items():\n                if isinstance(value, ComputedVar):\n                    fget = cls._copy_fn(value.fget)\n                    newcv = value._replace(fget=fget)\n                    # cleanup refs to mixin cls in var_data\n                    newcv._var_data = None\n                    newcv._var_set_state(cls)\n                    setattr(cls, name, newcv)\n                    cls.computed_vars[newcv._var_name] = newcv\n                    cls.vars[newcv._var_name] = newcv\n                    continue\n                if types.is_backend_base_variable(name, mixin):\n                    cls.backend_vars[name] = copy.deepcopy(value)\n                    continue\n                if events.get(name) is not None:\n                    continue\n                if not cls._item_is_event_handler(name, value):\n                    continue\n                if parent_state is not None and parent_state.event_handlers.get(name):\n                    continue\n                value = cls._copy_fn(value)\n                value.__qualname__ = f\"{cls.__name__}.{name}\"\n                events[name] = value\n\n        # Create the setvar event handler for this state\n        cls._create_setvar()\n\n        for name, fn in events.items():\n            handler = cls._create_event_handler(fn)\n            cls.event_handlers[name] = handler\n            setattr(cls, name, handler)\n\n        cls._init_var_dependency_dicts()\n\n    @staticmethod\n    def _copy_fn(fn: Callable) -> Callable:\n        \"\"\"Copy a function. Used to copy ComputedVars and EventHandlers from mixins.\n\n        Args:\n            fn: The function to copy.\n\n        Returns:\n            The copied function.\n        \"\"\"\n        newfn = FunctionType(\n            fn.__code__,\n            fn.__globals__,\n            name=fn.__name__,\n            argdefs=fn.__defaults__,\n            closure=fn.__closure__,\n        )\n        newfn.__annotations__ = fn.__annotations__\n        if mark := getattr(fn, BACKGROUND_TASK_MARKER, None):\n            setattr(newfn, BACKGROUND_TASK_MARKER, mark)\n        return newfn\n\n    @staticmethod\n    def _item_is_event_handler(name: str, value: Any) -> bool:\n        \"\"\"Check if the item is an event handler.\n\n        Args:\n            name: The name of the item.\n            value: The value of the item.\n\n        Returns:\n            Whether the item is an event handler.\n        \"\"\"\n        return (\n            not name.startswith(\"_\")\n            and isinstance(value, Callable)\n            and not isinstance(value, EventHandler)\n            and hasattr(value, \"__code__\")\n        )\n\n    @classmethod\n    def _mixins(cls) -> List[Type]:\n        \"\"\"Get the mixin classes of the state.\n\n        Returns:\n            The mixin classes of the state.\n        \"\"\"\n        return [\n            mixin\n            for mixin in cls.__mro__\n            if (\n                mixin not in [pydantic.BaseModel, Base, cls]\n                and issubclass(mixin, BaseState)\n                and mixin._mixin is True\n            )\n        ]\n\n    @classmethod\n    def _init_var_dependency_dicts(cls):\n        \"\"\"Initialize the var dependency tracking dicts.\n\n        Allows the state to know which vars each ComputedVar depends on and\n        whether a ComputedVar depends on a var in its parent state.\n\n        Additional updates tracking dicts for vars and substates that always\n        need to be recomputed.\n        \"\"\"\n        # Initialize per-class var dependency tracking.\n        cls._computed_var_dependencies = defaultdict(set)\n        cls._substate_var_dependencies = defaultdict(set)\n\n        inherited_vars = set(cls.inherited_vars).union(\n            set(cls.inherited_backend_vars),\n        )\n        for cvar_name, cvar in cls.computed_vars.items():\n            # Add the dependencies.\n            for var in cvar._deps(objclass=cls):\n                cls._computed_var_dependencies[var].add(cvar_name)\n                if var in inherited_vars:\n                    # track that this substate depends on its parent for this var\n                    state_name = cls.get_name()\n                    parent_state = cls.get_parent_state()\n                    while parent_state is not None and var in {\n                        **parent_state.vars,\n                        **parent_state.backend_vars,\n                    }:\n                        parent_state._substate_var_dependencies[var].add(state_name)\n                        state_name, parent_state = (\n                            parent_state.get_name(),\n                            parent_state.get_parent_state(),\n                        )\n\n        # ComputedVar with cache=False always need to be recomputed\n        cls._always_dirty_computed_vars = set(\n            cvar_name\n            for cvar_name, cvar in cls.computed_vars.items()\n            if not cvar._cache\n        )\n\n        # Any substate containing a ComputedVar with cache=False always needs to be recomputed\n        if cls._always_dirty_computed_vars:\n            # Tell parent classes that this substate has always dirty computed vars\n            state_name = cls.get_name()\n            parent_state = cls.get_parent_state()\n            while parent_state is not None:\n                parent_state._always_dirty_substates.add(state_name)\n                state_name, parent_state = (\n                    parent_state.get_name(),\n                    parent_state.get_parent_state(),\n                )\n\n    @classmethod\n    def _check_overridden_methods(cls):\n        \"\"\"Check for shadow methods and raise error if any.\n\n        Raises:\n            NameError: When an event handler shadows an inbuilt state method.\n        \"\"\"\n        overridden_methods = set()\n        state_base_functions = cls._get_base_functions()\n        for name, method in inspect.getmembers(cls, inspect.isfunction):\n            # Check if the method is overridden and not a dunder method\n            if (\n                not name.startswith(\"__\")\n                and method.__name__ in state_base_functions\n                and state_base_functions[method.__name__] != method\n            ):\n                overridden_methods.add(method.__name__)\n\n        for method_name in overridden_methods:\n            raise NameError(\n                f\"The event handler name `{method_name}` shadows a builtin State method; use a different name instead\"\n            )\n\n    @classmethod\n    def _check_overriden_basevars(cls):\n        \"\"\"Check for shadow base vars and raise error if any.\n\n        Raises:\n            NameError: When a computed var shadows a base var.\n        \"\"\"\n        for computed_var_ in cls._get_computed_vars():\n            if computed_var_._var_name in cls.__annotations__:\n                raise NameError(\n                    f\"The computed var name `{computed_var_._var_name}` shadows a base var in {cls.__module__}.{cls.__\n```\n# Function Signature: def get_delta(self) -> Delta:\n# Natural Language Annotations: \"\"\"Get the delta for the state.\n\n        Returns:\n            The delta for the state.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_delta(self) -> Delta:\n   ", "  ```\n@wrapt.synchronized\ndef get_delta", "  ```\ndef get_delta(self) -> Delta:\n   "]}, "reflex-reflex/model.py-get_db_engine": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/model.py:\n```\n\"\"\"Database built into Reflex.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, ClassVar, Optional, Type, Union\n\nimport alembic.autogenerate\nimport alembic.command\nimport alembic.config\nimport alembic.operations.ops\nimport alembic.runtime.environment\nimport alembic.script\nimport alembic.util\nimport sqlalchemy\nimport sqlalchemy.orm\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.utils import console\nfrom reflex.utils.compat import sqlmodel\n\n\ndef get_engine(url: str | None = None) -> sqlalchemy.engine.Engine:\n    \"\"\"Get the database engine.\n\n    Args:\n        url: the DB url to use.\n\n    Returns:\n        The database engine.\n\n    Raises:\n        ValueError: If the database url is None.\n    \"\"\"\n    conf = get_config()\n    url = url or conf.db_url\n    if url is None:\n        raise ValueError(\"No database url configured\")\n    if not Path(constants.ALEMBIC_CONFIG).exists():\n        console.warn(\n            \"Database is not initialized, run [bold]reflex db init[/bold] first.\"\n        )\n    # Print the SQL queries if the log level is INFO or lower.\n    echo_db_query = os.environ.get(\"SQLALCHEMY_ECHO\") == \"True\"\n    # Needed for the admin dash on sqlite.\n    connect_args = {\"check_same_thread\": False} if url.startswith(\"sqlite\") else {}\n    return sqlmodel.create_engine(url, echo=echo_db_query, connect_args=connect_args)\n\n\nSQLModelOrSqlAlchemy = Union[\n    Type[sqlmodel.SQLModel], Type[sqlalchemy.orm.DeclarativeBase]\n]\n\n\nclass ModelRegistry:\n    \"\"\"Registry for all models.\"\"\"\n\n    models: ClassVar[set[SQLModelOrSqlAlchemy]] = set()\n\n    # Cache the metadata to avoid re-creating it.\n    _metadata: ClassVar[sqlalchemy.MetaData | None] = None\n\n    @classmethod\n    def register(cls, model: SQLModelOrSqlAlchemy):\n        \"\"\"Register a model. Can be used directly or as a decorator.\n\n        Args:\n            model: The model to register.\n\n        Returns:\n            The model passed in as an argument (Allows decorator usage)\n        \"\"\"\n        cls.models.add(model)\n        return model\n\n    @classmethod\n    def get_models(cls, include_empty: bool = False) -> set[SQLModelOrSqlAlchemy]:\n        \"\"\"Get registered models.\n\n        Args:\n            include_empty: If True, include models with empty metadata.\n\n        Returns:\n            The registered models.\n        \"\"\"\n        if include_empty:\n            return cls.models\n        return {\n            model for model in cls.models if not cls._model_metadata_is_empty(model)\n        }\n\n    @staticmethod\n    def _model_metadata_is_empty(model: SQLModelOrSqlAlchemy) -> bool:\n        \"\"\"Check if the model metadata is empty.\n\n        Args:\n            model: The model to check.\n\n        Returns:\n            True if the model metadata is empty, False otherwise.\n        \"\"\"\n        return len(model.metadata.tables) == 0\n\n    @classmethod\n    def get_metadata(cls) -> sqlalchemy.MetaData:\n        \"\"\"Get the database metadata.\n\n        Returns:\n            The database metadata.\n        \"\"\"\n        if cls._metadata is not None:\n            return cls._metadata\n\n        models = cls.get_models(include_empty=False)\n\n        if len(models) == 1:\n            metadata = next(iter(models)).metadata\n        else:\n            # Merge the metadata from all the models.\n            # This allows mixing bare sqlalchemy models with sqlmodel models in one database.\n            metadata = sqlalchemy.MetaData()\n            for model in cls.get_models():\n                for table in model.metadata.tables.values():\n                    table.to_metadata(metadata)\n\n        # Cache the metadata\n        cls._metadata = metadata\n\n        return metadata\n\n\nclass Model(Base, sqlmodel.SQLModel):  # pyright: ignore [reportGeneralTypeIssues]\n    \"\"\"Base class to define a table in the database.\"\"\"\n\n    # The primary key for the table.\n    id: Optional[int] = sqlmodel.Field(default=None, primary_key=True)\n\n    def __init_subclass__(cls):\n        \"\"\"Drop the default primary key field if any primary key field is defined.\"\"\"\n        non_default_primary_key_fields = [\n            field_name\n            for field_name, field in cls.__fields__.items()\n            if field_name != \"id\"\n            and getattr(field.field_info, \"primary_key\", None) is True\n        ]\n        if non_default_primary_key_fields:\n            cls.__fields__.pop(\"id\", None)\n\n        super().__init_subclass__()\n\n    @classmethod\n    def _dict_recursive(cls, value):\n        \"\"\"Recursively serialize the relationship object(s).\n\n        Args:\n            value: The value to serialize.\n\n        Returns:\n            The serialized value.\n        \"\"\"\n        if hasattr(value, \"dict\"):\n            return value.dict()\n        elif isinstance(value, list):\n            return [cls._dict_recursive(item) for item in value]\n        return value\n\n    def dict(self, **kwargs):\n        \"\"\"Convert the object to a dictionary.\n\n        Args:\n            kwargs: Ignored but needed for compatibility.\n\n        Returns:\n            The object as a dictionary.\n        \"\"\"\n        base_fields = {name: getattr(self, name) for name in self.__fields__}\n        relationships = {}\n        # SQLModel relationships do not appear in __fields__, but should be included if present.\n        for name in self.__sqlmodel_relationships__:\n            try:\n                relationships[name] = self._dict_recursive(getattr(self, name))\n            except sqlalchemy.orm.exc.DetachedInstanceError:\n                # This happens when the relationship was never loaded and the session is closed.\n                continue\n        return {\n            **base_fields,\n            **relationships,\n        }\n\n    @staticmethod\n    def create_all():\n        \"\"\"Create all the tables.\"\"\"\n        engine = get_engine()\n        ModelRegistry.get_metadata().create_all(engine)\n\n    @staticmethod\n\n\n\n\n\n\n\n\n    @staticmethod\n    def _alembic_config():\n        \"\"\"Get the alembic configuration and script_directory.\n\n        Returns:\n            tuple of (config, script_directory)\n        \"\"\"\n        config = alembic.config.Config(constants.ALEMBIC_CONFIG)\n        return config, alembic.script.ScriptDirectory(\n            config.get_main_option(\"script_location\", default=\"version\"),\n        )\n\n    @staticmethod\n    def _alembic_render_item(\n        type_: str,\n        obj: Any,\n        autogen_context: \"alembic.autogenerate.api.AutogenContext\",\n    ):\n        \"\"\"Alembic render_item hook call.\n\n        This method is called to provide python code for the given obj,\n        but currently it is only used to add `sqlmodel` to the import list\n        when generating migration scripts.\n\n        See https://alembic.sqlalchemy.org/en/latest/api/runtime.html\n\n        Args:\n            type_: One of \"schema\", \"table\", \"column\", \"index\",\n                \"unique_constraint\", or \"foreign_key_constraint\".\n            obj: The object being rendered.\n            autogen_context: Shared AutogenContext passed to each render_item call.\n\n        Returns:\n            False - Indicating that the default rendering should be used.\n        \"\"\"\n        autogen_context.imports.add(\"import sqlmodel\")\n        return False\n\n    @classmethod\n    def alembic_init(cls):\n        \"\"\"Initialize alembic for the project.\"\"\"\n        alembic.command.init(\n            config=alembic.config.Config(constants.ALEMBIC_CONFIG),\n            directory=str(Path(constants.ALEMBIC_CONFIG).parent / \"alembic\"),\n        )\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def _alembic_upgrade(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        to_rev: str = \"head\",\n    ) -> None:\n        \"\"\"Apply alembic migrations up to the given revision.\n\n        Args:\n            connection: SQLAlchemy connection to use when performing upgrade.\n            to_rev: Revision to migrate towards.\n        \"\"\"\n        config, script_directory = cls._alembic_config()\n\n        def run_upgrade(rev, context):\n            return script_directory._upgrade_revs(to_rev, rev)\n\n        with alembic.runtime.environment.EnvironmentContext(\n            config=config,\n            script=script_directory,\n            fn=run_upgrade,\n        ) as env:\n            env.configure(connection=connection)\n            env.run_migrations()\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\nModelRegistry.register(Model)\n\n\ndef session(url: str | None = None) -> sqlmodel.Session:\n    \"\"\"Get a sqlmodel session to interact with the database.\n\n    Args:\n        url: The database url.\n\n    Returns:\n        A database session.\n    \"\"\"\n    return sqlmodel.Session(get_engine(url))\n\n\ndef sqla_session(url: str | None = None) -> sqlalchemy.orm.Session:\n    \"\"\"Get a bare sqlalchemy session to interact with the database.\n\n    Args:\n        url: The database url.\n\n    Returns:\n        A database session.\n    \"\"\"\n    return sqlalchemy.orm.Session(get_engine(url))\n\n```\n# Function Signature: def get_db_engine():\n# Natural Language Annotations: \"\"\"Get the database engine.\n\n        Returns:\n            The database engine.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_db_engine():\n    \"\"\"Get the database", "  ```\ndef get_db_engine() -> sqlalchemy.", "  ```\ndef get_db_engine():\n    \"\"\"Get the database"]}, "reflex-reflex/model.py-alembic_autogenerate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/model.py:\n```\n\"\"\"Database built into Reflex.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, ClassVar, Optional, Type, Union\n\nimport alembic.autogenerate\nimport alembic.command\nimport alembic.config\nimport alembic.operations.ops\nimport alembic.runtime.environment\nimport alembic.script\nimport alembic.util\nimport sqlalchemy\nimport sqlalchemy.orm\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.utils import console\nfrom reflex.utils.compat import sqlmodel\n\n\ndef get_engine(url: str | None = None) -> sqlalchemy.engine.Engine:\n    \"\"\"Get the database engine.\n\n    Args:\n        url: the DB url to use.\n\n    Returns:\n        The database engine.\n\n    Raises:\n        ValueError: If the database url is None.\n    \"\"\"\n    conf = get_config()\n    url = url or conf.db_url\n    if url is None:\n        raise ValueError(\"No database url configured\")\n    if not Path(constants.ALEMBIC_CONFIG).exists():\n        console.warn(\n            \"Database is not initialized, run [bold]reflex db init[/bold] first.\"\n        )\n    # Print the SQL queries if the log level is INFO or lower.\n    echo_db_query = os.environ.get(\"SQLALCHEMY_ECHO\") == \"True\"\n    # Needed for the admin dash on sqlite.\n    connect_args = {\"check_same_thread\": False} if url.startswith(\"sqlite\") else {}\n    return sqlmodel.create_engine(url, echo=echo_db_query, connect_args=connect_args)\n\n\nSQLModelOrSqlAlchemy = Union[\n    Type[sqlmodel.SQLModel], Type[sqlalchemy.orm.DeclarativeBase]\n]\n\n\nclass ModelRegistry:\n    \"\"\"Registry for all models.\"\"\"\n\n    models: ClassVar[set[SQLModelOrSqlAlchemy]] = set()\n\n    # Cache the metadata to avoid re-creating it.\n    _metadata: ClassVar[sqlalchemy.MetaData | None] = None\n\n    @classmethod\n    def register(cls, model: SQLModelOrSqlAlchemy):\n        \"\"\"Register a model. Can be used directly or as a decorator.\n\n        Args:\n            model: The model to register.\n\n        Returns:\n            The model passed in as an argument (Allows decorator usage)\n        \"\"\"\n        cls.models.add(model)\n        return model\n\n    @classmethod\n    def get_models(cls, include_empty: bool = False) -> set[SQLModelOrSqlAlchemy]:\n        \"\"\"Get registered models.\n\n        Args:\n            include_empty: If True, include models with empty metadata.\n\n        Returns:\n            The registered models.\n        \"\"\"\n        if include_empty:\n            return cls.models\n        return {\n            model for model in cls.models if not cls._model_metadata_is_empty(model)\n        }\n\n    @staticmethod\n    def _model_metadata_is_empty(model: SQLModelOrSqlAlchemy) -> bool:\n        \"\"\"Check if the model metadata is empty.\n\n        Args:\n            model: The model to check.\n\n        Returns:\n            True if the model metadata is empty, False otherwise.\n        \"\"\"\n        return len(model.metadata.tables) == 0\n\n    @classmethod\n    def get_metadata(cls) -> sqlalchemy.MetaData:\n        \"\"\"Get the database metadata.\n\n        Returns:\n            The database metadata.\n        \"\"\"\n        if cls._metadata is not None:\n            return cls._metadata\n\n        models = cls.get_models(include_empty=False)\n\n        if len(models) == 1:\n            metadata = next(iter(models)).metadata\n        else:\n            # Merge the metadata from all the models.\n            # This allows mixing bare sqlalchemy models with sqlmodel models in one database.\n            metadata = sqlalchemy.MetaData()\n            for model in cls.get_models():\n                for table in model.metadata.tables.values():\n                    table.to_metadata(metadata)\n\n        # Cache the metadata\n        cls._metadata = metadata\n\n        return metadata\n\n\nclass Model(Base, sqlmodel.SQLModel):  # pyright: ignore [reportGeneralTypeIssues]\n    \"\"\"Base class to define a table in the database.\"\"\"\n\n    # The primary key for the table.\n    id: Optional[int] = sqlmodel.Field(default=None, primary_key=True)\n\n    def __init_subclass__(cls):\n        \"\"\"Drop the default primary key field if any primary key field is defined.\"\"\"\n        non_default_primary_key_fields = [\n            field_name\n            for field_name, field in cls.__fields__.items()\n            if field_name != \"id\"\n            and getattr(field.field_info, \"primary_key\", None) is True\n        ]\n        if non_default_primary_key_fields:\n            cls.__fields__.pop(\"id\", None)\n\n        super().__init_subclass__()\n\n    @classmethod\n    def _dict_recursive(cls, value):\n        \"\"\"Recursively serialize the relationship object(s).\n\n        Args:\n            value: The value to serialize.\n\n        Returns:\n            The serialized value.\n        \"\"\"\n        if hasattr(value, \"dict\"):\n            return value.dict()\n        elif isinstance(value, list):\n            return [cls._dict_recursive(item) for item in value]\n        return value\n\n    def dict(self, **kwargs):\n        \"\"\"Convert the object to a dictionary.\n\n        Args:\n            kwargs: Ignored but needed for compatibility.\n\n        Returns:\n            The object as a dictionary.\n        \"\"\"\n        base_fields = {name: getattr(self, name) for name in self.__fields__}\n        relationships = {}\n        # SQLModel relationships do not appear in __fields__, but should be included if present.\n        for name in self.__sqlmodel_relationships__:\n            try:\n                relationships[name] = self._dict_recursive(getattr(self, name))\n            except sqlalchemy.orm.exc.DetachedInstanceError:\n                # This happens when the relationship was never loaded and the session is closed.\n                continue\n        return {\n            **base_fields,\n            **relationships,\n        }\n\n    @staticmethod\n    def create_all():\n        \"\"\"Create all the tables.\"\"\"\n        engine = get_engine()\n        ModelRegistry.get_metadata().create_all(engine)\n\n    @staticmethod\n\n\n\n\n\n\n\n\n    @staticmethod\n    def _alembic_config():\n        \"\"\"Get the alembic configuration and script_directory.\n\n        Returns:\n            tuple of (config, script_directory)\n        \"\"\"\n        config = alembic.config.Config(constants.ALEMBIC_CONFIG)\n        return config, alembic.script.ScriptDirectory(\n            config.get_main_option(\"script_location\", default=\"version\"),\n        )\n\n    @staticmethod\n    def _alembic_render_item(\n        type_: str,\n        obj: Any,\n        autogen_context: \"alembic.autogenerate.api.AutogenContext\",\n    ):\n        \"\"\"Alembic render_item hook call.\n\n        This method is called to provide python code for the given obj,\n        but currently it is only used to add `sqlmodel` to the import list\n        when generating migration scripts.\n\n        See https://alembic.sqlalchemy.org/en/latest/api/runtime.html\n\n        Args:\n            type_: One of \"schema\", \"table\", \"column\", \"index\",\n                \"unique_constraint\", or \"foreign_key_constraint\".\n            obj: The object being rendered.\n            autogen_context: Shared AutogenContext passed to each render_item call.\n\n        Returns:\n            False - Indicating that the default rendering should be used.\n        \"\"\"\n        autogen_context.imports.add(\"import sqlmodel\")\n        return False\n\n    @classmethod\n    def alembic_init(cls):\n        \"\"\"Initialize alembic for the project.\"\"\"\n        alembic.command.init(\n            config=alembic.config.Config(constants.ALEMBIC_CONFIG),\n            directory=str(Path(constants.ALEMBIC_CONFIG).parent / \"alembic\"),\n        )\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def _alembic_upgrade(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        to_rev: str = \"head\",\n    ) -> None:\n        \"\"\"Apply alembic migrations up to the given revision.\n\n        Args:\n            connection: SQLAlchemy connection to use when performing upgrade.\n            to_rev: Revision to migrate towards.\n        \"\"\"\n        config, script_directory = cls._alembic_config()\n\n        def run_upgrade(rev, context):\n            return script_directory._upgrade_revs(to_rev, rev)\n\n        with alembic.runtime.environment.EnvironmentContext(\n            config=config,\n            script=script_directory,\n            fn=run_upgrade,\n        ) as env:\n            env.configure(connection=connection)\n            env.run_migrations()\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\nModelRegistry.register(Model)\n\n\ndef session(url: str | None = None) -> sqlmodel.Session:\n    \"\"\"Get a sqlmodel session to interact with the database.\n\n    Args:\n        url: The database url.\n\n    Returns:\n        A database session.\n    \"\"\"\n    return sqlmodel.Session(get_engine(url))\n\n\ndef sqla_session(url: str | None = None) -> sqlalchemy.orm.Session:\n    \"\"\"Get a bare sqlalchemy session to interact with the database.\n\n    Args:\n        url: The database url.\n\n    Returns:\n        A database session.\n    \"\"\"\n    return sqlalchemy.orm.Session(get_engine(url))\n\n```\n# Function Signature: def alembic_autogenerate(\n# Natural Language Annotations: \"\"\"Generate migration scripts for alembic-detectable changes.\n\n        Args:\n            connection: SQLAlchemy connection to use when detecting changes.\n            message: Human readable identifier describing the generated revision.\n            write_migration_scripts: If True, write autogenerated revisions to script directory.\n\n        Returns:\n            True when changes have been detected.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef alembic_autogenerate(\n    connection", "  ```\ndef alembic_autogenerate(\n    connection", "  ```\ndef alembic_autogenerate(\n    connection"]}, "reflex-reflex/model.py-migrate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/model.py:\n```\n\"\"\"Database built into Reflex.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, ClassVar, Optional, Type, Union\n\nimport alembic.autogenerate\nimport alembic.command\nimport alembic.config\nimport alembic.operations.ops\nimport alembic.runtime.environment\nimport alembic.script\nimport alembic.util\nimport sqlalchemy\nimport sqlalchemy.orm\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.utils import console\nfrom reflex.utils.compat import sqlmodel\n\n\ndef get_engine(url: str | None = None) -> sqlalchemy.engine.Engine:\n    \"\"\"Get the database engine.\n\n    Args:\n        url: the DB url to use.\n\n    Returns:\n        The database engine.\n\n    Raises:\n        ValueError: If the database url is None.\n    \"\"\"\n    conf = get_config()\n    url = url or conf.db_url\n    if url is None:\n        raise ValueError(\"No database url configured\")\n    if not Path(constants.ALEMBIC_CONFIG).exists():\n        console.warn(\n            \"Database is not initialized, run [bold]reflex db init[/bold] first.\"\n        )\n    # Print the SQL queries if the log level is INFO or lower.\n    echo_db_query = os.environ.get(\"SQLALCHEMY_ECHO\") == \"True\"\n    # Needed for the admin dash on sqlite.\n    connect_args = {\"check_same_thread\": False} if url.startswith(\"sqlite\") else {}\n    return sqlmodel.create_engine(url, echo=echo_db_query, connect_args=connect_args)\n\n\nSQLModelOrSqlAlchemy = Union[\n    Type[sqlmodel.SQLModel], Type[sqlalchemy.orm.DeclarativeBase]\n]\n\n\nclass ModelRegistry:\n    \"\"\"Registry for all models.\"\"\"\n\n    models: ClassVar[set[SQLModelOrSqlAlchemy]] = set()\n\n    # Cache the metadata to avoid re-creating it.\n    _metadata: ClassVar[sqlalchemy.MetaData | None] = None\n\n    @classmethod\n    def register(cls, model: SQLModelOrSqlAlchemy):\n        \"\"\"Register a model. Can be used directly or as a decorator.\n\n        Args:\n            model: The model to register.\n\n        Returns:\n            The model passed in as an argument (Allows decorator usage)\n        \"\"\"\n        cls.models.add(model)\n        return model\n\n    @classmethod\n    def get_models(cls, include_empty: bool = False) -> set[SQLModelOrSqlAlchemy]:\n        \"\"\"Get registered models.\n\n        Args:\n            include_empty: If True, include models with empty metadata.\n\n        Returns:\n            The registered models.\n        \"\"\"\n        if include_empty:\n            return cls.models\n        return {\n            model for model in cls.models if not cls._model_metadata_is_empty(model)\n        }\n\n    @staticmethod\n    def _model_metadata_is_empty(model: SQLModelOrSqlAlchemy) -> bool:\n        \"\"\"Check if the model metadata is empty.\n\n        Args:\n            model: The model to check.\n\n        Returns:\n            True if the model metadata is empty, False otherwise.\n        \"\"\"\n        return len(model.metadata.tables) == 0\n\n    @classmethod\n    def get_metadata(cls) -> sqlalchemy.MetaData:\n        \"\"\"Get the database metadata.\n\n        Returns:\n            The database metadata.\n        \"\"\"\n        if cls._metadata is not None:\n            return cls._metadata\n\n        models = cls.get_models(include_empty=False)\n\n        if len(models) == 1:\n            metadata = next(iter(models)).metadata\n        else:\n            # Merge the metadata from all the models.\n            # This allows mixing bare sqlalchemy models with sqlmodel models in one database.\n            metadata = sqlalchemy.MetaData()\n            for model in cls.get_models():\n                for table in model.metadata.tables.values():\n                    table.to_metadata(metadata)\n\n        # Cache the metadata\n        cls._metadata = metadata\n\n        return metadata\n\n\nclass Model(Base, sqlmodel.SQLModel):  # pyright: ignore [reportGeneralTypeIssues]\n    \"\"\"Base class to define a table in the database.\"\"\"\n\n    # The primary key for the table.\n    id: Optional[int] = sqlmodel.Field(default=None, primary_key=True)\n\n    def __init_subclass__(cls):\n        \"\"\"Drop the default primary key field if any primary key field is defined.\"\"\"\n        non_default_primary_key_fields = [\n            field_name\n            for field_name, field in cls.__fields__.items()\n            if field_name != \"id\"\n            and getattr(field.field_info, \"primary_key\", None) is True\n        ]\n        if non_default_primary_key_fields:\n            cls.__fields__.pop(\"id\", None)\n\n        super().__init_subclass__()\n\n    @classmethod\n    def _dict_recursive(cls, value):\n        \"\"\"Recursively serialize the relationship object(s).\n\n        Args:\n            value: The value to serialize.\n\n        Returns:\n            The serialized value.\n        \"\"\"\n        if hasattr(value, \"dict\"):\n            return value.dict()\n        elif isinstance(value, list):\n            return [cls._dict_recursive(item) for item in value]\n        return value\n\n    def dict(self, **kwargs):\n        \"\"\"Convert the object to a dictionary.\n\n        Args:\n            kwargs: Ignored but needed for compatibility.\n\n        Returns:\n            The object as a dictionary.\n        \"\"\"\n        base_fields = {name: getattr(self, name) for name in self.__fields__}\n        relationships = {}\n        # SQLModel relationships do not appear in __fields__, but should be included if present.\n        for name in self.__sqlmodel_relationships__:\n            try:\n                relationships[name] = self._dict_recursive(getattr(self, name))\n            except sqlalchemy.orm.exc.DetachedInstanceError:\n                # This happens when the relationship was never loaded and the session is closed.\n                continue\n        return {\n            **base_fields,\n            **relationships,\n        }\n\n    @staticmethod\n    def create_all():\n        \"\"\"Create all the tables.\"\"\"\n        engine = get_engine()\n        ModelRegistry.get_metadata().create_all(engine)\n\n    @staticmethod\n\n\n\n\n\n\n\n\n    @staticmethod\n    def _alembic_config():\n        \"\"\"Get the alembic configuration and script_directory.\n\n        Returns:\n            tuple of (config, script_directory)\n        \"\"\"\n        config = alembic.config.Config(constants.ALEMBIC_CONFIG)\n        return config, alembic.script.ScriptDirectory(\n            config.get_main_option(\"script_location\", default=\"version\"),\n        )\n\n    @staticmethod\n    def _alembic_render_item(\n        type_: str,\n        obj: Any,\n        autogen_context: \"alembic.autogenerate.api.AutogenContext\",\n    ):\n        \"\"\"Alembic render_item hook call.\n\n        This method is called to provide python code for the given obj,\n        but currently it is only used to add `sqlmodel` to the import list\n        when generating migration scripts.\n\n        See https://alembic.sqlalchemy.org/en/latest/api/runtime.html\n\n        Args:\n            type_: One of \"schema\", \"table\", \"column\", \"index\",\n                \"unique_constraint\", or \"foreign_key_constraint\".\n            obj: The object being rendered.\n            autogen_context: Shared AutogenContext passed to each render_item call.\n\n        Returns:\n            False - Indicating that the default rendering should be used.\n        \"\"\"\n        autogen_context.imports.add(\"import sqlmodel\")\n        return False\n\n    @classmethod\n    def alembic_init(cls):\n        \"\"\"Initialize alembic for the project.\"\"\"\n        alembic.command.init(\n            config=alembic.config.Config(constants.ALEMBIC_CONFIG),\n            directory=str(Path(constants.ALEMBIC_CONFIG).parent / \"alembic\"),\n        )\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def _alembic_upgrade(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        to_rev: str = \"head\",\n    ) -> None:\n        \"\"\"Apply alembic migrations up to the given revision.\n\n        Args:\n            connection: SQLAlchemy connection to use when performing upgrade.\n            to_rev: Revision to migrate towards.\n        \"\"\"\n        config, script_directory = cls._alembic_config()\n\n        def run_upgrade(rev, context):\n            return script_directory._upgrade_revs(to_rev, rev)\n\n        with alembic.runtime.environment.EnvironmentContext(\n            config=config,\n            script=script_directory,\n            fn=run_upgrade,\n        ) as env:\n            env.configure(connection=connection)\n            env.run_migrations()\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\nModelRegistry.register(Model)\n\n\ndef session(url: str | None = None) -> sqlmodel.Session:\n    \"\"\"Get a sqlmodel session to interact with the database.\n\n    Args:\n        url: The database url.\n\n    Returns:\n        A database session.\n    \"\"\"\n    return sqlmodel.Session(get_engine(url))\n\n\ndef sqla_session(url: str | None = None) -> sqlalchemy.orm.Session:\n    \"\"\"Get a bare sqlalchemy session to interact with the database.\n\n    Args:\n        url: The database url.\n\n    Returns:\n        A database session.\n    \"\"\"\n    return sqlalchemy.orm.Session(get_engine(url))\n\n```\n# Function Signature: def migrate(cls, autogenerate: bool = False) -> bool | None:\n# Natural Language Annotations: \"\"\"Execute alembic migrations for all sqlmodel Model classes.\n\n        If alembic is not installed or has not been initialized for the project,\n        then no action is performed.\n\n        If there are no revisions currently tracked by alembic, then\n        an initial revision will be created based on sqlmodel metadata.\n\n        If models in the app have changed in incompatible ways that alembic\n        cannot automatically generate revisions for, the app may not be able to\n        start up until migration scripts have been corrected by hand.\n\n        Args:\n            autogenerate: If True, generate migration script and use it to upgrade schema\n                (otherwise, just bring the schema to current \"head\" revision).\n\n        Returns:\n            True - indicating the process was successful.\n            None - indicating the process was skipped.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  \n```\ndef migrate(cls, autogenerate: bool", "  ```\nfrom __future__ import annotations\n\nimport os\nfrom", "  ```\nimport logging\n\nimport sqlmodel\nfrom alembic."]}, "reflex-reflex/model.py-select": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/model.py:\n```\n\"\"\"Database built into Reflex.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, ClassVar, Optional, Type, Union\n\nimport alembic.autogenerate\nimport alembic.command\nimport alembic.config\nimport alembic.operations.ops\nimport alembic.runtime.environment\nimport alembic.script\nimport alembic.util\nimport sqlalchemy\nimport sqlalchemy.orm\n\nfrom reflex import constants\nfrom reflex.base import Base\nfrom reflex.config import get_config\nfrom reflex.utils import console\nfrom reflex.utils.compat import sqlmodel\n\n\ndef get_engine(url: str | None = None) -> sqlalchemy.engine.Engine:\n    \"\"\"Get the database engine.\n\n    Args:\n        url: the DB url to use.\n\n    Returns:\n        The database engine.\n\n    Raises:\n        ValueError: If the database url is None.\n    \"\"\"\n    conf = get_config()\n    url = url or conf.db_url\n    if url is None:\n        raise ValueError(\"No database url configured\")\n    if not Path(constants.ALEMBIC_CONFIG).exists():\n        console.warn(\n            \"Database is not initialized, run [bold]reflex db init[/bold] first.\"\n        )\n    # Print the SQL queries if the log level is INFO or lower.\n    echo_db_query = os.environ.get(\"SQLALCHEMY_ECHO\") == \"True\"\n    # Needed for the admin dash on sqlite.\n    connect_args = {\"check_same_thread\": False} if url.startswith(\"sqlite\") else {}\n    return sqlmodel.create_engine(url, echo=echo_db_query, connect_args=connect_args)\n\n\nSQLModelOrSqlAlchemy = Union[\n    Type[sqlmodel.SQLModel], Type[sqlalchemy.orm.DeclarativeBase]\n]\n\n\nclass ModelRegistry:\n    \"\"\"Registry for all models.\"\"\"\n\n    models: ClassVar[set[SQLModelOrSqlAlchemy]] = set()\n\n    # Cache the metadata to avoid re-creating it.\n    _metadata: ClassVar[sqlalchemy.MetaData | None] = None\n\n    @classmethod\n    def register(cls, model: SQLModelOrSqlAlchemy):\n        \"\"\"Register a model. Can be used directly or as a decorator.\n\n        Args:\n            model: The model to register.\n\n        Returns:\n            The model passed in as an argument (Allows decorator usage)\n        \"\"\"\n        cls.models.add(model)\n        return model\n\n    @classmethod\n    def get_models(cls, include_empty: bool = False) -> set[SQLModelOrSqlAlchemy]:\n        \"\"\"Get registered models.\n\n        Args:\n            include_empty: If True, include models with empty metadata.\n\n        Returns:\n            The registered models.\n        \"\"\"\n        if include_empty:\n            return cls.models\n        return {\n            model for model in cls.models if not cls._model_metadata_is_empty(model)\n        }\n\n    @staticmethod\n    def _model_metadata_is_empty(model: SQLModelOrSqlAlchemy) -> bool:\n        \"\"\"Check if the model metadata is empty.\n\n        Args:\n            model: The model to check.\n\n        Returns:\n            True if the model metadata is empty, False otherwise.\n        \"\"\"\n        return len(model.metadata.tables) == 0\n\n    @classmethod\n    def get_metadata(cls) -> sqlalchemy.MetaData:\n        \"\"\"Get the database metadata.\n\n        Returns:\n            The database metadata.\n        \"\"\"\n        if cls._metadata is not None:\n            return cls._metadata\n\n        models = cls.get_models(include_empty=False)\n\n        if len(models) == 1:\n            metadata = next(iter(models)).metadata\n        else:\n            # Merge the metadata from all the models.\n            # This allows mixing bare sqlalchemy models with sqlmodel models in one database.\n            metadata = sqlalchemy.MetaData()\n            for model in cls.get_models():\n                for table in model.metadata.tables.values():\n                    table.to_metadata(metadata)\n\n        # Cache the metadata\n        cls._metadata = metadata\n\n        return metadata\n\n\nclass Model(Base, sqlmodel.SQLModel):  # pyright: ignore [reportGeneralTypeIssues]\n    \"\"\"Base class to define a table in the database.\"\"\"\n\n    # The primary key for the table.\n    id: Optional[int] = sqlmodel.Field(default=None, primary_key=True)\n\n    def __init_subclass__(cls):\n        \"\"\"Drop the default primary key field if any primary key field is defined.\"\"\"\n        non_default_primary_key_fields = [\n            field_name\n            for field_name, field in cls.__fields__.items()\n            if field_name != \"id\"\n            and getattr(field.field_info, \"primary_key\", None) is True\n        ]\n        if non_default_primary_key_fields:\n            cls.__fields__.pop(\"id\", None)\n\n        super().__init_subclass__()\n\n    @classmethod\n    def _dict_recursive(cls, value):\n        \"\"\"Recursively serialize the relationship object(s).\n\n        Args:\n            value: The value to serialize.\n\n        Returns:\n            The serialized value.\n        \"\"\"\n        if hasattr(value, \"dict\"):\n            return value.dict()\n        elif isinstance(value, list):\n            return [cls._dict_recursive(item) for item in value]\n        return value\n\n    def dict(self, **kwargs):\n        \"\"\"Convert the object to a dictionary.\n\n        Args:\n            kwargs: Ignored but needed for compatibility.\n\n        Returns:\n            The object as a dictionary.\n        \"\"\"\n        base_fields = {name: getattr(self, name) for name in self.__fields__}\n        relationships = {}\n        # SQLModel relationships do not appear in __fields__, but should be included if present.\n        for name in self.__sqlmodel_relationships__:\n            try:\n                relationships[name] = self._dict_recursive(getattr(self, name))\n            except sqlalchemy.orm.exc.DetachedInstanceError:\n                # This happens when the relationship was never loaded and the session is closed.\n                continue\n        return {\n            **base_fields,\n            **relationships,\n        }\n\n    @staticmethod\n    def create_all():\n        \"\"\"Create all the tables.\"\"\"\n        engine = get_engine()\n        ModelRegistry.get_metadata().create_all(engine)\n\n    @staticmethod\n\n\n\n\n\n\n\n\n    @staticmethod\n    def _alembic_config():\n        \"\"\"Get the alembic configuration and script_directory.\n\n        Returns:\n            tuple of (config, script_directory)\n        \"\"\"\n        config = alembic.config.Config(constants.ALEMBIC_CONFIG)\n        return config, alembic.script.ScriptDirectory(\n            config.get_main_option(\"script_location\", default=\"version\"),\n        )\n\n    @staticmethod\n    def _alembic_render_item(\n        type_: str,\n        obj: Any,\n        autogen_context: \"alembic.autogenerate.api.AutogenContext\",\n    ):\n        \"\"\"Alembic render_item hook call.\n\n        This method is called to provide python code for the given obj,\n        but currently it is only used to add `sqlmodel` to the import list\n        when generating migration scripts.\n\n        See https://alembic.sqlalchemy.org/en/latest/api/runtime.html\n\n        Args:\n            type_: One of \"schema\", \"table\", \"column\", \"index\",\n                \"unique_constraint\", or \"foreign_key_constraint\".\n            obj: The object being rendered.\n            autogen_context: Shared AutogenContext passed to each render_item call.\n\n        Returns:\n            False - Indicating that the default rendering should be used.\n        \"\"\"\n        autogen_context.imports.add(\"import sqlmodel\")\n        return False\n\n    @classmethod\n    def alembic_init(cls):\n        \"\"\"Initialize alembic for the project.\"\"\"\n        alembic.command.init(\n            config=alembic.config.Config(constants.ALEMBIC_CONFIG),\n            directory=str(Path(constants.ALEMBIC_CONFIG).parent / \"alembic\"),\n        )\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def _alembic_upgrade(\n        cls,\n        connection: sqlalchemy.engine.Connection,\n        to_rev: str = \"head\",\n    ) -> None:\n        \"\"\"Apply alembic migrations up to the given revision.\n\n        Args:\n            connection: SQLAlchemy connection to use when performing upgrade.\n            to_rev: Revision to migrate towards.\n        \"\"\"\n        config, script_directory = cls._alembic_config()\n\n        def run_upgrade(rev, context):\n            return script_directory._upgrade_revs(to_rev, rev)\n\n        with alembic.runtime.environment.EnvironmentContext(\n            config=config,\n            script=script_directory,\n            fn=run_upgrade,\n        ) as env:\n            env.configure(connection=connection)\n            env.run_migrations()\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n\n\n\n\n\n\n\n\n\nModelRegistry.register(Model)\n\n\ndef session(url: str | None = None) -> sqlmodel.Session:\n    \"\"\"Get a sqlmodel session to interact with the database.\n\n    Args:\n        url: The database url.\n\n    Returns:\n        A database session.\n    \"\"\"\n    return sqlmodel.Session(get_engine(url))\n\n\ndef sqla_session(url: str | None = None) -> sqlalchemy.orm.Session:\n    \"\"\"Get a bare sqlalchemy session to interact with the database.\n\n    Args:\n        url: The database url.\n\n    Returns:\n        A database session.\n    \"\"\"\n    return sqlalchemy.orm.Session(get_engine(url))\n\n```\n# Function Signature: def select(cls):\n# Natural Language Annotations: \"\"\"Select rows from the table.\n\n        Returns:\n            The select statement.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef select(cls):\n    \"\"\"Select rows from the table", "  ```\ndef select(self):\n    \"\"\"Select rows from the table", "  ```\nfrom sqlmodel import select\n\nclass Model(Base, sql"]}, "reflex-reflex/compiler/compiler.py-compile_components": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/compiler/compiler.py:\n```\n\"\"\"Compiler for the reflex apps.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, Iterable, Optional, Type, Union\n\nfrom reflex import constants\nfrom reflex.compiler import templates, utils\nfrom reflex.components.component import (\n    BaseComponent,\n    Component,\n    ComponentStyle,\n    CustomComponent,\n    StatefulComponent,\n)\nfrom reflex.config import get_config\nfrom reflex.state import BaseState\nfrom reflex.style import SYSTEM_COLOR_MODE\nfrom reflex.utils.exec import is_prod_mode\nfrom reflex.utils.imports import ImportVar\nfrom reflex.utils.prerequisites import get_web_dir\nfrom reflex.vars import Var\n\n\ndef _compile_document_root(root: Component) -> str:\n    \"\"\"Compile the document root.\n\n    Args:\n        root: The document root to compile.\n\n    Returns:\n        The compiled document root.\n    \"\"\"\n    return templates.DOCUMENT_ROOT.render(\n        imports=utils.compile_imports(root._get_all_imports()),\n        document=root.render(),\n    )\n\n\ndef _compile_app(app_root: Component) -> str:\n    \"\"\"Compile the app template component.\n\n    Args:\n        app_root: The app root to compile.\n\n    Returns:\n        The compiled app.\n    \"\"\"\n    return templates.APP_ROOT.render(\n        imports=utils.compile_imports(app_root._get_all_imports()),\n        custom_codes=app_root._get_all_custom_code(),\n        hooks={**app_root._get_all_hooks_internal(), **app_root._get_all_hooks()},\n        render=app_root.render(),\n    )\n\n\ndef _compile_theme(theme: dict) -> str:\n    \"\"\"Compile the theme.\n\n    Args:\n        theme: The theme to compile.\n\n    Returns:\n        The compiled theme.\n    \"\"\"\n    return templates.THEME.render(theme=theme)\n\n\ndef _compile_contexts(state: Optional[Type[BaseState]], theme: Component | None) -> str:\n    \"\"\"Compile the initial state and contexts.\n\n    Args:\n        state: The app state.\n        theme: The top-level app theme.\n\n    Returns:\n        The compiled context file.\n    \"\"\"\n    appearance = getattr(theme, \"appearance\", None)\n    if appearance is None or Var.create_safe(appearance)._var_name == \"inherit\":\n        appearance = SYSTEM_COLOR_MODE\n\n    last_compiled_time = str(datetime.now())\n    return (\n        templates.CONTEXT.render(\n            initial_state=utils.compile_state(state),\n            state_name=state.get_name(),\n            client_storage=utils.compile_client_storage(state),\n            is_dev_mode=not is_prod_mode(),\n            last_compiled_time=last_compiled_time,\n            default_color_mode=appearance,\n        )\n        if state\n        else templates.CONTEXT.render(\n            is_dev_mode=not is_prod_mode(),\n            default_color_mode=appearance,\n            last_compiled_time=last_compiled_time,\n        )\n    )\n\n\ndef _compile_page(\n    component: Component,\n    state: Type[BaseState],\n) -> str:\n    \"\"\"Compile the component given the app state.\n\n    Args:\n        component: The component to compile.\n        state: The app state.\n\n    Returns:\n        The compiled component.\n    \"\"\"\n    imports = component._get_all_imports()\n    imports = utils.compile_imports(imports)\n\n    # Compile the code to render the component.\n    kwargs = {\"state_name\": state.get_name()} if state else {}\n\n    return templates.PAGE.render(\n        imports=imports,\n        dynamic_imports=component._get_all_dynamic_imports(),\n        custom_codes=component._get_all_custom_code(),\n        hooks={**component._get_all_hooks_internal(), **component._get_all_hooks()},\n        render=component.render(),\n        **kwargs,\n    )\n\n\ndef compile_root_stylesheet(stylesheets: list[str]) -> tuple[str, str]:\n    \"\"\"Compile the root stylesheet.\n\n    Args:\n        stylesheets: The stylesheets to include in the root stylesheet.\n\n    Returns:\n        The path and code of the compiled root stylesheet.\n    \"\"\"\n    output_path = utils.get_root_stylesheet_path()\n\n    code = _compile_root_stylesheet(stylesheets)\n\n    return output_path, code\n\n\ndef _compile_root_stylesheet(stylesheets: list[str]) -> str:\n    \"\"\"Compile the root stylesheet.\n\n    Args:\n        stylesheets: The stylesheets to include in the root stylesheet.\n\n    Returns:\n        The compiled root stylesheet.\n\n    Raises:\n        FileNotFoundError: If a specified stylesheet in assets directory does not exist.\n    \"\"\"\n    # Add tailwind css if enabled.\n    sheets = (\n        [constants.Tailwind.ROOT_STYLE_PATH]\n        if get_config().tailwind is not None\n        else []\n    )\n    for stylesheet in stylesheets:\n        if not utils.is_valid_url(stylesheet):\n            # check if stylesheet provided exists.\n            stylesheet_full_path = (\n                Path.cwd() / constants.Dirs.APP_ASSETS / stylesheet.strip(\"/\")\n            )\n            if not os.path.exists(stylesheet_full_path):\n                raise FileNotFoundError(\n                    f\"The stylesheet file {stylesheet_full_path} does not exist.\"\n                )\n            stylesheet = f\"../{constants.Dirs.PUBLIC}/{stylesheet.strip('/')}\"\n        sheets.append(stylesheet) if stylesheet not in sheets else None\n    return templates.STYLE.render(stylesheets=sheets)\n\n\ndef _compile_component(component: Component | StatefulComponent) -> str:\n    \"\"\"Compile a single component.\n\n    Args:\n        component: The component to compile.\n\n    Returns:\n        The compiled component.\n    \"\"\"\n    return templates.COMPONENT.render(component=component)\n\n\ndef _compile_components(\n    components: set[CustomComponent],\n) -> tuple[str, Dict[str, list[ImportVar]]]:\n    \"\"\"Compile the components.\n\n    Args:\n        components: The components to compile.\n\n    Returns:\n        The compiled components.\n    \"\"\"\n    imports = {\n        \"react\": [ImportVar(tag=\"memo\")],\n        f\"/{constants.Dirs.STATE_PATH}\": [ImportVar(tag=\"E\"), ImportVar(tag=\"isTrue\")],\n    }\n    component_renders = []\n\n    # Compile each component.\n    for component in components:\n        component_render, component_imports = utils.compile_custom_component(component)\n        component_renders.append(component_render)\n        imports = utils.merge_imports(imports, component_imports)\n\n    # Compile the components page.\n    return (\n        templates.COMPONENTS.render(\n            imports=utils.compile_imports(imports),\n            components=component_renders,\n        ),\n        imports,\n    )\n\n\ndef _compile_stateful_components(\n    page_components: list[BaseComponent],\n) -> str:\n    \"\"\"Walk the page components and extract shared stateful components.\n\n    Any StatefulComponent that is shared by more than one page will be rendered\n    to a separate module and marked rendered_as_shared so subsequent\n    renderings will import the component from the shared module instead of\n    directly including the code for it.\n\n    Args:\n        page_components: The Components or StatefulComponents to compile.\n\n    Returns:\n        The rendered stateful components code.\n    \"\"\"\n    all_import_dicts = []\n    rendered_components = {}\n\n    def get_shared_components_recursive(component: BaseComponent):\n        \"\"\"Get the shared components for a component and its children.\n\n        A shared component is a StatefulComponent that appears in 2 or more\n        pages and is a candidate for writing to a common file and importing\n        into each page where it is used.\n\n        Args:\n            component: The component to collect shared StatefulComponents for.\n        \"\"\"\n        for child in component.children:\n            # Depth-first traversal.\n            get_shared_components_recursive(child)\n\n        # When the component is referenced by more than one page, render it\n        # to be included in the STATEFUL_COMPONENTS module.\n        # Skip this step in dev mode, thereby avoiding potential hot reload errors for larger apps\n        if (\n            isinstance(component, StatefulComponent)\n            and component.references > 1\n            and is_prod_mode()\n        ):\n            # Reset this flag to render the actual component.\n            component.rendered_as_shared = False\n\n            # Include dynamic imports in the shared component.\n            if dynamic_imports := component._get_all_dynamic_imports():\n                rendered_components.update(\n                    {dynamic_import: None for dynamic_import in dynamic_imports}\n                )\n\n            # Include custom code in the shared component.\n            rendered_components.update(\n                {code: None for code in component._get_all_custom_code()},\n            )\n\n            # Include all imports in the shared component.\n            all_import_dicts.append(component._get_all_imports())\n\n            # Indicate that this component now imports from the shared file.\n            component.rendered_as_shared = True\n\n    for page_component in page_components:\n        get_shared_components_recursive(page_component)\n\n    # Don't import from the file that we're about to create.\n    all_imports = utils.merge_imports(*all_import_dicts)\n    all_imports.pop(\n        f\"/{constants.Dirs.UTILS}/{constants.PageNames.STATEFUL_COMPONENTS}\", None\n    )\n\n    return templates.STATEFUL_COMPONENTS.render(\n        imports=utils.compile_imports(all_imports),\n        memoized_code=\"\\n\".join(rendered_components),\n    )\n\n\ndef _compile_tailwind(\n    config: dict,\n) -> str:\n    \"\"\"Compile the Tailwind config.\n\n    Args:\n        config: The Tailwind config.\n\n    Returns:\n        The compiled Tailwind config.\n    \"\"\"\n    return templates.TAILWIND_CONFIG.render(\n        **config,\n    )\n\n\ndef compile_document_root(\n    head_components: list[Component],\n    html_lang: Optional[str] = None,\n    html_custom_attrs: Optional[Dict[str, Union[Var, str]]] = None,\n) -> tuple[str, str]:\n    \"\"\"Compile the document root.\n\n    Args:\n        head_components: The components to include in the head.\n        html_lang: The language of the document, will be added to the html root element.\n        html_custom_attrs: custom attributes added to the html root element.\n\n    Returns:\n        The path and code of the compiled document root.\n    \"\"\"\n    # Get the path for the output file.\n    output_path = utils.get_page_path(constants.PageNames.DOCUMENT_ROOT)\n\n    # Create the document root.\n    document_root = utils.create_document_root(\n        head_components, html_lang=html_lang, html_custom_attrs=html_custom_attrs\n    )\n\n    # Compile the document root.\n    code = _compile_document_root(document_root)\n    return output_path, code\n\n\ndef compile_app(app_root: Component) -> tuple[str, str]:\n    \"\"\"Compile the app root.\n\n    Args:\n        app_root: The app root component to compile.\n\n    Returns:\n        The path and code of the compiled app wrapper.\n    \"\"\"\n    # Get the path for the output file.\n    output_path = utils.get_page_path(constants.PageNames.APP_ROOT)\n\n    # Compile the document root.\n    code = _compile_app(app_root)\n    return output_path, code\n\n\ndef compile_theme(style: ComponentStyle) -> tuple[str, str]:\n    \"\"\"Compile the theme.\n\n    Args:\n        style: The style to compile.\n\n    Returns:\n        The path and code of the compiled theme.\n    \"\"\"\n    output_path = utils.get_theme_path()\n\n    # Create the theme.\n    theme = utils.create_theme(style)\n\n    # Compile the theme.\n    code = _compile_theme(theme)\n    return output_path, code\n\n\ndef compile_contexts(\n    state: Optional[Type[BaseState]],\n    theme: Component | None,\n) -> tuple[str, str]:\n    \"\"\"Compile the initial state / context.\n\n    Args:\n        state: The app state.\n        theme: The top-level app theme.\n\n    Returns:\n        The path and code of the compiled context.\n    \"\"\"\n    # Get the path for the output file.\n    output_path = utils.get_context_path()\n\n    return output_path, _compile_contexts(state, theme)\n\n\ndef compile_page(\n    path: str, component: Component, state: Type[BaseState]\n) -> tuple[str, str]:\n    \"\"\"Compile a single page.\n\n    Args:\n        path: The path to compile the page to.\n        component: The component to compile.\n        state: The app state.\n\n    Returns:\n        The path and code of the compiled page.\n    \"\"\"\n    # Get the path for the output file.\n    output_path = utils.get_page_path(path)\n\n    # Add the style to the component.\n    code = _compile_page(component, state)\n    return output_path, code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef compile_stateful_components(\n    pages: Iterable[Component],\n) -> tuple[str, str, list[BaseComponent]]:\n    \"\"\"Separately compile components that depend on State vars.\n\n    StatefulComponents are compiled as their own component functions with their own\n    useContext declarations, which allows page components to be stateless and avoid\n    re-rendering along with parts of the page that actually depend on state.\n\n    Args:\n        pages: The pages to extract stateful components from.\n\n    Returns:\n        The path and code of the compiled stateful components.\n    \"\"\"\n    output_path = utils.get_stateful_components_path()\n\n    # Compile the stateful components.\n    page_components = [StatefulComponent.compile_from(page) or page for page in pages]\n    code = _compile_stateful_components(page_components)\n    return output_path, code, page_components\n\n\ndef compile_tailwind(\n    config: dict,\n):\n    \"\"\"Compile the Tailwind config.\n\n    Args:\n        config: The Tailwind config.\n\n    Returns:\n        The compiled Tailwind config.\n    \"\"\"\n    # Get the path for the output file.\n    output_path = get_web_dir() / constants.Tailwind.CONFIG\n\n    # Compile the config.\n    code = _compile_tailwind(config)\n    return output_path, code\n\n\ndef remove_tailwind_from_postcss() -> tuple[str, str]:\n    \"\"\"If tailwind is not to be used, remove it from postcss.config.js.\n\n    Returns:\n        The path and code of the compiled postcss.config.js.\n    \"\"\"\n    # Get the path for the output file.\n    output_path = str(get_web_dir() / constants.Dirs.POSTCSS_JS)\n\n    code = [\n        line\n        for line in Path(output_path).read_text().splitlines(keepends=True)\n        if \"tailwindcss: \" not in line\n    ]\n\n    # Compile the config.\n    return output_path, \"\".join(code)\n\n\ndef purge_web_pages_dir():\n    \"\"\"Empty out .web/pages directory.\"\"\"\n    if not is_prod_mode() and os.environ.get(\"REFLEX_PERSIST_WEB_DIR\"):\n        # Skip purging the web directory in dev mode if REFLEX_PERSIST_WEB_DIR is set.\n        return\n\n    # Empty out the web pages directory.\n    utils.empty_dir(get_web_dir() / constants.Dirs.PAGES, keep_files=[\"_app.js\"])\n\n\nclass ExecutorSafeFunctions:\n    \"\"\"Helper class to allow parallelisation of parts of the compilation process.\n\n    This class (and its class attributes) are available at global scope.\n\n    In a multiprocessing context (like when using a ProcessPoolExecutor), the content of this\n    global class is logically replicated to any FORKED process.\n\n    How it works:\n    * Before the child process is forked, ensure that we stash any input data required by any future\n      function call in the child process.\n    * After the child process is forked, the child process will have a copy of the global class, which\n      includes the previously stashed input data.\n    * Any task submitted to the child process simply needs a way to communicate which input data the\n      requested function call requires.\n\n    Why do we need this? Passing input data directly to child process often not possible because the input data is not picklable.\n    The mechanic described here removes the need to pickle the input data at all.\n\n    Limitations:\n    * This can never support returning unpicklable OUTPUT data.\n    * Any object mutations done by the child process will not propagate back to the parent process (fork goes one way!).\n\n    \"\"\"\n\n    COMPILE_PAGE_ARGS_BY_ROUTE = {}\n    COMPILE_APP_APP_ROOT: Component | None = None\n    CUSTOM_COMPONENTS: set[CustomComponent] | None = None\n    STYLE: ComponentStyle | None = None\n\n    @classmethod\n    def compile_page(cls, route: str):\n        \"\"\"Compile a page.\n\n        Args:\n            route: The route of the page to compile.\n\n        Returns:\n            The path and code of the compiled page.\n        \"\"\"\n        return compile_page(*cls.COMPILE_PAGE_ARGS_BY_ROUTE[route])\n\n    @classmethod\n    def compile_app(cls):\n        \"\"\"Compile the app.\n\n        Returns:\n            The path and code of the compiled app.\n\n        Raises:\n            ValueError: If the app root is not set.\n        \"\"\"\n        if cls.COMPILE_APP_APP_ROOT is None:\n            raise ValueError(\"COMPILE_APP_APP_ROOT should be set\")\n        return compile_app(cls.COMPILE_APP_APP_ROOT)\n\n    @classmethod\n    def compile_custom_components(cls):\n        \"\"\"Compile the custom components.\n\n        Returns:\n            The path and code of the compiled custom components.\n\n        Raises:\n            ValueError: If the custom components are not set.\n        \"\"\"\n        if cls.CUSTOM_COMPONENTS is None:\n            raise ValueError(\"CUSTOM_COMPONENTS should be set\")\n        return compile_components(cls.CUSTOM_COMPONENTS)\n\n    @classmethod\n    def compile_theme(cls):\n        \"\"\"Compile the theme.\n\n        Returns:\n            The path and code of the compiled theme.\n\n        Raises:\n            ValueError: If the style is not set.\n        \"\"\"\n        if cls.STYLE is None:\n            raise ValueError(\"STYLE should be set\")\n        return compile_theme(cls.STYLE)\n\n```\n# Function Signature: def compile_components(\n# Natural Language Annotations: \"\"\"Compile the custom components.\n\n    Args:\n        components: The custom components to compile.\n\n    Returns:\n        The path and code of the compiled components.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```python\ndef compile_components(\n        components: set[Custom", "  ```\ndef compile_components(components):\n    output_path,", "  ```\ndef compile_components(components: set[CustomComponent]) ->"]}, "reflex-reflex/utils/prerequisites.py-get_or_set_last_reflex_version_check_datetime": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/utils/prerequisites.py:\n```\n\"\"\"Everything related to fetching or initializing build prerequisites.\"\"\"\n\nfrom __future__ import annotations\n\nimport functools\nimport glob\nimport importlib\nimport inspect\nimport json\nimport os\nimport platform\nimport random\nimport re\nimport shutil\nimport stat\nimport sys\nimport tempfile\nimport zipfile\nfrom datetime import datetime\nfrom fileinput import FileInput\nfrom pathlib import Path\nfrom types import ModuleType\nfrom typing import Callable, List, Optional\n\nimport httpx\nimport pkg_resources\nimport typer\nfrom alembic.util.exc import CommandError\nfrom packaging import version\nfrom redis import Redis as RedisSync\nfrom redis.asyncio import Redis\n\nimport reflex\nfrom reflex import constants, model\nfrom reflex.base import Base\nfrom reflex.compiler import templates\nfrom reflex.config import Config, get_config\nfrom reflex.utils import console, path_ops, processes\nfrom reflex.utils.format import format_library_name\n\nCURRENTLY_INSTALLING_NODE = False\n\n\nclass Template(Base):\n    \"\"\"A template for a Reflex app.\"\"\"\n\n    name: str\n    description: str\n    code_url: str\n    demo_url: str\n\n\nclass CpuInfo(Base):\n    \"\"\"Model to save cpu info.\"\"\"\n\n    manufacturer_id: Optional[str]\n    model_name: Optional[str]\n    address_width: Optional[int]\n\n\ndef get_web_dir() -> Path:\n    \"\"\"Get the working directory for the next.js commands.\n\n    Can be overriden with REFLEX_WEB_WORKDIR.\n\n    Returns:\n        The working directory.\n    \"\"\"\n    workdir = Path(os.getenv(\"REFLEX_WEB_WORKDIR\", constants.Dirs.WEB))\n    return workdir\n\n\ndef check_latest_package_version(package_name: str):\n    \"\"\"Check if the latest version of the package is installed.\n\n    Args:\n        package_name: The name of the package.\n    \"\"\"\n    try:\n        # Get the latest version from PyPI\n        current_version = pkg_resources.get_distribution(package_name).version\n        url = f\"https://pypi.org/pypi/{package_name}/json\"\n        response = httpx.get(url)\n        latest_version = response.json()[\"info\"][\"version\"]\n        if (\n            version.parse(current_version) < version.parse(latest_version)\n            and not get_or_set_last_reflex_version_check_datetime()\n        ):\n            # only show a warning when the host version is outdated and\n            # the last_version_check_datetime is not set in reflex.json\n            console.warn(\n                f\"Your version ({current_version}) of {package_name} is out of date. Upgrade to {latest_version} with 'pip install {package_name} --upgrade'\"\n            )\n    except Exception:\n        pass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef check_node_version() -> bool:\n    \"\"\"Check the version of Node.js.\n\n    Returns:\n        Whether the version of Node.js is valid.\n    \"\"\"\n    current_version = get_node_version()\n    if current_version:\n        # Compare the version numbers\n        return (\n            current_version >= version.parse(constants.Node.MIN_VERSION)\n            if constants.IS_WINDOWS\n            else current_version == version.parse(constants.Node.VERSION)\n        )\n    return False\n\n\ndef get_node_version() -> version.Version | None:\n    \"\"\"Get the version of node.\n\n    Returns:\n        The version of node.\n    \"\"\"\n    node_path = path_ops.get_node_path()\n    if node_path is None:\n        return None\n    try:\n        result = processes.new_process([node_path, \"-v\"], run=True)\n        # The output will be in the form \"vX.Y.Z\", but version.parse() can handle it\n        return version.parse(result.stdout)  # type: ignore\n    except (FileNotFoundError, TypeError):\n        return None\n\n\ndef get_fnm_version() -> version.Version | None:\n    \"\"\"Get the version of fnm.\n\n    Returns:\n        The version of FNM.\n    \"\"\"\n    try:\n        result = processes.new_process([constants.Fnm.EXE, \"--version\"], run=True)\n        return version.parse(result.stdout.split(\" \")[1])  # type: ignore\n    except (FileNotFoundError, TypeError):\n        return None\n    except version.InvalidVersion as e:\n        console.warn(\n            f\"The detected fnm version ({e.args[0]}) is not valid. Defaulting to None.\"\n        )\n        return None\n\n\ndef get_bun_version() -> version.Version | None:\n    \"\"\"Get the version of bun.\n\n    Returns:\n        The version of bun.\n    \"\"\"\n    try:\n        # Run the bun -v command and capture the output\n        result = processes.new_process([get_config().bun_path, \"-v\"], run=True)\n        return version.parse(result.stdout)  # type: ignore\n    except FileNotFoundError:\n        return None\n    except version.InvalidVersion as e:\n        console.warn(\n            f\"The detected bun version ({e.args[0]}) is not valid. Defaulting to None.\"\n        )\n        return None\n\n\ndef get_install_package_manager() -> str | None:\n    \"\"\"Get the package manager executable for installation.\n      Currently, bun is used for installation only.\n\n    Returns:\n        The path to the package manager.\n    \"\"\"\n    if (\n        constants.IS_WINDOWS\n        and not is_windows_bun_supported()\n        or windows_check_onedrive_in_path()\n        or windows_npm_escape_hatch()\n    ):\n        return get_package_manager()\n    return get_config().bun_path\n\n\ndef get_package_manager() -> str | None:\n    \"\"\"Get the package manager executable for running app.\n      Currently on unix systems, npm is used for running the app only.\n\n    Returns:\n        The path to the package manager.\n    \"\"\"\n    npm_path = path_ops.get_npm_path()\n    if npm_path is not None:\n        npm_path = str(Path(npm_path).resolve())\n    return npm_path\n\n\ndef windows_check_onedrive_in_path() -> bool:\n    \"\"\"For windows, check if oneDrive is present in the project dir path.\n\n    Returns:\n        If oneDrive is in the path of the project directory.\n    \"\"\"\n    return \"onedrive\" in str(Path.cwd()).lower()\n\n\ndef windows_npm_escape_hatch() -> bool:\n    \"\"\"For windows, if the user sets REFLEX_USE_NPM, use npm instead of bun.\n\n    Returns:\n        If the user has set REFLEX_USE_NPM.\n    \"\"\"\n    return os.environ.get(\"REFLEX_USE_NPM\", \"\").lower() in [\"true\", \"1\", \"yes\"]\n\n\ndef get_app(reload: bool = False) -> ModuleType:\n    \"\"\"Get the app module based on the default config.\n\n    Args:\n        reload: Re-import the app module from disk\n\n    Returns:\n        The app based on the default config.\n\n    Raises:\n        RuntimeError: If the app name is not set in the config.\n    \"\"\"\n    from reflex.utils import telemetry\n\n    try:\n        os.environ[constants.RELOAD_CONFIG] = str(reload)\n        config = get_config()\n        if not config.app_name:\n            raise RuntimeError(\n                \"Cannot get the app module because `app_name` is not set in rxconfig! \"\n                \"If this error occurs in a reflex test case, ensure that `get_app` is mocked.\"\n            )\n        module = config.module\n        sys.path.insert(0, os.getcwd())\n        app = __import__(module, fromlist=(constants.CompileVars.APP,))\n\n        if reload:\n            from reflex.state import reload_state_module\n\n            # Reset rx.State subclasses to avoid conflict when reloading.\n            reload_state_module(module=module)\n\n            # Reload the app module.\n            importlib.reload(app)\n\n        return app\n    except Exception as ex:\n        telemetry.send_error(ex, context=\"frontend\")\n        raise\n\n\ndef get_compiled_app(reload: bool = False, export: bool = False) -> ModuleType:\n    \"\"\"Get the app module based on the default config after first compiling it.\n\n    Args:\n        reload: Re-import the app module from disk\n        export: Compile the app for export\n\n    Returns:\n        The compiled app based on the default config.\n    \"\"\"\n    app_module = get_app(reload=reload)\n    app = getattr(app_module, constants.CompileVars.APP)\n    # For py3.8 and py3.9 compatibility when redis is used, we MUST add any decorator pages\n    # before compiling the app in a thread to avoid event loop error (REF-2172).\n    app._apply_decorated_pages()\n    app._compile(export=export)\n    return app_module\n\n\ndef get_redis() -> Redis | None:\n    \"\"\"Get the asynchronous redis client.\n\n    Returns:\n        The asynchronous redis client.\n    \"\"\"\n    if isinstance((redis_url_or_options := parse_redis_url()), str):\n        return Redis.from_url(redis_url_or_options)\n    elif isinstance(redis_url_or_options, dict):\n        return Redis(**redis_url_or_options)\n    return None\n\n\ndef get_redis_sync() -> RedisSync | None:\n    \"\"\"Get the synchronous redis client.\n\n    Returns:\n        The synchronous redis client.\n    \"\"\"\n    if isinstance((redis_url_or_options := parse_redis_url()), str):\n        return RedisSync.from_url(redis_url_or_options)\n    elif isinstance(redis_url_or_options, dict):\n        return RedisSync(**redis_url_or_options)\n    return None\n\n\ndef parse_redis_url() -> str | dict | None:\n    \"\"\"Parse the REDIS_URL in config if applicable.\n\n    Returns:\n        If redis-py syntax, return the URL as it is. Otherwise, return the host/port/db as a dict.\n    \"\"\"\n    config = get_config()\n    if not config.redis_url:\n        return None\n    if config.redis_url.startswith((\"redis://\", \"rediss://\", \"unix://\")):\n        return config.redis_url\n    console.deprecate(\n        feature_name=\"host[:port] style redis urls\",\n        reason=\"redis-py url syntax is now being used\",\n        deprecation_version=\"0.3.6\",\n        removal_version=\"0.6.0\",\n    )\n    redis_url, has_port, redis_port = config.redis_url.partition(\":\")\n    if not has_port:\n        redis_port = 6379\n    console.info(f\"Using redis at {config.redis_url}\")\n    return dict(host=redis_url, port=int(redis_port), db=0)\n\n\ndef validate_app_name(app_name: str | None = None) -> str:\n    \"\"\"Validate the app name.\n\n    The default app name is the name of the current directory.\n\n    Args:\n        app_name: the name passed by user during reflex init\n\n    Returns:\n        The app name after validation.\n\n    Raises:\n        Exit: if the app directory name is reflex or if the name is not standard for a python package name.\n    \"\"\"\n    app_name = (\n        app_name if app_name else os.getcwd().split(os.path.sep)[-1].replace(\"-\", \"_\")\n    )\n    # Make sure the app is not named \"reflex\".\n    if app_name.lower() == constants.Reflex.MODULE_NAME:\n        console.error(\n            f\"The app directory cannot be named [bold]{constants.Reflex.MODULE_NAME}[/bold].\"\n        )\n        raise typer.Exit(1)\n\n    # Make sure the app name is standard for a python package name.\n    if not re.match(r\"^[a-zA-Z][a-zA-Z0-9_]*$\", app_name):\n        console.error(\n            \"The app directory name must start with a letter and can contain letters, numbers, and underscores.\"\n        )\n        raise typer.Exit(1)\n\n    return app_name\n\n\ndef create_config(app_name: str):\n    \"\"\"Create a new rxconfig file.\n\n    Args:\n        app_name: The name of the app.\n    \"\"\"\n    # Import here to avoid circular imports.\n    from reflex.compiler import templates\n\n    config_name = f\"{re.sub(r'[^a-zA-Z]', '', app_name).capitalize()}Config\"\n    with open(constants.Config.FILE, \"w\") as f:\n        console.debug(f\"Creating {constants.Config.FILE}\")\n        f.write(templates.RXCONFIG.render(app_name=app_name, config_name=config_name))\n\n\ndef initialize_gitignore(\n    gitignore_file: str = constants.GitIgnore.FILE,\n    files_to_ignore: set[str] = constants.GitIgnore.DEFAULTS,\n):\n    \"\"\"Initialize the template .gitignore file.\n\n    Args:\n        gitignore_file: The .gitignore file to create.\n        files_to_ignore: The files to add to the .gitignore file.\n    \"\"\"\n    # Combine with the current ignored files.\n    if os.path.exists(gitignore_file):\n        with open(gitignore_file, \"r\") as f:\n            files_to_ignore |= set([line.strip() for line in f.readlines()])\n\n    # Write files to the .gitignore file.\n    with open(gitignore_file, \"w\", newline=\"\\n\") as f:\n        console.debug(f\"Creating {gitignore_file}\")\n        f.write(f\"{(path_ops.join(sorted(files_to_ignore))).lstrip()}\\n\")\n\n\ndef initialize_requirements_txt():\n    \"\"\"Initialize the requirements.txt file.\n    If absent, generate one for the user.\n    If the requirements.txt does not have reflex as dependency,\n    generate a requirement pinning current version and append to\n    the requirements.txt file.\n    \"\"\"\n    fp = Path(constants.RequirementsTxt.FILE)\n    encoding = \"utf-8\"\n    if not fp.exists():\n        fp.touch()\n    else:\n        # Detect the encoding of the original file\n        import charset_normalizer\n\n        charset_matches = charset_normalizer.from_path(fp)\n        maybe_charset_match = charset_matches.best()\n        if maybe_charset_match is None:\n            console.debug(f\"Unable to detect encoding for {fp}, exiting.\")\n            return\n        encoding = maybe_charset_match.encoding\n        console.debug(f\"Detected encoding for {fp} as {encoding}.\")\n    try:\n        other_requirements_exist = False\n        with open(fp, \"r\", encoding=encoding) as f:\n            for req in f.readlines():\n                # Check if we have a package name that is reflex\n                if re.match(r\"^reflex[^a-zA-Z0-9]\", req):\n                    console.debug(f\"{fp} already has reflex as dependency.\")\n                    return\n                other_requirements_exist = True\n        with open(fp, \"a\", encoding=encoding) as f:\n            preceding_newline = \"\\n\" if other_requirements_exist else \"\"\n            f.write(\n                f\"{preceding_newline}{constants.RequirementsTxt.DEFAULTS_STUB}{constants.Reflex.VERSION}\\n\"\n            )\n    except Exception:\n        console.info(f\"Unable to check {fp} for reflex dependency.\")\n\n\ndef initialize_app_directory(\n    app_name: str,\n    template_name: str = constants.Templates.DEFAULT,\n    template_code_dir_name: str | None = None,\n    template_dir: Path | None = None,\n):\n    \"\"\"Initialize the app directory on reflex init.\n\n    Args:\n        app_name: The name of the app.\n        template_name: The name of the template to use.\n        template_code_dir_name: The name of the code directory in the template.\n        template_dir: The directory of the template source files.\n\n    Raises:\n        Exit: If template_name, template_code_dir_name, template_dir combination is not supported.\n    \"\"\"\n    console.log(\"Initializing the app directory.\")\n\n    # By default, use the blank template from local assets.\n    if template_name == constants.Templates.DEFAULT:\n        if template_code_dir_name is not None or template_dir is not None:\n            console.error(\n                f\"Only {template_name=} should be provided, got {template_code_dir_name=}, {template_dir=}.\"\n            )\n            raise typer.Exit(1)\n        template_code_dir_name = constants.Templates.Dirs.CODE\n        template_dir = Path(constants.Templates.Dirs.BASE, \"apps\", template_name)\n    else:\n        if template_code_dir_name is None or template_dir is None:\n            console.error(\n                f\"For `{template_name}` template, `template_code_dir_name` and `template_dir` should both be provided.\"\n            )\n            raise typer.Exit(1)\n\n    console.debug(f\"Using {template_name=} {template_dir=} {template_code_dir_name=}.\")\n\n    # Remove all pyc and __pycache__ dirs in template directory.\n    for pyc_file in template_dir.glob(\"**/*.pyc\"):\n        pyc_file.unlink()\n    for pycache_dir in template_dir.glob(\"**/__pycache__\"):\n        pycache_dir.rmdir()\n\n    for file in template_dir.iterdir():\n        # Copy the file to current directory but keep the name the same.\n        path_ops.cp(str(file), file.name)\n\n    # Rename the template app to the app name.\n    path_ops.mv(template_code_dir_name, app_name)\n    path_ops.mv(\n        os.path.join(app_name, template_name + constants.Ext.PY),\n        os.path.join(app_name, app_name + constants.Ext.PY),\n    )\n\n    # Fix up the imports.\n    path_ops.find_replace(\n        app_name,\n        f\"from {template_name}\",\n        f\"from {app_name}\",\n    )\n\n\ndef get_project_hash(raise_on_fail: bool = False) -> int | None:\n    \"\"\"Get the project hash from the reflex.json file if the file exists.\n\n    Args:\n        raise_on_fail: Whether to raise an error if the file does not exist.\n\n    Returns:\n        project_hash: The app hash.\n    \"\"\"\n    json_file = get_web_dir() / constants.Reflex.JSON\n    if not json_file.exists() and not raise_on_fail:\n        return None\n    data = json.loads(json_file.read_text())\n    return data.get(\"project_hash\")\n\n\ndef initialize_web_directory():\n    \"\"\"Initialize the web directory on reflex init.\"\"\"\n    console.log(\"Initializing the web directory.\")\n\n    # Re-use the hash if one is already created, so we don't over-write it when running reflex init\n    project_hash = get_project_hash()\n\n    path_ops.cp(constants.Templates.Dirs.WEB_TEMPLATE, str(get_web_dir()))\n\n    initialize_package_json()\n\n    path_ops.mkdir(get_web_dir() / constants.Dirs.PUBLIC)\n\n    update_next_config()\n\n    # Initialize the reflex json file.\n    init_reflex_json(project_hash=project_hash)\n\n\ndef _compile_package_json():\n    return templates.PACKAGE_JSON.render(\n        scripts={\n            \"dev\": constants.PackageJson.Commands.DEV,\n            \"export\": constants.PackageJson.Commands.EXPORT,\n            \"export_sitemap\": constants.PackageJson.Commands.EXPORT_SITEMAP,\n            \"prod\": constants.PackageJson.Commands.PROD,\n        },\n        dependencies=constants.PackageJson.DEPENDENCIES,\n        dev_dependencies=constants.PackageJson.DEV_DEPENDENCIES,\n    )\n\n\ndef initialize_package_json():\n    \"\"\"Render and write in .web the package.json file.\"\"\"\n    output_path = get_web_dir() / constants.PackageJson.PATH\n    code = _compile_package_json()\n    output_path.write_text(code)\n\n\ndef init_reflex_json(project_hash: int | None):\n    \"\"\"Write the hash of the Reflex project to a REFLEX_JSON.\n\n    Re-use the hash if one is already created, therefore do not\n    overwrite it every time we run the reflex init command\n    .\n\n    Args:\n        project_hash: The app hash.\n    \"\"\"\n    if project_hash is not None:\n        console.debug(f\"Project hash is already set to {project_hash}.\")\n    else:\n        # Get a random project hash.\n        project_hash = random.getrandbits(128)\n        console.debug(f\"Setting project hash to {project_hash}.\")\n\n    # Write the hash and version to the reflex json file.\n    reflex_json = {\n        \"version\": constants.Reflex.VERSION,\n        \"project_hash\": project_hash,\n    }\n    path_ops.update_json_file(get_web_dir() / constants.Reflex.JSON, reflex_json)\n\n\ndef update_next_config(export=False, transpile_packages: Optional[List[str]] = None):\n    \"\"\"Update Next.js config from Reflex config.\n\n    Args:\n        export: if the method run during reflex export.\n        transpile_packages: list of packages to transpile via next.config.js.\n    \"\"\"\n    next_config_file = get_web_dir() / constants.Next.CONFIG_FILE\n\n    next_config = _update_next_config(\n        get_config(), export=export, transpile_packages=transpile_packages\n    )\n\n    # Overwriting the next.config.js triggers a full server reload, so make sure\n    # there is actually a diff.\n    orig_next_config = next_config_file.read_text() if next_config_file.exists() else \"\"\n    if orig_next_config != next_config:\n        next_config_file.write_text(next_config)\n\n\ndef _update_next_config(\n    config: Config, export: bool = False, transpile_packages: Optional[List[str]] = None\n):\n    next_config = {\n        \"basePath\": config.frontend_path or \"\",\n        \"compress\": config.next_compression,\n        \"reactStrictMode\": config.react_strict_mode,\n        \"trailingSlash\": True,\n    }\n    if transpile_packages:\n        next_config[\"transpilePackages\"] = list(\n            set((format_library_name(p) for p in transpile_packages))\n        )\n    if export:\n        next_config[\"output\"] = \"export\"\n        next_config[\"distDir\"] = constants.Dirs.STATIC\n\n    next_config_json = re.sub(r'\"([^\"]+)\"(?=:)', r\"\\1\", json.dumps(next_config))\n    return f\"module.exports = {next_config_json};\"\n\n\ndef remove_existing_bun_installation():\n    \"\"\"Remove existing bun installation.\"\"\"\n    console.debug(\"Removing existing bun installation.\")\n    if os.path.exists(get_config().bun_path):\n        path_ops.rm(constants.Bun.ROOT_PATH)\n\n\ndef download_and_run(url: str, *args, show_status: bool = False, **env):\n    \"\"\"Download and run a script.\n\n    Args:\n        url: The url of the script.\n        args: The arguments to pass to the script.\n        show_status: Whether to show the status of the script.\n        env: The environment variables to use.\n    \"\"\"\n    # Download the script\n    console.debug(f\"Downloading {url}\")\n    response = httpx.get(url)\n    if response.status_code != httpx.codes.OK:\n        response.raise_for_status()\n\n    # Save the script to a temporary file.\n    script = tempfile.NamedTemporaryFile()\n    with open(script.name, \"w\") as f:\n        f.write(response.text)\n\n    # Run the script.\n    env = {**os.environ, **env}\n    process = processes.new_process([\"bash\", f.name, *args], env=env)\n    show = processes.show_status if show_status else processes.show_logs\n    show(f\"Installing {url}\", process)\n\n\ndef download_and_extract_fnm_zip():\n    \"\"\"Download and run a script.\n\n    Raises:\n        Exit: If an error occurs while downloading or extracting the FNM zip.\n    \"\"\"\n    # Download the zip file\n    url = constants.Fnm.INSTALL_URL\n    console.debug(f\"Downloading {url}\")\n    fnm_zip_file = os.path.join(constants.Fnm.DIR, f\"{constants.Fnm.FILENAME}.zip\")\n    # Function to download and extract the FNM zip release.\n    try:\n        # Download the FNM zip release.\n        # TODO: show progress to improve UX\n        with httpx.stream(\"GET\", url, follow_redirects=True) as response:\n            response.raise_for_status()\n            with open(fnm_zip_file, \"wb\") as output_file:\n                for chunk in response.iter_bytes():\n                    output_file.write(chunk)\n\n        # Extract the downloaded zip file.\n        with zipfile.ZipFile(fnm_zip_file, \"r\") as zip_ref:\n            zip_ref.extractall(constants.Fnm.DIR)\n\n        console.debug(\"FNM package downloaded and extracted successfully.\")\n    except Exception as e:\n        console.error(f\"An error occurred while downloading fnm package: {e}\")\n        raise typer.Exit(1) from e\n    finally:\n        # Clean up the downloaded zip file.\n        path_ops.rm(fnm_zip_file)\n\n\ndef install_node():\n    \"\"\"Install fnm and nodejs for use by Reflex.\n    Independent of any existing system installations.\n    \"\"\"\n    if not constants\n```\n# Function Signature: def get_or_set_last_reflex_version_check_datetime():\n# Natural Language Annotations: \"\"\"Get the last time a check was made for the latest reflex version.\n    This is typically useful for cases where the host reflex version is\n    less than that on Pypi.\n\n    Returns:\n        The last version check datetime.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfrom typing import Optional\nfrom datetime import datetime\n\ndef get", "  ```\nimport datetime\n\n\ndef get_or_set_last", "  ```\ndef get_or_set_last_reflex_version"]}, "reflex-reflex/middleware/hydrate_middleware.py-preprocess": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/middleware/hydrate_middleware.py:\n```\n\"\"\"Middleware to hydrate the state.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Optional\n\nfrom reflex import constants\nfrom reflex.event import Event, get_hydrate_event\nfrom reflex.middleware.middleware import Middleware\nfrom reflex.state import BaseState, StateUpdate\nfrom reflex.utils import format\n\nif TYPE_CHECKING:\n    from reflex.app import App\n\n\nclass HydrateMiddleware(Middleware):\n    \"\"\"Middleware to handle initial app hydration.\"\"\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: async def preprocess(\n# Natural Language Annotations: \"\"\"Preprocess the event.\n\n        Args:\n            app: The app to apply the middleware to.\n            state: The client state.\n            event: The event to preprocess.\n\n        Returns:\n            An optional delta or list of state updates to return.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def preprocess(self, app: \"App\", state", "  ```\nasync def preprocess(app: 'App', state: Base", "  ```\nimport typing\nfrom typing import TYPE_CHECKING,"]}, "reflex-reflex/components/component.py-get_event_triggers": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def get_event_triggers(self) -> Dict[str, Any]:\n# Natural Language Annotations: \"\"\"Get the event triggers for the component.\n\n        Returns:\n            The event triggers.\n\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport reflex.components\n\nclass Component(reflex.", "  ```\nget_event_triggers(self):\n    return", "  ```\ndef get_event_triggers(self):\n   "]}, "reflex-reflex/components/component.py-get_props": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def get_props(cls) -> Set[str]:\n# Natural Language Annotations: \"\"\"Get the unique fields for the component.\n\n        Returns:\n            The unique fields.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```python\ndef get_props(cls) -> Set[str]:", "  ```\ndef get_props(cls) -> Set[str]:\n", "  ```\ndef get_props(cls) -> Set[str]:\n"]}, "reflex-reflex/components/component.py-create": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def create(cls, *children, **props) -> Component:\n# Natural Language Annotations: \"\"\"Create the component.\n\n        Args:\n            *children: The children of the component.\n            **props: The props of the component.\n\n        Returns:\n            The component.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef create(cls, *children, **props):\n   ", "  ```\nimport inspect\nimport typing\nfrom typing import Any, Callable", "  ```\nimport typing\n\nfrom reflex.components.component import Component"]}, "reflex-reflex/components/component.py-_add_style_recursive": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def _add_style_recursive(\n# Natural Language Annotations: \"\"\"Add additional style to the component and its children.\n\n        Apply order is as follows (with the latest overriding the earliest):\n        1. Default style from `_add_style`/`add_style`.\n        2. User-defined style from `App.style`.\n        3. User-defined style from `Component.style`.\n        4. style dict and css props passed to the component instance.\n\n        Args:\n            style: A dict from component to styling.\n            theme: The theme to apply. (for retro-compatibility with deprecated _apply_theme API)\n\n        Raises:\n            UserWarning: If `_add_style` has been overridden.\n\n        Returns:\n            The component with the additional style.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```python\ndef _add_style_recursive(self):\n", "  ```\ndef _add_style_recursive(\n    self:", "  ```\ndef _add_style_recursive(self, style:"]}, "reflex-reflex/components/component.py-render": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def render(self) -> Dict:\n# Natural Language Annotations: \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of component.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef render(self) -> Dict:\n    \"\"\"Render", "  ```\ndef render(self):\n    \"\"\"Render the component.\n", "  ```\nimport json\n\ndef render(self) -> Dict:"]}, "reflex-reflex/components/component.py-_get_vars": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def _get_vars(self, include_children: bool = False) -> list[Var]:\n# Natural Language Annotations: \"\"\"Walk all Vars used in this component.\n\n        Args:\n            include_children: Whether to include Vars from children.\n\n        Returns:\n            Each var referenced by the component (props, styles, event handlers).\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _get_vars(self, include_children: bool", "  ```\ndef _get_vars(self, include_children: bool", "  ```\ndef _get_vars(self, include_children: bool"]}, "reflex-reflex/components/component.py-_has_stateful_event_triggers": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def _has_stateful_event_triggers(self):\n# Natural Language Annotations: \"\"\"Check if component or children have any event triggers that use state.\n\n        Returns:\n            True if the component or children have any event triggers that uses state.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _has_stateful_event_triggers(", "  ```\ndef _has_stateful_event_triggers(", "  ```\ndef _has_stateful_event_triggers("]}, "reflex-reflex/components/component.py-_get_all_custom_code": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def _get_all_custom_code(self) -> set[str]:\n# Natural Language Annotations: \"\"\"Get custom code for the component and its children.\n\n        Returns:\n            The custom code.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```python\ndef _get_all_custom_code(self)", "  ```\ndef _get_all_custom_code(self):\n", "  ```\nimport builtins, types, typing\n\ndef _get_"]}, "reflex-reflex/components/component.py-_get_all_imports": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def _get_all_imports(self, collapse: bool = False) -> ParsedImportDict:\n# Natural Language Annotations: \"\"\"Get all the libraries and fields that are used by the component and its children.\n\n        Args:\n            collapse: Whether to collapse the imports by removing duplicates.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _get_all_imports(self, collapse:", "  ```\nimport { Fragment } from \"react\";\nimport { Text }", "  ```\ndef _get_all_imports(self, collapse:"]}, "reflex-reflex/components/component.py-_get_all_hooks_internal": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def _get_all_hooks_internal(self) -> dict[str, None]:\n# Natural Language Annotations: \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n@classmethod\n@lru_cache(maxsize=", "  def _get_all_hooks_internal(self) -> dict", "  ```\n{\n    \"trigger_dependency_map\": {},\n   "]}, "reflex-reflex/components/component.py-_get_all_hooks": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def _get_all_hooks(self) -> dict[str, None]:\n# Natural Language Annotations: \"\"\"Get the React hooks for this component and its children.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport { useState } from \"react\";\n\nconst hook", "  ```\ndef _get_all_hooks(self) -> dict", "  ```\ndef _get_all_hooks(self):\n   "]}, "reflex-reflex/components/component.py-_get_all_custom_components": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/component.py:\n```\n\"\"\"Base component definitions.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport typing\nfrom abc import ABC, abstractmethod\nfrom functools import lru_cache, wraps\nfrom hashlib import md5\nfrom types import SimpleNamespace\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Iterator,\n    List,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nimport reflex.state\nfrom reflex.base import Base\nfrom reflex.compiler.templates import STATEFUL_COMPONENT\nfrom reflex.components.tags import Tag\nfrom reflex.constants import (\n    Dirs,\n    EventTriggers,\n    Hooks,\n    Imports,\n    MemoizationDisposition,\n    MemoizationMode,\n    PageNames,\n)\nfrom reflex.event import (\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_event_handler,\n    get_handler_args,\n)\nfrom reflex.style import Style, format_as_emotion\nfrom reflex.utils import console, format, imports, types\nfrom reflex.utils.imports import ImportDict, ImportVar, ParsedImportDict, parse_imports\nfrom reflex.utils.serializers import serializer\nfrom reflex.vars import BaseVar, Var, VarData\n\n\nclass BaseComponent(Base, ABC):\n    \"\"\"The base class for all Reflex components.\n\n    This is something that can be rendered as a Component via the Reflex compiler.\n    \"\"\"\n\n    # The children nested within the component.\n    children: List[BaseComponent] = []\n\n    # The library that the component is based on.\n    library: Optional[str] = None\n\n    # List here the non-react dependency needed by `library`\n    lib_dependencies: List[str] = []\n\n    # List here the dependencies that need to be transpiled by Next.js\n    transpile_packages: List[str] = []\n\n    # The tag to use when rendering the component.\n    tag: Optional[str] = None\n\n    @abstractmethod\n    def render(self) -> dict:\n        \"\"\"Render the component.\n\n        Returns:\n            The dictionary for template of the component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks_internal(self) -> dict[str, None]:\n        \"\"\"Get the reflex internal hooks for the component and its children.\n\n        Returns:\n            The code that should appear just before user-defined hooks.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_hooks(self) -> dict[str, None]:\n        \"\"\"Get the React hooks for this component.\n\n        Returns:\n            The code that should appear just before returning the rendered component.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_imports(self) -> ParsedImportDict:\n        \"\"\"Get all the libraries and fields that are used by the component.\n\n        Returns:\n            The import dict with the required imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_dynamic_imports(self) -> set[str]:\n        \"\"\"Get dynamic imports for the component.\n\n        Returns:\n            The dynamic imports.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_custom_code(self) -> set[str]:\n        \"\"\"Get custom code for the component.\n\n        Returns:\n            The custom code.\n        \"\"\"\n\n    @abstractmethod\n    def _get_all_refs(self) -> set[str]:\n        \"\"\"Get the refs for the children of the component.\n\n        Returns:\n            The refs for the children.\n        \"\"\"\n\n\nclass ComponentNamespace(SimpleNamespace):\n    \"\"\"A namespace to manage components with subcomponents.\"\"\"\n\n    def __hash__(self) -> int:\n        \"\"\"Get the hash of the namespace.\n\n\n        Returns:\n            The hash of the namespace.\n        \"\"\"\n        return hash(self.__class__.__name__)\n\n\ndef evaluate_style_namespaces(style: ComponentStyle) -> dict:\n    \"\"\"Evaluate namespaces in the style.\n\n    Args:\n        style: The style to evaluate.\n\n    Returns:\n        The evaluated style.\n    \"\"\"\n    return {\n        k.__call__ if isinstance(k, ComponentNamespace) else k: v\n        for k, v in style.items()\n    }\n\n\n# Map from component to styling.\nComponentStyle = Dict[\n    Union[str, Type[BaseComponent], Callable, ComponentNamespace], Any\n]\nComponentChild = Union[types.PrimitiveType, Var, BaseComponent]\n\n\nclass Component(BaseComponent, ABC):\n    \"\"\"A component with style, event trigger and other props.\"\"\"\n\n    # The style of the component.\n    style: Style = Style()\n\n    # A mapping from event triggers to event chains.\n    event_triggers: Dict[str, Union[EventChain, Var]] = {}\n\n    # The alias for the tag.\n    alias: Optional[str] = None\n\n    # Whether the import is default or named.\n    is_default: Optional[bool] = False\n\n    # A unique key for the component.\n    key: Any = None\n\n    # The id for the component.\n    id: Any = None\n\n    # The class name for the component.\n    class_name: Any = None\n\n    # Special component props.\n    special_props: Set[Var] = set()\n\n    # Whether the component should take the focus once the page is loaded\n    autofocus: bool = False\n\n    # components that cannot be children\n    _invalid_children: List[str] = []\n\n    # only components that are allowed as children\n    _valid_children: List[str] = []\n\n    # only components that are allowed as parent\n    _valid_parents: List[str] = []\n\n    # props to change the name of\n    _rename_props: Dict[str, str] = {}\n\n    # custom attribute\n    custom_attrs: Dict[str, Union[Var, str]] = {}\n\n    # When to memoize this component and its children.\n    _memoization_mode: MemoizationMode = MemoizationMode()\n\n    # State class associated with this component instance\n    State: Optional[Type[reflex.state.State]] = None\n\n    def add_imports(self) -> ImportDict | list[ImportDict]:\n        \"\"\"Add imports for the component.\n\n        This method should be implemented by subclasses to add new imports for the component.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_imports in each parent class will be merged internally.\n\n        Returns:\n            The additional imports for this component subclass.\n\n        The format of the return value is a dictionary where the keys are the\n        library names (with optional npm-style version specifications) mapping\n        to a single name to be imported, or a list names to be imported.\n\n        For advanced use cases, the values can be ImportVar instances (for\n        example, to provide an alias or mark that an import is the default\n        export from the given library).\n\n        ```python\n        return {\n            \"react\": \"useEffect\",\n            \"react-draggable\": [\"DraggableCore\", rx.ImportVar(tag=\"Draggable\", is_default=True)],\n        }\n        ```\n        \"\"\"\n        return {}\n\n    def add_hooks(self) -> list[str | Var]:\n        \"\"\"Add hooks inside the component function.\n\n        Hooks are pieces of literal Javascript code that is inserted inside the\n        React component function.\n\n        Each logical hook should be a separate string in the list.\n\n        Common strings will be deduplicated and inserted into the component\n        function only once, so define const variables and other identical code\n        in their own strings to avoid defining the same const or hook multiple\n        times.\n\n        If a hook depends on specific data from the component instance, be sure\n        to use unique values inside the string to _avoid_ deduplication.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_hooks in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional hooks for this component subclass.\n\n        ```python\n        return [\n            \"const [count, setCount] = useState(0);\",\n            \"useEffect(() => { setCount((prev) => prev + 1); console.log(`mounted ${count} times`); }, []);\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    def add_custom_code(self) -> list[str]:\n        \"\"\"Add custom Javascript code into the page that contains this component.\n\n        Custom code is inserted at module level, after any imports.\n\n        Each string of custom code is deduplicated per-page, so take care to\n        avoid defining the same const or function differently from different\n        component instances.\n\n        Custom code is useful for defining global functions or constants which\n        can then be referenced inside hooks or used by component vars.\n\n        Implementations do NOT need to call super(). The result of calling\n        add_custom_code in each parent class will be merged and deduplicated internally.\n\n        Returns:\n            The additional custom code for this component subclass.\n\n        ```python\n        return [\n            \"const translatePoints = (event) => { return { x: event.clientX, y: event.clientY }; };\",\n        ]\n        ```\n        \"\"\"\n        return []\n\n    @classmethod\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Set default properties.\n\n        Args:\n            **kwargs: The kwargs to pass to the superclass.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Get all the props for the component.\n        props = cls.get_props()\n\n        # Convert fields to props, setting default values.\n        for field in cls.get_fields().values():\n            # If the field is not a component prop, skip it.\n            if field.name not in props:\n                continue\n\n            # Set default values for any props.\n            if types._issubclass(field.type_, Var):\n                field.required = False\n                field.default = Var.create(\n                    field.default, _var_is_string=isinstance(field.default, str)\n                )\n            elif types._issubclass(field.type_, EventHandler):\n                field.required = False\n\n        # Ensure renamed props from parent classes are applied to the subclass.\n        if cls._rename_props:\n            inherited_rename_props = {}\n            for parent in reversed(cls.mro()):\n                if issubclass(parent, Component) and parent._rename_props:\n                    inherited_rename_props.update(parent._rename_props)\n            cls._rename_props = inherited_rename_props\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the component.\n\n        Args:\n            *args: Args to initialize the component.\n            **kwargs: Kwargs to initialize the component.\n\n        Raises:\n            TypeError: If an invalid prop is passed.\n            ValueError: If an event trigger passed is not valid.\n        \"\"\"\n        # Set the id and children initially.\n        children = kwargs.get(\"children\", [])\n        initial_kwargs = {\n            \"id\": kwargs.get(\"id\"),\n            \"children\": children,\n            **{\n                prop: Var.create(\n                    kwargs[prop],\n                    _var_is_string=False if isinstance(kwargs[prop], str) else None,\n                )\n                for prop in self.get_initial_props()\n                if prop in kwargs\n            },\n        }\n        super().__init__(**initial_kwargs)\n\n        self._validate_component_children(children)\n\n        # Get the component fields, triggers, and props.\n        fields = self.get_fields()\n        component_specific_triggers = self.get_event_triggers()\n        props = self.get_props()\n\n        # Add any events triggers.\n        if \"event_triggers\" not in kwargs:\n            kwargs[\"event_triggers\"] = {}\n        kwargs[\"event_triggers\"] = kwargs[\"event_triggers\"].copy()\n\n        # Iterate through the kwargs and set the props.\n        for key, value in kwargs.items():\n            if (\n                key.startswith(\"on_\")\n                and key not in component_specific_triggers\n                and key not in props\n            ):\n                raise ValueError(\n                    f\"The {(comp_name := type(self).__name__)} does not take in an `{key}` event trigger. If {comp_name}\"\n                    f\" is a third party component make sure to add `{key}` to the component's event triggers. \"\n                    f\"visit https://reflex.dev/docs/wrapping-react/guide/#event-triggers for more info.\"\n                )\n            if key in component_specific_triggers:\n                # Event triggers are bound to event chains.\n                field_type = EventChain\n            elif key in props:\n                # Set the field type.\n                field_type = fields[key].type_\n\n            else:\n                continue\n\n            # Check whether the key is a component prop.\n            if types._issubclass(field_type, Var):\n                # Used to store the passed types if var type is a union.\n                passed_types = None\n                try:\n                    # Try to create a var from the value.\n                    kwargs[key] = Var.create(\n                        value,\n                        _var_is_string=False if isinstance(value, str) else None,\n                    )\n\n                    # Check that the var type is not None.\n                    if kwargs[key] is None:\n                        raise TypeError\n\n                    expected_type = fields[key].outer_type_.__args__[0]\n                    # validate literal fields.\n                    types.validate_literal(\n                        key, value, expected_type, type(self).__name__\n                    )\n                    # Get the passed type and the var type.\n                    passed_type = kwargs[key]._var_type\n                    expected_type = (\n                        type(expected_type.__args__[0])\n                        if types.is_literal(expected_type)\n                        else expected_type\n                    )\n                except TypeError:\n                    # If it is not a valid var, check the base types.\n                    passed_type = type(value)\n                    expected_type = fields[key].outer_type_\n                if types.is_union(passed_type):\n                    # We need to check all possible types in the union.\n                    passed_types = (\n                        arg\n                        for arg in passed_type.__args__  # type: ignore\n                        if arg is not type(None)\n                    )\n                if (\n                    # If the passed var is a union, check if all possible types are valid.\n                    passed_types\n                    and not all(\n                        types._issubclass(pt, expected_type) for pt in passed_types\n                    )\n                ) or (\n                    # Else just check if the passed var type is valid.\n                    not passed_types\n                    and not types._issubclass(passed_type, expected_type, value)\n                ):\n                    value_name = value._var_name if isinstance(value, Var) else value\n                    raise TypeError(\n                        f\"Invalid var passed for prop {type(self).__name__}.{key}, expected type {expected_type}, got value {value_name} of type {passed_types or passed_type}.\"\n                    )\n\n            # Check if the key is an event trigger.\n            if key in component_specific_triggers:\n                # Temporarily disable full control for event triggers.\n                kwargs[\"event_triggers\"][key] = self._create_event_chain(\n                    value=value,  # type: ignore\n                    args_spec=component_specific_triggers[key],\n                )\n\n        # Remove any keys that were added as events.\n        for key in kwargs[\"event_triggers\"]:\n            del kwargs[key]\n\n        # Add style props to the component.\n        style = kwargs.get(\"style\", {})\n        if isinstance(style, List):\n            # Merge styles, the later ones overriding keys in the earlier ones.\n            style = {k: v for style_dict in style for k, v in style_dict.items()}\n\n        kwargs[\"style\"] = Style(\n            {\n                **self.get_fields()[\"style\"].default,\n                **style,\n                **{attr: value for attr, value in kwargs.items() if attr not in fields},\n            }\n        )\n        if \"custom_attrs\" not in kwargs:\n            kwargs[\"custom_attrs\"] = {}\n\n        # Convert class_name to str if it's list\n        class_name = kwargs.get(\"class_name\", \"\")\n        if isinstance(class_name, (List, tuple)):\n            kwargs[\"class_name\"] = \" \".join(class_name)\n\n        # Construct the component.\n        super().__init__(*args, **kwargs)\n\n    def _create_event_chain(\n        self,\n        args_spec: Any,\n        value: Union[\n            Var, EventHandler, EventSpec, List[Union[EventHandler, EventSpec]], Callable\n        ],\n    ) -> Union[EventChain, Var]:\n        \"\"\"Create an event chain from a variety of input types.\n\n        Args:\n            args_spec: The args_spec of the event trigger being bound.\n            value: The value to create the event chain from.\n\n        Returns:\n            The event chain.\n\n        Raises:\n            ValueError: If the value is not a valid event chain.\n        \"\"\"\n        # If it's an event chain var, return it.\n        if isinstance(value, Var):\n            if value._var_type is not EventChain:\n                raise ValueError(f\"Invalid event chain: {value}\")\n            return value\n        elif isinstance(value, EventChain):\n            # Trust that the caller knows what they're doing passing an EventChain directly\n            return value\n\n        # If the input is a single event handler, wrap it in a list.\n        if isinstance(value, (EventHandler, EventSpec)):\n            value = [value]\n\n        # If the input is a list of event handlers, create an event chain.\n        if isinstance(value, List):\n            events: list[EventSpec] = []\n            for v in value:\n                if isinstance(v, (EventHandler, EventSpec)):\n                    # Call the event handler to get the event.\n                    try:\n                        event = call_event_handler(v, args_spec)\n                    except ValueError as err:\n                        raise ValueError(\n                            f\" {err} defined in the `{type(self).__name__}` component\"\n                        ) from err\n\n                    # Add the event to the chain.\n                    events.append(event)\n                elif isinstance(v, Callable):\n                    # Call the lambda to get the event chain.\n                    result = call_event_fn(v, args_spec)\n                    if isinstance(result, Var):\n                        raise ValueError(\n                            f\"Invalid event chain: {v}. Cannot use a Var-returning \"\n                            \"lambda inside an EventChain list.\"\n                        )\n                    events.extend(result)\n                else:\n                    raise ValueError(f\"Invalid event: {v}\")\n\n        # If the input is a callable, create an event chain.\n        elif isinstance(value, Callable):\n            result = call_event_fn(value, args_spec)\n            if isinstance(result, Var):\n                # Recursively call this function if the lambda returned an EventChain Var.\n                return self._create_event_chain(args_spec, result)\n            events = result\n\n        # Otherwise, raise an error.\n        else:\n            raise ValueError(f\"Invalid event chain: {value}\")\n\n        # Add args to the event specs if necessary.\n        events = [e.with_args(get_handler_args(e)) for e in events]\n\n        # Collect event_actions from each spec\n        event_actions = {}\n        for e in events:\n            event_actions.update(e.event_actions)\n\n        # Return the event chain.\n        if isinstance(args_spec, Var):\n            return EventChain(\n                events=events,\n                args_spec=None,\n                event_actions=event_actions,\n            )\n        else:\n            return EventChain(\n                events=events,\n                args_spec=args_spec,\n                event_actions=event_actions,\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def __repr__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        return format.json_dumps(self.render())\n\n    def __str__(self) -> str:\n        \"\"\"Represent the component in React.\n\n        Returns:\n            The code to render the component.\n        \"\"\"\n        from reflex.compiler.compiler import _compile_component\n\n        return _compile_component(self)\n\n    def _apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply the theme to this component.\n\n        Deprecated. Use add_style instead.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        pass\n\n    def apply_theme(self, theme: Optional[Component]):\n        \"\"\"Apply a theme to the component and its children.\n\n        Args:\n            theme: The theme to apply.\n        \"\"\"\n        self._apply_theme(theme)\n        for child in self.children:\n            if isinstance(child, Component):\n                child.apply_theme(theme)\n\n    def _exclude_props(self) -> list[str]:\n        \"\"\"Props to exclude when adding the component props to the Tag.\n\n        Returns:\n            A list of component props to exclude.\n        \"\"\"\n        return []\n\n    def _render(self, props: dict[str, Any] | None = None) -> Tag:\n        \"\"\"Define how to render the component in React.\n\n        Args:\n            props: The props to render (if None, then use get_props).\n\n        Returns:\n            The tag to render.\n        \"\"\"\n        # Create the base tag.\n        tag = Tag(\n            name=self.tag if not self.alias else self.alias,\n            special_props=self.special_props,\n        )\n\n        if props is None:\n            # Add component props to the tag.\n            props = {\n                attr[:-1] if attr.endswith(\"_\") else attr: getattr(self, attr)\n                for attr in self.get_props()\n            }\n\n            # Add ref to element if `id` is not None.\n            ref = self.get_ref()\n            if ref is not None:\n                props[\"ref\"] = Var.create(\n                    ref, _var_is_local=False, _var_is_string=False\n                )\n        else:\n            props = props.copy()\n\n        props.update(\n            **{\n                trigger: handler\n                for trigger, handler in self.event_triggers.items()\n                if trigger not in {EventTriggers.ON_MOUNT, EventTriggers.ON_UNMOUNT}\n            },\n            key=self.key,\n            id=self.id,\n            class_name=self.class_name,\n        )\n        props.update(self._get_style())\n        props.update(self.custom_attrs)\n\n        # remove excluded props from prop dict before adding to tag.\n        for prop_to_exclude in self._exclude_props():\n            props.pop(prop_to_exclude, None)\n\n        return tag.add_props(**props)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n\n\n\n\n\n\n\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_initial_props(cls) -> Set[str]:\n        \"\"\"Get the initial props to set for the component.\n\n        Returns:\n            The initial props to set.\n        \"\"\"\n        return set()\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def get_component_props(cls) -> set[str]:\n        \"\"\"Get the props that expected a component as value.\n\n        Returns:\n            The components props.\n        \"\"\"\n        return {\n            name\n            for name, field in cls.get_fields().items()\n            if name in cls.get_props()\n            and types._issubclass(field.outer_type_, Component)\n        }\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def add_style(self) -> dict[str, Any] | None:\n        \"\"\"Add style to the component.\n\n        Downstream components can override this method to return a style dict\n        that will be applied to the component.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        return None\n\n    def _add_style(self) -> Style:\n        \"\"\"Call add_style for all bases in the MRO.\n\n        Downstream components should NOT override. Use add_style instead.\n\n        Returns:\n            The style to add.\n        \"\"\"\n        styles = []\n\n        # Walk the MRO to call all `add_style` methods.\n        for base in self._iter_parent_classes_with_method(\"add_style\"):\n            s = base.add_style(self)  # type: ignore\n            if s is not None:\n                styles.append(s)\n\n        _style = Style()\n        for s in reversed(styles):\n            _style.update(s)\n        return _style\n\n    def _get_component_style(self, styles: ComponentStyle) -> Style | None:\n        \"\"\"Get the style to the component from `App.style`.\n\n        Args:\n            styles: The style to apply.\n\n        Returns:\n            The style of the component.\n        \"\"\"\n        component_style = None\n        if type(self) in styles:\n            component_style = Style(styles[type(self)])\n        if self.create in styles:\n            component_style = Style(styles[self.create])\n        return component_style\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _get_style(self) -> dict:\n        \"\"\"Get the style for the component.\n\n        Returns:\n            The dictionary of the component style as value and the style notation as key.\n        \"\"\"\n        if isinstance(self.style, Var):\n            return {\"css\": self.style}\n        return {\"css\": Var.create(format_as_emotion(self.style))}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _replace_prop_names(self, rendered_dict) -> None:\n       \n```\n# Function Signature: def _get_all_custom_components(\n# Natural Language Annotations: \"\"\"Get all the custom components used by the component.\n\n        Args:\n            seen: The tags of the components that have already been seen.\n\n        Returns:\n            The set of custom components.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _get_all_custom_components(self, seen", "  ```\ndef _get_all_custom_components(\n    seen", "  ```\n\n\n\nimport types\n\nfrom reflex.constants"]}, "reflex-reflex/components/core/upload.py-create": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/core/upload.py:\n```\n\"\"\"A file upload component.\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Callable, ClassVar, Dict, List, Optional\n\nfrom reflex.components.component import Component, ComponentNamespace, MemoizationLeaf\nfrom reflex.components.el.elements.forms import Input\nfrom reflex.components.radix.themes.layout.box import Box\nfrom reflex.constants import Dirs\nfrom reflex.event import (\n    CallableEventSpec,\n    EventChain,\n    EventHandler,\n    EventSpec,\n    call_event_fn,\n    call_script,\n    parse_args_spec,\n)\nfrom reflex.utils.imports import ImportVar\nfrom reflex.vars import BaseVar, CallableVar, Var, VarData\n\nDEFAULT_UPLOAD_ID: str = \"default\"\n\nupload_files_context_var_data: VarData = VarData(\n    imports={\n        \"react\": \"useContext\",\n        f\"/{Dirs.CONTEXTS_PATH}\": \"UploadFilesContext\",\n    },\n    hooks={\n        \"const [filesById, setFilesById] = useContext(UploadFilesContext);\": None,\n    },\n)\n\n\n@CallableVar\ndef upload_file(id_: str = DEFAULT_UPLOAD_ID) -> BaseVar:\n    \"\"\"Get the file upload drop trigger.\n\n    This var is passed to the dropzone component to update the file list when a\n    drop occurs.\n\n    Args:\n        id_: The id of the upload to get the drop trigger for.\n\n    Returns:\n        A var referencing the file upload drop trigger.\n    \"\"\"\n    id_var = Var.create_safe(id_, _var_is_string=True)\n    var_name = f\"\"\"e => setFilesById(filesById => {{\n    const updatedFilesById = Object.assign({{}}, filesById);\n    updatedFilesById[{id_var._var_name_unwrapped}] = e;\n    return updatedFilesById;\n  }})\n    \"\"\"\n\n    return BaseVar(\n        _var_name=var_name,\n        _var_type=EventChain,\n        _var_data=VarData.merge(upload_files_context_var_data, id_var._var_data),\n    )\n\n\n@CallableVar\ndef selected_files(id_: str = DEFAULT_UPLOAD_ID) -> BaseVar:\n    \"\"\"Get the list of selected files.\n\n    Args:\n        id_: The id of the upload to get the selected files for.\n\n    Returns:\n        A var referencing the list of selected file paths.\n    \"\"\"\n    id_var = Var.create_safe(id_, _var_is_string=True)\n    return BaseVar(\n        _var_name=f\"(filesById[{id_var._var_name_unwrapped}] ? filesById[{id_var._var_name_unwrapped}].map((f) => (f.path || f.name)) : [])\",\n        _var_type=List[str],\n        _var_data=VarData.merge(upload_files_context_var_data, id_var._var_data),\n    )\n\n\n@CallableEventSpec\ndef clear_selected_files(id_: str = DEFAULT_UPLOAD_ID) -> EventSpec:\n    \"\"\"Clear the list of selected files.\n\n    Args:\n        id_: The id of the upload to clear.\n\n    Returns:\n        An event spec that clears the list of selected files when triggered.\n    \"\"\"\n    # UploadFilesProvider assigns a special function to clear selected files\n    # into the shared global refs object to make it accessible outside a React\n    # component via `call_script` (otherwise backend could never clear files).\n    return call_script(f\"refs['__clear_selected_files']({id_!r})\")\n\n\ndef cancel_upload(upload_id: str) -> EventSpec:\n    \"\"\"Cancel an upload.\n\n    Args:\n        upload_id: The id of the upload to cancel.\n\n    Returns:\n        An event spec that cancels the upload when triggered.\n    \"\"\"\n    return call_script(\n        f\"upload_controllers[{Var.create_safe(upload_id, _var_is_string=True)._var_name_unwrapped}]?.abort()\"\n    )\n\n\ndef get_upload_dir() -> Path:\n    \"\"\"Get the directory where uploaded files are stored.\n\n    Returns:\n        The directory where uploaded files are stored.\n    \"\"\"\n    Upload.is_used = True\n\n    uploaded_files_dir = Path(\n        os.environ.get(\"REFLEX_UPLOADED_FILES_DIR\", \"./uploaded_files\")\n    )\n    uploaded_files_dir.mkdir(parents=True, exist_ok=True)\n    return uploaded_files_dir\n\n\nuploaded_files_url_prefix: Var = Var.create_safe(\n    \"${getBackendURL(env.UPLOAD)}\",\n    _var_is_string=False,\n    _var_data=VarData(\n        imports={\n            f\"/{Dirs.STATE_PATH}\": \"getBackendURL\",\n            \"/env.json\": ImportVar(tag=\"env\", is_default=True),\n        }\n    ),\n)\n\n\ndef get_upload_url(file_path: str) -> Var[str]:\n    \"\"\"Get the URL of an uploaded file.\n\n    Args:\n        file_path: The path of the uploaded file.\n\n    Returns:\n        The URL of the uploaded file to be rendered from the frontend (as a str-encoded Var).\n    \"\"\"\n    Upload.is_used = True\n\n    return Var.create_safe(\n        f\"{uploaded_files_url_prefix}/{file_path}\", _var_is_string=True\n    )\n\n\ndef _on_drop_spec(files: Var):\n    \"\"\"Args spec for the on_drop event trigger.\n\n    Args:\n        files: The files to upload.\n\n    Returns:\n        Signature for on_drop handler including the files to upload.\n    \"\"\"\n    return [files]\n\n\nclass UploadFilesProvider(Component):\n    \"\"\"AppWrap component that provides a dict of selected files by ID via useContext.\"\"\"\n\n    library = f\"/{Dirs.CONTEXTS_PATH}\"\n    tag = \"UploadFilesProvider\"\n\n\nclass Upload(MemoizationLeaf):\n    \"\"\"A file upload component.\"\"\"\n\n    library = \"react-dropzone@14.2.3\"\n\n    tag = \"ReactDropzone\"\n\n    is_default = True\n\n    # The list of accepted file types. This should be a dictionary of MIME types as keys and array of file formats as\n    # values.\n    # supported MIME types: https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types\n    accept: Var[Optional[Dict[str, List]]]\n\n    # Whether the dropzone is disabled.\n    disabled: Var[bool]\n\n    # The maximum number of files that can be uploaded.\n    max_files: Var[int]\n\n    # The maximum file size (bytes) that can be uploaded.\n    max_size: Var[int]\n\n    # The minimum file size (bytes) that can be uploaded.\n    min_size: Var[int]\n\n    # Whether to allow multiple files to be uploaded.\n    multiple: Var[bool] = True  # type: ignore\n\n    # Whether to disable click to upload.\n    no_click: Var[bool]\n\n    # Whether to disable drag and drop.\n    no_drag: Var[bool]\n\n    # Whether to disable using the space/enter keys to upload.\n    no_keyboard: Var[bool]\n\n    # Marked True when any Upload component is created.\n    is_used: ClassVar[bool] = False\n\n    # Fired when files are dropped.\n    on_drop: EventHandler[_on_drop_spec]\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def _update_arg_tuple_for_on_drop(cls, arg_value: tuple[Var, Var]):\n        \"\"\"Helper to update caller-provided EventSpec args for direct use with on_drop.\n\n        Args:\n            arg_value: The arg tuple to update (if necessary).\n\n        Returns:\n            The updated arg_value tuple when arg is \"files\", otherwise the original arg_value.\n        \"\"\"\n        if arg_value[0]._var_name == \"files\":\n            placeholder = parse_args_spec(_on_drop_spec)[0]\n            return (arg_value[0], placeholder)\n        return arg_value\n\n    def _render(self):\n        out = super()._render()\n        out.args = (\"getRootProps\", \"getInputProps\")\n        return out\n\n    @staticmethod\n    def _get_app_wrap_components() -> dict[tuple[int, str], Component]:\n        return {\n            (5, \"UploadFilesProvider\"): UploadFilesProvider.create(),\n        }\n\n\nclass StyledUpload(Upload):\n    \"\"\"The styled Upload Component.\"\"\"\n\n    @classmethod\n    def create(cls, *children, **props) -> Component:\n        \"\"\"Create the styled upload component.\n\n        Args:\n            *children: The children of the component.\n            **props: The properties of the component.\n\n        Returns:\n            The styled upload component.\n        \"\"\"\n        # Set default props.\n        props.setdefault(\"border\", \"1px dashed var(--accent-12)\")\n        props.setdefault(\"padding\", \"5em\")\n        props.setdefault(\"textAlign\", \"center\")\n\n        # Mark the Upload component as used in the app.\n        Upload.is_used = True\n\n        return super().create(\n            *children,\n            **props,\n        )\n\n\nclass UploadNamespace(ComponentNamespace):\n    \"\"\"Upload component namespace.\"\"\"\n\n    root = Upload.create\n    __call__ = StyledUpload.create\n\n\nupload = UploadNamespace()\n\n```\n# Function Signature: def create(cls, *children, **props) -> Component:\n# Natural Language Annotations: \"\"\"Create an upload component.\n\n        Args:\n            *children: The children of the component.\n            **props: The properties of the component.\n\n        Returns:\n            The upload component.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef create(cls, *children, **props) -> Component", "  ```\ndef create(cls, *children, **props):\n   ", "  ```\ndef create(cls, *children, **props) -> Component"]}, "reflex-reflex/components/next/image.py-create": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/next/image.py:\n```\n\"\"\"Image component from next/image.\"\"\"\n\nfrom typing import Any, Literal, Optional, Union\n\nfrom reflex.event import EventHandler\nfrom reflex.utils import types\nfrom reflex.vars import Var\n\nfrom .base import NextComponent\n\n\nclass Image(NextComponent):\n    \"\"\"Display an image.\"\"\"\n\n    tag = \"Image\"\n    library = \"next/image\"\n    is_default = True\n\n    # This can be either an absolute external URL, or an internal path\n    src: Var[Any]\n\n    # Represents the rendered width in pixels, so it will affect how large the image appears.\n    width: Var[Any]\n\n    # Represents the rendered height in pixels, so it will affect how large the image appears.\n    height: Var[Any]\n\n    # Used to describe the image for screen readers and search engines.\n    alt: Var[str]\n\n    # A custom function used to resolve image URLs.\n    loader: Var[Any]\n\n    # A boolean that causes the image to fill the parent element, which is useful when the width and height are unknown. Default to True\n    fill: Var[bool]\n\n    # A string, similar to a media query, that provides information about how wide the image will be at different breakpoints.\n    sizes: Var[str]\n\n    # The quality of the optimized image, an integer between 1 and 100, where 100 is the best quality and therefore largest file size. Defaults to 75.\n    quality: Var[int]\n\n    # When true, the image will be considered high priority and preload. Lazy loading is automatically disabled for images using priority.\n    priority: Var[bool]\n\n    # A placeholder to use while the image is loading. Possible values are blur, empty, or data:image/.... Defaults to empty.\n    placeholder: Var[str]\n\n    # Allows passing CSS styles to the underlying image element.\n    # style: Var[Any]\n\n    # The loading behavior of the image. Defaults to lazy. Can hurt performance, recommended to use `priority` instead.\n    loading: Var[Literal[\"lazy\", \"eager\"]]\n\n    # A Data URL to be used as a placeholder image before the src image successfully loads. Only takes effect when combined with placeholder=\"blur\".\n    blurDataURL: Var[str]\n\n    # Fires when the image has loaded.\n    on_load: EventHandler[lambda: []]\n\n    # Fires when the image has an error.\n    on_error: EventHandler[lambda: []]\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def create(\n# Natural Language Annotations: \"\"\"Create an Image component from next/image.\n\n        Args:\n            *children: The children of the component.\n            width: The width of the image.\n            height: The height of the image.\n            **props:The props of the component.\n\n        Returns:\n            _type_: _description_\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef create(children, width, height, **props):\n", "  ```\ndef create_image(\n    *children: Any,\n", "  ```\nimport NextComponent from \"reflex/components/next/image"]}, "reflex-reflex/components/lucide/icon.py-create": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/lucide/icon.py:\n```\n\"\"\"Lucide Icon component.\"\"\"\n\nfrom reflex.components.component import Component\nfrom reflex.utils import console, format\nfrom reflex.vars import Var\n\n\nclass LucideIconComponent(Component):\n    \"\"\"Lucide Icon Component.\"\"\"\n\n    library = \"lucide-react@0.359.0\"\n\n\nclass Icon(LucideIconComponent):\n    \"\"\"An Icon component.\"\"\"\n\n    tag = \"None\"\n\n    # The size of the icon in pixels.\n    size: Var[int]\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRENAMED_ICONS_05 = {\n    \"activity_square\": \"square_activity\",\n    \"alert_circle\": \"circle_alert\",\n    \"alert_octagon\": \"octagon_alert\",\n    \"alert_triangle\": \"triangle_alert\",\n    \"arrow_down_circle\": \"circle_arrow_down\",\n    \"arrow_down_left_from_circle\": \"circle_arrow_out_down_left\",\n    \"arrow_down_left_from_square\": \"square_arrow_out_down_left\",\n    \"arrow_down_left_square\": \"square_arrow_down_left\",\n    \"arrow_down_right_from_circle\": \"circle_arrow_out_down_right\",\n    \"arrow_down_right_from_square\": \"square_arrow_out_down_right\",\n    \"arrow_down_right_square\": \"square_arrow_down_right\",\n    \"arrow_down_square\": \"square_arrow_down\",\n    \"arrow_left_circle\": \"circle_arrow_left\",\n    \"arrow_left_square\": \"square_arrow_left\",\n    \"arrow_right_circle\": \"circle_arrow_right\",\n    \"arrow_right_square\": \"square_arrow_right\",\n    \"arrow_up_circle\": \"circle_arrow_up\",\n    \"arrow_up_left_from_circle\": \"circle_arrow_out_up_left\",\n    \"arrow_up_left_from_square\": \"square_arrow_out_up_left\",\n    \"arrow_up_left_square\": \"square_arrow_up_left\",\n    \"arrow_up_right_from_circle\": \"circle_arrow_out_up_right\",\n    \"arrow_up_right_from_square\": \"square_arrow_out_up_right\",\n    \"arrow_up_right_square\": \"square_arrow_up_right\",\n    \"arrow_up_square\": \"square_arrow_up\",\n    \"asterisk_square\": \"square_asterisk\",\n    \"check_circle\": \"circle_check_big\",\n    \"check_circle_2\": \"circle_check\",\n    \"check_square\": \"square_check_big\",\n    \"check_square_2\": \"square_check\",\n    \"chevron_down_circle\": \"circle_chevron_down\",\n    \"chevron_down_square\": \"square_chevron_down\",\n    \"chevron_left_circle\": \"circle_chevron_left\",\n    \"chevron_left_square\": \"square_chevron_left\",\n    \"chevron_right_circle\": \"circle_chevron_right\",\n    \"chevron_right_square\": \"square_chevron_right\",\n    \"chevron_up_circle\": \"circle_chevron_up\",\n    \"chevron_up_square\": \"square_chevron_up\",\n    \"code_2\": \"code_xml\",\n    \"code_square\": \"square_code\",\n    \"contact_2\": \"contact_round\",\n    \"divide_circle\": \"circle_divide\",\n    \"divide_square\": \"square_divide\",\n    \"dot_square\": \"square_dot\",\n    \"download_cloud\": \"cloud_download\",\n    \"equal_square\": \"square_equal\",\n    \"form_input\": \"rectangle_ellipsis\",\n    \"function_square\": \"square_function\",\n    \"gantt_chart_square\": \"square_gantt_chart\",\n    \"gauge_circle\": \"circle_gauge\",\n    \"globe_2\": \"earth\",\n    \"help_circle\": \"circle_help\",\n    \"helping_hand\": \"hand_helping\",\n    \"ice_cream\": \"ice_cream_cone\",\n    \"ice_cream_2\": \"ice_cream_bowl\",\n    \"indent\": \"indent_increase\",\n    \"kanban_square\": \"square_kanban\",\n    \"kanban_square_dashed\": \"square_dashed_kanban\",\n    \"laptop_2\": \"laptop_minimal\",\n    \"library_square\": \"square_library\",\n    \"loader_2\": \"loader_circle\",\n    \"m_square\": \"square_m\",\n    \"menu_square\": \"square_menu\",\n    \"mic_2\": \"mic_vocal\",\n    \"minus_circle\": \"circle_minus\",\n    \"minus_square\": \"square_minus\",\n    \"more_horizontal\": \"ellipsis\",\n    \"more_vertical\": \"ellipsis_vertical\",\n    \"mouse_pointer_square\": \"square_mouse_pointer\",\n    \"mouse_pointer_square_dashed\": \"square_dashed_mouse_pointer\",\n    \"outdent\": \"indent_decrease\",\n    \"palm_tree\": \"tree_palm\",\n    \"parking_circle\": \"circle_parking\",\n    \"parking_circle_off\": \"circle_parking_off\",\n    \"parking_square\": \"square_parking\",\n    \"parking_square_off\": \"square_parking_off\",\n    \"pause_circle\": \"circle_pause\",\n    \"pause_octagon\": \"octagon_pause\",\n    \"percent_circle\": \"circle_percent\",\n    \"percent_diamond\": \"diamond_percent\",\n    \"percent_square\": \"square_percent\",\n    \"pi_square\": \"square_pi\",\n    \"pilcrow_square\": \"square_pilcrow\",\n    \"play_circle\": \"circle_play\",\n    \"play_square\": \"square_play\",\n    \"plus_circle\": \"circle_plus\",\n    \"plus_square\": \"square_plus\",\n    \"power_circle\": \"circle_power\",\n    \"power_square\": \"square_power\",\n    \"school_2\": \"university\",\n    \"scissors_square\": \"square_scissors\",\n    \"scissors_square_dashed_bottom\": \"square_bottom_dashed_scissors\",\n    \"sigma_square\": \"square_sigma\",\n    \"slash_circle\": \"circle_slash\",\n    \"sliders\": \"sliders_vertical\",\n    \"split_square_horizontal\": \"square_split_horizontal\",\n    \"split_square_vertical\": \"square_split_vertical\",\n    \"stop_circle\": \"circle_stop\",\n    \"subtitles\": \"captions\",\n    \"test_tube_2\": \"test_tube_diagonal\",\n    \"unlock\": \"lock_open\",\n    \"unlock_keyhole\": \"lock_keyhole_open\",\n    \"upload_cloud\": \"cloud_upload\",\n    \"wallet_2\": \"wallet_minimal\",\n    \"wand_2\": \"wand_sparkles\",\n    \"x_circle\": \"circle_x\",\n    \"x_octagon\": \"octagon_x\",\n    \"x_square\": \"square_x\",\n}\n\nLUCIDE_ICON_LIST = [\n    \"a_arrow_down\",\n    \"a_arrow_up\",\n    \"a_large_small\",\n    \"accessibility\",\n    \"activity\",\n    \"air_vent\",\n    \"airplay\",\n    \"alarm_clock\",\n    \"alarm_clock_check\",\n    \"alarm_clock_minus\",\n    \"alarm_clock_off\",\n    \"alarm_clock_plus\",\n    \"alarm_smoke\",\n    \"album\",\n    \"align_center\",\n    \"align_center_horizontal\",\n    \"align_center_vertical\",\n    \"align_end_horizontal\",\n    \"align_end_vertical\",\n    \"align_horizontal_distribute_center\",\n    \"align_horizontal_distribute_end\",\n    \"align_horizontal_distribute_start\",\n    \"align_horizontal_justify_center\",\n    \"align_horizontal_justify_end\",\n    \"align_horizontal_justify_start\",\n    \"align_horizontal_space_around\",\n    \"align_horizontal_space_between\",\n    \"align_justify\",\n    \"align_left\",\n    \"align_right\",\n    \"align_start_horizontal\",\n    \"align_start_vertical\",\n    \"align_vertical_distribute_center\",\n    \"align_vertical_distribute_end\",\n    \"align_vertical_distribute_start\",\n    \"align_vertical_justify_center\",\n    \"align_vertical_justify_end\",\n    \"align_vertical_justify_start\",\n    \"align_vertical_space_around\",\n    \"align_vertical_space_between\",\n    \"ambulance\",\n    \"ampersand\",\n    \"ampersands\",\n    \"anchor\",\n    \"angry\",\n    \"annoyed\",\n    \"antenna\",\n    \"anvil\",\n    \"aperture\",\n    \"app_window\",\n    \"app_window_mac\",\n    \"apple\",\n    \"archive\",\n    \"archive_restore\",\n    \"archive_x\",\n    \"area_chart\",\n    \"armchair\",\n    \"arrow_big_down\",\n    \"arrow_big_down_dash\",\n    \"arrow_big_left\",\n    \"arrow_big_left_dash\",\n    \"arrow_big_right\",\n    \"arrow_big_right_dash\",\n    \"arrow_big_up\",\n    \"arrow_big_up_dash\",\n    \"arrow_down\",\n    \"arrow_down_0_1\",\n    \"arrow_down_1_0\",\n    \"arrow_down_a_z\",\n    \"arrow_down_from_line\",\n    \"arrow_down_left\",\n    \"arrow_down_narrow_wide\",\n    \"arrow_down_right\",\n    \"arrow_down_to_dot\",\n    \"arrow_down_to_line\",\n    \"arrow_down_up\",\n    \"arrow_down_wide_narrow\",\n    \"arrow_down_z_a\",\n    \"arrow_left\",\n    \"arrow_left_from_line\",\n    \"arrow_left_right\",\n    \"arrow_left_to_line\",\n    \"arrow_right\",\n    \"arrow_right_from_line\",\n    \"arrow_right_left\",\n    \"arrow_right_to_line\",\n    \"arrow_up\",\n    \"arrow_up_0_1\",\n    \"arrow_up_1_0\",\n    \"arrow_up_a_z\",\n    \"arrow_up_down\",\n    \"arrow_up_from_dot\",\n    \"arrow_up_from_line\",\n    \"arrow_up_left\",\n    \"arrow_up_narrow_wide\",\n    \"arrow_up_right\",\n    \"arrow_up_to_line\",\n    \"arrow_up_wide_narrow\",\n    \"arrow_up_z_a\",\n    \"arrows_up_from_line\",\n    \"asterisk\",\n    \"at_sign\",\n    \"atom\",\n    \"audio_lines\",\n    \"audio_waveform\",\n    \"award\",\n    \"axe\",\n    \"axis_3d\",\n    \"baby\",\n    \"backpack\",\n    \"badge\",\n    \"badge_alert\",\n    \"badge_cent\",\n    \"badge_check\",\n    \"badge_dollar_sign\",\n    \"badge_euro\",\n    \"badge_help\",\n    \"badge_indian_rupee\",\n    \"badge_info\",\n    \"badge_japanese_yen\",\n    \"badge_minus\",\n    \"badge_percent\",\n    \"badge_plus\",\n    \"badge_pound_sterling\",\n    \"badge_russian_ruble\",\n    \"badge_swiss_franc\",\n    \"badge_x\",\n    \"baggage_claim\",\n    \"ban\",\n    \"banana\",\n    \"banknote\",\n    \"bar_chart\",\n    \"bar_chart_2\",\n    \"bar_chart_3\",\n    \"bar_chart_4\",\n    \"bar_chart_big\",\n    \"bar_chart_horizontal\",\n    \"bar_chart_horizontal_big\",\n    \"barcode\",\n    \"baseline\",\n    \"bath\",\n    \"battery\",\n    \"battery_charging\",\n    \"battery_full\",\n    \"battery_low\",\n    \"battery_medium\",\n    \"battery_warning\",\n    \"beaker\",\n    \"bean\",\n    \"bean_off\",\n    \"bed\",\n    \"bed_double\",\n    \"bed_single\",\n    \"beef\",\n    \"beer\",\n    \"beer_off\",\n    \"bell\",\n    \"bell_dot\",\n    \"bell_electric\",\n    \"bell_minus\",\n    \"bell_off\",\n    \"bell_plus\",\n    \"bell_ring\",\n    \"between_horizontal_end\",\n    \"between_horizontal_start\",\n    \"between_vertical_end\",\n    \"between_vertical_start\",\n    \"bike\",\n    \"binary\",\n    \"biohazard\",\n    \"bird\",\n    \"bitcoin\",\n    \"blend\",\n    \"blinds\",\n    \"blocks\",\n    \"bluetooth\",\n    \"bluetooth_connected\",\n    \"bluetooth_off\",\n    \"bluetooth_searching\",\n    \"bold\",\n    \"bolt\",\n    \"bomb\",\n    \"bone\",\n    \"book\",\n    \"book_a\",\n    \"book_audio\",\n    \"book_check\",\n    \"book_copy\",\n    \"book_dashed\",\n    \"book_down\",\n    \"book_headphones\",\n    \"book_heart\",\n    \"book_image\",\n    \"book_key\",\n    \"book_lock\",\n    \"book_marked\",\n    \"book_minus\",\n    \"book_open\",\n    \"book_open_check\",\n    \"book_open_text\",\n    \"book_plus\",\n    \"book_text\",\n    \"book_type\",\n    \"book_up\",\n    \"book_up_2\",\n    \"book_user\",\n    \"book_x\",\n    \"bookmark\",\n    \"bookmark_check\",\n    \"bookmark_minus\",\n    \"bookmark_plus\",\n    \"bookmark_x\",\n    \"boom_box\",\n    \"bot\",\n    \"bot_message_square\",\n    \"box\",\n    \"box_select\",\n    \"boxes\",\n    \"braces\",\n    \"brackets\",\n    \"brain\",\n    \"brain_circuit\",\n    \"brain_cog\",\n    \"brick_wall\",\n    \"briefcase\",\n    \"briefcase_business\",\n    \"briefcase_medical\",\n    \"bring_to_front\",\n    \"brush\",\n    \"bug\",\n    \"bug_off\",\n    \"bug_play\",\n    \"building\",\n    \"building_2\",\n    \"bus\",\n    \"bus_front\",\n    \"cable\",\n    \"cable_car\",\n    \"cake\",\n    \"cake_slice\",\n    \"calculator\",\n    \"calendar\",\n    \"calendar_check\",\n    \"calendar_check_2\",\n    \"calendar_clock\",\n    \"calendar_days\",\n    \"calendar_fold\",\n    \"calendar_heart\",\n    \"calendar_minus\",\n    \"calendar_minus_2\",\n    \"calendar_off\",\n    \"calendar_plus\",\n    \"calendar_plus_2\",\n    \"calendar_range\",\n    \"calendar_search\",\n    \"calendar_x\",\n    \"calendar_x_2\",\n    \"camera\",\n    \"camera_off\",\n    \"candlestick_chart\",\n    \"candy\",\n    \"candy_cane\",\n    \"candy_off\",\n    \"cannabis\",\n    \"captions\",\n    \"captions_off\",\n    \"car\",\n    \"car_front\",\n    \"car_taxi_front\",\n    \"caravan\",\n    \"carrot\",\n    \"case_lower\",\n    \"case_sensitive\",\n    \"case_upper\",\n    \"cassette_tape\",\n    \"cast\",\n    \"castle\",\n    \"cat\",\n    \"cctv\",\n    \"check\",\n    \"check_check\",\n    \"chef_hat\",\n    \"cherry\",\n    \"chevron_down\",\n    \"chevron_first\",\n    \"chevron_last\",\n    \"chevron_left\",\n    \"chevron_right\",\n    \"chevron_up\",\n    \"chevrons_down\",\n    \"chevrons_down_up\",\n    \"chevrons_left\",\n    \"chevrons_left_right\",\n    \"chevrons_right\",\n    \"chevrons_right_left\",\n    \"chevrons_up\",\n    \"chevrons_up_down\",\n    \"chrome\",\n    \"church\",\n    \"cigarette\",\n    \"cigarette_off\",\n    \"circle\",\n    \"circle_alert\",\n    \"circle_arrow_down\",\n    \"circle_arrow_left\",\n    \"circle_arrow_out_down_left\",\n    \"circle_arrow_out_down_right\",\n    \"circle_arrow_out_up_left\",\n    \"circle_arrow_out_up_right\",\n    \"circle_arrow_right\",\n    \"circle_arrow_up\",\n    \"circle_check_big\",\n    \"circle_check\",\n    \"circle_chevron_down\",\n    \"circle_chevron_left\",\n    \"circle_chevron_right\",\n    \"circle_chevron_up\",\n    \"circle_dashed\",\n    \"circle_divide\",\n    \"circle_dollar_sign\",\n    \"circle_dot\",\n    \"circle_dot_dashed\",\n    \"circle_ellipsis\",\n    \"circle_equal\",\n    \"circle_fading_plus\",\n    \"circle_gauge\",\n    \"circle_help\",\n    \"circle_minus\",\n    \"circle_off\",\n    \"circle_parking_off\",\n    \"circle_parking\",\n    \"circle_pause\",\n    \"circle_percent\",\n    \"circle_play\",\n    \"circle_plus\",\n    \"circle_power\",\n    \"circle_slash\",\n    \"circle_slash_2\",\n    \"circle_stop\",\n    \"circle_user\",\n    \"circle_user_round\",\n    \"circle_x\",\n    \"circuit_board\",\n    \"citrus\",\n    \"clapperboard\",\n    \"clipboard\",\n    \"clipboard_check\",\n    \"clipboard_copy\",\n    \"clipboard_list\",\n    \"clipboard_minus\",\n    \"clipboard_paste\",\n    \"clipboard_pen\",\n    \"clipboard_pen_line\",\n    \"clipboard_plus\",\n    \"clipboard_type\",\n    \"clipboard_x\",\n    \"clock\",\n    \"clock_1\",\n    \"clock_10\",\n    \"clock_11\",\n    \"clock_12\",\n    \"clock_2\",\n    \"clock_3\",\n    \"clock_4\",\n    \"clock_5\",\n    \"clock_6\",\n    \"clock_7\",\n    \"clock_8\",\n    \"clock_9\",\n    \"cloud\",\n    \"cloud_cog\",\n    \"cloud_download\",\n    \"cloud_drizzle\",\n    \"cloud_fog\",\n    \"cloud_hail\",\n    \"cloud_lightning\",\n    \"cloud_moon\",\n    \"cloud_moon_rain\",\n    \"cloud_off\",\n    \"cloud_rain\",\n    \"cloud_rain_wind\",\n    \"cloud_snow\",\n    \"cloud_sun\",\n    \"cloud_sun_rain\",\n    \"cloud_upload\",\n    \"cloudy\",\n    \"clover\",\n    \"club\",\n    \"code\",\n    \"code_xml\",\n    \"codepen\",\n    \"codesandbox\",\n    \"coffee\",\n    \"cog\",\n    \"coins\",\n    \"columns_2\",\n    \"columns_3\",\n    \"columns_4\",\n    \"combine\",\n    \"command\",\n    \"compass\",\n    \"component\",\n    \"computer\",\n    \"concierge_bell\",\n    \"cone\",\n    \"construction\",\n    \"contact\",\n    \"contact_round\",\n    \"container\",\n    \"contrast\",\n    \"cookie\",\n    \"cooking_pot\",\n    \"copy\",\n    \"copy_check\",\n    \"copy_minus\",\n    \"copy_plus\",\n    \"copy_slash\",\n    \"copy_x\",\n    \"copyleft\",\n    \"copyright\",\n    \"corner_down_left\",\n    \"corner_down_right\",\n    \"corner_left_down\",\n    \"corner_left_up\",\n    \"corner_right_down\",\n    \"corner_right_up\",\n    \"corner_up_left\",\n    \"corner_up_right\",\n    \"cpu\",\n    \"creative_commons\",\n    \"credit_card\",\n    \"croissant\",\n    \"crop\",\n    \"cross\",\n    \"crosshair\",\n    \"crown\",\n    \"cuboid\",\n    \"cup_soda\",\n    \"currency\",\n    \"cylinder\",\n    \"database\",\n    \"database_backup\",\n    \"database_zap\",\n    \"delete\",\n    \"dessert\",\n    \"diameter\",\n    \"diamond\",\n    \"diamond_percent\",\n    \"dice_1\",\n    \"dice_2\",\n    \"dice_3\",\n    \"dice_4\",\n    \"dice_5\",\n    \"dice_6\",\n    \"dices\",\n    \"diff\",\n    \"disc\",\n    \"disc_2\",\n    \"disc_3\",\n    \"disc_album\",\n    \"divide\",\n    \"dna\",\n    \"dna_off\",\n    \"dock\",\n    \"dog\",\n    \"dollar_sign\",\n    \"donut\",\n    \"door_closed\",\n    \"door_open\",\n    \"dot\",\n    \"download\",\n    \"drafting_compass\",\n    \"drama\",\n    \"dribbble\",\n    \"drill\",\n    \"droplet\",\n    \"droplets\",\n    \"drum\",\n    \"drumstick\",\n    \"dumbbell\",\n    \"ear\",\n    \"ear_off\",\n    \"earth\",\n    \"earth_lock\",\n    \"eclipse\",\n    \"egg\",\n    \"egg_fried\",\n    \"egg_off\",\n    \"ellipsis\",\n    \"ellipsis_vertical\",\n    \"equal\",\n    \"equal_not\",\n    \"eraser\",\n    \"euro\",\n    \"expand\",\n    \"external_link\",\n    \"eye\",\n    \"eye_off\",\n    \"facebook\",\n    \"factory\",\n    \"fan\",\n    \"fast_forward\",\n    \"feather\",\n    \"fence\",\n    \"ferris_wheel\",\n    \"figma\",\n    \"file\",\n    \"file_archive\",\n    \"file_audio\",\n    \"file_audio_2\",\n    \"file_axis_3d\",\n    \"file_badge\",\n    \"file_badge_2\",\n    \"file_bar_chart\",\n    \"file_bar_chart_2\",\n    \"file_box\",\n    \"file_check\",\n    \"file_check_2\",\n    \"file_clock\",\n    \"file_code\",\n    \"file_code_2\",\n    \"file_cog\",\n    \"file_diff\",\n    \"file_digit\",\n    \"file_down\",\n    \"file_heart\",\n    \"file_image\",\n    \"file_input\",\n    \"file_json\",\n    \"file_json_2\",\n    \"file_key\",\n    \"file_key_2\",\n    \"file_line_chart\",\n    \"file_lock\",\n    \"file_lock_2\",\n    \"file_minus\",\n    \"file_minus_2\",\n    \"file_music\",\n    \"file_output\",\n    \"file_pen\",\n    \"file_pen_line\",\n    \"file_pie_chart\",\n    \"file_plus\",\n    \"file_plus_2\",\n    \"file_question\",\n    \"file_scan\",\n    \"file_search\",\n    \"file_search_2\",\n    \"file_sliders\",\n    \"file_spreadsheet\",\n    \"file_stack\",\n    \"file_symlink\",\n    \"file_terminal\",\n    \"file_text\",\n    \"file_type\",\n    \"file_type_2\",\n    \"file_up\",\n    \"file_video\",\n    \"file_video_2\",\n    \"file_volume\",\n    \"file_volume_2\",\n    \"file_warning\",\n    \"file_x\",\n    \"file_x_2\",\n    \"files\",\n    \"film\",\n    \"filter\",\n    \"filter_x\",\n    \"fingerprint\",\n    \"fire_extinguisher\",\n    \"fish\",\n    \"fish_off\",\n    \"fish_symbol\",\n    \"flag\",\n    \"flag_off\",\n    \"flag_triangle_left\",\n    \"flag_triangle_right\",\n    \"flame\",\n    \"flame_kindling\",\n    \"flashlight\",\n    \"flashlight_off\",\n    \"flask_conical\",\n    \"flask_conical_off\",\n    \"flask_round\",\n    \"flip_horizontal\",\n    \"flip_horizontal_2\",\n    \"flip_vertical\",\n    \"flip_vertical_2\",\n    \"flower\",\n    \"flower_2\",\n    \"focus\",\n    \"fold_horizontal\",\n    \"fold_vertical\",\n    \"folder\",\n    \"folder_archive\",\n    \"folder_check\",\n    \"folder_clock\",\n    \"folder_closed\",\n    \"folder_cog\",\n    \"folder_dot\",\n    \"folder_down\",\n    \"folder_git\",\n    \"folder_git_2\",\n    \"folder_heart\",\n    \"folder_input\",\n    \"folder_kanban\",\n    \"folder_key\",\n    \"folder_lock\",\n    \"folder_minus\",\n    \"folder_open\",\n    \"folder_open_dot\",\n    \"folder_output\",\n    \"folder_pen\",\n    \"folder_plus\",\n    \"folder_root\",\n    \"folder_search\",\n    \"folder_search_2\",\n    \"folder_symlink\",\n    \"folder_sync\",\n    \"folder_tree\",\n    \"folder_up\",\n    \"folder_x\",\n    \"folders\",\n    \"footprints\",\n    \"forklift\",\n    \"forward\",\n    \"frame\",\n    \"framer\",\n    \"frown\",\n    \"fuel\",\n    \"fullscreen\",\n    \"gallery_horizontal\",\n    \"gallery_horizontal_end\",\n    \"gallery_thumbnails\",\n    \"gallery_vertical\",\n    \"gallery_vertical_end\",\n    \"gamepad\",\n    \"gamepad_2\",\n    \"gantt_chart\",\n    \"gauge\",\n    \"gavel\",\n    \"gem\",\n    \"ghost\",\n    \"gift\",\n    \"git_branch\",\n    \"git_branch_plus\",\n    \"git_commit_horizontal\",\n    \"git_commit_vertical\",\n    \"git_compare\",\n    \"git_compare_arrows\",\n    \"git_fork\",\n    \"git_graph\",\n    \"git_merge\",\n    \"git_pull_request\",\n    \"git_pull_request_arrow\",\n    \"git_pull_request_closed\",\n    \"git_pull_request_create\",\n    \"git_pull_request_create_arrow\",\n    \"git_pull_request_draft\",\n    \"github\",\n    \"gitlab\",\n    \"glass_water\",\n    \"glasses\",\n    \"globe\",\n    \"globe_lock\",\n    \"goal\",\n    \"grab\",\n    \"graduation_cap\",\n    \"gr\n```\n# Function Signature: def create(cls, *children, **props) -> Component:\n# Natural Language Annotations: \"\"\"Initialize the Icon component.\n\n        Run some additional checks on Icon component.\n\n        Args:\n            *children: The positional arguments\n            **props: The keyword arguments\n\n        Raises:\n            AttributeError: The errors tied to bad usage of the Icon component.\n            ValueError: If the icon tag is invalid.\n\n        Returns:\n            The created component.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef create(cls, *children, **props):\n   ", "  ```\ndef create(cls, *children, **props):\n   ", "  ```\ndef create(cls, *children, **props):\n   "]}, "reflex-reflex/components/tags/tag.py-format_props": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/reflex/reflex/components/tags/tag.py:\n```\n\"\"\"A React tag.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List, Optional, Set, Tuple, Union\n\nfrom reflex.base import Base\nfrom reflex.event import EventChain\nfrom reflex.utils import format, types\nfrom reflex.vars import Var\n\n\nclass Tag(Base):\n    \"\"\"A React tag.\"\"\"\n\n    # The name of the tag.\n    name: str = \"\"\n\n    # The props of the tag.\n    props: Dict[str, Any] = {}\n\n    # The inner contents of the tag.\n    contents: str = \"\"\n\n    # Args to pass to the tag.\n    args: Optional[Tuple[str, ...]] = None\n\n    # Special props that aren't key value pairs.\n    special_props: Set[Var] = set()\n\n    # The children components.\n    children: List[Any] = []\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the tag.\n\n        Args:\n            *args: Args to initialize the tag.\n            **kwargs: Kwargs to initialize the tag.\n        \"\"\"\n        # Convert any props to vars.\n        if \"props\" in kwargs:\n            kwargs[\"props\"] = {\n                name: Var.create(value, _var_is_string=False)\n                for name, value in kwargs[\"props\"].items()\n            }\n        super().__init__(*args, **kwargs)\n\n\n\n\n\n\n\n\n\n    def add_props(self, **kwargs: Optional[Any]) -> Tag:\n        \"\"\"Add props to the tag.\n\n        Args:\n            **kwargs: The props to add.\n\n        Returns:\n            The tag with the props added.\n        \"\"\"\n        from reflex.components.core.colors import Color\n\n        self.props.update(\n            {\n                format.to_camel_case(name, allow_hyphens=True): prop\n                if types._isinstance(prop, Union[EventChain, dict])\n                else Var.create(\n                    prop, _var_is_string=isinstance(prop, Color)\n                )  # rx.color is always a string\n                for name, prop in kwargs.items()\n                if self.is_valid_prop(prop)\n            }\n        )\n        return self\n\n    def remove_props(self, *args: str) -> Tag:\n        \"\"\"Remove props from the tag.\n\n        Args:\n            *args: The props to remove.\n\n        Returns:\n            The tag with the props removed.\n        \"\"\"\n        for name in args:\n            prop_name = format.to_camel_case(name)\n            if prop_name in self.props:\n                del self.props[prop_name]\n        return self\n\n    @staticmethod\n    def is_valid_prop(prop: Optional[Var]) -> bool:\n        \"\"\"Check if the prop is valid.\n\n        Args:\n            prop: The prop to check.\n\n        Returns:\n            Whether the prop is valid.\n        \"\"\"\n        return prop is not None and not (isinstance(prop, dict) and len(prop) == 0)\n\n```\n# Function Signature: def format_props(self) -> List:\n# Natural Language Annotations: \"\"\"Format the tag's props.\n\n        Returns:\n            The formatted props list.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef format_props(self) -> List:\n    \"\"\"", "  ```\ndef format_props(self) -> List:\n    \"\"\"", "  ```\ndef format_props(self):\n    \"\"\"Format the tag"]}, "textual-src/textual/screen.py-focus_next": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/screen.py:\n```\n\"\"\"\n\nThe `Screen` class is a special widget which represents the content in the terminal. See [Screens](/guide/screens/) for details.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom functools import partial\nfrom operator import attrgetter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generic,\n    Iterable,\n    Iterator,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport rich.repr\nfrom rich.console import RenderableType\nfrom rich.style import Style\n\nfrom . import constants, errors, events, messages\nfrom ._callback import invoke\nfrom ._compositor import Compositor, MapGeometry\nfrom ._context import active_message_pump, visible_screen_stack\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import CallbackType\nfrom .await_complete import AwaitComplete\nfrom .binding import ActiveBinding, Binding, _Bindings\nfrom .css.match import match\nfrom .css.parse import parse_selectors\nfrom .css.query import NoMatches, QueryType\nfrom .dom import DOMNode\nfrom .errors import NoWidget\nfrom .geometry import Offset, Region, Size\nfrom .reactive import Reactive\nfrom .renderables.background_screen import BackgroundScreen\nfrom .renderables.blank import Blank\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import Widget\nfrom .widgets import Tooltip\nfrom .widgets._toast import ToastRack\n\nif TYPE_CHECKING:\n    from typing_extensions import Final\n\n    from .command import Provider\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .message_pump import MessagePump\n\n# Screen updates will be batched so that they don't happen more often than 60 times per second:\nUPDATE_PERIOD: Final[float] = 1 / constants.MAX_FPS\n\nScreenResultType = TypeVar(\"ScreenResultType\")\n\"\"\"The result type of a screen.\"\"\"\n\nScreenResultCallbackType = Union[\n    Callable[[ScreenResultType], None], Callable[[ScreenResultType], Awaitable[None]]\n]\n\"\"\"Type of a screen result callback function.\"\"\"\n\n\n@rich.repr.auto\nclass ResultCallback(Generic[ScreenResultType]):\n    \"\"\"Holds the details of a callback.\"\"\"\n\n    def __init__(\n        self,\n        requester: MessagePump,\n        callback: ScreenResultCallbackType[ScreenResultType] | None,\n        future: asyncio.Future[ScreenResultType] | None = None,\n    ) -> None:\n        \"\"\"Initialise the result callback object.\n\n        Args:\n            requester: The object making a request for the callback.\n            callback: The callback function.\n            future: A Future to hold the result.\n        \"\"\"\n        self.requester = requester\n        \"\"\"The object in the DOM that requested the callback.\"\"\"\n        self.callback: ScreenResultCallbackType | None = callback\n        \"\"\"The callback function.\"\"\"\n        self.future = future\n        \"\"\"A future for the result\"\"\"\n\n    def __call__(self, result: ScreenResultType) -> None:\n        \"\"\"Call the callback, passing the given result.\n\n        Args:\n            result: The result to pass to the callback.\n\n        Note:\n            If the requested or the callback are `None` this will be a no-op.\n        \"\"\"\n        if self.future is not None:\n            self.future.set_result(result)\n        if self.requester is not None and self.callback is not None:\n            self.requester.call_next(self.callback, result)\n\n\n@rich.repr.auto\nclass Screen(Generic[ScreenResultType], Widget):\n    \"\"\"The base class for screens.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = None\n    \"\"\"A selector to determine what to focus automatically when the screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Set to `None` to inherit the value from the screen's app.\n    Set to `\"\"` to disable auto focus.\n    \"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. Rules here take priority over CSS_PATH.\n\n    Note:\n        This CSS applies to the whole app.\n    \"\"\"\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\n\n    Note:\n        This CSS applies to the whole app.\n    \"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    Screen {\n        layout: vertical;\n        overflow-y: auto;\n        background: $surface;        \n        \n        &:inline {\n            height: auto;\n            min-height: 1;\n            border-top: tall $background;\n            border-bottom: tall $background;\n        }\n    }\n    \"\"\"\n\n    TITLE: ClassVar[str | None] = None\n    \"\"\"A class variable to set the *default* title for the screen.\n\n    This overrides the app title.\n    To update the title while the screen is running,\n    you can set the [title][textual.screen.Screen.title] attribute.\n    \"\"\"\n\n    SUB_TITLE: ClassVar[str | None] = None\n    \"\"\"A class variable to set the *default* sub-title for the screen.\n\n    This overrides the app sub-title.\n    To update the sub-title while the screen is running,\n    you can set the [sub_title][textual.screen.Screen.sub_title] attribute.\n    \"\"\"\n\n    focused: Reactive[Widget | None] = Reactive(None)\n    \"\"\"The focused [widget][textual.widget.Widget] or `None` for no focus.\n    To set focus, do not update this value directly. Use [set_focus][textual.screen.Screen.set_focus] instead.\"\"\"\n    stack_updates: Reactive[int] = Reactive(0, repaint=False)\n    \"\"\"An integer that updates when the screen is resumed.\"\"\"\n    sub_title: Reactive[str | None] = Reactive(None, compute=False)\n    \"\"\"Screen sub-title to override [the app sub-title][textual.app.App.sub_title].\"\"\"\n    title: Reactive[str | None] = Reactive(None, compute=False)\n    \"\"\"Screen title to override [the app title][textual.app.App.title].\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = set()\n    \"\"\"Command providers used by the [command palette](/guide/command_palette), associated with the screen.\n\n    Should be a set of [`command.Provider`][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS = [\n        Binding(\"tab\", \"app.focus_next\", \"Focus Next\", show=False),\n        Binding(\"shift+tab\", \"app.focus_previous\", \"Focus Previous\", show=False),\n    ]\n\n    def __init__(\n        self,\n        name: str | None = None,\n        id: str | None = None,\n        classes: str | None = None,\n    ) -> None:\n        \"\"\"\n        Initialize the screen.\n\n        Args:\n            name: The name of the screen.\n            id: The ID of the screen in the DOM.\n            classes: The CSS classes for the screen.\n        \"\"\"\n        self._modal = False\n        super().__init__(name=name, id=id, classes=classes)\n        self._compositor = Compositor()\n        self._dirty_widgets: set[Widget] = set()\n        self.__update_timer: Timer | None = None\n        self._callbacks: list[tuple[CallbackType, MessagePump]] = []\n        self._result_callbacks: list[ResultCallback[ScreenResultType]] = []\n\n        self._tooltip_widget: Widget | None = None\n        self._tooltip_timer: Timer | None = None\n\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(self.CSS_PATH)\n                if self.CSS_PATH is not None\n                else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self.title = self.TITLE\n        self.sub_title = self.SUB_TITLE\n\n        self.screen_layout_refresh_signal: Signal[Screen] = Signal(\n            self, \"layout-refresh\"\n        )\n        \"\"\"The signal that is published when the screen's layout is refreshed.\"\"\"\n\n        self._bindings_updated = False\n        \"\"\"Indicates that a binding update was requested.\"\"\"\n        self.bindings_updated_signal: Signal[Screen] = Signal(self, \"bindings_updated\")\n        \"\"\"A signal published when the bindings have been updated\"\"\"\n\n    @property\n    def is_modal(self) -> bool:\n        \"\"\"Is the screen modal?\"\"\"\n        return self._modal\n\n    @property\n    def is_current(self) -> bool:\n        \"\"\"Is the screen current (i.e. visible to user)?\"\"\"\n        from .app import ScreenStackError\n\n        try:\n            return self.app.screen is self or self in self.app._background_screens\n        except ScreenStackError:\n            return False\n\n    @property\n    def _update_timer(self) -> Timer:\n        \"\"\"Timer used to perform updates.\"\"\"\n        if self.__update_timer is None:\n            self.__update_timer = self.set_interval(\n                UPDATE_PERIOD, self._on_timer_update, name=\"screen_update\", pause=True\n            )\n        return self.__update_timer\n\n    @property\n    def layers(self) -> tuple[str, ...]:\n        \"\"\"Layers from parent.\n\n        Returns:\n            Tuple of layer names.\n        \"\"\"\n        extras = [\"_loading\"]\n        if not self.app._disable_notifications:\n            extras.append(\"_toastrack\")\n        if not self.app._disable_tooltips:\n            extras.append(\"_tooltips\")\n        return (*super().layers, *extras)\n\n    def _watch_focused(self):\n        self.refresh_bindings()\n\n    def _watch_stack_updates(self):\n        self.refresh_bindings()\n\n    def refresh_bindings(self) -> None:\n        \"\"\"Call to request a refresh of bindings.\"\"\"\n        self.log.debug(\"Bindings updated\")\n        self._bindings_updated = True\n        self.check_idle()\n\n    @property\n    def _binding_chain(self) -> list[tuple[DOMNode, _Bindings]]:\n        \"\"\"Binding chain from this screen.\"\"\"\n        focused = self.focused\n        if focused is not None and focused.loading:\n            focused = None\n        namespace_bindings: list[tuple[DOMNode, _Bindings]]\n\n        if focused is None:\n            namespace_bindings = [\n                (self, self._bindings),\n                (self.app, self.app._bindings),\n            ]\n        else:\n            namespace_bindings = [\n                (node, node._bindings) for node in focused.ancestors_with_self\n            ]\n\n        return namespace_bindings\n\n    @property\n    def _modal_binding_chain(self) -> list[tuple[DOMNode, _Bindings]]:\n        \"\"\"The binding chain, ignoring everything before the last modal.\"\"\"\n        binding_chain = self._binding_chain\n        for index, (node, _bindings) in enumerate(binding_chain, 1):\n            if node.is_modal:\n                return binding_chain[:index]\n        return binding_chain\n\n    @property\n    def active_bindings(self) -> dict[str, ActiveBinding]:\n        \"\"\"Get currently active bindings for this screen.\n\n        If no widget is focused, then app-level bindings are returned.\n        If a widget is focused, then any bindings present in the screen and app are merged and returned.\n\n        This property may be used to inspect current bindings.\n\n        Returns:\n            A map of keys to a tuple containing (namespace, binding, enabled boolean).\n        \"\"\"\n\n        bindings_map: dict[str, ActiveBinding] = {}\n        for namespace, bindings in self._modal_binding_chain:\n            for key, binding in bindings.keys.items():\n                action_state = self.app._check_action_state(binding.action, namespace)\n                if action_state is False:\n                    continue\n                if existing_key_and_binding := bindings_map.get(key):\n                    _, existing_binding, _ = existing_key_and_binding\n                    if binding.priority and not existing_binding.priority:\n                        bindings_map[key] = ActiveBinding(\n                            namespace, binding, bool(action_state)\n                        )\n                else:\n                    bindings_map[key] = ActiveBinding(\n                        namespace, binding, bool(action_state)\n                    )\n\n        return bindings_map\n\n    @property\n    def is_active(self) -> bool:\n        \"\"\"Is the screen active (i.e. visible and top of the stack)?\"\"\"\n        try:\n            return self.app.screen is self\n        except Exception:\n            return False\n\n    def render(self) -> RenderableType:\n        \"\"\"Render method inherited from widget, used to render the screen's background.\n\n        Returns:\n            Background renderable.\n        \"\"\"\n        background = self.styles.background\n        try:\n            base_screen = visible_screen_stack.get().pop()\n        except IndexError:\n            base_screen = None\n\n        if base_screen is not None and background.a < 1:\n            # If background is translucent, render a background screen\n            return BackgroundScreen(base_screen, background)\n\n        if background.is_transparent:\n            # If the background is transparent, defer to App.render\n            return self.app.render()\n        # Render a screen of a solid color.\n        return Blank(background)\n\n    def get_offset(self, widget: Widget) -> Offset:\n        \"\"\"Get the absolute offset of a given Widget.\n\n        Args:\n            widget: A widget\n\n        Returns:\n            The widget's offset relative to the top left of the terminal.\n        \"\"\"\n        return self._compositor.get_offset(widget)\n\n    def get_widget_at(self, x: int, y: int) -> tuple[Widget, Region]:\n        \"\"\"Get the widget at a given coordinate.\n\n        Args:\n            x: X Coordinate.\n            y: Y Coordinate.\n\n        Returns:\n            Widget and screen region.\n\n        Raises:\n            NoWidget: If there is no widget under the screen coordinate.\n        \"\"\"\n        return self._compositor.get_widget_at(x, y)\n\n    def get_widgets_at(self, x: int, y: int) -> Iterable[tuple[Widget, Region]]:\n        \"\"\"Get all widgets under a given coordinate.\n\n        Args:\n            x: X coordinate.\n            y: Y coordinate.\n\n        Returns:\n            Sequence of (WIDGET, REGION) tuples.\n        \"\"\"\n        return self._compositor.get_widgets_at(x, y)\n\n    def get_focusable_widget_at(self, x: int, y: int) -> Widget | None:\n        \"\"\"Get the focusable widget under a given coordinate.\n\n        If the widget directly under the given coordinate is not focusable, then this method will check\n        if any of the ancestors are focusable. If no ancestors are focusable, then `None` will be returned.\n\n        Args:\n            x: X coordinate.\n            y: Y coordinate.\n\n        Returns:\n            A `Widget`, or `None` if there is no focusable widget underneath the coordinate.\n        \"\"\"\n        try:\n            widget, _region = self.get_widget_at(x, y)\n        except NoWidget:\n            return None\n\n        for node in widget.ancestors_with_self:\n            if isinstance(node, Widget) and node.focusable:\n                return node\n        return None\n\n    def get_style_at(self, x: int, y: int) -> Style:\n        \"\"\"Get the style under a given coordinate.\n\n        Args:\n            x: X Coordinate.\n            y: Y Coordinate.\n\n        Returns:\n            Rich Style object.\n        \"\"\"\n        return self._compositor.get_style_at(x, y)\n\n    def find_widget(self, widget: Widget) -> MapGeometry:\n        \"\"\"Get the screen region of a Widget.\n\n        Args:\n            widget: A Widget within the composition.\n\n        Returns:\n            Region relative to screen.\n\n        Raises:\n            NoWidget: If the widget could not be found in this screen.\n        \"\"\"\n        return self._compositor.find_widget(widget)\n\n    @property\n    def focus_chain(self) -> list[Widget]:\n        \"\"\"A list of widgets that may receive focus, in focus order.\"\"\"\n        # TODO: Calculating a focus chain is moderately expensive.\n        # Suspect we can move focus without calculating the entire thing again.\n\n        widgets: list[Widget] = []\n        add_widget = widgets.append\n        focus_sorter = attrgetter(\"_focus_sort_key\")\n        # We traverse the DOM and keep track of where we are at with a node stack.\n        # Additionally, we manually keep track of the visibility of the DOM\n        # instead of relying on the property `.visible` to save on DOM traversals.\n        # node_stack: list[tuple[iterator over node children, node visibility]]\n        node_stack: list[tuple[Iterator[Widget], bool]] = [\n            (\n                iter(sorted(self.displayed_children, key=focus_sorter)),\n                self.visible,\n            )\n        ]\n        pop = node_stack.pop\n        push = node_stack.append\n\n        while node_stack:\n            children_iterator, parent_visibility = node_stack[-1]\n            node = next(children_iterator, None)\n            if node is None:\n                pop()\n            else:\n                if node._check_disabled():\n                    continue\n                node_styles_visibility = node.styles.get_rule(\"visibility\")\n                node_is_visible = (\n                    node_styles_visibility != \"hidden\"\n                    if node_styles_visibility\n                    else parent_visibility  # Inherit visibility if the style is unset.\n                )\n                if node.is_container and node.allow_focus_children():\n                    sorted_displayed_children = sorted(\n                        node.displayed_children, key=focus_sorter\n                    )\n                    push((iter(sorted_displayed_children), node_is_visible))\n                # Same check as `if node.focusable`, but we cached inherited visibility\n                # and we also skipped disabled nodes altogether.\n                if node_is_visible and node.allow_focus():\n                    add_widget(node)\n\n        return widgets\n\n    def _move_focus(\n        self, direction: int = 0, selector: str | type[QueryType] = \"*\"\n    ) -> Widget | None:\n        \"\"\"Move the focus in the given direction.\n\n        If no widget is currently focused, this will focus the first focusable widget.\n        If no focusable widget matches the given CSS selector, focus is set to `None`.\n\n        Args:\n            direction: 1 to move forward, -1 to move backward, or\n                0 to keep the current focus.\n            selector: CSS selector to filter\n                what nodes can be focused.\n\n        Returns:\n            Newly focused widget, or None for no focus. If the return\n                is not `None`, then it is guaranteed that the widget returned matches\n                the CSS selectors given in the argument.\n        \"\"\"\n        # TODO: This shouldn't be required\n        self._compositor._full_map_invalidated = True\n        if not isinstance(selector, str):\n            selector = selector.__name__\n        selector_set = parse_selectors(selector)\n        focus_chain = self.focus_chain\n        filtered_focus_chain = (\n            node for node in focus_chain if match(selector_set, node)\n        )\n\n        if not focus_chain:\n            # Nothing focusable, so nothing to do\n            return self.focused\n        if self.focused is None:\n            # Nothing currently focused, so focus the first one.\n            to_focus = next(filtered_focus_chain, None)\n            self.set_focus(to_focus)\n            return self.focused\n\n        # Ensure focus will be in a node that matches the selectors.\n        if not direction and not match(selector_set, self.focused):\n            direction = 1\n\n        try:\n            # Find the index of the currently focused widget\n            current_index = focus_chain.index(self.focused)\n        except ValueError:\n            # Focused widget was removed in the interim, start again\n            self.set_focus(next(filtered_focus_chain, None))\n        else:\n            # Only move the focus if we are currently showing the focus\n            if direction:\n                to_focus = None\n                chain_length = len(focus_chain)\n                for step in range(1, len(focus_chain) + 1):\n                    node = focus_chain[\n                        (current_index + direction * step) % chain_length\n                    ]\n                    if match(selector_set, node):\n                        to_focus = node\n                        break\n                self.set_focus(to_focus)\n\n        return self.focused\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _reset_focus(\n        self, widget: Widget, avoiding: list[Widget] | None = None\n    ) -> None:\n        \"\"\"Reset the focus when a widget is removed\n\n        Args:\n            widget: A widget that is removed.\n            avoiding: Optional list of nodes to avoid.\n        \"\"\"\n\n        avoiding = avoiding or []\n\n        # Make this a NOP if we're being asked to deal with a widget that\n        # isn't actually the currently-focused widget.\n        if self.focused is not widget:\n            return\n\n        # Grab the list of widgets that we can set focus to.\n        focusable_widgets = self.focus_chain\n        if not focusable_widgets:\n            # If there's nothing to focus... give up now.\n            self.set_focus(None)\n            return\n\n        try:\n            # Find the location of the widget we're taking focus from, in\n            # the focus chain.\n            widget_index = focusable_widgets.index(widget)\n        except ValueError:\n            # widget is not in focusable widgets\n            # It may have been made invisible\n            # Move to a sibling if possible\n            for sibling in widget.visible_siblings:\n                if sibling not in avoiding and sibling.focusable:\n                    self.set_focus(sibling)\n                    break\n            else:\n                self.set_focus(None)\n            return\n\n        # Now go looking for something before it, that isn't about to be\n        # removed, and which can receive focus, and go focus that.\n        chosen: Widget | None = None\n        for candidate in reversed(\n            focusable_widgets[widget_index + 1 :] + focusable_widgets[:widget_index]\n        ):\n            if candidate not in avoiding:\n                chosen = candidate\n                break\n\n        # Go with what was found.\n        self.set_focus(chosen)\n\n    def _update_focus_styles(\n        self, focused: Widget | None = None, blurred: Widget | None = None\n    ) -> None:\n        \"\"\"Update CSS for focus changes.\n\n        Args:\n            focused: The widget that was focused.\n            blurred: The widget that was blurred.\n        \"\"\"\n        widgets: set[DOMNode] = set()\n\n        if focused is not None:\n            for widget in reversed(focused.ancestors_with_self):\n                if widget._has_focus_within:\n                    widgets.update(widget.walk_children(with_self=True))\n                    break\n        if blurred is not None:\n            for widget in reversed(blurred.ancestors_with_self):\n                if widget._has_focus_within:\n                    widgets.update(widget.walk_children(with_self=True))\n                    break\n        if widgets:\n            self.app.stylesheet.update_nodes(\n                [widget for widget in widgets if widget._has_focus_within], animate=True\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _extend_compose(self, widgets: list[Widget]) -> None:\n        \"\"\"Insert Textual's own internal widgets.\n\n        Args:\n            widgets: The list of widgets to be composed.\n\n        This method adds the tooltip, if required, and also adds the\n        container for `Toast`s.\n        \"\"\"\n        if not self.app._disable_tooltips:\n            widgets.insert(0, Tooltip(id=\"textual-tooltip\"))\n        if not self.app._disable_notifications:\n            widgets.insert(0, ToastRack(id=\"textual-toastrack\"))\n\n    def _on_mount(self, event: events.Mount) -> None:\n        \"\"\"Set up the tooltip-clearing signal when we mount.\"\"\"\n        self.screen_layout_refresh_signal.subscribe(\n            self, self._maybe_clear_tooltip, immediate=True\n        )\n        self.refresh_bindings()\n\n    async def _on_idle(self, event: events.Idle) -> None:\n        # Check for any widgets marked as 'dirty' (needs a repaint)\n        event.prevent_default()\n\n        try:\n            if not self.app._batch_count and self.is_current:\n                if (\n                    self._layout_required\n                    or self._scroll_required\n                    or self._repaint_required\n                    or self._recompose_required\n                    or self._dirty_widgets\n                ):\n                    self._update_timer.resume()\n                    return\n\n            await self._invoke_and_clear_callbacks()\n        finally:\n            if self._bindings_updated:\n                self._bindings_updated\n```\n# Function Signature: def focus_next(self, selector: str | type[QueryType] = \"*\") -> Widget | None:\n# Natural Language Annotations: \"\"\"Focus the next widget, optionally filtered by a CSS selector.\n\n        If no widget is currently focused, this will focus the first focusable widget.\n        If no focusable widget matches the given CSS selector, focus is set to `None`.\n\n        Args:\n            selector: CSS selector to filter\n                what nodes can be focused.\n\n        Returns:\n            Newly focused widget, or None for no focus. If the return\n                is not `None`, then it is guaranteed that the widget returned matches\n                the CSS selectors given in the argument.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef focus_next(self, selector: str | type[", "  ```python\ndef focus_next(self, selector: str | type", "  ```\ndef focus_next(self, selector: str | type["]}, "textual-src/textual/screen.py-focus_previous": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/screen.py:\n```\n\"\"\"\n\nThe `Screen` class is a special widget which represents the content in the terminal. See [Screens](/guide/screens/) for details.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom functools import partial\nfrom operator import attrgetter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generic,\n    Iterable,\n    Iterator,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport rich.repr\nfrom rich.console import RenderableType\nfrom rich.style import Style\n\nfrom . import constants, errors, events, messages\nfrom ._callback import invoke\nfrom ._compositor import Compositor, MapGeometry\nfrom ._context import active_message_pump, visible_screen_stack\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import CallbackType\nfrom .await_complete import AwaitComplete\nfrom .binding import ActiveBinding, Binding, _Bindings\nfrom .css.match import match\nfrom .css.parse import parse_selectors\nfrom .css.query import NoMatches, QueryType\nfrom .dom import DOMNode\nfrom .errors import NoWidget\nfrom .geometry import Offset, Region, Size\nfrom .reactive import Reactive\nfrom .renderables.background_screen import BackgroundScreen\nfrom .renderables.blank import Blank\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import Widget\nfrom .widgets import Tooltip\nfrom .widgets._toast import ToastRack\n\nif TYPE_CHECKING:\n    from typing_extensions import Final\n\n    from .command import Provider\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .message_pump import MessagePump\n\n# Screen updates will be batched so that they don't happen more often than 60 times per second:\nUPDATE_PERIOD: Final[float] = 1 / constants.MAX_FPS\n\nScreenResultType = TypeVar(\"ScreenResultType\")\n\"\"\"The result type of a screen.\"\"\"\n\nScreenResultCallbackType = Union[\n    Callable[[ScreenResultType], None], Callable[[ScreenResultType], Awaitable[None]]\n]\n\"\"\"Type of a screen result callback function.\"\"\"\n\n\n@rich.repr.auto\nclass ResultCallback(Generic[ScreenResultType]):\n    \"\"\"Holds the details of a callback.\"\"\"\n\n    def __init__(\n        self,\n        requester: MessagePump,\n        callback: ScreenResultCallbackType[ScreenResultType] | None,\n        future: asyncio.Future[ScreenResultType] | None = None,\n    ) -> None:\n        \"\"\"Initialise the result callback object.\n\n        Args:\n            requester: The object making a request for the callback.\n            callback: The callback function.\n            future: A Future to hold the result.\n        \"\"\"\n        self.requester = requester\n        \"\"\"The object in the DOM that requested the callback.\"\"\"\n        self.callback: ScreenResultCallbackType | None = callback\n        \"\"\"The callback function.\"\"\"\n        self.future = future\n        \"\"\"A future for the result\"\"\"\n\n    def __call__(self, result: ScreenResultType) -> None:\n        \"\"\"Call the callback, passing the given result.\n\n        Args:\n            result: The result to pass to the callback.\n\n        Note:\n            If the requested or the callback are `None` this will be a no-op.\n        \"\"\"\n        if self.future is not None:\n            self.future.set_result(result)\n        if self.requester is not None and self.callback is not None:\n            self.requester.call_next(self.callback, result)\n\n\n@rich.repr.auto\nclass Screen(Generic[ScreenResultType], Widget):\n    \"\"\"The base class for screens.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = None\n    \"\"\"A selector to determine what to focus automatically when the screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Set to `None` to inherit the value from the screen's app.\n    Set to `\"\"` to disable auto focus.\n    \"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. Rules here take priority over CSS_PATH.\n\n    Note:\n        This CSS applies to the whole app.\n    \"\"\"\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\n\n    Note:\n        This CSS applies to the whole app.\n    \"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    Screen {\n        layout: vertical;\n        overflow-y: auto;\n        background: $surface;        \n        \n        &:inline {\n            height: auto;\n            min-height: 1;\n            border-top: tall $background;\n            border-bottom: tall $background;\n        }\n    }\n    \"\"\"\n\n    TITLE: ClassVar[str | None] = None\n    \"\"\"A class variable to set the *default* title for the screen.\n\n    This overrides the app title.\n    To update the title while the screen is running,\n    you can set the [title][textual.screen.Screen.title] attribute.\n    \"\"\"\n\n    SUB_TITLE: ClassVar[str | None] = None\n    \"\"\"A class variable to set the *default* sub-title for the screen.\n\n    This overrides the app sub-title.\n    To update the sub-title while the screen is running,\n    you can set the [sub_title][textual.screen.Screen.sub_title] attribute.\n    \"\"\"\n\n    focused: Reactive[Widget | None] = Reactive(None)\n    \"\"\"The focused [widget][textual.widget.Widget] or `None` for no focus.\n    To set focus, do not update this value directly. Use [set_focus][textual.screen.Screen.set_focus] instead.\"\"\"\n    stack_updates: Reactive[int] = Reactive(0, repaint=False)\n    \"\"\"An integer that updates when the screen is resumed.\"\"\"\n    sub_title: Reactive[str | None] = Reactive(None, compute=False)\n    \"\"\"Screen sub-title to override [the app sub-title][textual.app.App.sub_title].\"\"\"\n    title: Reactive[str | None] = Reactive(None, compute=False)\n    \"\"\"Screen title to override [the app title][textual.app.App.title].\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = set()\n    \"\"\"Command providers used by the [command palette](/guide/command_palette), associated with the screen.\n\n    Should be a set of [`command.Provider`][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS = [\n        Binding(\"tab\", \"app.focus_next\", \"Focus Next\", show=False),\n        Binding(\"shift+tab\", \"app.focus_previous\", \"Focus Previous\", show=False),\n    ]\n\n    def __init__(\n        self,\n        name: str | None = None,\n        id: str | None = None,\n        classes: str | None = None,\n    ) -> None:\n        \"\"\"\n        Initialize the screen.\n\n        Args:\n            name: The name of the screen.\n            id: The ID of the screen in the DOM.\n            classes: The CSS classes for the screen.\n        \"\"\"\n        self._modal = False\n        super().__init__(name=name, id=id, classes=classes)\n        self._compositor = Compositor()\n        self._dirty_widgets: set[Widget] = set()\n        self.__update_timer: Timer | None = None\n        self._callbacks: list[tuple[CallbackType, MessagePump]] = []\n        self._result_callbacks: list[ResultCallback[ScreenResultType]] = []\n\n        self._tooltip_widget: Widget | None = None\n        self._tooltip_timer: Timer | None = None\n\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(self.CSS_PATH)\n                if self.CSS_PATH is not None\n                else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self.title = self.TITLE\n        self.sub_title = self.SUB_TITLE\n\n        self.screen_layout_refresh_signal: Signal[Screen] = Signal(\n            self, \"layout-refresh\"\n        )\n        \"\"\"The signal that is published when the screen's layout is refreshed.\"\"\"\n\n        self._bindings_updated = False\n        \"\"\"Indicates that a binding update was requested.\"\"\"\n        self.bindings_updated_signal: Signal[Screen] = Signal(self, \"bindings_updated\")\n        \"\"\"A signal published when the bindings have been updated\"\"\"\n\n    @property\n    def is_modal(self) -> bool:\n        \"\"\"Is the screen modal?\"\"\"\n        return self._modal\n\n    @property\n    def is_current(self) -> bool:\n        \"\"\"Is the screen current (i.e. visible to user)?\"\"\"\n        from .app import ScreenStackError\n\n        try:\n            return self.app.screen is self or self in self.app._background_screens\n        except ScreenStackError:\n            return False\n\n    @property\n    def _update_timer(self) -> Timer:\n        \"\"\"Timer used to perform updates.\"\"\"\n        if self.__update_timer is None:\n            self.__update_timer = self.set_interval(\n                UPDATE_PERIOD, self._on_timer_update, name=\"screen_update\", pause=True\n            )\n        return self.__update_timer\n\n    @property\n    def layers(self) -> tuple[str, ...]:\n        \"\"\"Layers from parent.\n\n        Returns:\n            Tuple of layer names.\n        \"\"\"\n        extras = [\"_loading\"]\n        if not self.app._disable_notifications:\n            extras.append(\"_toastrack\")\n        if not self.app._disable_tooltips:\n            extras.append(\"_tooltips\")\n        return (*super().layers, *extras)\n\n    def _watch_focused(self):\n        self.refresh_bindings()\n\n    def _watch_stack_updates(self):\n        self.refresh_bindings()\n\n    def refresh_bindings(self) -> None:\n        \"\"\"Call to request a refresh of bindings.\"\"\"\n        self.log.debug(\"Bindings updated\")\n        self._bindings_updated = True\n        self.check_idle()\n\n    @property\n    def _binding_chain(self) -> list[tuple[DOMNode, _Bindings]]:\n        \"\"\"Binding chain from this screen.\"\"\"\n        focused = self.focused\n        if focused is not None and focused.loading:\n            focused = None\n        namespace_bindings: list[tuple[DOMNode, _Bindings]]\n\n        if focused is None:\n            namespace_bindings = [\n                (self, self._bindings),\n                (self.app, self.app._bindings),\n            ]\n        else:\n            namespace_bindings = [\n                (node, node._bindings) for node in focused.ancestors_with_self\n            ]\n\n        return namespace_bindings\n\n    @property\n    def _modal_binding_chain(self) -> list[tuple[DOMNode, _Bindings]]:\n        \"\"\"The binding chain, ignoring everything before the last modal.\"\"\"\n        binding_chain = self._binding_chain\n        for index, (node, _bindings) in enumerate(binding_chain, 1):\n            if node.is_modal:\n                return binding_chain[:index]\n        return binding_chain\n\n    @property\n    def active_bindings(self) -> dict[str, ActiveBinding]:\n        \"\"\"Get currently active bindings for this screen.\n\n        If no widget is focused, then app-level bindings are returned.\n        If a widget is focused, then any bindings present in the screen and app are merged and returned.\n\n        This property may be used to inspect current bindings.\n\n        Returns:\n            A map of keys to a tuple containing (namespace, binding, enabled boolean).\n        \"\"\"\n\n        bindings_map: dict[str, ActiveBinding] = {}\n        for namespace, bindings in self._modal_binding_chain:\n            for key, binding in bindings.keys.items():\n                action_state = self.app._check_action_state(binding.action, namespace)\n                if action_state is False:\n                    continue\n                if existing_key_and_binding := bindings_map.get(key):\n                    _, existing_binding, _ = existing_key_and_binding\n                    if binding.priority and not existing_binding.priority:\n                        bindings_map[key] = ActiveBinding(\n                            namespace, binding, bool(action_state)\n                        )\n                else:\n                    bindings_map[key] = ActiveBinding(\n                        namespace, binding, bool(action_state)\n                    )\n\n        return bindings_map\n\n    @property\n    def is_active(self) -> bool:\n        \"\"\"Is the screen active (i.e. visible and top of the stack)?\"\"\"\n        try:\n            return self.app.screen is self\n        except Exception:\n            return False\n\n    def render(self) -> RenderableType:\n        \"\"\"Render method inherited from widget, used to render the screen's background.\n\n        Returns:\n            Background renderable.\n        \"\"\"\n        background = self.styles.background\n        try:\n            base_screen = visible_screen_stack.get().pop()\n        except IndexError:\n            base_screen = None\n\n        if base_screen is not None and background.a < 1:\n            # If background is translucent, render a background screen\n            return BackgroundScreen(base_screen, background)\n\n        if background.is_transparent:\n            # If the background is transparent, defer to App.render\n            return self.app.render()\n        # Render a screen of a solid color.\n        return Blank(background)\n\n    def get_offset(self, widget: Widget) -> Offset:\n        \"\"\"Get the absolute offset of a given Widget.\n\n        Args:\n            widget: A widget\n\n        Returns:\n            The widget's offset relative to the top left of the terminal.\n        \"\"\"\n        return self._compositor.get_offset(widget)\n\n    def get_widget_at(self, x: int, y: int) -> tuple[Widget, Region]:\n        \"\"\"Get the widget at a given coordinate.\n\n        Args:\n            x: X Coordinate.\n            y: Y Coordinate.\n\n        Returns:\n            Widget and screen region.\n\n        Raises:\n            NoWidget: If there is no widget under the screen coordinate.\n        \"\"\"\n        return self._compositor.get_widget_at(x, y)\n\n    def get_widgets_at(self, x: int, y: int) -> Iterable[tuple[Widget, Region]]:\n        \"\"\"Get all widgets under a given coordinate.\n\n        Args:\n            x: X coordinate.\n            y: Y coordinate.\n\n        Returns:\n            Sequence of (WIDGET, REGION) tuples.\n        \"\"\"\n        return self._compositor.get_widgets_at(x, y)\n\n    def get_focusable_widget_at(self, x: int, y: int) -> Widget | None:\n        \"\"\"Get the focusable widget under a given coordinate.\n\n        If the widget directly under the given coordinate is not focusable, then this method will check\n        if any of the ancestors are focusable. If no ancestors are focusable, then `None` will be returned.\n\n        Args:\n            x: X coordinate.\n            y: Y coordinate.\n\n        Returns:\n            A `Widget`, or `None` if there is no focusable widget underneath the coordinate.\n        \"\"\"\n        try:\n            widget, _region = self.get_widget_at(x, y)\n        except NoWidget:\n            return None\n\n        for node in widget.ancestors_with_self:\n            if isinstance(node, Widget) and node.focusable:\n                return node\n        return None\n\n    def get_style_at(self, x: int, y: int) -> Style:\n        \"\"\"Get the style under a given coordinate.\n\n        Args:\n            x: X Coordinate.\n            y: Y Coordinate.\n\n        Returns:\n            Rich Style object.\n        \"\"\"\n        return self._compositor.get_style_at(x, y)\n\n    def find_widget(self, widget: Widget) -> MapGeometry:\n        \"\"\"Get the screen region of a Widget.\n\n        Args:\n            widget: A Widget within the composition.\n\n        Returns:\n            Region relative to screen.\n\n        Raises:\n            NoWidget: If the widget could not be found in this screen.\n        \"\"\"\n        return self._compositor.find_widget(widget)\n\n    @property\n    def focus_chain(self) -> list[Widget]:\n        \"\"\"A list of widgets that may receive focus, in focus order.\"\"\"\n        # TODO: Calculating a focus chain is moderately expensive.\n        # Suspect we can move focus without calculating the entire thing again.\n\n        widgets: list[Widget] = []\n        add_widget = widgets.append\n        focus_sorter = attrgetter(\"_focus_sort_key\")\n        # We traverse the DOM and keep track of where we are at with a node stack.\n        # Additionally, we manually keep track of the visibility of the DOM\n        # instead of relying on the property `.visible` to save on DOM traversals.\n        # node_stack: list[tuple[iterator over node children, node visibility]]\n        node_stack: list[tuple[Iterator[Widget], bool]] = [\n            (\n                iter(sorted(self.displayed_children, key=focus_sorter)),\n                self.visible,\n            )\n        ]\n        pop = node_stack.pop\n        push = node_stack.append\n\n        while node_stack:\n            children_iterator, parent_visibility = node_stack[-1]\n            node = next(children_iterator, None)\n            if node is None:\n                pop()\n            else:\n                if node._check_disabled():\n                    continue\n                node_styles_visibility = node.styles.get_rule(\"visibility\")\n                node_is_visible = (\n                    node_styles_visibility != \"hidden\"\n                    if node_styles_visibility\n                    else parent_visibility  # Inherit visibility if the style is unset.\n                )\n                if node.is_container and node.allow_focus_children():\n                    sorted_displayed_children = sorted(\n                        node.displayed_children, key=focus_sorter\n                    )\n                    push((iter(sorted_displayed_children), node_is_visible))\n                # Same check as `if node.focusable`, but we cached inherited visibility\n                # and we also skipped disabled nodes altogether.\n                if node_is_visible and node.allow_focus():\n                    add_widget(node)\n\n        return widgets\n\n    def _move_focus(\n        self, direction: int = 0, selector: str | type[QueryType] = \"*\"\n    ) -> Widget | None:\n        \"\"\"Move the focus in the given direction.\n\n        If no widget is currently focused, this will focus the first focusable widget.\n        If no focusable widget matches the given CSS selector, focus is set to `None`.\n\n        Args:\n            direction: 1 to move forward, -1 to move backward, or\n                0 to keep the current focus.\n            selector: CSS selector to filter\n                what nodes can be focused.\n\n        Returns:\n            Newly focused widget, or None for no focus. If the return\n                is not `None`, then it is guaranteed that the widget returned matches\n                the CSS selectors given in the argument.\n        \"\"\"\n        # TODO: This shouldn't be required\n        self._compositor._full_map_invalidated = True\n        if not isinstance(selector, str):\n            selector = selector.__name__\n        selector_set = parse_selectors(selector)\n        focus_chain = self.focus_chain\n        filtered_focus_chain = (\n            node for node in focus_chain if match(selector_set, node)\n        )\n\n        if not focus_chain:\n            # Nothing focusable, so nothing to do\n            return self.focused\n        if self.focused is None:\n            # Nothing currently focused, so focus the first one.\n            to_focus = next(filtered_focus_chain, None)\n            self.set_focus(to_focus)\n            return self.focused\n\n        # Ensure focus will be in a node that matches the selectors.\n        if not direction and not match(selector_set, self.focused):\n            direction = 1\n\n        try:\n            # Find the index of the currently focused widget\n            current_index = focus_chain.index(self.focused)\n        except ValueError:\n            # Focused widget was removed in the interim, start again\n            self.set_focus(next(filtered_focus_chain, None))\n        else:\n            # Only move the focus if we are currently showing the focus\n            if direction:\n                to_focus = None\n                chain_length = len(focus_chain)\n                for step in range(1, len(focus_chain) + 1):\n                    node = focus_chain[\n                        (current_index + direction * step) % chain_length\n                    ]\n                    if match(selector_set, node):\n                        to_focus = node\n                        break\n                self.set_focus(to_focus)\n\n        return self.focused\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _reset_focus(\n        self, widget: Widget, avoiding: list[Widget] | None = None\n    ) -> None:\n        \"\"\"Reset the focus when a widget is removed\n\n        Args:\n            widget: A widget that is removed.\n            avoiding: Optional list of nodes to avoid.\n        \"\"\"\n\n        avoiding = avoiding or []\n\n        # Make this a NOP if we're being asked to deal with a widget that\n        # isn't actually the currently-focused widget.\n        if self.focused is not widget:\n            return\n\n        # Grab the list of widgets that we can set focus to.\n        focusable_widgets = self.focus_chain\n        if not focusable_widgets:\n            # If there's nothing to focus... give up now.\n            self.set_focus(None)\n            return\n\n        try:\n            # Find the location of the widget we're taking focus from, in\n            # the focus chain.\n            widget_index = focusable_widgets.index(widget)\n        except ValueError:\n            # widget is not in focusable widgets\n            # It may have been made invisible\n            # Move to a sibling if possible\n            for sibling in widget.visible_siblings:\n                if sibling not in avoiding and sibling.focusable:\n                    self.set_focus(sibling)\n                    break\n            else:\n                self.set_focus(None)\n            return\n\n        # Now go looking for something before it, that isn't about to be\n        # removed, and which can receive focus, and go focus that.\n        chosen: Widget | None = None\n        for candidate in reversed(\n            focusable_widgets[widget_index + 1 :] + focusable_widgets[:widget_index]\n        ):\n            if candidate not in avoiding:\n                chosen = candidate\n                break\n\n        # Go with what was found.\n        self.set_focus(chosen)\n\n    def _update_focus_styles(\n        self, focused: Widget | None = None, blurred: Widget | None = None\n    ) -> None:\n        \"\"\"Update CSS for focus changes.\n\n        Args:\n            focused: The widget that was focused.\n            blurred: The widget that was blurred.\n        \"\"\"\n        widgets: set[DOMNode] = set()\n\n        if focused is not None:\n            for widget in reversed(focused.ancestors_with_self):\n                if widget._has_focus_within:\n                    widgets.update(widget.walk_children(with_self=True))\n                    break\n        if blurred is not None:\n            for widget in reversed(blurred.ancestors_with_self):\n                if widget._has_focus_within:\n                    widgets.update(widget.walk_children(with_self=True))\n                    break\n        if widgets:\n            self.app.stylesheet.update_nodes(\n                [widget for widget in widgets if widget._has_focus_within], animate=True\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _extend_compose(self, widgets: list[Widget]) -> None:\n        \"\"\"Insert Textual's own internal widgets.\n\n        Args:\n            widgets: The list of widgets to be composed.\n\n        This method adds the tooltip, if required, and also adds the\n        container for `Toast`s.\n        \"\"\"\n        if not self.app._disable_tooltips:\n            widgets.insert(0, Tooltip(id=\"textual-tooltip\"))\n        if not self.app._disable_notifications:\n            widgets.insert(0, ToastRack(id=\"textual-toastrack\"))\n\n    def _on_mount(self, event: events.Mount) -> None:\n        \"\"\"Set up the tooltip-clearing signal when we mount.\"\"\"\n        self.screen_layout_refresh_signal.subscribe(\n            self, self._maybe_clear_tooltip, immediate=True\n        )\n        self.refresh_bindings()\n\n    async def _on_idle(self, event: events.Idle) -> None:\n        # Check for any widgets marked as 'dirty' (needs a repaint)\n        event.prevent_default()\n\n        try:\n            if not self.app._batch_count and self.is_current:\n                if (\n                    self._layout_required\n                    or self._scroll_required\n                    or self._repaint_required\n                    or self._recompose_required\n                    or self._dirty_widgets\n                ):\n                    self._update_timer.resume()\n                    return\n\n            await self._invoke_and_clear_callbacks()\n        finally:\n            if self._bindings_updated:\n                self._bindings_updated\n```\n# Function Signature: def focus_previous(self, selector: str | type[QueryType] = \"*\") -> Widget | None:\n# Natural Language Annotations: \"\"\"Focus the previous widget, optionally filtered by a CSS selector.\n\n        If no widget is currently focused, this will focus the first focusable widget.\n        If no focusable widget matches the given CSS selector, focus is set to `None`.\n\n        Args:\n            selector: CSS selector to filter\n                what nodes can be focused.\n\n        Returns:\n            Newly focused widget, or None for no focus. If the return\n                is not `None`, then it is guaranteed that the widget returned matches\n                the CSS selectors given in the argument.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef focus_previous(self, selector: str | type[", "  ```\ndef focus_previous(self, selector: str | type[", "  ```\ndef focus_previous(\n    self, selector: str |"]}, "textual-src/textual/screen.py-set_focus": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/screen.py:\n```\n\"\"\"\n\nThe `Screen` class is a special widget which represents the content in the terminal. See [Screens](/guide/screens/) for details.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom functools import partial\nfrom operator import attrgetter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generic,\n    Iterable,\n    Iterator,\n    Type,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport rich.repr\nfrom rich.console import RenderableType\nfrom rich.style import Style\n\nfrom . import constants, errors, events, messages\nfrom ._callback import invoke\nfrom ._compositor import Compositor, MapGeometry\nfrom ._context import active_message_pump, visible_screen_stack\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import CallbackType\nfrom .await_complete import AwaitComplete\nfrom .binding import ActiveBinding, Binding, _Bindings\nfrom .css.match import match\nfrom .css.parse import parse_selectors\nfrom .css.query import NoMatches, QueryType\nfrom .dom import DOMNode\nfrom .errors import NoWidget\nfrom .geometry import Offset, Region, Size\nfrom .reactive import Reactive\nfrom .renderables.background_screen import BackgroundScreen\nfrom .renderables.blank import Blank\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import Widget\nfrom .widgets import Tooltip\nfrom .widgets._toast import ToastRack\n\nif TYPE_CHECKING:\n    from typing_extensions import Final\n\n    from .command import Provider\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .message_pump import MessagePump\n\n# Screen updates will be batched so that they don't happen more often than 60 times per second:\nUPDATE_PERIOD: Final[float] = 1 / constants.MAX_FPS\n\nScreenResultType = TypeVar(\"ScreenResultType\")\n\"\"\"The result type of a screen.\"\"\"\n\nScreenResultCallbackType = Union[\n    Callable[[ScreenResultType], None], Callable[[ScreenResultType], Awaitable[None]]\n]\n\"\"\"Type of a screen result callback function.\"\"\"\n\n\n@rich.repr.auto\nclass ResultCallback(Generic[ScreenResultType]):\n    \"\"\"Holds the details of a callback.\"\"\"\n\n    def __init__(\n        self,\n        requester: MessagePump,\n        callback: ScreenResultCallbackType[ScreenResultType] | None,\n        future: asyncio.Future[ScreenResultType] | None = None,\n    ) -> None:\n        \"\"\"Initialise the result callback object.\n\n        Args:\n            requester: The object making a request for the callback.\n            callback: The callback function.\n            future: A Future to hold the result.\n        \"\"\"\n        self.requester = requester\n        \"\"\"The object in the DOM that requested the callback.\"\"\"\n        self.callback: ScreenResultCallbackType | None = callback\n        \"\"\"The callback function.\"\"\"\n        self.future = future\n        \"\"\"A future for the result\"\"\"\n\n    def __call__(self, result: ScreenResultType) -> None:\n        \"\"\"Call the callback, passing the given result.\n\n        Args:\n            result: The result to pass to the callback.\n\n        Note:\n            If the requested or the callback are `None` this will be a no-op.\n        \"\"\"\n        if self.future is not None:\n            self.future.set_result(result)\n        if self.requester is not None and self.callback is not None:\n            self.requester.call_next(self.callback, result)\n\n\n@rich.repr.auto\nclass Screen(Generic[ScreenResultType], Widget):\n    \"\"\"The base class for screens.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = None\n    \"\"\"A selector to determine what to focus automatically when the screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Set to `None` to inherit the value from the screen's app.\n    Set to `\"\"` to disable auto focus.\n    \"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. Rules here take priority over CSS_PATH.\n\n    Note:\n        This CSS applies to the whole app.\n    \"\"\"\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\n\n    Note:\n        This CSS applies to the whole app.\n    \"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    Screen {\n        layout: vertical;\n        overflow-y: auto;\n        background: $surface;        \n        \n        &:inline {\n            height: auto;\n            min-height: 1;\n            border-top: tall $background;\n            border-bottom: tall $background;\n        }\n    }\n    \"\"\"\n\n    TITLE: ClassVar[str | None] = None\n    \"\"\"A class variable to set the *default* title for the screen.\n\n    This overrides the app title.\n    To update the title while the screen is running,\n    you can set the [title][textual.screen.Screen.title] attribute.\n    \"\"\"\n\n    SUB_TITLE: ClassVar[str | None] = None\n    \"\"\"A class variable to set the *default* sub-title for the screen.\n\n    This overrides the app sub-title.\n    To update the sub-title while the screen is running,\n    you can set the [sub_title][textual.screen.Screen.sub_title] attribute.\n    \"\"\"\n\n    focused: Reactive[Widget | None] = Reactive(None)\n    \"\"\"The focused [widget][textual.widget.Widget] or `None` for no focus.\n    To set focus, do not update this value directly. Use [set_focus][textual.screen.Screen.set_focus] instead.\"\"\"\n    stack_updates: Reactive[int] = Reactive(0, repaint=False)\n    \"\"\"An integer that updates when the screen is resumed.\"\"\"\n    sub_title: Reactive[str | None] = Reactive(None, compute=False)\n    \"\"\"Screen sub-title to override [the app sub-title][textual.app.App.sub_title].\"\"\"\n    title: Reactive[str | None] = Reactive(None, compute=False)\n    \"\"\"Screen title to override [the app title][textual.app.App.title].\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = set()\n    \"\"\"Command providers used by the [command palette](/guide/command_palette), associated with the screen.\n\n    Should be a set of [`command.Provider`][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS = [\n        Binding(\"tab\", \"app.focus_next\", \"Focus Next\", show=False),\n        Binding(\"shift+tab\", \"app.focus_previous\", \"Focus Previous\", show=False),\n    ]\n\n    def __init__(\n        self,\n        name: str | None = None,\n        id: str | None = None,\n        classes: str | None = None,\n    ) -> None:\n        \"\"\"\n        Initialize the screen.\n\n        Args:\n            name: The name of the screen.\n            id: The ID of the screen in the DOM.\n            classes: The CSS classes for the screen.\n        \"\"\"\n        self._modal = False\n        super().__init__(name=name, id=id, classes=classes)\n        self._compositor = Compositor()\n        self._dirty_widgets: set[Widget] = set()\n        self.__update_timer: Timer | None = None\n        self._callbacks: list[tuple[CallbackType, MessagePump]] = []\n        self._result_callbacks: list[ResultCallback[ScreenResultType]] = []\n\n        self._tooltip_widget: Widget | None = None\n        self._tooltip_timer: Timer | None = None\n\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(self.CSS_PATH)\n                if self.CSS_PATH is not None\n                else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self.title = self.TITLE\n        self.sub_title = self.SUB_TITLE\n\n        self.screen_layout_refresh_signal: Signal[Screen] = Signal(\n            self, \"layout-refresh\"\n        )\n        \"\"\"The signal that is published when the screen's layout is refreshed.\"\"\"\n\n        self._bindings_updated = False\n        \"\"\"Indicates that a binding update was requested.\"\"\"\n        self.bindings_updated_signal: Signal[Screen] = Signal(self, \"bindings_updated\")\n        \"\"\"A signal published when the bindings have been updated\"\"\"\n\n    @property\n    def is_modal(self) -> bool:\n        \"\"\"Is the screen modal?\"\"\"\n        return self._modal\n\n    @property\n    def is_current(self) -> bool:\n        \"\"\"Is the screen current (i.e. visible to user)?\"\"\"\n        from .app import ScreenStackError\n\n        try:\n            return self.app.screen is self or self in self.app._background_screens\n        except ScreenStackError:\n            return False\n\n    @property\n    def _update_timer(self) -> Timer:\n        \"\"\"Timer used to perform updates.\"\"\"\n        if self.__update_timer is None:\n            self.__update_timer = self.set_interval(\n                UPDATE_PERIOD, self._on_timer_update, name=\"screen_update\", pause=True\n            )\n        return self.__update_timer\n\n    @property\n    def layers(self) -> tuple[str, ...]:\n        \"\"\"Layers from parent.\n\n        Returns:\n            Tuple of layer names.\n        \"\"\"\n        extras = [\"_loading\"]\n        if not self.app._disable_notifications:\n            extras.append(\"_toastrack\")\n        if not self.app._disable_tooltips:\n            extras.append(\"_tooltips\")\n        return (*super().layers, *extras)\n\n    def _watch_focused(self):\n        self.refresh_bindings()\n\n    def _watch_stack_updates(self):\n        self.refresh_bindings()\n\n    def refresh_bindings(self) -> None:\n        \"\"\"Call to request a refresh of bindings.\"\"\"\n        self.log.debug(\"Bindings updated\")\n        self._bindings_updated = True\n        self.check_idle()\n\n    @property\n    def _binding_chain(self) -> list[tuple[DOMNode, _Bindings]]:\n        \"\"\"Binding chain from this screen.\"\"\"\n        focused = self.focused\n        if focused is not None and focused.loading:\n            focused = None\n        namespace_bindings: list[tuple[DOMNode, _Bindings]]\n\n        if focused is None:\n            namespace_bindings = [\n                (self, self._bindings),\n                (self.app, self.app._bindings),\n            ]\n        else:\n            namespace_bindings = [\n                (node, node._bindings) for node in focused.ancestors_with_self\n            ]\n\n        return namespace_bindings\n\n    @property\n    def _modal_binding_chain(self) -> list[tuple[DOMNode, _Bindings]]:\n        \"\"\"The binding chain, ignoring everything before the last modal.\"\"\"\n        binding_chain = self._binding_chain\n        for index, (node, _bindings) in enumerate(binding_chain, 1):\n            if node.is_modal:\n                return binding_chain[:index]\n        return binding_chain\n\n    @property\n    def active_bindings(self) -> dict[str, ActiveBinding]:\n        \"\"\"Get currently active bindings for this screen.\n\n        If no widget is focused, then app-level bindings are returned.\n        If a widget is focused, then any bindings present in the screen and app are merged and returned.\n\n        This property may be used to inspect current bindings.\n\n        Returns:\n            A map of keys to a tuple containing (namespace, binding, enabled boolean).\n        \"\"\"\n\n        bindings_map: dict[str, ActiveBinding] = {}\n        for namespace, bindings in self._modal_binding_chain:\n            for key, binding in bindings.keys.items():\n                action_state = self.app._check_action_state(binding.action, namespace)\n                if action_state is False:\n                    continue\n                if existing_key_and_binding := bindings_map.get(key):\n                    _, existing_binding, _ = existing_key_and_binding\n                    if binding.priority and not existing_binding.priority:\n                        bindings_map[key] = ActiveBinding(\n                            namespace, binding, bool(action_state)\n                        )\n                else:\n                    bindings_map[key] = ActiveBinding(\n                        namespace, binding, bool(action_state)\n                    )\n\n        return bindings_map\n\n    @property\n    def is_active(self) -> bool:\n        \"\"\"Is the screen active (i.e. visible and top of the stack)?\"\"\"\n        try:\n            return self.app.screen is self\n        except Exception:\n            return False\n\n    def render(self) -> RenderableType:\n        \"\"\"Render method inherited from widget, used to render the screen's background.\n\n        Returns:\n            Background renderable.\n        \"\"\"\n        background = self.styles.background\n        try:\n            base_screen = visible_screen_stack.get().pop()\n        except IndexError:\n            base_screen = None\n\n        if base_screen is not None and background.a < 1:\n            # If background is translucent, render a background screen\n            return BackgroundScreen(base_screen, background)\n\n        if background.is_transparent:\n            # If the background is transparent, defer to App.render\n            return self.app.render()\n        # Render a screen of a solid color.\n        return Blank(background)\n\n    def get_offset(self, widget: Widget) -> Offset:\n        \"\"\"Get the absolute offset of a given Widget.\n\n        Args:\n            widget: A widget\n\n        Returns:\n            The widget's offset relative to the top left of the terminal.\n        \"\"\"\n        return self._compositor.get_offset(widget)\n\n    def get_widget_at(self, x: int, y: int) -> tuple[Widget, Region]:\n        \"\"\"Get the widget at a given coordinate.\n\n        Args:\n            x: X Coordinate.\n            y: Y Coordinate.\n\n        Returns:\n            Widget and screen region.\n\n        Raises:\n            NoWidget: If there is no widget under the screen coordinate.\n        \"\"\"\n        return self._compositor.get_widget_at(x, y)\n\n    def get_widgets_at(self, x: int, y: int) -> Iterable[tuple[Widget, Region]]:\n        \"\"\"Get all widgets under a given coordinate.\n\n        Args:\n            x: X coordinate.\n            y: Y coordinate.\n\n        Returns:\n            Sequence of (WIDGET, REGION) tuples.\n        \"\"\"\n        return self._compositor.get_widgets_at(x, y)\n\n    def get_focusable_widget_at(self, x: int, y: int) -> Widget | None:\n        \"\"\"Get the focusable widget under a given coordinate.\n\n        If the widget directly under the given coordinate is not focusable, then this method will check\n        if any of the ancestors are focusable. If no ancestors are focusable, then `None` will be returned.\n\n        Args:\n            x: X coordinate.\n            y: Y coordinate.\n\n        Returns:\n            A `Widget`, or `None` if there is no focusable widget underneath the coordinate.\n        \"\"\"\n        try:\n            widget, _region = self.get_widget_at(x, y)\n        except NoWidget:\n            return None\n\n        for node in widget.ancestors_with_self:\n            if isinstance(node, Widget) and node.focusable:\n                return node\n        return None\n\n    def get_style_at(self, x: int, y: int) -> Style:\n        \"\"\"Get the style under a given coordinate.\n\n        Args:\n            x: X Coordinate.\n            y: Y Coordinate.\n\n        Returns:\n            Rich Style object.\n        \"\"\"\n        return self._compositor.get_style_at(x, y)\n\n    def find_widget(self, widget: Widget) -> MapGeometry:\n        \"\"\"Get the screen region of a Widget.\n\n        Args:\n            widget: A Widget within the composition.\n\n        Returns:\n            Region relative to screen.\n\n        Raises:\n            NoWidget: If the widget could not be found in this screen.\n        \"\"\"\n        return self._compositor.find_widget(widget)\n\n    @property\n    def focus_chain(self) -> list[Widget]:\n        \"\"\"A list of widgets that may receive focus, in focus order.\"\"\"\n        # TODO: Calculating a focus chain is moderately expensive.\n        # Suspect we can move focus without calculating the entire thing again.\n\n        widgets: list[Widget] = []\n        add_widget = widgets.append\n        focus_sorter = attrgetter(\"_focus_sort_key\")\n        # We traverse the DOM and keep track of where we are at with a node stack.\n        # Additionally, we manually keep track of the visibility of the DOM\n        # instead of relying on the property `.visible` to save on DOM traversals.\n        # node_stack: list[tuple[iterator over node children, node visibility]]\n        node_stack: list[tuple[Iterator[Widget], bool]] = [\n            (\n                iter(sorted(self.displayed_children, key=focus_sorter)),\n                self.visible,\n            )\n        ]\n        pop = node_stack.pop\n        push = node_stack.append\n\n        while node_stack:\n            children_iterator, parent_visibility = node_stack[-1]\n            node = next(children_iterator, None)\n            if node is None:\n                pop()\n            else:\n                if node._check_disabled():\n                    continue\n                node_styles_visibility = node.styles.get_rule(\"visibility\")\n                node_is_visible = (\n                    node_styles_visibility != \"hidden\"\n                    if node_styles_visibility\n                    else parent_visibility  # Inherit visibility if the style is unset.\n                )\n                if node.is_container and node.allow_focus_children():\n                    sorted_displayed_children = sorted(\n                        node.displayed_children, key=focus_sorter\n                    )\n                    push((iter(sorted_displayed_children), node_is_visible))\n                # Same check as `if node.focusable`, but we cached inherited visibility\n                # and we also skipped disabled nodes altogether.\n                if node_is_visible and node.allow_focus():\n                    add_widget(node)\n\n        return widgets\n\n    def _move_focus(\n        self, direction: int = 0, selector: str | type[QueryType] = \"*\"\n    ) -> Widget | None:\n        \"\"\"Move the focus in the given direction.\n\n        If no widget is currently focused, this will focus the first focusable widget.\n        If no focusable widget matches the given CSS selector, focus is set to `None`.\n\n        Args:\n            direction: 1 to move forward, -1 to move backward, or\n                0 to keep the current focus.\n            selector: CSS selector to filter\n                what nodes can be focused.\n\n        Returns:\n            Newly focused widget, or None for no focus. If the return\n                is not `None`, then it is guaranteed that the widget returned matches\n                the CSS selectors given in the argument.\n        \"\"\"\n        # TODO: This shouldn't be required\n        self._compositor._full_map_invalidated = True\n        if not isinstance(selector, str):\n            selector = selector.__name__\n        selector_set = parse_selectors(selector)\n        focus_chain = self.focus_chain\n        filtered_focus_chain = (\n            node for node in focus_chain if match(selector_set, node)\n        )\n\n        if not focus_chain:\n            # Nothing focusable, so nothing to do\n            return self.focused\n        if self.focused is None:\n            # Nothing currently focused, so focus the first one.\n            to_focus = next(filtered_focus_chain, None)\n            self.set_focus(to_focus)\n            return self.focused\n\n        # Ensure focus will be in a node that matches the selectors.\n        if not direction and not match(selector_set, self.focused):\n            direction = 1\n\n        try:\n            # Find the index of the currently focused widget\n            current_index = focus_chain.index(self.focused)\n        except ValueError:\n            # Focused widget was removed in the interim, start again\n            self.set_focus(next(filtered_focus_chain, None))\n        else:\n            # Only move the focus if we are currently showing the focus\n            if direction:\n                to_focus = None\n                chain_length = len(focus_chain)\n                for step in range(1, len(focus_chain) + 1):\n                    node = focus_chain[\n                        (current_index + direction * step) % chain_length\n                    ]\n                    if match(selector_set, node):\n                        to_focus = node\n                        break\n                self.set_focus(to_focus)\n\n        return self.focused\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _reset_focus(\n        self, widget: Widget, avoiding: list[Widget] | None = None\n    ) -> None:\n        \"\"\"Reset the focus when a widget is removed\n\n        Args:\n            widget: A widget that is removed.\n            avoiding: Optional list of nodes to avoid.\n        \"\"\"\n\n        avoiding = avoiding or []\n\n        # Make this a NOP if we're being asked to deal with a widget that\n        # isn't actually the currently-focused widget.\n        if self.focused is not widget:\n            return\n\n        # Grab the list of widgets that we can set focus to.\n        focusable_widgets = self.focus_chain\n        if not focusable_widgets:\n            # If there's nothing to focus... give up now.\n            self.set_focus(None)\n            return\n\n        try:\n            # Find the location of the widget we're taking focus from, in\n            # the focus chain.\n            widget_index = focusable_widgets.index(widget)\n        except ValueError:\n            # widget is not in focusable widgets\n            # It may have been made invisible\n            # Move to a sibling if possible\n            for sibling in widget.visible_siblings:\n                if sibling not in avoiding and sibling.focusable:\n                    self.set_focus(sibling)\n                    break\n            else:\n                self.set_focus(None)\n            return\n\n        # Now go looking for something before it, that isn't about to be\n        # removed, and which can receive focus, and go focus that.\n        chosen: Widget | None = None\n        for candidate in reversed(\n            focusable_widgets[widget_index + 1 :] + focusable_widgets[:widget_index]\n        ):\n            if candidate not in avoiding:\n                chosen = candidate\n                break\n\n        # Go with what was found.\n        self.set_focus(chosen)\n\n    def _update_focus_styles(\n        self, focused: Widget | None = None, blurred: Widget | None = None\n    ) -> None:\n        \"\"\"Update CSS for focus changes.\n\n        Args:\n            focused: The widget that was focused.\n            blurred: The widget that was blurred.\n        \"\"\"\n        widgets: set[DOMNode] = set()\n\n        if focused is not None:\n            for widget in reversed(focused.ancestors_with_self):\n                if widget._has_focus_within:\n                    widgets.update(widget.walk_children(with_self=True))\n                    break\n        if blurred is not None:\n            for widget in reversed(blurred.ancestors_with_self):\n                if widget._has_focus_within:\n                    widgets.update(widget.walk_children(with_self=True))\n                    break\n        if widgets:\n            self.app.stylesheet.update_nodes(\n                [widget for widget in widgets if widget._has_focus_within], animate=True\n            )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _extend_compose(self, widgets: list[Widget]) -> None:\n        \"\"\"Insert Textual's own internal widgets.\n\n        Args:\n            widgets: The list of widgets to be composed.\n\n        This method adds the tooltip, if required, and also adds the\n        container for `Toast`s.\n        \"\"\"\n        if not self.app._disable_tooltips:\n            widgets.insert(0, Tooltip(id=\"textual-tooltip\"))\n        if not self.app._disable_notifications:\n            widgets.insert(0, ToastRack(id=\"textual-toastrack\"))\n\n    def _on_mount(self, event: events.Mount) -> None:\n        \"\"\"Set up the tooltip-clearing signal when we mount.\"\"\"\n        self.screen_layout_refresh_signal.subscribe(\n            self, self._maybe_clear_tooltip, immediate=True\n        )\n        self.refresh_bindings()\n\n    async def _on_idle(self, event: events.Idle) -> None:\n        # Check for any widgets marked as 'dirty' (needs a repaint)\n        event.prevent_default()\n\n        try:\n            if not self.app._batch_count and self.is_current:\n                if (\n                    self._layout_required\n                    or self._scroll_required\n                    or self._repaint_required\n                    or self._recompose_required\n                    or self._dirty_widgets\n                ):\n                    self._update_timer.resume()\n                    return\n\n            await self._invoke_and_clear_callbacks()\n        finally:\n            if self._bindings_updated:\n                self._bindings_updated\n```\n# Function Signature: def set_focus(self, widget: Widget | None, scroll_visible: bool = True) -> None:\n# Natural Language Annotations: \"\"\"Focus (or un-focus) a widget. A focused widget will receive key events first.\n\n        Args:\n            widget: Widget to focus, or None to un-focus.\n            scroll_visible: Scroll widget in to view.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfrom typing import Optional\n\nclass Screen:\n    def set", "  ```\ndef set_focus(self, widget: Widget | None,", "  ```\nimport asyncio\nfrom functools import partial\nfrom"]}, "textual-src/textual/strip.py-index_to_cell_position": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/strip.py:\n```\n\"\"\"\nA Strip contains the result of rendering a widget.\nSee [line API](/guide/widgets#line-api) for how to use Strips.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom itertools import chain\nfrom typing import Iterable, Iterator, Sequence\n\nimport rich.repr\nfrom rich.cells import cell_len, set_cell_size\nfrom rich.console import Console, ConsoleOptions, RenderResult\nfrom rich.measure import Measurement\nfrom rich.segment import Segment\nfrom rich.style import Style, StyleType\n\nfrom ._segment_tools import index_to_cell_position\nfrom .cache import FIFOCache\nfrom .color import Color\nfrom .constants import DEBUG\nfrom .filter import LineFilter\n\n\ndef get_line_length(segments: Iterable[Segment]) -> int:\n    \"\"\"Get the line length (total length of all segments).\n\n    Args:\n        segments: Iterable of segments.\n\n    Returns:\n        Length of line in cells.\n    \"\"\"\n    _cell_len = cell_len\n    return sum([_cell_len(text) for text, _, control in segments if not control])\n\n\nclass StripRenderable:\n    \"\"\"A renderable which renders a list of strips in to lines.\"\"\"\n\n    def __init__(self, strips: list[Strip], width: int | None = None) -> None:\n        self._strips = strips\n        self._width = width\n\n    def __rich_console__(\n        self, console: Console, options: ConsoleOptions\n    ) -> RenderResult:\n        new_line = Segment.line()\n        for strip in self._strips:\n            yield from strip\n            yield new_line\n\n    def __rich_measure__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> Measurement:\n        if self._width is None:\n            width = max(strip.cell_length for strip in self._strips)\n        else:\n            width = self._width\n        return Measurement(width, width)\n\n\n@rich.repr.auto\nclass Strip:\n    \"\"\"Represents a 'strip' (horizontal line) of a Textual Widget.\n\n    A Strip is like an immutable list of Segments. The immutability allows for effective caching.\n\n    Args:\n        segments: An iterable of segments.\n        cell_length: The cell length if known, or None to calculate on demand.\n    \"\"\"\n\n    __slots__ = [\n        \"_segments\",\n        \"_cell_length\",\n        \"_divide_cache\",\n        \"_crop_cache\",\n        \"_style_cache\",\n        \"_filter_cache\",\n        \"_render_cache\",\n        \"_line_length_cache\",\n        \"_crop_extend_cache\",\n        \"_link_ids\",\n    ]\n\n    def __init__(\n        self, segments: Iterable[Segment], cell_length: int | None = None\n    ) -> None:\n        self._segments = list(segments)\n        self._cell_length = cell_length\n        self._divide_cache: FIFOCache[tuple[int, ...], list[Strip]] = FIFOCache(4)\n        self._crop_cache: FIFOCache[tuple[int, int], Strip] = FIFOCache(16)\n        self._style_cache: FIFOCache[Style, Strip] = FIFOCache(16)\n        self._filter_cache: FIFOCache[tuple[LineFilter, Color], Strip] = FIFOCache(4)\n        self._line_length_cache: FIFOCache[\n            tuple[int, Style | None],\n            Strip,\n        ] = FIFOCache(4)\n        self._crop_extend_cache: FIFOCache[\n            tuple[int, int, Style | None],\n            Strip,\n        ] = FIFOCache(4)\n        self._render_cache: str | None = None\n        self._link_ids: set[str] | None = None\n\n        if DEBUG and cell_length is not None:\n            # If `cell_length` is incorrect, render will be fubar\n            assert get_line_length(self._segments) == cell_length\n\n    def __rich_repr__(self) -> rich.repr.Result:\n        yield self._segments\n        yield self.cell_length\n\n    @property\n    def text(self) -> str:\n        \"\"\"Segment text.\"\"\"\n        return \"\".join(segment.text for segment in self._segments)\n\n    @property\n    def link_ids(self) -> set[str]:\n        \"\"\"A set of the link ids in this Strip.\"\"\"\n        if self._link_ids is None:\n            self._link_ids = {\n                style._link_id for _, style, _ in self._segments if style is not None\n            }\n        return self._link_ids\n\n    @classmethod\n    def blank(cls, cell_length: int, style: StyleType | None = None) -> Strip:\n        \"\"\"Create a blank strip.\n\n        Args:\n            cell_length: Desired cell length.\n            style: Style of blank.\n\n        Returns:\n            New strip.\n        \"\"\"\n        segment_style = Style.parse(style) if isinstance(style, str) else style\n        return cls([Segment(\" \" * cell_length, segment_style)], cell_length)\n\n    @classmethod\n    def from_lines(\n        cls, lines: list[list[Segment]], cell_length: int | None = None\n    ) -> list[Strip]:\n        \"\"\"Convert lines (lists of segments) to a list of Strips.\n\n        Args:\n            lines: List of lines, where a line is a list of segments.\n            cell_length: Cell length of lines (must be same) or None if not known.\n\n        Returns:\n            List of strips.\n        \"\"\"\n        return [cls(segments, cell_length) for segments in lines]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @property\n    def cell_length(self) -> int:\n        \"\"\"Get the number of cells required to render this object.\"\"\"\n        # Done on demand and cached, as this is an O(n) operation\n        if self._cell_length is None:\n            self._cell_length = get_line_length(self._segments)\n        return self._cell_length\n\n    @classmethod\n    def join(cls, strips: Iterable[Strip | None]) -> Strip:\n        \"\"\"Join a number of strips in to one.\n\n        Args:\n            strips: An iterable of Strips.\n\n        Returns:\n            A new combined strip.\n        \"\"\"\n\n        segments: list[list[Segment]] = []\n        add_segments = segments.append\n        total_cell_length = 0\n        for strip in strips:\n            if strip is not None:\n                total_cell_length += strip.cell_length\n                add_segments(strip._segments)\n        strip = cls(chain.from_iterable(segments), total_cell_length)\n        return strip\n\n    def __bool__(self) -> bool:\n        return not not self._segments  # faster than bool(...)\n\n    def __iter__(self) -> Iterator[Segment]:\n        return iter(self._segments)\n\n    def __reversed__(self) -> Iterator[Segment]:\n        return reversed(self._segments)\n\n    def __len__(self) -> int:\n        return len(self._segments)\n\n    def __eq__(self, strip: object) -> bool:\n        return isinstance(strip, Strip) and (\n            self._segments == strip._segments and self.cell_length == strip.cell_length\n        )\n\n    def extend_cell_length(self, cell_length: int, style: Style | None = None) -> Strip:\n        \"\"\"Extend the cell length if it is less than the given value.\n\n        Args:\n            cell_length: Required minimum cell length.\n            style: Style for padding if the cell length is extended.\n\n        Returns:\n            A new Strip.\n        \"\"\"\n        if self.cell_length < cell_length:\n            missing_space = cell_length - self.cell_length\n            segments = self._segments + [Segment(\" \" * missing_space, style)]\n            return Strip(segments, cell_length)\n        else:\n            return self\n\n    def adjust_cell_length(self, cell_length: int, style: Style | None = None) -> Strip:\n        \"\"\"Adjust the cell length, possibly truncating or extending.\n\n        Args:\n            cell_length: New desired cell length.\n            style: Style when extending, or `None`.\n\n        Returns:\n            A new strip with the supplied cell length.\n        \"\"\"\n\n        cache_key = (cell_length, style)\n        cached_strip = self._line_length_cache.get(cache_key)\n        if cached_strip is not None:\n            return cached_strip\n\n        new_line: list[Segment]\n        line = self._segments\n        current_cell_length = self.cell_length\n\n        _Segment = Segment\n\n        if current_cell_length < cell_length:\n            # Cell length is larger, so pad with spaces.\n            new_line = line + [\n                _Segment(\" \" * (cell_length - current_cell_length), style)\n            ]\n            strip = Strip(new_line, cell_length)\n\n        elif current_cell_length > cell_length:\n            # Cell length is shorter so we need to truncate.\n            new_line = []\n            append = new_line.append\n            line_length = 0\n            for segment in line:\n                segment_length = segment.cell_length\n                if line_length + segment_length < cell_length:\n                    append(segment)\n                    line_length += segment_length\n                else:\n                    text, segment_style, _ = segment\n                    text = set_cell_size(text, cell_length - line_length)\n                    append(_Segment(text, segment_style))\n                    break\n            strip = Strip(new_line, cell_length)\n        else:\n            # Strip is already the required cell length, so return self.\n            strip = self\n\n        self._line_length_cache[cache_key] = strip\n\n        return strip\n\n    def simplify(self) -> Strip:\n        \"\"\"Simplify the segments (join segments with same style)\n\n        Returns:\n            New strip.\n        \"\"\"\n        line = Strip(\n            Segment.simplify(self._segments),\n            self._cell_length,\n        )\n        return line\n\n    def apply_filter(self, filter: LineFilter, background: Color) -> Strip:\n        \"\"\"Apply a filter to all segments in the strip.\n\n        Args:\n            filter: A line filter object.\n\n        Returns:\n            A new Strip.\n        \"\"\"\n        cached_strip = self._filter_cache.get((filter, background))\n        if cached_strip is None:\n            cached_strip = Strip(\n                filter.apply(self._segments, background), self._cell_length\n            )\n            self._filter_cache[(filter, background)] = cached_strip\n        return cached_strip\n\n    def style_links(self, link_id: str, link_style: Style) -> Strip:\n        \"\"\"Apply a style to Segments with the given link_id.\n\n        Args:\n            link_id: A link id.\n            link_style: Style to apply.\n\n        Returns:\n            New strip (or same Strip if no changes).\n        \"\"\"\n\n        _Segment = Segment\n        if link_id not in self.link_ids:\n            return self\n        segments = [\n            _Segment(\n                text,\n                (\n                    (style + link_style if style is not None else None)\n                    if (style and not style._null and style._link_id == link_id)\n                    else style\n                ),\n                control,\n            )\n            for text, style, control in self._segments\n        ]\n        return Strip(segments, self._cell_length)\n\n    def crop_extend(self, start: int, end: int, style: Style | None) -> Strip:\n        \"\"\"Crop between two points, extending the length if required.\n\n        Args:\n            start: Start offset of crop.\n            end: End offset of crop.\n            style: Style of additional padding.\n\n        Returns:\n            New cropped Strip.\n        \"\"\"\n        cache_key = (start, end, style)\n        cached_result = self._crop_extend_cache.get(cache_key)\n        if cached_result is not None:\n            return cached_result\n        strip = self.extend_cell_length(end, style).crop(start, end)\n        self._crop_extend_cache[cache_key] = strip\n        return strip\n\n    def crop(self, start: int, end: int | None = None) -> Strip:\n        \"\"\"Crop a strip between two cell positions.\n\n        Args:\n            start: The start cell position (inclusive).\n            end: The end cell position (exclusive).\n\n        Returns:\n            A new Strip.\n        \"\"\"\n        start = max(0, start)\n        end = self.cell_length if end is None else min(self.cell_length, end)\n        if start == 0 and end == self.cell_length:\n            return self\n        cache_key = (start, end)\n        cached = self._crop_cache.get(cache_key)\n        if cached is not None:\n            return cached\n        _cell_len = cell_len\n        pos = 0\n        output_segments: list[Segment] = []\n        add_segment = output_segments.append\n        iter_segments = iter(self._segments)\n        segment: Segment | None = None\n        if start >= self.cell_length:\n            strip = Strip([], 0)\n        else:\n            for segment in iter_segments:\n                end_pos = pos + _cell_len(segment.text)\n                if end_pos > start:\n                    segment = segment.split_cells(start - pos)[1]\n                    break\n                pos = end_pos\n\n            if end >= self.cell_length:\n                # The end crop is the end of the segments, so we can collect all remaining segments\n                if segment:\n                    add_segment(segment)\n                output_segments.extend(iter_segments)\n                strip = Strip(output_segments, self.cell_length - start)\n            else:\n                pos = start\n                while segment is not None:\n                    end_pos = pos + _cell_len(segment.text)\n                    if end_pos < end:\n                        add_segment(segment)\n                    else:\n                        add_segment(segment.split_cells(end - pos)[0])\n                        break\n                    pos = end_pos\n                    segment = next(iter_segments, None)\n                strip = Strip(output_segments, end - start)\n        self._crop_cache[cache_key] = strip\n        return strip\n\n    def divide(self, cuts: Iterable[int]) -> Sequence[Strip]:\n        \"\"\"Divide the strip in to multiple smaller strips by cutting at given (cell) indices.\n\n        Args:\n            cuts: An iterable of cell positions as ints.\n\n        Returns:\n            A new list of strips.\n        \"\"\"\n\n        pos = 0\n        cell_length = self.cell_length\n        cuts = [cut for cut in cuts if cut <= cell_length]\n        cache_key = tuple(cuts)\n        cached = self._divide_cache.get(cache_key)\n        if cached is not None:\n            return cached\n\n        strips: list[Strip]\n        if cuts == [cell_length]:\n            strips = [self]\n        else:\n            strips = []\n            add_strip = strips.append\n            for segments, cut in zip(Segment.divide(self._segments, cuts), cuts):\n                add_strip(Strip(segments, cut - pos))\n                pos = cut\n\n        self._divide_cache[cache_key] = strips\n        return strips\n\n    def apply_style(self, style: Style) -> Strip:\n        \"\"\"Apply a style to the Strip.\n\n        Args:\n            style: A Rich style.\n\n        Returns:\n            A new strip.\n        \"\"\"\n        cached = self._style_cache.get(style)\n        if cached is not None:\n            return cached\n        styled_strip = Strip(\n            Segment.apply_style(self._segments, style), self.cell_length\n        )\n        self._style_cache[style] = styled_strip\n        return styled_strip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def index_to_cell_position(self, index: int) -> int:\n# Natural Language Annotations: \"\"\"Given a character index, return the cell position of that character.\n        This is the sum of the cell lengths of all the characters *before* the character\n        at `index`.\n\n        Args:\n            index: The index to convert.\n\n        Returns:\n            The cell position of the character at `index`.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef index_to_cell_position(self, index:", "  ```\ndef index_to_cell_position(self, index:", "  ```\nimport typing\nfrom . import Strip\n\nclass IndexTo"]}, "textual-src/textual/strip.py-render": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/strip.py:\n```\n\"\"\"\nA Strip contains the result of rendering a widget.\nSee [line API](/guide/widgets#line-api) for how to use Strips.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom itertools import chain\nfrom typing import Iterable, Iterator, Sequence\n\nimport rich.repr\nfrom rich.cells import cell_len, set_cell_size\nfrom rich.console import Console, ConsoleOptions, RenderResult\nfrom rich.measure import Measurement\nfrom rich.segment import Segment\nfrom rich.style import Style, StyleType\n\nfrom ._segment_tools import index_to_cell_position\nfrom .cache import FIFOCache\nfrom .color import Color\nfrom .constants import DEBUG\nfrom .filter import LineFilter\n\n\ndef get_line_length(segments: Iterable[Segment]) -> int:\n    \"\"\"Get the line length (total length of all segments).\n\n    Args:\n        segments: Iterable of segments.\n\n    Returns:\n        Length of line in cells.\n    \"\"\"\n    _cell_len = cell_len\n    return sum([_cell_len(text) for text, _, control in segments if not control])\n\n\nclass StripRenderable:\n    \"\"\"A renderable which renders a list of strips in to lines.\"\"\"\n\n    def __init__(self, strips: list[Strip], width: int | None = None) -> None:\n        self._strips = strips\n        self._width = width\n\n    def __rich_console__(\n        self, console: Console, options: ConsoleOptions\n    ) -> RenderResult:\n        new_line = Segment.line()\n        for strip in self._strips:\n            yield from strip\n            yield new_line\n\n    def __rich_measure__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> Measurement:\n        if self._width is None:\n            width = max(strip.cell_length for strip in self._strips)\n        else:\n            width = self._width\n        return Measurement(width, width)\n\n\n@rich.repr.auto\nclass Strip:\n    \"\"\"Represents a 'strip' (horizontal line) of a Textual Widget.\n\n    A Strip is like an immutable list of Segments. The immutability allows for effective caching.\n\n    Args:\n        segments: An iterable of segments.\n        cell_length: The cell length if known, or None to calculate on demand.\n    \"\"\"\n\n    __slots__ = [\n        \"_segments\",\n        \"_cell_length\",\n        \"_divide_cache\",\n        \"_crop_cache\",\n        \"_style_cache\",\n        \"_filter_cache\",\n        \"_render_cache\",\n        \"_line_length_cache\",\n        \"_crop_extend_cache\",\n        \"_link_ids\",\n    ]\n\n    def __init__(\n        self, segments: Iterable[Segment], cell_length: int | None = None\n    ) -> None:\n        self._segments = list(segments)\n        self._cell_length = cell_length\n        self._divide_cache: FIFOCache[tuple[int, ...], list[Strip]] = FIFOCache(4)\n        self._crop_cache: FIFOCache[tuple[int, int], Strip] = FIFOCache(16)\n        self._style_cache: FIFOCache[Style, Strip] = FIFOCache(16)\n        self._filter_cache: FIFOCache[tuple[LineFilter, Color], Strip] = FIFOCache(4)\n        self._line_length_cache: FIFOCache[\n            tuple[int, Style | None],\n            Strip,\n        ] = FIFOCache(4)\n        self._crop_extend_cache: FIFOCache[\n            tuple[int, int, Style | None],\n            Strip,\n        ] = FIFOCache(4)\n        self._render_cache: str | None = None\n        self._link_ids: set[str] | None = None\n\n        if DEBUG and cell_length is not None:\n            # If `cell_length` is incorrect, render will be fubar\n            assert get_line_length(self._segments) == cell_length\n\n    def __rich_repr__(self) -> rich.repr.Result:\n        yield self._segments\n        yield self.cell_length\n\n    @property\n    def text(self) -> str:\n        \"\"\"Segment text.\"\"\"\n        return \"\".join(segment.text for segment in self._segments)\n\n    @property\n    def link_ids(self) -> set[str]:\n        \"\"\"A set of the link ids in this Strip.\"\"\"\n        if self._link_ids is None:\n            self._link_ids = {\n                style._link_id for _, style, _ in self._segments if style is not None\n            }\n        return self._link_ids\n\n    @classmethod\n    def blank(cls, cell_length: int, style: StyleType | None = None) -> Strip:\n        \"\"\"Create a blank strip.\n\n        Args:\n            cell_length: Desired cell length.\n            style: Style of blank.\n\n        Returns:\n            New strip.\n        \"\"\"\n        segment_style = Style.parse(style) if isinstance(style, str) else style\n        return cls([Segment(\" \" * cell_length, segment_style)], cell_length)\n\n    @classmethod\n    def from_lines(\n        cls, lines: list[list[Segment]], cell_length: int | None = None\n    ) -> list[Strip]:\n        \"\"\"Convert lines (lists of segments) to a list of Strips.\n\n        Args:\n            lines: List of lines, where a line is a list of segments.\n            cell_length: Cell length of lines (must be same) or None if not known.\n\n        Returns:\n            List of strips.\n        \"\"\"\n        return [cls(segments, cell_length) for segments in lines]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @property\n    def cell_length(self) -> int:\n        \"\"\"Get the number of cells required to render this object.\"\"\"\n        # Done on demand and cached, as this is an O(n) operation\n        if self._cell_length is None:\n            self._cell_length = get_line_length(self._segments)\n        return self._cell_length\n\n    @classmethod\n    def join(cls, strips: Iterable[Strip | None]) -> Strip:\n        \"\"\"Join a number of strips in to one.\n\n        Args:\n            strips: An iterable of Strips.\n\n        Returns:\n            A new combined strip.\n        \"\"\"\n\n        segments: list[list[Segment]] = []\n        add_segments = segments.append\n        total_cell_length = 0\n        for strip in strips:\n            if strip is not None:\n                total_cell_length += strip.cell_length\n                add_segments(strip._segments)\n        strip = cls(chain.from_iterable(segments), total_cell_length)\n        return strip\n\n    def __bool__(self) -> bool:\n        return not not self._segments  # faster than bool(...)\n\n    def __iter__(self) -> Iterator[Segment]:\n        return iter(self._segments)\n\n    def __reversed__(self) -> Iterator[Segment]:\n        return reversed(self._segments)\n\n    def __len__(self) -> int:\n        return len(self._segments)\n\n    def __eq__(self, strip: object) -> bool:\n        return isinstance(strip, Strip) and (\n            self._segments == strip._segments and self.cell_length == strip.cell_length\n        )\n\n    def extend_cell_length(self, cell_length: int, style: Style | None = None) -> Strip:\n        \"\"\"Extend the cell length if it is less than the given value.\n\n        Args:\n            cell_length: Required minimum cell length.\n            style: Style for padding if the cell length is extended.\n\n        Returns:\n            A new Strip.\n        \"\"\"\n        if self.cell_length < cell_length:\n            missing_space = cell_length - self.cell_length\n            segments = self._segments + [Segment(\" \" * missing_space, style)]\n            return Strip(segments, cell_length)\n        else:\n            return self\n\n    def adjust_cell_length(self, cell_length: int, style: Style | None = None) -> Strip:\n        \"\"\"Adjust the cell length, possibly truncating or extending.\n\n        Args:\n            cell_length: New desired cell length.\n            style: Style when extending, or `None`.\n\n        Returns:\n            A new strip with the supplied cell length.\n        \"\"\"\n\n        cache_key = (cell_length, style)\n        cached_strip = self._line_length_cache.get(cache_key)\n        if cached_strip is not None:\n            return cached_strip\n\n        new_line: list[Segment]\n        line = self._segments\n        current_cell_length = self.cell_length\n\n        _Segment = Segment\n\n        if current_cell_length < cell_length:\n            # Cell length is larger, so pad with spaces.\n            new_line = line + [\n                _Segment(\" \" * (cell_length - current_cell_length), style)\n            ]\n            strip = Strip(new_line, cell_length)\n\n        elif current_cell_length > cell_length:\n            # Cell length is shorter so we need to truncate.\n            new_line = []\n            append = new_line.append\n            line_length = 0\n            for segment in line:\n                segment_length = segment.cell_length\n                if line_length + segment_length < cell_length:\n                    append(segment)\n                    line_length += segment_length\n                else:\n                    text, segment_style, _ = segment\n                    text = set_cell_size(text, cell_length - line_length)\n                    append(_Segment(text, segment_style))\n                    break\n            strip = Strip(new_line, cell_length)\n        else:\n            # Strip is already the required cell length, so return self.\n            strip = self\n\n        self._line_length_cache[cache_key] = strip\n\n        return strip\n\n    def simplify(self) -> Strip:\n        \"\"\"Simplify the segments (join segments with same style)\n\n        Returns:\n            New strip.\n        \"\"\"\n        line = Strip(\n            Segment.simplify(self._segments),\n            self._cell_length,\n        )\n        return line\n\n    def apply_filter(self, filter: LineFilter, background: Color) -> Strip:\n        \"\"\"Apply a filter to all segments in the strip.\n\n        Args:\n            filter: A line filter object.\n\n        Returns:\n            A new Strip.\n        \"\"\"\n        cached_strip = self._filter_cache.get((filter, background))\n        if cached_strip is None:\n            cached_strip = Strip(\n                filter.apply(self._segments, background), self._cell_length\n            )\n            self._filter_cache[(filter, background)] = cached_strip\n        return cached_strip\n\n    def style_links(self, link_id: str, link_style: Style) -> Strip:\n        \"\"\"Apply a style to Segments with the given link_id.\n\n        Args:\n            link_id: A link id.\n            link_style: Style to apply.\n\n        Returns:\n            New strip (or same Strip if no changes).\n        \"\"\"\n\n        _Segment = Segment\n        if link_id not in self.link_ids:\n            return self\n        segments = [\n            _Segment(\n                text,\n                (\n                    (style + link_style if style is not None else None)\n                    if (style and not style._null and style._link_id == link_id)\n                    else style\n                ),\n                control,\n            )\n            for text, style, control in self._segments\n        ]\n        return Strip(segments, self._cell_length)\n\n    def crop_extend(self, start: int, end: int, style: Style | None) -> Strip:\n        \"\"\"Crop between two points, extending the length if required.\n\n        Args:\n            start: Start offset of crop.\n            end: End offset of crop.\n            style: Style of additional padding.\n\n        Returns:\n            New cropped Strip.\n        \"\"\"\n        cache_key = (start, end, style)\n        cached_result = self._crop_extend_cache.get(cache_key)\n        if cached_result is not None:\n            return cached_result\n        strip = self.extend_cell_length(end, style).crop(start, end)\n        self._crop_extend_cache[cache_key] = strip\n        return strip\n\n    def crop(self, start: int, end: int | None = None) -> Strip:\n        \"\"\"Crop a strip between two cell positions.\n\n        Args:\n            start: The start cell position (inclusive).\n            end: The end cell position (exclusive).\n\n        Returns:\n            A new Strip.\n        \"\"\"\n        start = max(0, start)\n        end = self.cell_length if end is None else min(self.cell_length, end)\n        if start == 0 and end == self.cell_length:\n            return self\n        cache_key = (start, end)\n        cached = self._crop_cache.get(cache_key)\n        if cached is not None:\n            return cached\n        _cell_len = cell_len\n        pos = 0\n        output_segments: list[Segment] = []\n        add_segment = output_segments.append\n        iter_segments = iter(self._segments)\n        segment: Segment | None = None\n        if start >= self.cell_length:\n            strip = Strip([], 0)\n        else:\n            for segment in iter_segments:\n                end_pos = pos + _cell_len(segment.text)\n                if end_pos > start:\n                    segment = segment.split_cells(start - pos)[1]\n                    break\n                pos = end_pos\n\n            if end >= self.cell_length:\n                # The end crop is the end of the segments, so we can collect all remaining segments\n                if segment:\n                    add_segment(segment)\n                output_segments.extend(iter_segments)\n                strip = Strip(output_segments, self.cell_length - start)\n            else:\n                pos = start\n                while segment is not None:\n                    end_pos = pos + _cell_len(segment.text)\n                    if end_pos < end:\n                        add_segment(segment)\n                    else:\n                        add_segment(segment.split_cells(end - pos)[0])\n                        break\n                    pos = end_pos\n                    segment = next(iter_segments, None)\n                strip = Strip(output_segments, end - start)\n        self._crop_cache[cache_key] = strip\n        return strip\n\n    def divide(self, cuts: Iterable[int]) -> Sequence[Strip]:\n        \"\"\"Divide the strip in to multiple smaller strips by cutting at given (cell) indices.\n\n        Args:\n            cuts: An iterable of cell positions as ints.\n\n        Returns:\n            A new list of strips.\n        \"\"\"\n\n        pos = 0\n        cell_length = self.cell_length\n        cuts = [cut for cut in cuts if cut <= cell_length]\n        cache_key = tuple(cuts)\n        cached = self._divide_cache.get(cache_key)\n        if cached is not None:\n            return cached\n\n        strips: list[Strip]\n        if cuts == [cell_length]:\n            strips = [self]\n        else:\n            strips = []\n            add_strip = strips.append\n            for segments, cut in zip(Segment.divide(self._segments, cuts), cuts):\n                add_strip(Strip(segments, cut - pos))\n                pos = cut\n\n        self._divide_cache[cache_key] = strips\n        return strips\n\n    def apply_style(self, style: Style) -> Strip:\n        \"\"\"Apply a style to the Strip.\n\n        Args:\n            style: A Rich style.\n\n        Returns:\n            A new strip.\n        \"\"\"\n        cached = self._style_cache.get(style)\n        if cached is not None:\n            return cached\n        styled_strip = Strip(\n            Segment.apply_style(self._segments, style), self.cell_length\n        )\n        self._style_cache[style] = styled_strip\n        return styled_strip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def render(self, console: Console) -> str:\n# Natural Language Annotations: \"\"\"Render the strip into terminal sequences.\n\n        Args:\n            console: Console instance.\n\n        Returns:\n            Rendered sequences.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef render(self, console: Console) -> str:\n", "  ```\ndef render(self, console: Console) -> str:\n", "  ```\ndef render(self, console: Console) -> str:\n"]}, "textual-src/textual/app.py-batch_update": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/app.py:\n```\n\"\"\"\n\nHere you will find the [App][textual.app.App] class, which is the base class for Textual apps.\n\nSee [app basics](/guide/app) for how to build Textual apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport inspect\nimport io\nimport os\nimport signal\nimport sys\nimport threading\nimport warnings\nfrom asyncio import Task, create_task\nfrom concurrent.futures import Future\nfrom contextlib import (\n    asynccontextmanager,\n    contextmanager,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom datetime import datetime\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    Sequence,\n    Type,\n    TypeVar,\n    overload,\n)\nfrom weakref import WeakKeyDictionary, WeakSet\n\nimport rich\nimport rich.repr\nfrom rich.console import Console, RenderableType\nfrom rich.control import Control\nfrom rich.protocol import is_renderable\nfrom rich.segment import Segment, Segments\nfrom rich.terminal_theme import TerminalTheme\n\nfrom . import (\n    Logger,\n    LogGroup,\n    LogVerbosity,\n    actions,\n    constants,\n    events,\n    log,\n    messages,\n    on,\n)\nfrom ._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction\nfrom ._ansi_sequences import SYNC_END, SYNC_START\nfrom ._ansi_theme import ALABASTER, MONOKAI\nfrom ._callback import invoke\nfrom ._compose import compose\nfrom ._compositor import CompositorUpdate\nfrom ._context import active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._event_broker import NoHandler, extract_handler_actions\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import AnimationLevel\nfrom ._wait import wait_for_idle\nfrom ._worker_manager import WorkerManager\nfrom .actions import ActionParseResult, SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .binding import Binding, BindingType, _Bindings\nfrom .command import CommandPalette, Provider\nfrom .css.errors import StylesheetError\nfrom .css.query import NoMatches\nfrom .css.stylesheet import RulesMap, Stylesheet\nfrom .design import ColorSystem\nfrom .dom import DOMNode, NoScreen\nfrom .driver import Driver\nfrom .errors import NoWidget\nfrom .features import FeatureFlag, parse_features\nfrom .file_monitor import FileMonitor\nfrom .filter import ANSIToTruecolor, DimFilter, Monochrome\nfrom .geometry import Offset, Region, Size\nfrom .keys import (\n    REPLACED_KEYS,\n    _character_to_key,\n    _get_key_display,\n    _get_unicode_name_from_key,\n)\nfrom .messages import CallbackType\nfrom .notifications import Notification, Notifications, Notify, SeverityLevel\nfrom .reactive import Reactive\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .screen import (\n    ActiveBinding,\n    Screen,\n    ScreenResultCallbackType,\n    ScreenResultType,\n    SystemModalScreen,\n)\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import AwaitMount, Widget\nfrom .widgets._toast import ToastRack\nfrom .worker import NoActiveWorker, get_current_worker\n\nif TYPE_CHECKING:\n    from textual_dev.client import DevtoolsClient\n    from typing_extensions import Coroutine, Literal, Self, TypeAlias\n\n    from ._system_commands import SystemCommands\n    from ._types import MessageTarget\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import WrongType  # type: ignore  # noqa: F401\n    from .filter import LineFilter\n    from .message import Message\n    from .pilot import Pilot\n    from .widget import MountError  # type: ignore  # noqa: F401\n\nWINDOWS = sys.platform == \"win32\"\n\n# asyncio will warn against resources not being cleared\nif constants.DEBUG:\n    warnings.simplefilter(\"always\", ResourceWarning)\n\n# `asyncio.get_event_loop()` is deprecated since Python 3.10:\n_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)\n\nLayoutDefinition = \"dict[str, Any]\"\n\nDEFAULT_COLORS = {\n    \"dark\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=True,\n    ),\n    \"light\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=False,\n    ),\n}\n\nComposeResult = Iterable[Widget]\nRenderResult = RenderableType\n\nAutopilotCallbackType: TypeAlias = (\n    \"Callable[[Pilot[object]], Coroutine[Any, Any, None]]\"\n)\n\"\"\"Signature for valid callbacks that can be used to control apps.\"\"\"\n\n\ndef get_system_commands() -> type[SystemCommands]:\n    \"\"\"Callable to lazy load the system commands.\n\n    Returns:\n        System commands class.\n    \"\"\"\n    from ._system_commands import SystemCommands\n\n    return SystemCommands\n\n\nclass AppError(Exception):\n    \"\"\"Base class for general App related exceptions.\"\"\"\n\n\nclass ActionError(Exception):\n    \"\"\"Base class for exceptions relating to actions.\"\"\"\n\n\nclass ScreenError(Exception):\n    \"\"\"Base class for exceptions that relate to screens.\"\"\"\n\n\nclass ScreenStackError(ScreenError):\n    \"\"\"Raised when trying to manipulate the screen stack incorrectly.\"\"\"\n\n\nclass ModeError(Exception):\n    \"\"\"Base class for exceptions related to modes.\"\"\"\n\n\nclass InvalidModeError(ModeError):\n    \"\"\"Raised if there is an issue with a mode name.\"\"\"\n\n\nclass UnknownModeError(ModeError):\n    \"\"\"Raised when attempting to use a mode that is not known.\"\"\"\n\n\nclass ActiveModeError(ModeError):\n    \"\"\"Raised when attempting to remove the currently active mode.\"\"\"\n\n\nclass SuspendNotSupported(Exception):\n    \"\"\"Raised if suspending the application is not supported.\n\n    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while\n    the application is running in an environment where this isn't supported.\n    \"\"\"\n\n\nReturnType = TypeVar(\"ReturnType\")\nCallThreadReturnType = TypeVar(\"CallThreadReturnType\")\n\n\nclass _NullFile:\n    \"\"\"A file-like where writes go nowhere.\"\"\"\n\n    def write(self, text: str) -> None:\n        pass\n\n    def flush(self) -> None:\n        pass\n\n    def isatty(self) -> bool:\n        return True\n\n\nclass _PrintCapture:\n    \"\"\"A file-like which captures output.\"\"\"\n\n    def __init__(self, app: App, stderr: bool = False) -> None:\n        \"\"\"\n\n        Args:\n            app: App instance.\n            stderr: Write from stderr.\n        \"\"\"\n        self.app = app\n        self.stderr = stderr\n\n    def write(self, text: str) -> None:\n        \"\"\"Called when writing to stdout or stderr.\n\n        Args:\n            text: Text that was \"printed\".\n        \"\"\"\n        self.app._print(text, stderr=self.stderr)\n\n    def flush(self) -> None:\n        \"\"\"Called when stdout or stderr was flushed.\"\"\"\n        self.app._flush(stderr=self.stderr)\n\n    def isatty(self) -> bool:\n        \"\"\"Pretend we're a terminal.\"\"\"\n        # TODO: should this be configurable?\n        return True\n\n    def fileno(self) -> int:\n        \"\"\"Return invalid fileno.\"\"\"\n        return -1\n\n\n@rich.repr.auto\nclass App(Generic[ReturnType], DOMNode):\n    \"\"\"The base class for Textual Applications.\"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,\n    and therefore takes priority in the event of a specificity clash.\"\"\"\n\n    # Default (the lowest priority) CSS\n    DEFAULT_CSS: ClassVar[str]\n    DEFAULT_CSS = \"\"\"\n    App {\n        background: $background;\n        color: $text;\n    }\n    *:disabled:can-focus {\n        opacity: 0.7;\n    }\n    \"\"\"\n\n    MODES: ClassVar[dict[str, str | Screen | Callable[[], Screen]]] = {}\n    \"\"\"Modes associated with the app and their base screens.\n\n    The base screen is the screen at the bottom of the mode stack. You can think of\n    it as the default screen for that stack.\n    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],\n    [`Screen`][textual.screen.Screen] instances, or callables that return screens.\n\n    Example:\n        ```py\n        class HelpScreen(Screen[None]):\n            ...\n\n        class MainAppScreen(Screen[None]):\n            ...\n\n        class MyApp(App[None]):\n            MODES = {\n                \"default\": \"main\",\n                \"help\": HelpScreen,\n            }\n\n            SCREENS = {\n                \"main\": MainAppScreen,\n            }\n\n            ...\n        ```\n    \"\"\"\n    SCREENS: ClassVar[dict[str, Screen[Any] | Callable[[], Screen[Any]]]] = {}\n    \"\"\"Screens associated with the app for the lifetime of the app.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = \"*\"\n    \"\"\"A selector to determine what to focus automatically when a screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Setting to `None` or `\"\"` disables auto focus.\n    \"\"\"\n\n    _BASE_PATH: str | None = None\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\"\"\"\n\n    TITLE: str | None = None\n    \"\"\"A class variable to set the *default* title for the application.\n\n    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.\n    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].\n    \"\"\"\n\n    SUB_TITLE: str | None = None\n    \"\"\"A class variable to set the default sub-title for the application.\n\n    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.\n    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True\n    \"\"\"Should the [command palette][textual.command.CommandPalette] be enabled for the application?\"\"\"\n\n    NOTIFICATION_TIMEOUT: ClassVar[float] = 5\n    \"\"\"Default number of seconds to show notifications before removing them.\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {\n        get_system_commands\n    }\n    \"\"\"Command providers used by the [command palette](/guide/command_palette).\n\n    Should be a set of [command.Provider][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS: ClassVar[list[BindingType]] = [\n        Binding(\"ctrl+c\", \"quit\", \"Quit\", show=False, priority=True),\n        Binding(\"ctrl+backslash\", \"command_palette\", show=False, priority=True),\n    ]\n\n    CLOSE_TIMEOUT: float | None = 5.0\n    \"\"\"Timeout waiting for widget's to close, or `None` for no timeout.\"\"\"\n\n    title: Reactive[str] = Reactive(\"\", compute=False)\n    sub_title: Reactive[str] = Reactive(\"\", compute=False)\n\n    dark: Reactive[bool] = Reactive(True, compute=False)\n    \"\"\"Use a dark theme if `True`, otherwise use a light theme.\n\n    Modify this attribute to switch between light and dark themes.\n\n    Example:\n        ```python\n        self.app.dark = not self.app.dark  # Toggle dark mode\n        ```\n    \"\"\"\n    app_focus = Reactive(True, compute=False)\n    \"\"\"Indicates if the app has focus.\n\n    When run in the terminal, the app always has focus. When run in the web, the app will\n    get focus when the terminal widget has focus.\n    \"\"\"\n\n    ansi_theme_dark = Reactive(MONOKAI, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in dark mode.\"\"\"\n\n    ansi_theme_light = Reactive(ALABASTER, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in light mode.\"\"\"\n\n    def __init__(\n        self,\n        driver_class: Type[Driver] | None = None,\n        css_path: CSSPathType | None = None,\n        watch_css: bool = False,\n    ):\n        \"\"\"Create an instance of an app.\n\n        Args:\n            driver_class: Driver class or `None` to auto-detect.\n                This will be used by some Textual tools.\n            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.\n                To load multiple CSS files, pass a list of strings or paths which\n                will be loaded in order.\n            watch_css: Reload CSS if the files changed. This is set automatically if\n                you are using `textual run` with the `dev` switch.\n\n        Raises:\n            CssPathError: When the supplied CSS path(s) are an unexpected type.\n        \"\"\"\n        self._start_time = perf_counter()\n        super().__init__()\n        self.features: frozenset[FeatureFlag] = parse_features(os.getenv(\"TEXTUAL\", \"\"))\n\n        ansi_theme = self.ansi_theme_dark if self.dark else self.ansi_theme_light\n        self._filters: list[LineFilter] = [ANSIToTruecolor(ansi_theme)]\n\n        environ = dict(os.environ)\n        no_color = environ.pop(\"NO_COLOR\", None)\n        if no_color is not None:\n            self._filters.append(Monochrome())\n\n        for filter_name in constants.FILTERS.split(\",\"):\n            filter = filter_name.lower().strip()\n            if filter == \"dim\":\n                self._filters.append(DimFilter())\n\n        self.console = Console(\n            color_system=constants.COLOR_SYSTEM,\n            file=_NullFile(),\n            markup=True,\n            highlight=False,\n            emoji=False,\n            legacy_windows=False,\n            _environ=environ,\n            force_terminal=True,\n            safe_box=False,\n            soft_wrap=False,\n        )\n        self._workers = WorkerManager(self)\n        self.error_console = Console(markup=False, highlight=False, stderr=True)\n        self.driver_class = driver_class or self.get_driver_class()\n        self._screen_stacks: dict[str, list[Screen[Any]]] = {\"_default\": []}\n        \"\"\"A stack of screens per mode.\"\"\"\n        self._current_mode: str = \"_default\"\n        \"\"\"The current mode the app is in.\"\"\"\n        self._sync_available = False\n\n        self.mouse_over: Widget | None = None\n        self.mouse_captured: Widget | None = None\n        self._driver: Driver | None = None\n        self._exit_renderables: list[RenderableType] = []\n\n        self._action_targets = {\"app\", \"screen\", \"focused\"}\n        self._animator = Animator(self)\n        self._animate = self._animator.bind(self)\n        self.mouse_position = Offset(0, 0)\n\n        self._mouse_down_widget: Widget | None = None\n        \"\"\"The widget that was most recently mouse downed (used to create click events).\"\"\"\n\n        self._previous_cursor_position = Offset(0, 0)\n        \"\"\"The previous cursor position\"\"\"\n\n        self.cursor_position = Offset(0, 0)\n        \"\"\"The position of the terminal cursor in screen-space.\n\n        This can be set by widgets and is useful for controlling the\n        positioning of OS IME and emoji popup menus.\"\"\"\n\n        self._exception: Exception | None = None\n        \"\"\"The unhandled exception which is leading to the app shutting down,\n        or None if the app is still running with no unhandled exceptions.\"\"\"\n\n        self._exception_event: asyncio.Event = asyncio.Event()\n        \"\"\"An event that will be set when the first exception is encountered.\"\"\"\n\n        self.title = (\n            self.TITLE if self.TITLE is not None else f\"{self.__class__.__name__}\"\n        )\n        \"\"\"The title for the application.\n\n        The initial value for `title` will be set to the `TITLE` class variable if it exists, or\n        the name of the app if it doesn't.\n\n        Assign a new value to this attribute to change the title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else \"\"\n        \"\"\"The sub-title for the application.\n\n        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or\n        an empty string if it doesn't.\n\n        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to\n        the file being worked on.\n\n        Assign a new value to this attribute to change the sub-title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE\n        \"\"\"A flag to say if the application should use the command palette.\n\n        If set to `False` any call to\n        [`action_command_palette`][textual.app.App.action_command_palette]\n        will be ignored.\n        \"\"\"\n\n        self._logger = Logger(self._log)\n\n        self._refresh_required = False\n\n        self.design = DEFAULT_COLORS\n\n        self._css_has_errors = False\n        self.stylesheet = Stylesheet(variables=self.get_css_variables())\n\n        css_path = css_path or self.CSS_PATH\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(css_path) if css_path is not None else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self._registry: WeakSet[DOMNode] = WeakSet()\n\n        # Sensitivity on X is double the sensitivity on Y to account for\n        # cells being twice as tall as wide\n        self.scroll_sensitivity_x: float = 4.0\n        \"\"\"Number of columns to scroll in the X direction with wheel or trackpad.\"\"\"\n        self.scroll_sensitivity_y: float = 2.0\n        \"\"\"Number of lines to scroll in the Y direction with wheel or trackpad.\"\"\"\n\n        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}\n        self._installed_screens.update(**self.SCREENS)\n\n        self._compose_stacks: list[list[Widget]] = []\n        self._composed: list[list[Widget]] = []\n        self._recompose_required = False\n\n        self.devtools: DevtoolsClient | None = None\n        self._devtools_redirector: StdoutRedirector | None = None\n        if \"devtools\" in self.features:\n            try:\n                from textual_dev.client import DevtoolsClient\n                from textual_dev.redirect_output import StdoutRedirector\n            except ImportError:\n                # Dev dependencies not installed\n                pass\n            else:\n                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)\n                self._devtools_redirector = StdoutRedirector(self.devtools)\n\n        self._loop: asyncio.AbstractEventLoop | None = None\n        self._return_value: ReturnType | None = None\n        \"\"\"Internal attribute used to set the return value for the app.\"\"\"\n        self._return_code: int | None = None\n        \"\"\"Internal attribute used to set the return code for the app.\"\"\"\n        self._exit = False\n        self._disable_tooltips = False\n        self._disable_notifications = False\n\n        self.css_monitor = (\n            FileMonitor(self.css_path, self._on_css_change)\n            if watch_css or self.debug\n            else None\n        )\n        self._screenshot: str | None = None\n        self._dom_lock = RLock()\n        self._dom_ready = False\n        self._batch_count = 0\n        self._notifications = Notifications()\n\n        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (\n            WeakKeyDictionary()\n        )\n        \"\"\"Registry of the MessageTargets which are capturing output at any given time.\"\"\"\n        self._capture_stdout = _PrintCapture(self, stderr=False)\n        \"\"\"File-like object capturing data written to stdout.\"\"\"\n        self._capture_stderr = _PrintCapture(self, stderr=True)\n        \"\"\"File-like object capturing data written to stderr.\"\"\"\n        self._original_stdout = sys.__stdout__\n        \"\"\"The original stdout stream (before redirection etc).\"\"\"\n        self._original_stderr = sys.__stderr__\n        \"\"\"The original stderr stream (before redirection etc).\"\"\"\n\n        self.app_suspend_signal: Signal[App] = Signal(self, \"app-suspend\")\n        \"\"\"The signal that is published when the app is suspended.\n\n        When [`App.suspend`][textual.app.App.suspend] is called this signal\n        will be [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work before the suspension takes place.\n        \"\"\"\n        self.app_resume_signal: Signal[App] = Signal(self, \"app-resume\")\n        \"\"\"The signal that is published when the app is resumed after a suspend.\n\n        When the app is resumed after a\n        [`App.suspend`][textual.app.App.suspend] call this signal will be\n        [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work after the app has resumed.\n        \"\"\"\n\n        self.set_class(self.dark, \"-dark-mode\")\n        self.set_class(not self.dark, \"-light-mode\")\n\n        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS\n        \"\"\"Determines what type of animations the app will display.\n\n        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].\n        \"\"\"\n\n        self._last_focused_on_app_blur: Widget | None = None\n        \"\"\"The widget that had focus when the last `AppBlur` happened.\n\n        This will be used to restore correct focus when an `AppFocus`\n        happens.\n        \"\"\"\n\n        # Size of previous inline update\n        self._previous_inline_height: int | None = None\n\n    def validate_title(self, title: Any) -> str:\n        \"\"\"Make sure the title is set to a string.\"\"\"\n        return str(title)\n\n    def validate_sub_title(self, sub_title: Any) -> str:\n        \"\"\"Make sure the sub-title is set to a string.\"\"\"\n        return str(sub_title)\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The [worker](/guide/workers/) manager.\n\n        Returns:\n            An object to manage workers.\n        \"\"\"\n        return self._workers\n\n    @property\n    def return_value(self) -> ReturnType | None:\n        \"\"\"The return value of the app, or `None` if it has not yet been set.\n\n        The return value is set when calling [exit][textual.app.App.exit].\n        \"\"\"\n        return self._return_value\n\n    @property\n    def return_code(self) -> int | None:\n        \"\"\"The return code with which the app exited.\n\n        Non-zero codes indicate errors.\n        A value of 1 means the app exited with a fatal error.\n        If the app wasn't exited yet, this will be `None`.\n\n        Example:\n            The return code can be used to exit the process via `sys.exit`.\n            ```py\n            my_app.run()\n            sys.exit(my_app.return_code)\n            ```\n        \"\"\"\n        return self._return\n```\n# Function Signature: def batch_update(self) -> Generator[None, None, None]:\n# Natural Language Annotations: \"\"\"A context manager to suspend all repaints until the end of the batch.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n@contextmanager\ndef batch_update(self) -> Gener", "  ```\nimport asyncio\nfrom weakref import WeakKeyDictionary", "  ```python\n@contextmanager\ndef batch_update(self) ->"]}, "textual-src/textual/app.py-call_from_thread": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/app.py:\n```\n\"\"\"\n\nHere you will find the [App][textual.app.App] class, which is the base class for Textual apps.\n\nSee [app basics](/guide/app) for how to build Textual apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport inspect\nimport io\nimport os\nimport signal\nimport sys\nimport threading\nimport warnings\nfrom asyncio import Task, create_task\nfrom concurrent.futures import Future\nfrom contextlib import (\n    asynccontextmanager,\n    contextmanager,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom datetime import datetime\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    Sequence,\n    Type,\n    TypeVar,\n    overload,\n)\nfrom weakref import WeakKeyDictionary, WeakSet\n\nimport rich\nimport rich.repr\nfrom rich.console import Console, RenderableType\nfrom rich.control import Control\nfrom rich.protocol import is_renderable\nfrom rich.segment import Segment, Segments\nfrom rich.terminal_theme import TerminalTheme\n\nfrom . import (\n    Logger,\n    LogGroup,\n    LogVerbosity,\n    actions,\n    constants,\n    events,\n    log,\n    messages,\n    on,\n)\nfrom ._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction\nfrom ._ansi_sequences import SYNC_END, SYNC_START\nfrom ._ansi_theme import ALABASTER, MONOKAI\nfrom ._callback import invoke\nfrom ._compose import compose\nfrom ._compositor import CompositorUpdate\nfrom ._context import active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._event_broker import NoHandler, extract_handler_actions\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import AnimationLevel\nfrom ._wait import wait_for_idle\nfrom ._worker_manager import WorkerManager\nfrom .actions import ActionParseResult, SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .binding import Binding, BindingType, _Bindings\nfrom .command import CommandPalette, Provider\nfrom .css.errors import StylesheetError\nfrom .css.query import NoMatches\nfrom .css.stylesheet import RulesMap, Stylesheet\nfrom .design import ColorSystem\nfrom .dom import DOMNode, NoScreen\nfrom .driver import Driver\nfrom .errors import NoWidget\nfrom .features import FeatureFlag, parse_features\nfrom .file_monitor import FileMonitor\nfrom .filter import ANSIToTruecolor, DimFilter, Monochrome\nfrom .geometry import Offset, Region, Size\nfrom .keys import (\n    REPLACED_KEYS,\n    _character_to_key,\n    _get_key_display,\n    _get_unicode_name_from_key,\n)\nfrom .messages import CallbackType\nfrom .notifications import Notification, Notifications, Notify, SeverityLevel\nfrom .reactive import Reactive\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .screen import (\n    ActiveBinding,\n    Screen,\n    ScreenResultCallbackType,\n    ScreenResultType,\n    SystemModalScreen,\n)\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import AwaitMount, Widget\nfrom .widgets._toast import ToastRack\nfrom .worker import NoActiveWorker, get_current_worker\n\nif TYPE_CHECKING:\n    from textual_dev.client import DevtoolsClient\n    from typing_extensions import Coroutine, Literal, Self, TypeAlias\n\n    from ._system_commands import SystemCommands\n    from ._types import MessageTarget\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import WrongType  # type: ignore  # noqa: F401\n    from .filter import LineFilter\n    from .message import Message\n    from .pilot import Pilot\n    from .widget import MountError  # type: ignore  # noqa: F401\n\nWINDOWS = sys.platform == \"win32\"\n\n# asyncio will warn against resources not being cleared\nif constants.DEBUG:\n    warnings.simplefilter(\"always\", ResourceWarning)\n\n# `asyncio.get_event_loop()` is deprecated since Python 3.10:\n_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)\n\nLayoutDefinition = \"dict[str, Any]\"\n\nDEFAULT_COLORS = {\n    \"dark\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=True,\n    ),\n    \"light\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=False,\n    ),\n}\n\nComposeResult = Iterable[Widget]\nRenderResult = RenderableType\n\nAutopilotCallbackType: TypeAlias = (\n    \"Callable[[Pilot[object]], Coroutine[Any, Any, None]]\"\n)\n\"\"\"Signature for valid callbacks that can be used to control apps.\"\"\"\n\n\ndef get_system_commands() -> type[SystemCommands]:\n    \"\"\"Callable to lazy load the system commands.\n\n    Returns:\n        System commands class.\n    \"\"\"\n    from ._system_commands import SystemCommands\n\n    return SystemCommands\n\n\nclass AppError(Exception):\n    \"\"\"Base class for general App related exceptions.\"\"\"\n\n\nclass ActionError(Exception):\n    \"\"\"Base class for exceptions relating to actions.\"\"\"\n\n\nclass ScreenError(Exception):\n    \"\"\"Base class for exceptions that relate to screens.\"\"\"\n\n\nclass ScreenStackError(ScreenError):\n    \"\"\"Raised when trying to manipulate the screen stack incorrectly.\"\"\"\n\n\nclass ModeError(Exception):\n    \"\"\"Base class for exceptions related to modes.\"\"\"\n\n\nclass InvalidModeError(ModeError):\n    \"\"\"Raised if there is an issue with a mode name.\"\"\"\n\n\nclass UnknownModeError(ModeError):\n    \"\"\"Raised when attempting to use a mode that is not known.\"\"\"\n\n\nclass ActiveModeError(ModeError):\n    \"\"\"Raised when attempting to remove the currently active mode.\"\"\"\n\n\nclass SuspendNotSupported(Exception):\n    \"\"\"Raised if suspending the application is not supported.\n\n    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while\n    the application is running in an environment where this isn't supported.\n    \"\"\"\n\n\nReturnType = TypeVar(\"ReturnType\")\nCallThreadReturnType = TypeVar(\"CallThreadReturnType\")\n\n\nclass _NullFile:\n    \"\"\"A file-like where writes go nowhere.\"\"\"\n\n    def write(self, text: str) -> None:\n        pass\n\n    def flush(self) -> None:\n        pass\n\n    def isatty(self) -> bool:\n        return True\n\n\nclass _PrintCapture:\n    \"\"\"A file-like which captures output.\"\"\"\n\n    def __init__(self, app: App, stderr: bool = False) -> None:\n        \"\"\"\n\n        Args:\n            app: App instance.\n            stderr: Write from stderr.\n        \"\"\"\n        self.app = app\n        self.stderr = stderr\n\n    def write(self, text: str) -> None:\n        \"\"\"Called when writing to stdout or stderr.\n\n        Args:\n            text: Text that was \"printed\".\n        \"\"\"\n        self.app._print(text, stderr=self.stderr)\n\n    def flush(self) -> None:\n        \"\"\"Called when stdout or stderr was flushed.\"\"\"\n        self.app._flush(stderr=self.stderr)\n\n    def isatty(self) -> bool:\n        \"\"\"Pretend we're a terminal.\"\"\"\n        # TODO: should this be configurable?\n        return True\n\n    def fileno(self) -> int:\n        \"\"\"Return invalid fileno.\"\"\"\n        return -1\n\n\n@rich.repr.auto\nclass App(Generic[ReturnType], DOMNode):\n    \"\"\"The base class for Textual Applications.\"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,\n    and therefore takes priority in the event of a specificity clash.\"\"\"\n\n    # Default (the lowest priority) CSS\n    DEFAULT_CSS: ClassVar[str]\n    DEFAULT_CSS = \"\"\"\n    App {\n        background: $background;\n        color: $text;\n    }\n    *:disabled:can-focus {\n        opacity: 0.7;\n    }\n    \"\"\"\n\n    MODES: ClassVar[dict[str, str | Screen | Callable[[], Screen]]] = {}\n    \"\"\"Modes associated with the app and their base screens.\n\n    The base screen is the screen at the bottom of the mode stack. You can think of\n    it as the default screen for that stack.\n    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],\n    [`Screen`][textual.screen.Screen] instances, or callables that return screens.\n\n    Example:\n        ```py\n        class HelpScreen(Screen[None]):\n            ...\n\n        class MainAppScreen(Screen[None]):\n            ...\n\n        class MyApp(App[None]):\n            MODES = {\n                \"default\": \"main\",\n                \"help\": HelpScreen,\n            }\n\n            SCREENS = {\n                \"main\": MainAppScreen,\n            }\n\n            ...\n        ```\n    \"\"\"\n    SCREENS: ClassVar[dict[str, Screen[Any] | Callable[[], Screen[Any]]]] = {}\n    \"\"\"Screens associated with the app for the lifetime of the app.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = \"*\"\n    \"\"\"A selector to determine what to focus automatically when a screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Setting to `None` or `\"\"` disables auto focus.\n    \"\"\"\n\n    _BASE_PATH: str | None = None\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\"\"\"\n\n    TITLE: str | None = None\n    \"\"\"A class variable to set the *default* title for the application.\n\n    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.\n    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].\n    \"\"\"\n\n    SUB_TITLE: str | None = None\n    \"\"\"A class variable to set the default sub-title for the application.\n\n    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.\n    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True\n    \"\"\"Should the [command palette][textual.command.CommandPalette] be enabled for the application?\"\"\"\n\n    NOTIFICATION_TIMEOUT: ClassVar[float] = 5\n    \"\"\"Default number of seconds to show notifications before removing them.\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {\n        get_system_commands\n    }\n    \"\"\"Command providers used by the [command palette](/guide/command_palette).\n\n    Should be a set of [command.Provider][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS: ClassVar[list[BindingType]] = [\n        Binding(\"ctrl+c\", \"quit\", \"Quit\", show=False, priority=True),\n        Binding(\"ctrl+backslash\", \"command_palette\", show=False, priority=True),\n    ]\n\n    CLOSE_TIMEOUT: float | None = 5.0\n    \"\"\"Timeout waiting for widget's to close, or `None` for no timeout.\"\"\"\n\n    title: Reactive[str] = Reactive(\"\", compute=False)\n    sub_title: Reactive[str] = Reactive(\"\", compute=False)\n\n    dark: Reactive[bool] = Reactive(True, compute=False)\n    \"\"\"Use a dark theme if `True`, otherwise use a light theme.\n\n    Modify this attribute to switch between light and dark themes.\n\n    Example:\n        ```python\n        self.app.dark = not self.app.dark  # Toggle dark mode\n        ```\n    \"\"\"\n    app_focus = Reactive(True, compute=False)\n    \"\"\"Indicates if the app has focus.\n\n    When run in the terminal, the app always has focus. When run in the web, the app will\n    get focus when the terminal widget has focus.\n    \"\"\"\n\n    ansi_theme_dark = Reactive(MONOKAI, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in dark mode.\"\"\"\n\n    ansi_theme_light = Reactive(ALABASTER, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in light mode.\"\"\"\n\n    def __init__(\n        self,\n        driver_class: Type[Driver] | None = None,\n        css_path: CSSPathType | None = None,\n        watch_css: bool = False,\n    ):\n        \"\"\"Create an instance of an app.\n\n        Args:\n            driver_class: Driver class or `None` to auto-detect.\n                This will be used by some Textual tools.\n            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.\n                To load multiple CSS files, pass a list of strings or paths which\n                will be loaded in order.\n            watch_css: Reload CSS if the files changed. This is set automatically if\n                you are using `textual run` with the `dev` switch.\n\n        Raises:\n            CssPathError: When the supplied CSS path(s) are an unexpected type.\n        \"\"\"\n        self._start_time = perf_counter()\n        super().__init__()\n        self.features: frozenset[FeatureFlag] = parse_features(os.getenv(\"TEXTUAL\", \"\"))\n\n        ansi_theme = self.ansi_theme_dark if self.dark else self.ansi_theme_light\n        self._filters: list[LineFilter] = [ANSIToTruecolor(ansi_theme)]\n\n        environ = dict(os.environ)\n        no_color = environ.pop(\"NO_COLOR\", None)\n        if no_color is not None:\n            self._filters.append(Monochrome())\n\n        for filter_name in constants.FILTERS.split(\",\"):\n            filter = filter_name.lower().strip()\n            if filter == \"dim\":\n                self._filters.append(DimFilter())\n\n        self.console = Console(\n            color_system=constants.COLOR_SYSTEM,\n            file=_NullFile(),\n            markup=True,\n            highlight=False,\n            emoji=False,\n            legacy_windows=False,\n            _environ=environ,\n            force_terminal=True,\n            safe_box=False,\n            soft_wrap=False,\n        )\n        self._workers = WorkerManager(self)\n        self.error_console = Console(markup=False, highlight=False, stderr=True)\n        self.driver_class = driver_class or self.get_driver_class()\n        self._screen_stacks: dict[str, list[Screen[Any]]] = {\"_default\": []}\n        \"\"\"A stack of screens per mode.\"\"\"\n        self._current_mode: str = \"_default\"\n        \"\"\"The current mode the app is in.\"\"\"\n        self._sync_available = False\n\n        self.mouse_over: Widget | None = None\n        self.mouse_captured: Widget | None = None\n        self._driver: Driver | None = None\n        self._exit_renderables: list[RenderableType] = []\n\n        self._action_targets = {\"app\", \"screen\", \"focused\"}\n        self._animator = Animator(self)\n        self._animate = self._animator.bind(self)\n        self.mouse_position = Offset(0, 0)\n\n        self._mouse_down_widget: Widget | None = None\n        \"\"\"The widget that was most recently mouse downed (used to create click events).\"\"\"\n\n        self._previous_cursor_position = Offset(0, 0)\n        \"\"\"The previous cursor position\"\"\"\n\n        self.cursor_position = Offset(0, 0)\n        \"\"\"The position of the terminal cursor in screen-space.\n\n        This can be set by widgets and is useful for controlling the\n        positioning of OS IME and emoji popup menus.\"\"\"\n\n        self._exception: Exception | None = None\n        \"\"\"The unhandled exception which is leading to the app shutting down,\n        or None if the app is still running with no unhandled exceptions.\"\"\"\n\n        self._exception_event: asyncio.Event = asyncio.Event()\n        \"\"\"An event that will be set when the first exception is encountered.\"\"\"\n\n        self.title = (\n            self.TITLE if self.TITLE is not None else f\"{self.__class__.__name__}\"\n        )\n        \"\"\"The title for the application.\n\n        The initial value for `title` will be set to the `TITLE` class variable if it exists, or\n        the name of the app if it doesn't.\n\n        Assign a new value to this attribute to change the title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else \"\"\n        \"\"\"The sub-title for the application.\n\n        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or\n        an empty string if it doesn't.\n\n        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to\n        the file being worked on.\n\n        Assign a new value to this attribute to change the sub-title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE\n        \"\"\"A flag to say if the application should use the command palette.\n\n        If set to `False` any call to\n        [`action_command_palette`][textual.app.App.action_command_palette]\n        will be ignored.\n        \"\"\"\n\n        self._logger = Logger(self._log)\n\n        self._refresh_required = False\n\n        self.design = DEFAULT_COLORS\n\n        self._css_has_errors = False\n        self.stylesheet = Stylesheet(variables=self.get_css_variables())\n\n        css_path = css_path or self.CSS_PATH\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(css_path) if css_path is not None else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self._registry: WeakSet[DOMNode] = WeakSet()\n\n        # Sensitivity on X is double the sensitivity on Y to account for\n        # cells being twice as tall as wide\n        self.scroll_sensitivity_x: float = 4.0\n        \"\"\"Number of columns to scroll in the X direction with wheel or trackpad.\"\"\"\n        self.scroll_sensitivity_y: float = 2.0\n        \"\"\"Number of lines to scroll in the Y direction with wheel or trackpad.\"\"\"\n\n        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}\n        self._installed_screens.update(**self.SCREENS)\n\n        self._compose_stacks: list[list[Widget]] = []\n        self._composed: list[list[Widget]] = []\n        self._recompose_required = False\n\n        self.devtools: DevtoolsClient | None = None\n        self._devtools_redirector: StdoutRedirector | None = None\n        if \"devtools\" in self.features:\n            try:\n                from textual_dev.client import DevtoolsClient\n                from textual_dev.redirect_output import StdoutRedirector\n            except ImportError:\n                # Dev dependencies not installed\n                pass\n            else:\n                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)\n                self._devtools_redirector = StdoutRedirector(self.devtools)\n\n        self._loop: asyncio.AbstractEventLoop | None = None\n        self._return_value: ReturnType | None = None\n        \"\"\"Internal attribute used to set the return value for the app.\"\"\"\n        self._return_code: int | None = None\n        \"\"\"Internal attribute used to set the return code for the app.\"\"\"\n        self._exit = False\n        self._disable_tooltips = False\n        self._disable_notifications = False\n\n        self.css_monitor = (\n            FileMonitor(self.css_path, self._on_css_change)\n            if watch_css or self.debug\n            else None\n        )\n        self._screenshot: str | None = None\n        self._dom_lock = RLock()\n        self._dom_ready = False\n        self._batch_count = 0\n        self._notifications = Notifications()\n\n        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (\n            WeakKeyDictionary()\n        )\n        \"\"\"Registry of the MessageTargets which are capturing output at any given time.\"\"\"\n        self._capture_stdout = _PrintCapture(self, stderr=False)\n        \"\"\"File-like object capturing data written to stdout.\"\"\"\n        self._capture_stderr = _PrintCapture(self, stderr=True)\n        \"\"\"File-like object capturing data written to stderr.\"\"\"\n        self._original_stdout = sys.__stdout__\n        \"\"\"The original stdout stream (before redirection etc).\"\"\"\n        self._original_stderr = sys.__stderr__\n        \"\"\"The original stderr stream (before redirection etc).\"\"\"\n\n        self.app_suspend_signal: Signal[App] = Signal(self, \"app-suspend\")\n        \"\"\"The signal that is published when the app is suspended.\n\n        When [`App.suspend`][textual.app.App.suspend] is called this signal\n        will be [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work before the suspension takes place.\n        \"\"\"\n        self.app_resume_signal: Signal[App] = Signal(self, \"app-resume\")\n        \"\"\"The signal that is published when the app is resumed after a suspend.\n\n        When the app is resumed after a\n        [`App.suspend`][textual.app.App.suspend] call this signal will be\n        [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work after the app has resumed.\n        \"\"\"\n\n        self.set_class(self.dark, \"-dark-mode\")\n        self.set_class(not self.dark, \"-light-mode\")\n\n        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS\n        \"\"\"Determines what type of animations the app will display.\n\n        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].\n        \"\"\"\n\n        self._last_focused_on_app_blur: Widget | None = None\n        \"\"\"The widget that had focus when the last `AppBlur` happened.\n\n        This will be used to restore correct focus when an `AppFocus`\n        happens.\n        \"\"\"\n\n        # Size of previous inline update\n        self._previous_inline_height: int | None = None\n\n    def validate_title(self, title: Any) -> str:\n        \"\"\"Make sure the title is set to a string.\"\"\"\n        return str(title)\n\n    def validate_sub_title(self, sub_title: Any) -> str:\n        \"\"\"Make sure the sub-title is set to a string.\"\"\"\n        return str(sub_title)\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The [worker](/guide/workers/) manager.\n\n        Returns:\n            An object to manage workers.\n        \"\"\"\n        return self._workers\n\n    @property\n    def return_value(self) -> ReturnType | None:\n        \"\"\"The return value of the app, or `None` if it has not yet been set.\n\n        The return value is set when calling [exit][textual.app.App.exit].\n        \"\"\"\n        return self._return_value\n\n    @property\n    def return_code(self) -> int | None:\n        \"\"\"The return code with which the app exited.\n\n        Non-zero codes indicate errors.\n        A value of 1 means the app exited with a fatal error.\n        If the app wasn't exited yet, this will be `None`.\n\n        Example:\n            The return code can be used to exit the process via `sys.exit`.\n            ```py\n            my_app.run()\n            sys.exit(my_app.return_code)\n            ```\n        \"\"\"\n        return self._return\n```\n# Function Signature: def call_from_thread(\n# Natural Language Annotations: \"\"\"Run a callable from another thread, and return the result.\n\n        Like asyncio apps in general, Textual apps are not thread-safe. If you call methods\n        or set attributes on Textual objects from a thread, you may get unpredictable results.\n\n        This method will ensure that your code runs within the correct context.\n\n        !!! tip\n\n            Consider using [post_message][textual.message_pump.MessagePump.post_message] which is also thread-safe.\n\n        Args:\n            callback: A callable to run.\n            *args: Arguments to the callback.\n            **kwargs: Keyword arguments for the callback.\n\n        Raises:\n            RuntimeError: If the app isn't running or if this method is called from the same\n                thread where the app is running.\n\n        Returns:\n            The result of the callback.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nimport asyncio\nimport inspect\nimport sys\nimport thread", "  ```\ndef call_from_thread(\n    callback: Callable", "  ```\ndef call_from_thread(\n    self,\n   "]}, "textual-src/textual/app.py-run_test": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/app.py:\n```\n\"\"\"\n\nHere you will find the [App][textual.app.App] class, which is the base class for Textual apps.\n\nSee [app basics](/guide/app) for how to build Textual apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport inspect\nimport io\nimport os\nimport signal\nimport sys\nimport threading\nimport warnings\nfrom asyncio import Task, create_task\nfrom concurrent.futures import Future\nfrom contextlib import (\n    asynccontextmanager,\n    contextmanager,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom datetime import datetime\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    Sequence,\n    Type,\n    TypeVar,\n    overload,\n)\nfrom weakref import WeakKeyDictionary, WeakSet\n\nimport rich\nimport rich.repr\nfrom rich.console import Console, RenderableType\nfrom rich.control import Control\nfrom rich.protocol import is_renderable\nfrom rich.segment import Segment, Segments\nfrom rich.terminal_theme import TerminalTheme\n\nfrom . import (\n    Logger,\n    LogGroup,\n    LogVerbosity,\n    actions,\n    constants,\n    events,\n    log,\n    messages,\n    on,\n)\nfrom ._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction\nfrom ._ansi_sequences import SYNC_END, SYNC_START\nfrom ._ansi_theme import ALABASTER, MONOKAI\nfrom ._callback import invoke\nfrom ._compose import compose\nfrom ._compositor import CompositorUpdate\nfrom ._context import active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._event_broker import NoHandler, extract_handler_actions\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import AnimationLevel\nfrom ._wait import wait_for_idle\nfrom ._worker_manager import WorkerManager\nfrom .actions import ActionParseResult, SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .binding import Binding, BindingType, _Bindings\nfrom .command import CommandPalette, Provider\nfrom .css.errors import StylesheetError\nfrom .css.query import NoMatches\nfrom .css.stylesheet import RulesMap, Stylesheet\nfrom .design import ColorSystem\nfrom .dom import DOMNode, NoScreen\nfrom .driver import Driver\nfrom .errors import NoWidget\nfrom .features import FeatureFlag, parse_features\nfrom .file_monitor import FileMonitor\nfrom .filter import ANSIToTruecolor, DimFilter, Monochrome\nfrom .geometry import Offset, Region, Size\nfrom .keys import (\n    REPLACED_KEYS,\n    _character_to_key,\n    _get_key_display,\n    _get_unicode_name_from_key,\n)\nfrom .messages import CallbackType\nfrom .notifications import Notification, Notifications, Notify, SeverityLevel\nfrom .reactive import Reactive\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .screen import (\n    ActiveBinding,\n    Screen,\n    ScreenResultCallbackType,\n    ScreenResultType,\n    SystemModalScreen,\n)\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import AwaitMount, Widget\nfrom .widgets._toast import ToastRack\nfrom .worker import NoActiveWorker, get_current_worker\n\nif TYPE_CHECKING:\n    from textual_dev.client import DevtoolsClient\n    from typing_extensions import Coroutine, Literal, Self, TypeAlias\n\n    from ._system_commands import SystemCommands\n    from ._types import MessageTarget\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import WrongType  # type: ignore  # noqa: F401\n    from .filter import LineFilter\n    from .message import Message\n    from .pilot import Pilot\n    from .widget import MountError  # type: ignore  # noqa: F401\n\nWINDOWS = sys.platform == \"win32\"\n\n# asyncio will warn against resources not being cleared\nif constants.DEBUG:\n    warnings.simplefilter(\"always\", ResourceWarning)\n\n# `asyncio.get_event_loop()` is deprecated since Python 3.10:\n_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)\n\nLayoutDefinition = \"dict[str, Any]\"\n\nDEFAULT_COLORS = {\n    \"dark\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=True,\n    ),\n    \"light\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=False,\n    ),\n}\n\nComposeResult = Iterable[Widget]\nRenderResult = RenderableType\n\nAutopilotCallbackType: TypeAlias = (\n    \"Callable[[Pilot[object]], Coroutine[Any, Any, None]]\"\n)\n\"\"\"Signature for valid callbacks that can be used to control apps.\"\"\"\n\n\ndef get_system_commands() -> type[SystemCommands]:\n    \"\"\"Callable to lazy load the system commands.\n\n    Returns:\n        System commands class.\n    \"\"\"\n    from ._system_commands import SystemCommands\n\n    return SystemCommands\n\n\nclass AppError(Exception):\n    \"\"\"Base class for general App related exceptions.\"\"\"\n\n\nclass ActionError(Exception):\n    \"\"\"Base class for exceptions relating to actions.\"\"\"\n\n\nclass ScreenError(Exception):\n    \"\"\"Base class for exceptions that relate to screens.\"\"\"\n\n\nclass ScreenStackError(ScreenError):\n    \"\"\"Raised when trying to manipulate the screen stack incorrectly.\"\"\"\n\n\nclass ModeError(Exception):\n    \"\"\"Base class for exceptions related to modes.\"\"\"\n\n\nclass InvalidModeError(ModeError):\n    \"\"\"Raised if there is an issue with a mode name.\"\"\"\n\n\nclass UnknownModeError(ModeError):\n    \"\"\"Raised when attempting to use a mode that is not known.\"\"\"\n\n\nclass ActiveModeError(ModeError):\n    \"\"\"Raised when attempting to remove the currently active mode.\"\"\"\n\n\nclass SuspendNotSupported(Exception):\n    \"\"\"Raised if suspending the application is not supported.\n\n    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while\n    the application is running in an environment where this isn't supported.\n    \"\"\"\n\n\nReturnType = TypeVar(\"ReturnType\")\nCallThreadReturnType = TypeVar(\"CallThreadReturnType\")\n\n\nclass _NullFile:\n    \"\"\"A file-like where writes go nowhere.\"\"\"\n\n    def write(self, text: str) -> None:\n        pass\n\n    def flush(self) -> None:\n        pass\n\n    def isatty(self) -> bool:\n        return True\n\n\nclass _PrintCapture:\n    \"\"\"A file-like which captures output.\"\"\"\n\n    def __init__(self, app: App, stderr: bool = False) -> None:\n        \"\"\"\n\n        Args:\n            app: App instance.\n            stderr: Write from stderr.\n        \"\"\"\n        self.app = app\n        self.stderr = stderr\n\n    def write(self, text: str) -> None:\n        \"\"\"Called when writing to stdout or stderr.\n\n        Args:\n            text: Text that was \"printed\".\n        \"\"\"\n        self.app._print(text, stderr=self.stderr)\n\n    def flush(self) -> None:\n        \"\"\"Called when stdout or stderr was flushed.\"\"\"\n        self.app._flush(stderr=self.stderr)\n\n    def isatty(self) -> bool:\n        \"\"\"Pretend we're a terminal.\"\"\"\n        # TODO: should this be configurable?\n        return True\n\n    def fileno(self) -> int:\n        \"\"\"Return invalid fileno.\"\"\"\n        return -1\n\n\n@rich.repr.auto\nclass App(Generic[ReturnType], DOMNode):\n    \"\"\"The base class for Textual Applications.\"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,\n    and therefore takes priority in the event of a specificity clash.\"\"\"\n\n    # Default (the lowest priority) CSS\n    DEFAULT_CSS: ClassVar[str]\n    DEFAULT_CSS = \"\"\"\n    App {\n        background: $background;\n        color: $text;\n    }\n    *:disabled:can-focus {\n        opacity: 0.7;\n    }\n    \"\"\"\n\n    MODES: ClassVar[dict[str, str | Screen | Callable[[], Screen]]] = {}\n    \"\"\"Modes associated with the app and their base screens.\n\n    The base screen is the screen at the bottom of the mode stack. You can think of\n    it as the default screen for that stack.\n    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],\n    [`Screen`][textual.screen.Screen] instances, or callables that return screens.\n\n    Example:\n        ```py\n        class HelpScreen(Screen[None]):\n            ...\n\n        class MainAppScreen(Screen[None]):\n            ...\n\n        class MyApp(App[None]):\n            MODES = {\n                \"default\": \"main\",\n                \"help\": HelpScreen,\n            }\n\n            SCREENS = {\n                \"main\": MainAppScreen,\n            }\n\n            ...\n        ```\n    \"\"\"\n    SCREENS: ClassVar[dict[str, Screen[Any] | Callable[[], Screen[Any]]]] = {}\n    \"\"\"Screens associated with the app for the lifetime of the app.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = \"*\"\n    \"\"\"A selector to determine what to focus automatically when a screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Setting to `None` or `\"\"` disables auto focus.\n    \"\"\"\n\n    _BASE_PATH: str | None = None\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\"\"\"\n\n    TITLE: str | None = None\n    \"\"\"A class variable to set the *default* title for the application.\n\n    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.\n    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].\n    \"\"\"\n\n    SUB_TITLE: str | None = None\n    \"\"\"A class variable to set the default sub-title for the application.\n\n    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.\n    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True\n    \"\"\"Should the [command palette][textual.command.CommandPalette] be enabled for the application?\"\"\"\n\n    NOTIFICATION_TIMEOUT: ClassVar[float] = 5\n    \"\"\"Default number of seconds to show notifications before removing them.\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {\n        get_system_commands\n    }\n    \"\"\"Command providers used by the [command palette](/guide/command_palette).\n\n    Should be a set of [command.Provider][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS: ClassVar[list[BindingType]] = [\n        Binding(\"ctrl+c\", \"quit\", \"Quit\", show=False, priority=True),\n        Binding(\"ctrl+backslash\", \"command_palette\", show=False, priority=True),\n    ]\n\n    CLOSE_TIMEOUT: float | None = 5.0\n    \"\"\"Timeout waiting for widget's to close, or `None` for no timeout.\"\"\"\n\n    title: Reactive[str] = Reactive(\"\", compute=False)\n    sub_title: Reactive[str] = Reactive(\"\", compute=False)\n\n    dark: Reactive[bool] = Reactive(True, compute=False)\n    \"\"\"Use a dark theme if `True`, otherwise use a light theme.\n\n    Modify this attribute to switch between light and dark themes.\n\n    Example:\n        ```python\n        self.app.dark = not self.app.dark  # Toggle dark mode\n        ```\n    \"\"\"\n    app_focus = Reactive(True, compute=False)\n    \"\"\"Indicates if the app has focus.\n\n    When run in the terminal, the app always has focus. When run in the web, the app will\n    get focus when the terminal widget has focus.\n    \"\"\"\n\n    ansi_theme_dark = Reactive(MONOKAI, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in dark mode.\"\"\"\n\n    ansi_theme_light = Reactive(ALABASTER, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in light mode.\"\"\"\n\n    def __init__(\n        self,\n        driver_class: Type[Driver] | None = None,\n        css_path: CSSPathType | None = None,\n        watch_css: bool = False,\n    ):\n        \"\"\"Create an instance of an app.\n\n        Args:\n            driver_class: Driver class or `None` to auto-detect.\n                This will be used by some Textual tools.\n            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.\n                To load multiple CSS files, pass a list of strings or paths which\n                will be loaded in order.\n            watch_css: Reload CSS if the files changed. This is set automatically if\n                you are using `textual run` with the `dev` switch.\n\n        Raises:\n            CssPathError: When the supplied CSS path(s) are an unexpected type.\n        \"\"\"\n        self._start_time = perf_counter()\n        super().__init__()\n        self.features: frozenset[FeatureFlag] = parse_features(os.getenv(\"TEXTUAL\", \"\"))\n\n        ansi_theme = self.ansi_theme_dark if self.dark else self.ansi_theme_light\n        self._filters: list[LineFilter] = [ANSIToTruecolor(ansi_theme)]\n\n        environ = dict(os.environ)\n        no_color = environ.pop(\"NO_COLOR\", None)\n        if no_color is not None:\n            self._filters.append(Monochrome())\n\n        for filter_name in constants.FILTERS.split(\",\"):\n            filter = filter_name.lower().strip()\n            if filter == \"dim\":\n                self._filters.append(DimFilter())\n\n        self.console = Console(\n            color_system=constants.COLOR_SYSTEM,\n            file=_NullFile(),\n            markup=True,\n            highlight=False,\n            emoji=False,\n            legacy_windows=False,\n            _environ=environ,\n            force_terminal=True,\n            safe_box=False,\n            soft_wrap=False,\n        )\n        self._workers = WorkerManager(self)\n        self.error_console = Console(markup=False, highlight=False, stderr=True)\n        self.driver_class = driver_class or self.get_driver_class()\n        self._screen_stacks: dict[str, list[Screen[Any]]] = {\"_default\": []}\n        \"\"\"A stack of screens per mode.\"\"\"\n        self._current_mode: str = \"_default\"\n        \"\"\"The current mode the app is in.\"\"\"\n        self._sync_available = False\n\n        self.mouse_over: Widget | None = None\n        self.mouse_captured: Widget | None = None\n        self._driver: Driver | None = None\n        self._exit_renderables: list[RenderableType] = []\n\n        self._action_targets = {\"app\", \"screen\", \"focused\"}\n        self._animator = Animator(self)\n        self._animate = self._animator.bind(self)\n        self.mouse_position = Offset(0, 0)\n\n        self._mouse_down_widget: Widget | None = None\n        \"\"\"The widget that was most recently mouse downed (used to create click events).\"\"\"\n\n        self._previous_cursor_position = Offset(0, 0)\n        \"\"\"The previous cursor position\"\"\"\n\n        self.cursor_position = Offset(0, 0)\n        \"\"\"The position of the terminal cursor in screen-space.\n\n        This can be set by widgets and is useful for controlling the\n        positioning of OS IME and emoji popup menus.\"\"\"\n\n        self._exception: Exception | None = None\n        \"\"\"The unhandled exception which is leading to the app shutting down,\n        or None if the app is still running with no unhandled exceptions.\"\"\"\n\n        self._exception_event: asyncio.Event = asyncio.Event()\n        \"\"\"An event that will be set when the first exception is encountered.\"\"\"\n\n        self.title = (\n            self.TITLE if self.TITLE is not None else f\"{self.__class__.__name__}\"\n        )\n        \"\"\"The title for the application.\n\n        The initial value for `title` will be set to the `TITLE` class variable if it exists, or\n        the name of the app if it doesn't.\n\n        Assign a new value to this attribute to change the title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else \"\"\n        \"\"\"The sub-title for the application.\n\n        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or\n        an empty string if it doesn't.\n\n        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to\n        the file being worked on.\n\n        Assign a new value to this attribute to change the sub-title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE\n        \"\"\"A flag to say if the application should use the command palette.\n\n        If set to `False` any call to\n        [`action_command_palette`][textual.app.App.action_command_palette]\n        will be ignored.\n        \"\"\"\n\n        self._logger = Logger(self._log)\n\n        self._refresh_required = False\n\n        self.design = DEFAULT_COLORS\n\n        self._css_has_errors = False\n        self.stylesheet = Stylesheet(variables=self.get_css_variables())\n\n        css_path = css_path or self.CSS_PATH\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(css_path) if css_path is not None else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self._registry: WeakSet[DOMNode] = WeakSet()\n\n        # Sensitivity on X is double the sensitivity on Y to account for\n        # cells being twice as tall as wide\n        self.scroll_sensitivity_x: float = 4.0\n        \"\"\"Number of columns to scroll in the X direction with wheel or trackpad.\"\"\"\n        self.scroll_sensitivity_y: float = 2.0\n        \"\"\"Number of lines to scroll in the Y direction with wheel or trackpad.\"\"\"\n\n        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}\n        self._installed_screens.update(**self.SCREENS)\n\n        self._compose_stacks: list[list[Widget]] = []\n        self._composed: list[list[Widget]] = []\n        self._recompose_required = False\n\n        self.devtools: DevtoolsClient | None = None\n        self._devtools_redirector: StdoutRedirector | None = None\n        if \"devtools\" in self.features:\n            try:\n                from textual_dev.client import DevtoolsClient\n                from textual_dev.redirect_output import StdoutRedirector\n            except ImportError:\n                # Dev dependencies not installed\n                pass\n            else:\n                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)\n                self._devtools_redirector = StdoutRedirector(self.devtools)\n\n        self._loop: asyncio.AbstractEventLoop | None = None\n        self._return_value: ReturnType | None = None\n        \"\"\"Internal attribute used to set the return value for the app.\"\"\"\n        self._return_code: int | None = None\n        \"\"\"Internal attribute used to set the return code for the app.\"\"\"\n        self._exit = False\n        self._disable_tooltips = False\n        self._disable_notifications = False\n\n        self.css_monitor = (\n            FileMonitor(self.css_path, self._on_css_change)\n            if watch_css or self.debug\n            else None\n        )\n        self._screenshot: str | None = None\n        self._dom_lock = RLock()\n        self._dom_ready = False\n        self._batch_count = 0\n        self._notifications = Notifications()\n\n        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (\n            WeakKeyDictionary()\n        )\n        \"\"\"Registry of the MessageTargets which are capturing output at any given time.\"\"\"\n        self._capture_stdout = _PrintCapture(self, stderr=False)\n        \"\"\"File-like object capturing data written to stdout.\"\"\"\n        self._capture_stderr = _PrintCapture(self, stderr=True)\n        \"\"\"File-like object capturing data written to stderr.\"\"\"\n        self._original_stdout = sys.__stdout__\n        \"\"\"The original stdout stream (before redirection etc).\"\"\"\n        self._original_stderr = sys.__stderr__\n        \"\"\"The original stderr stream (before redirection etc).\"\"\"\n\n        self.app_suspend_signal: Signal[App] = Signal(self, \"app-suspend\")\n        \"\"\"The signal that is published when the app is suspended.\n\n        When [`App.suspend`][textual.app.App.suspend] is called this signal\n        will be [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work before the suspension takes place.\n        \"\"\"\n        self.app_resume_signal: Signal[App] = Signal(self, \"app-resume\")\n        \"\"\"The signal that is published when the app is resumed after a suspend.\n\n        When the app is resumed after a\n        [`App.suspend`][textual.app.App.suspend] call this signal will be\n        [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work after the app has resumed.\n        \"\"\"\n\n        self.set_class(self.dark, \"-dark-mode\")\n        self.set_class(not self.dark, \"-light-mode\")\n\n        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS\n        \"\"\"Determines what type of animations the app will display.\n\n        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].\n        \"\"\"\n\n        self._last_focused_on_app_blur: Widget | None = None\n        \"\"\"The widget that had focus when the last `AppBlur` happened.\n\n        This will be used to restore correct focus when an `AppFocus`\n        happens.\n        \"\"\"\n\n        # Size of previous inline update\n        self._previous_inline_height: int | None = None\n\n    def validate_title(self, title: Any) -> str:\n        \"\"\"Make sure the title is set to a string.\"\"\"\n        return str(title)\n\n    def validate_sub_title(self, sub_title: Any) -> str:\n        \"\"\"Make sure the sub-title is set to a string.\"\"\"\n        return str(sub_title)\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The [worker](/guide/workers/) manager.\n\n        Returns:\n            An object to manage workers.\n        \"\"\"\n        return self._workers\n\n    @property\n    def return_value(self) -> ReturnType | None:\n        \"\"\"The return value of the app, or `None` if it has not yet been set.\n\n        The return value is set when calling [exit][textual.app.App.exit].\n        \"\"\"\n        return self._return_value\n\n    @property\n    def return_code(self) -> int | None:\n        \"\"\"The return code with which the app exited.\n\n        Non-zero codes indicate errors.\n        A value of 1 means the app exited with a fatal error.\n        If the app wasn't exited yet, this will be `None`.\n\n        Example:\n            The return code can be used to exit the process via `sys.exit`.\n            ```py\n            my_app.run()\n            sys.exit(my_app.return_code)\n            ```\n        \"\"\"\n        return self._return\n```\n# Function Signature: async def run_test(\n# Natural Language Annotations: \"\"\"An asynchronous context manager for testing apps.\n\n        !!! tip\n\n            See the guide for [testing](/guide/testing) Textual apps.\n\n        Use this to run your app in \"headless\" mode (no output) and drive the app via a [Pilot][textual.pilot.Pilot] object.\n\n        Example:\n\n            ```python\n            async with app.run_test() as pilot:\n                await pilot.click(\"#Button.ok\")\n                assert ...\n            ```\n\n        Args:\n            headless: Run in headless mode (no output or input).\n            size: Force terminal size to `(WIDTH, HEIGHT)`,\n                or None to auto-detect.\n            tooltips: Enable tooltips when testing.\n            notifications: Enable notifications when testing.\n            message_hook: An optional callback that will be called each time any message arrives at any\n                message pump in the app.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def run_test(\n    headless: bool =", "  ```python\nasync def run_test(\n    self,\n   ", "  ```\nasync def run_test(\n    self,\n    head"]}, "textual-src/textual/app.py-run": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/app.py:\n```\n\"\"\"\n\nHere you will find the [App][textual.app.App] class, which is the base class for Textual apps.\n\nSee [app basics](/guide/app) for how to build Textual apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport inspect\nimport io\nimport os\nimport signal\nimport sys\nimport threading\nimport warnings\nfrom asyncio import Task, create_task\nfrom concurrent.futures import Future\nfrom contextlib import (\n    asynccontextmanager,\n    contextmanager,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom datetime import datetime\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    Sequence,\n    Type,\n    TypeVar,\n    overload,\n)\nfrom weakref import WeakKeyDictionary, WeakSet\n\nimport rich\nimport rich.repr\nfrom rich.console import Console, RenderableType\nfrom rich.control import Control\nfrom rich.protocol import is_renderable\nfrom rich.segment import Segment, Segments\nfrom rich.terminal_theme import TerminalTheme\n\nfrom . import (\n    Logger,\n    LogGroup,\n    LogVerbosity,\n    actions,\n    constants,\n    events,\n    log,\n    messages,\n    on,\n)\nfrom ._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction\nfrom ._ansi_sequences import SYNC_END, SYNC_START\nfrom ._ansi_theme import ALABASTER, MONOKAI\nfrom ._callback import invoke\nfrom ._compose import compose\nfrom ._compositor import CompositorUpdate\nfrom ._context import active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._event_broker import NoHandler, extract_handler_actions\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import AnimationLevel\nfrom ._wait import wait_for_idle\nfrom ._worker_manager import WorkerManager\nfrom .actions import ActionParseResult, SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .binding import Binding, BindingType, _Bindings\nfrom .command import CommandPalette, Provider\nfrom .css.errors import StylesheetError\nfrom .css.query import NoMatches\nfrom .css.stylesheet import RulesMap, Stylesheet\nfrom .design import ColorSystem\nfrom .dom import DOMNode, NoScreen\nfrom .driver import Driver\nfrom .errors import NoWidget\nfrom .features import FeatureFlag, parse_features\nfrom .file_monitor import FileMonitor\nfrom .filter import ANSIToTruecolor, DimFilter, Monochrome\nfrom .geometry import Offset, Region, Size\nfrom .keys import (\n    REPLACED_KEYS,\n    _character_to_key,\n    _get_key_display,\n    _get_unicode_name_from_key,\n)\nfrom .messages import CallbackType\nfrom .notifications import Notification, Notifications, Notify, SeverityLevel\nfrom .reactive import Reactive\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .screen import (\n    ActiveBinding,\n    Screen,\n    ScreenResultCallbackType,\n    ScreenResultType,\n    SystemModalScreen,\n)\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import AwaitMount, Widget\nfrom .widgets._toast import ToastRack\nfrom .worker import NoActiveWorker, get_current_worker\n\nif TYPE_CHECKING:\n    from textual_dev.client import DevtoolsClient\n    from typing_extensions import Coroutine, Literal, Self, TypeAlias\n\n    from ._system_commands import SystemCommands\n    from ._types import MessageTarget\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import WrongType  # type: ignore  # noqa: F401\n    from .filter import LineFilter\n    from .message import Message\n    from .pilot import Pilot\n    from .widget import MountError  # type: ignore  # noqa: F401\n\nWINDOWS = sys.platform == \"win32\"\n\n# asyncio will warn against resources not being cleared\nif constants.DEBUG:\n    warnings.simplefilter(\"always\", ResourceWarning)\n\n# `asyncio.get_event_loop()` is deprecated since Python 3.10:\n_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)\n\nLayoutDefinition = \"dict[str, Any]\"\n\nDEFAULT_COLORS = {\n    \"dark\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=True,\n    ),\n    \"light\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=False,\n    ),\n}\n\nComposeResult = Iterable[Widget]\nRenderResult = RenderableType\n\nAutopilotCallbackType: TypeAlias = (\n    \"Callable[[Pilot[object]], Coroutine[Any, Any, None]]\"\n)\n\"\"\"Signature for valid callbacks that can be used to control apps.\"\"\"\n\n\ndef get_system_commands() -> type[SystemCommands]:\n    \"\"\"Callable to lazy load the system commands.\n\n    Returns:\n        System commands class.\n    \"\"\"\n    from ._system_commands import SystemCommands\n\n    return SystemCommands\n\n\nclass AppError(Exception):\n    \"\"\"Base class for general App related exceptions.\"\"\"\n\n\nclass ActionError(Exception):\n    \"\"\"Base class for exceptions relating to actions.\"\"\"\n\n\nclass ScreenError(Exception):\n    \"\"\"Base class for exceptions that relate to screens.\"\"\"\n\n\nclass ScreenStackError(ScreenError):\n    \"\"\"Raised when trying to manipulate the screen stack incorrectly.\"\"\"\n\n\nclass ModeError(Exception):\n    \"\"\"Base class for exceptions related to modes.\"\"\"\n\n\nclass InvalidModeError(ModeError):\n    \"\"\"Raised if there is an issue with a mode name.\"\"\"\n\n\nclass UnknownModeError(ModeError):\n    \"\"\"Raised when attempting to use a mode that is not known.\"\"\"\n\n\nclass ActiveModeError(ModeError):\n    \"\"\"Raised when attempting to remove the currently active mode.\"\"\"\n\n\nclass SuspendNotSupported(Exception):\n    \"\"\"Raised if suspending the application is not supported.\n\n    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while\n    the application is running in an environment where this isn't supported.\n    \"\"\"\n\n\nReturnType = TypeVar(\"ReturnType\")\nCallThreadReturnType = TypeVar(\"CallThreadReturnType\")\n\n\nclass _NullFile:\n    \"\"\"A file-like where writes go nowhere.\"\"\"\n\n    def write(self, text: str) -> None:\n        pass\n\n    def flush(self) -> None:\n        pass\n\n    def isatty(self) -> bool:\n        return True\n\n\nclass _PrintCapture:\n    \"\"\"A file-like which captures output.\"\"\"\n\n    def __init__(self, app: App, stderr: bool = False) -> None:\n        \"\"\"\n\n        Args:\n            app: App instance.\n            stderr: Write from stderr.\n        \"\"\"\n        self.app = app\n        self.stderr = stderr\n\n    def write(self, text: str) -> None:\n        \"\"\"Called when writing to stdout or stderr.\n\n        Args:\n            text: Text that was \"printed\".\n        \"\"\"\n        self.app._print(text, stderr=self.stderr)\n\n    def flush(self) -> None:\n        \"\"\"Called when stdout or stderr was flushed.\"\"\"\n        self.app._flush(stderr=self.stderr)\n\n    def isatty(self) -> bool:\n        \"\"\"Pretend we're a terminal.\"\"\"\n        # TODO: should this be configurable?\n        return True\n\n    def fileno(self) -> int:\n        \"\"\"Return invalid fileno.\"\"\"\n        return -1\n\n\n@rich.repr.auto\nclass App(Generic[ReturnType], DOMNode):\n    \"\"\"The base class for Textual Applications.\"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,\n    and therefore takes priority in the event of a specificity clash.\"\"\"\n\n    # Default (the lowest priority) CSS\n    DEFAULT_CSS: ClassVar[str]\n    DEFAULT_CSS = \"\"\"\n    App {\n        background: $background;\n        color: $text;\n    }\n    *:disabled:can-focus {\n        opacity: 0.7;\n    }\n    \"\"\"\n\n    MODES: ClassVar[dict[str, str | Screen | Callable[[], Screen]]] = {}\n    \"\"\"Modes associated with the app and their base screens.\n\n    The base screen is the screen at the bottom of the mode stack. You can think of\n    it as the default screen for that stack.\n    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],\n    [`Screen`][textual.screen.Screen] instances, or callables that return screens.\n\n    Example:\n        ```py\n        class HelpScreen(Screen[None]):\n            ...\n\n        class MainAppScreen(Screen[None]):\n            ...\n\n        class MyApp(App[None]):\n            MODES = {\n                \"default\": \"main\",\n                \"help\": HelpScreen,\n            }\n\n            SCREENS = {\n                \"main\": MainAppScreen,\n            }\n\n            ...\n        ```\n    \"\"\"\n    SCREENS: ClassVar[dict[str, Screen[Any] | Callable[[], Screen[Any]]]] = {}\n    \"\"\"Screens associated with the app for the lifetime of the app.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = \"*\"\n    \"\"\"A selector to determine what to focus automatically when a screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Setting to `None` or `\"\"` disables auto focus.\n    \"\"\"\n\n    _BASE_PATH: str | None = None\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\"\"\"\n\n    TITLE: str | None = None\n    \"\"\"A class variable to set the *default* title for the application.\n\n    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.\n    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].\n    \"\"\"\n\n    SUB_TITLE: str | None = None\n    \"\"\"A class variable to set the default sub-title for the application.\n\n    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.\n    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True\n    \"\"\"Should the [command palette][textual.command.CommandPalette] be enabled for the application?\"\"\"\n\n    NOTIFICATION_TIMEOUT: ClassVar[float] = 5\n    \"\"\"Default number of seconds to show notifications before removing them.\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {\n        get_system_commands\n    }\n    \"\"\"Command providers used by the [command palette](/guide/command_palette).\n\n    Should be a set of [command.Provider][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS: ClassVar[list[BindingType]] = [\n        Binding(\"ctrl+c\", \"quit\", \"Quit\", show=False, priority=True),\n        Binding(\"ctrl+backslash\", \"command_palette\", show=False, priority=True),\n    ]\n\n    CLOSE_TIMEOUT: float | None = 5.0\n    \"\"\"Timeout waiting for widget's to close, or `None` for no timeout.\"\"\"\n\n    title: Reactive[str] = Reactive(\"\", compute=False)\n    sub_title: Reactive[str] = Reactive(\"\", compute=False)\n\n    dark: Reactive[bool] = Reactive(True, compute=False)\n    \"\"\"Use a dark theme if `True`, otherwise use a light theme.\n\n    Modify this attribute to switch between light and dark themes.\n\n    Example:\n        ```python\n        self.app.dark = not self.app.dark  # Toggle dark mode\n        ```\n    \"\"\"\n    app_focus = Reactive(True, compute=False)\n    \"\"\"Indicates if the app has focus.\n\n    When run in the terminal, the app always has focus. When run in the web, the app will\n    get focus when the terminal widget has focus.\n    \"\"\"\n\n    ansi_theme_dark = Reactive(MONOKAI, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in dark mode.\"\"\"\n\n    ansi_theme_light = Reactive(ALABASTER, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in light mode.\"\"\"\n\n    def __init__(\n        self,\n        driver_class: Type[Driver] | None = None,\n        css_path: CSSPathType | None = None,\n        watch_css: bool = False,\n    ):\n        \"\"\"Create an instance of an app.\n\n        Args:\n            driver_class: Driver class or `None` to auto-detect.\n                This will be used by some Textual tools.\n            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.\n                To load multiple CSS files, pass a list of strings or paths which\n                will be loaded in order.\n            watch_css: Reload CSS if the files changed. This is set automatically if\n                you are using `textual run` with the `dev` switch.\n\n        Raises:\n            CssPathError: When the supplied CSS path(s) are an unexpected type.\n        \"\"\"\n        self._start_time = perf_counter()\n        super().__init__()\n        self.features: frozenset[FeatureFlag] = parse_features(os.getenv(\"TEXTUAL\", \"\"))\n\n        ansi_theme = self.ansi_theme_dark if self.dark else self.ansi_theme_light\n        self._filters: list[LineFilter] = [ANSIToTruecolor(ansi_theme)]\n\n        environ = dict(os.environ)\n        no_color = environ.pop(\"NO_COLOR\", None)\n        if no_color is not None:\n            self._filters.append(Monochrome())\n\n        for filter_name in constants.FILTERS.split(\",\"):\n            filter = filter_name.lower().strip()\n            if filter == \"dim\":\n                self._filters.append(DimFilter())\n\n        self.console = Console(\n            color_system=constants.COLOR_SYSTEM,\n            file=_NullFile(),\n            markup=True,\n            highlight=False,\n            emoji=False,\n            legacy_windows=False,\n            _environ=environ,\n            force_terminal=True,\n            safe_box=False,\n            soft_wrap=False,\n        )\n        self._workers = WorkerManager(self)\n        self.error_console = Console(markup=False, highlight=False, stderr=True)\n        self.driver_class = driver_class or self.get_driver_class()\n        self._screen_stacks: dict[str, list[Screen[Any]]] = {\"_default\": []}\n        \"\"\"A stack of screens per mode.\"\"\"\n        self._current_mode: str = \"_default\"\n        \"\"\"The current mode the app is in.\"\"\"\n        self._sync_available = False\n\n        self.mouse_over: Widget | None = None\n        self.mouse_captured: Widget | None = None\n        self._driver: Driver | None = None\n        self._exit_renderables: list[RenderableType] = []\n\n        self._action_targets = {\"app\", \"screen\", \"focused\"}\n        self._animator = Animator(self)\n        self._animate = self._animator.bind(self)\n        self.mouse_position = Offset(0, 0)\n\n        self._mouse_down_widget: Widget | None = None\n        \"\"\"The widget that was most recently mouse downed (used to create click events).\"\"\"\n\n        self._previous_cursor_position = Offset(0, 0)\n        \"\"\"The previous cursor position\"\"\"\n\n        self.cursor_position = Offset(0, 0)\n        \"\"\"The position of the terminal cursor in screen-space.\n\n        This can be set by widgets and is useful for controlling the\n        positioning of OS IME and emoji popup menus.\"\"\"\n\n        self._exception: Exception | None = None\n        \"\"\"The unhandled exception which is leading to the app shutting down,\n        or None if the app is still running with no unhandled exceptions.\"\"\"\n\n        self._exception_event: asyncio.Event = asyncio.Event()\n        \"\"\"An event that will be set when the first exception is encountered.\"\"\"\n\n        self.title = (\n            self.TITLE if self.TITLE is not None else f\"{self.__class__.__name__}\"\n        )\n        \"\"\"The title for the application.\n\n        The initial value for `title` will be set to the `TITLE` class variable if it exists, or\n        the name of the app if it doesn't.\n\n        Assign a new value to this attribute to change the title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else \"\"\n        \"\"\"The sub-title for the application.\n\n        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or\n        an empty string if it doesn't.\n\n        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to\n        the file being worked on.\n\n        Assign a new value to this attribute to change the sub-title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE\n        \"\"\"A flag to say if the application should use the command palette.\n\n        If set to `False` any call to\n        [`action_command_palette`][textual.app.App.action_command_palette]\n        will be ignored.\n        \"\"\"\n\n        self._logger = Logger(self._log)\n\n        self._refresh_required = False\n\n        self.design = DEFAULT_COLORS\n\n        self._css_has_errors = False\n        self.stylesheet = Stylesheet(variables=self.get_css_variables())\n\n        css_path = css_path or self.CSS_PATH\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(css_path) if css_path is not None else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self._registry: WeakSet[DOMNode] = WeakSet()\n\n        # Sensitivity on X is double the sensitivity on Y to account for\n        # cells being twice as tall as wide\n        self.scroll_sensitivity_x: float = 4.0\n        \"\"\"Number of columns to scroll in the X direction with wheel or trackpad.\"\"\"\n        self.scroll_sensitivity_y: float = 2.0\n        \"\"\"Number of lines to scroll in the Y direction with wheel or trackpad.\"\"\"\n\n        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}\n        self._installed_screens.update(**self.SCREENS)\n\n        self._compose_stacks: list[list[Widget]] = []\n        self._composed: list[list[Widget]] = []\n        self._recompose_required = False\n\n        self.devtools: DevtoolsClient | None = None\n        self._devtools_redirector: StdoutRedirector | None = None\n        if \"devtools\" in self.features:\n            try:\n                from textual_dev.client import DevtoolsClient\n                from textual_dev.redirect_output import StdoutRedirector\n            except ImportError:\n                # Dev dependencies not installed\n                pass\n            else:\n                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)\n                self._devtools_redirector = StdoutRedirector(self.devtools)\n\n        self._loop: asyncio.AbstractEventLoop | None = None\n        self._return_value: ReturnType | None = None\n        \"\"\"Internal attribute used to set the return value for the app.\"\"\"\n        self._return_code: int | None = None\n        \"\"\"Internal attribute used to set the return code for the app.\"\"\"\n        self._exit = False\n        self._disable_tooltips = False\n        self._disable_notifications = False\n\n        self.css_monitor = (\n            FileMonitor(self.css_path, self._on_css_change)\n            if watch_css or self.debug\n            else None\n        )\n        self._screenshot: str | None = None\n        self._dom_lock = RLock()\n        self._dom_ready = False\n        self._batch_count = 0\n        self._notifications = Notifications()\n\n        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (\n            WeakKeyDictionary()\n        )\n        \"\"\"Registry of the MessageTargets which are capturing output at any given time.\"\"\"\n        self._capture_stdout = _PrintCapture(self, stderr=False)\n        \"\"\"File-like object capturing data written to stdout.\"\"\"\n        self._capture_stderr = _PrintCapture(self, stderr=True)\n        \"\"\"File-like object capturing data written to stderr.\"\"\"\n        self._original_stdout = sys.__stdout__\n        \"\"\"The original stdout stream (before redirection etc).\"\"\"\n        self._original_stderr = sys.__stderr__\n        \"\"\"The original stderr stream (before redirection etc).\"\"\"\n\n        self.app_suspend_signal: Signal[App] = Signal(self, \"app-suspend\")\n        \"\"\"The signal that is published when the app is suspended.\n\n        When [`App.suspend`][textual.app.App.suspend] is called this signal\n        will be [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work before the suspension takes place.\n        \"\"\"\n        self.app_resume_signal: Signal[App] = Signal(self, \"app-resume\")\n        \"\"\"The signal that is published when the app is resumed after a suspend.\n\n        When the app is resumed after a\n        [`App.suspend`][textual.app.App.suspend] call this signal will be\n        [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work after the app has resumed.\n        \"\"\"\n\n        self.set_class(self.dark, \"-dark-mode\")\n        self.set_class(not self.dark, \"-light-mode\")\n\n        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS\n        \"\"\"Determines what type of animations the app will display.\n\n        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].\n        \"\"\"\n\n        self._last_focused_on_app_blur: Widget | None = None\n        \"\"\"The widget that had focus when the last `AppBlur` happened.\n\n        This will be used to restore correct focus when an `AppFocus`\n        happens.\n        \"\"\"\n\n        # Size of previous inline update\n        self._previous_inline_height: int | None = None\n\n    def validate_title(self, title: Any) -> str:\n        \"\"\"Make sure the title is set to a string.\"\"\"\n        return str(title)\n\n    def validate_sub_title(self, sub_title: Any) -> str:\n        \"\"\"Make sure the sub-title is set to a string.\"\"\"\n        return str(sub_title)\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The [worker](/guide/workers/) manager.\n\n        Returns:\n            An object to manage workers.\n        \"\"\"\n        return self._workers\n\n    @property\n    def return_value(self) -> ReturnType | None:\n        \"\"\"The return value of the app, or `None` if it has not yet been set.\n\n        The return value is set when calling [exit][textual.app.App.exit].\n        \"\"\"\n        return self._return_value\n\n    @property\n    def return_code(self) -> int | None:\n        \"\"\"The return code with which the app exited.\n\n        Non-zero codes indicate errors.\n        A value of 1 means the app exited with a fatal error.\n        If the app wasn't exited yet, this will be `None`.\n\n        Example:\n            The return code can be used to exit the process via `sys.exit`.\n            ```py\n            my_app.run()\n            sys.exit(my_app.return_code)\n            ```\n        \"\"\"\n        return self._return\n```\n# Function Signature: def run(\n# Natural Language Annotations: \"\"\"Run the app.\n\n        Args:\n            headless: Run in headless mode (no output).\n            inline: Run the app inline (under the prompt).\n            inline_no_clear: Don't clear the app output when exiting an inline app.\n            mouse: Enable mouse support.\n            size: Force terminal size to `(WIDTH, HEIGHT)`,\n                or None to auto-detect.\n            auto_pilot: An auto pilot coroutine.\n\n        Returns:\n            App return value.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nrun(\n    headless: bool = False,\n   ", "  ```\ndef run(\n    self,\n    headless: bool", "  ```python\ndef run(\n    headless: bool = False,"]}, "textual-src/textual/app.py-switch_mode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/app.py:\n```\n\"\"\"\n\nHere you will find the [App][textual.app.App] class, which is the base class for Textual apps.\n\nSee [app basics](/guide/app) for how to build Textual apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport inspect\nimport io\nimport os\nimport signal\nimport sys\nimport threading\nimport warnings\nfrom asyncio import Task, create_task\nfrom concurrent.futures import Future\nfrom contextlib import (\n    asynccontextmanager,\n    contextmanager,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom datetime import datetime\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    Sequence,\n    Type,\n    TypeVar,\n    overload,\n)\nfrom weakref import WeakKeyDictionary, WeakSet\n\nimport rich\nimport rich.repr\nfrom rich.console import Console, RenderableType\nfrom rich.control import Control\nfrom rich.protocol import is_renderable\nfrom rich.segment import Segment, Segments\nfrom rich.terminal_theme import TerminalTheme\n\nfrom . import (\n    Logger,\n    LogGroup,\n    LogVerbosity,\n    actions,\n    constants,\n    events,\n    log,\n    messages,\n    on,\n)\nfrom ._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction\nfrom ._ansi_sequences import SYNC_END, SYNC_START\nfrom ._ansi_theme import ALABASTER, MONOKAI\nfrom ._callback import invoke\nfrom ._compose import compose\nfrom ._compositor import CompositorUpdate\nfrom ._context import active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._event_broker import NoHandler, extract_handler_actions\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import AnimationLevel\nfrom ._wait import wait_for_idle\nfrom ._worker_manager import WorkerManager\nfrom .actions import ActionParseResult, SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .binding import Binding, BindingType, _Bindings\nfrom .command import CommandPalette, Provider\nfrom .css.errors import StylesheetError\nfrom .css.query import NoMatches\nfrom .css.stylesheet import RulesMap, Stylesheet\nfrom .design import ColorSystem\nfrom .dom import DOMNode, NoScreen\nfrom .driver import Driver\nfrom .errors import NoWidget\nfrom .features import FeatureFlag, parse_features\nfrom .file_monitor import FileMonitor\nfrom .filter import ANSIToTruecolor, DimFilter, Monochrome\nfrom .geometry import Offset, Region, Size\nfrom .keys import (\n    REPLACED_KEYS,\n    _character_to_key,\n    _get_key_display,\n    _get_unicode_name_from_key,\n)\nfrom .messages import CallbackType\nfrom .notifications import Notification, Notifications, Notify, SeverityLevel\nfrom .reactive import Reactive\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .screen import (\n    ActiveBinding,\n    Screen,\n    ScreenResultCallbackType,\n    ScreenResultType,\n    SystemModalScreen,\n)\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import AwaitMount, Widget\nfrom .widgets._toast import ToastRack\nfrom .worker import NoActiveWorker, get_current_worker\n\nif TYPE_CHECKING:\n    from textual_dev.client import DevtoolsClient\n    from typing_extensions import Coroutine, Literal, Self, TypeAlias\n\n    from ._system_commands import SystemCommands\n    from ._types import MessageTarget\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import WrongType  # type: ignore  # noqa: F401\n    from .filter import LineFilter\n    from .message import Message\n    from .pilot import Pilot\n    from .widget import MountError  # type: ignore  # noqa: F401\n\nWINDOWS = sys.platform == \"win32\"\n\n# asyncio will warn against resources not being cleared\nif constants.DEBUG:\n    warnings.simplefilter(\"always\", ResourceWarning)\n\n# `asyncio.get_event_loop()` is deprecated since Python 3.10:\n_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)\n\nLayoutDefinition = \"dict[str, Any]\"\n\nDEFAULT_COLORS = {\n    \"dark\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=True,\n    ),\n    \"light\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=False,\n    ),\n}\n\nComposeResult = Iterable[Widget]\nRenderResult = RenderableType\n\nAutopilotCallbackType: TypeAlias = (\n    \"Callable[[Pilot[object]], Coroutine[Any, Any, None]]\"\n)\n\"\"\"Signature for valid callbacks that can be used to control apps.\"\"\"\n\n\ndef get_system_commands() -> type[SystemCommands]:\n    \"\"\"Callable to lazy load the system commands.\n\n    Returns:\n        System commands class.\n    \"\"\"\n    from ._system_commands import SystemCommands\n\n    return SystemCommands\n\n\nclass AppError(Exception):\n    \"\"\"Base class for general App related exceptions.\"\"\"\n\n\nclass ActionError(Exception):\n    \"\"\"Base class for exceptions relating to actions.\"\"\"\n\n\nclass ScreenError(Exception):\n    \"\"\"Base class for exceptions that relate to screens.\"\"\"\n\n\nclass ScreenStackError(ScreenError):\n    \"\"\"Raised when trying to manipulate the screen stack incorrectly.\"\"\"\n\n\nclass ModeError(Exception):\n    \"\"\"Base class for exceptions related to modes.\"\"\"\n\n\nclass InvalidModeError(ModeError):\n    \"\"\"Raised if there is an issue with a mode name.\"\"\"\n\n\nclass UnknownModeError(ModeError):\n    \"\"\"Raised when attempting to use a mode that is not known.\"\"\"\n\n\nclass ActiveModeError(ModeError):\n    \"\"\"Raised when attempting to remove the currently active mode.\"\"\"\n\n\nclass SuspendNotSupported(Exception):\n    \"\"\"Raised if suspending the application is not supported.\n\n    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while\n    the application is running in an environment where this isn't supported.\n    \"\"\"\n\n\nReturnType = TypeVar(\"ReturnType\")\nCallThreadReturnType = TypeVar(\"CallThreadReturnType\")\n\n\nclass _NullFile:\n    \"\"\"A file-like where writes go nowhere.\"\"\"\n\n    def write(self, text: str) -> None:\n        pass\n\n    def flush(self) -> None:\n        pass\n\n    def isatty(self) -> bool:\n        return True\n\n\nclass _PrintCapture:\n    \"\"\"A file-like which captures output.\"\"\"\n\n    def __init__(self, app: App, stderr: bool = False) -> None:\n        \"\"\"\n\n        Args:\n            app: App instance.\n            stderr: Write from stderr.\n        \"\"\"\n        self.app = app\n        self.stderr = stderr\n\n    def write(self, text: str) -> None:\n        \"\"\"Called when writing to stdout or stderr.\n\n        Args:\n            text: Text that was \"printed\".\n        \"\"\"\n        self.app._print(text, stderr=self.stderr)\n\n    def flush(self) -> None:\n        \"\"\"Called when stdout or stderr was flushed.\"\"\"\n        self.app._flush(stderr=self.stderr)\n\n    def isatty(self) -> bool:\n        \"\"\"Pretend we're a terminal.\"\"\"\n        # TODO: should this be configurable?\n        return True\n\n    def fileno(self) -> int:\n        \"\"\"Return invalid fileno.\"\"\"\n        return -1\n\n\n@rich.repr.auto\nclass App(Generic[ReturnType], DOMNode):\n    \"\"\"The base class for Textual Applications.\"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,\n    and therefore takes priority in the event of a specificity clash.\"\"\"\n\n    # Default (the lowest priority) CSS\n    DEFAULT_CSS: ClassVar[str]\n    DEFAULT_CSS = \"\"\"\n    App {\n        background: $background;\n        color: $text;\n    }\n    *:disabled:can-focus {\n        opacity: 0.7;\n    }\n    \"\"\"\n\n    MODES: ClassVar[dict[str, str | Screen | Callable[[], Screen]]] = {}\n    \"\"\"Modes associated with the app and their base screens.\n\n    The base screen is the screen at the bottom of the mode stack. You can think of\n    it as the default screen for that stack.\n    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],\n    [`Screen`][textual.screen.Screen] instances, or callables that return screens.\n\n    Example:\n        ```py\n        class HelpScreen(Screen[None]):\n            ...\n\n        class MainAppScreen(Screen[None]):\n            ...\n\n        class MyApp(App[None]):\n            MODES = {\n                \"default\": \"main\",\n                \"help\": HelpScreen,\n            }\n\n            SCREENS = {\n                \"main\": MainAppScreen,\n            }\n\n            ...\n        ```\n    \"\"\"\n    SCREENS: ClassVar[dict[str, Screen[Any] | Callable[[], Screen[Any]]]] = {}\n    \"\"\"Screens associated with the app for the lifetime of the app.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = \"*\"\n    \"\"\"A selector to determine what to focus automatically when a screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Setting to `None` or `\"\"` disables auto focus.\n    \"\"\"\n\n    _BASE_PATH: str | None = None\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\"\"\"\n\n    TITLE: str | None = None\n    \"\"\"A class variable to set the *default* title for the application.\n\n    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.\n    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].\n    \"\"\"\n\n    SUB_TITLE: str | None = None\n    \"\"\"A class variable to set the default sub-title for the application.\n\n    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.\n    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True\n    \"\"\"Should the [command palette][textual.command.CommandPalette] be enabled for the application?\"\"\"\n\n    NOTIFICATION_TIMEOUT: ClassVar[float] = 5\n    \"\"\"Default number of seconds to show notifications before removing them.\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {\n        get_system_commands\n    }\n    \"\"\"Command providers used by the [command palette](/guide/command_palette).\n\n    Should be a set of [command.Provider][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS: ClassVar[list[BindingType]] = [\n        Binding(\"ctrl+c\", \"quit\", \"Quit\", show=False, priority=True),\n        Binding(\"ctrl+backslash\", \"command_palette\", show=False, priority=True),\n    ]\n\n    CLOSE_TIMEOUT: float | None = 5.0\n    \"\"\"Timeout waiting for widget's to close, or `None` for no timeout.\"\"\"\n\n    title: Reactive[str] = Reactive(\"\", compute=False)\n    sub_title: Reactive[str] = Reactive(\"\", compute=False)\n\n    dark: Reactive[bool] = Reactive(True, compute=False)\n    \"\"\"Use a dark theme if `True`, otherwise use a light theme.\n\n    Modify this attribute to switch between light and dark themes.\n\n    Example:\n        ```python\n        self.app.dark = not self.app.dark  # Toggle dark mode\n        ```\n    \"\"\"\n    app_focus = Reactive(True, compute=False)\n    \"\"\"Indicates if the app has focus.\n\n    When run in the terminal, the app always has focus. When run in the web, the app will\n    get focus when the terminal widget has focus.\n    \"\"\"\n\n    ansi_theme_dark = Reactive(MONOKAI, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in dark mode.\"\"\"\n\n    ansi_theme_light = Reactive(ALABASTER, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in light mode.\"\"\"\n\n    def __init__(\n        self,\n        driver_class: Type[Driver] | None = None,\n        css_path: CSSPathType | None = None,\n        watch_css: bool = False,\n    ):\n        \"\"\"Create an instance of an app.\n\n        Args:\n            driver_class: Driver class or `None` to auto-detect.\n                This will be used by some Textual tools.\n            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.\n                To load multiple CSS files, pass a list of strings or paths which\n                will be loaded in order.\n            watch_css: Reload CSS if the files changed. This is set automatically if\n                you are using `textual run` with the `dev` switch.\n\n        Raises:\n            CssPathError: When the supplied CSS path(s) are an unexpected type.\n        \"\"\"\n        self._start_time = perf_counter()\n        super().__init__()\n        self.features: frozenset[FeatureFlag] = parse_features(os.getenv(\"TEXTUAL\", \"\"))\n\n        ansi_theme = self.ansi_theme_dark if self.dark else self.ansi_theme_light\n        self._filters: list[LineFilter] = [ANSIToTruecolor(ansi_theme)]\n\n        environ = dict(os.environ)\n        no_color = environ.pop(\"NO_COLOR\", None)\n        if no_color is not None:\n            self._filters.append(Monochrome())\n\n        for filter_name in constants.FILTERS.split(\",\"):\n            filter = filter_name.lower().strip()\n            if filter == \"dim\":\n                self._filters.append(DimFilter())\n\n        self.console = Console(\n            color_system=constants.COLOR_SYSTEM,\n            file=_NullFile(),\n            markup=True,\n            highlight=False,\n            emoji=False,\n            legacy_windows=False,\n            _environ=environ,\n            force_terminal=True,\n            safe_box=False,\n            soft_wrap=False,\n        )\n        self._workers = WorkerManager(self)\n        self.error_console = Console(markup=False, highlight=False, stderr=True)\n        self.driver_class = driver_class or self.get_driver_class()\n        self._screen_stacks: dict[str, list[Screen[Any]]] = {\"_default\": []}\n        \"\"\"A stack of screens per mode.\"\"\"\n        self._current_mode: str = \"_default\"\n        \"\"\"The current mode the app is in.\"\"\"\n        self._sync_available = False\n\n        self.mouse_over: Widget | None = None\n        self.mouse_captured: Widget | None = None\n        self._driver: Driver | None = None\n        self._exit_renderables: list[RenderableType] = []\n\n        self._action_targets = {\"app\", \"screen\", \"focused\"}\n        self._animator = Animator(self)\n        self._animate = self._animator.bind(self)\n        self.mouse_position = Offset(0, 0)\n\n        self._mouse_down_widget: Widget | None = None\n        \"\"\"The widget that was most recently mouse downed (used to create click events).\"\"\"\n\n        self._previous_cursor_position = Offset(0, 0)\n        \"\"\"The previous cursor position\"\"\"\n\n        self.cursor_position = Offset(0, 0)\n        \"\"\"The position of the terminal cursor in screen-space.\n\n        This can be set by widgets and is useful for controlling the\n        positioning of OS IME and emoji popup menus.\"\"\"\n\n        self._exception: Exception | None = None\n        \"\"\"The unhandled exception which is leading to the app shutting down,\n        or None if the app is still running with no unhandled exceptions.\"\"\"\n\n        self._exception_event: asyncio.Event = asyncio.Event()\n        \"\"\"An event that will be set when the first exception is encountered.\"\"\"\n\n        self.title = (\n            self.TITLE if self.TITLE is not None else f\"{self.__class__.__name__}\"\n        )\n        \"\"\"The title for the application.\n\n        The initial value for `title` will be set to the `TITLE` class variable if it exists, or\n        the name of the app if it doesn't.\n\n        Assign a new value to this attribute to change the title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else \"\"\n        \"\"\"The sub-title for the application.\n\n        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or\n        an empty string if it doesn't.\n\n        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to\n        the file being worked on.\n\n        Assign a new value to this attribute to change the sub-title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE\n        \"\"\"A flag to say if the application should use the command palette.\n\n        If set to `False` any call to\n        [`action_command_palette`][textual.app.App.action_command_palette]\n        will be ignored.\n        \"\"\"\n\n        self._logger = Logger(self._log)\n\n        self._refresh_required = False\n\n        self.design = DEFAULT_COLORS\n\n        self._css_has_errors = False\n        self.stylesheet = Stylesheet(variables=self.get_css_variables())\n\n        css_path = css_path or self.CSS_PATH\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(css_path) if css_path is not None else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self._registry: WeakSet[DOMNode] = WeakSet()\n\n        # Sensitivity on X is double the sensitivity on Y to account for\n        # cells being twice as tall as wide\n        self.scroll_sensitivity_x: float = 4.0\n        \"\"\"Number of columns to scroll in the X direction with wheel or trackpad.\"\"\"\n        self.scroll_sensitivity_y: float = 2.0\n        \"\"\"Number of lines to scroll in the Y direction with wheel or trackpad.\"\"\"\n\n        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}\n        self._installed_screens.update(**self.SCREENS)\n\n        self._compose_stacks: list[list[Widget]] = []\n        self._composed: list[list[Widget]] = []\n        self._recompose_required = False\n\n        self.devtools: DevtoolsClient | None = None\n        self._devtools_redirector: StdoutRedirector | None = None\n        if \"devtools\" in self.features:\n            try:\n                from textual_dev.client import DevtoolsClient\n                from textual_dev.redirect_output import StdoutRedirector\n            except ImportError:\n                # Dev dependencies not installed\n                pass\n            else:\n                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)\n                self._devtools_redirector = StdoutRedirector(self.devtools)\n\n        self._loop: asyncio.AbstractEventLoop | None = None\n        self._return_value: ReturnType | None = None\n        \"\"\"Internal attribute used to set the return value for the app.\"\"\"\n        self._return_code: int | None = None\n        \"\"\"Internal attribute used to set the return code for the app.\"\"\"\n        self._exit = False\n        self._disable_tooltips = False\n        self._disable_notifications = False\n\n        self.css_monitor = (\n            FileMonitor(self.css_path, self._on_css_change)\n            if watch_css or self.debug\n            else None\n        )\n        self._screenshot: str | None = None\n        self._dom_lock = RLock()\n        self._dom_ready = False\n        self._batch_count = 0\n        self._notifications = Notifications()\n\n        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (\n            WeakKeyDictionary()\n        )\n        \"\"\"Registry of the MessageTargets which are capturing output at any given time.\"\"\"\n        self._capture_stdout = _PrintCapture(self, stderr=False)\n        \"\"\"File-like object capturing data written to stdout.\"\"\"\n        self._capture_stderr = _PrintCapture(self, stderr=True)\n        \"\"\"File-like object capturing data written to stderr.\"\"\"\n        self._original_stdout = sys.__stdout__\n        \"\"\"The original stdout stream (before redirection etc).\"\"\"\n        self._original_stderr = sys.__stderr__\n        \"\"\"The original stderr stream (before redirection etc).\"\"\"\n\n        self.app_suspend_signal: Signal[App] = Signal(self, \"app-suspend\")\n        \"\"\"The signal that is published when the app is suspended.\n\n        When [`App.suspend`][textual.app.App.suspend] is called this signal\n        will be [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work before the suspension takes place.\n        \"\"\"\n        self.app_resume_signal: Signal[App] = Signal(self, \"app-resume\")\n        \"\"\"The signal that is published when the app is resumed after a suspend.\n\n        When the app is resumed after a\n        [`App.suspend`][textual.app.App.suspend] call this signal will be\n        [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work after the app has resumed.\n        \"\"\"\n\n        self.set_class(self.dark, \"-dark-mode\")\n        self.set_class(not self.dark, \"-light-mode\")\n\n        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS\n        \"\"\"Determines what type of animations the app will display.\n\n        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].\n        \"\"\"\n\n        self._last_focused_on_app_blur: Widget | None = None\n        \"\"\"The widget that had focus when the last `AppBlur` happened.\n\n        This will be used to restore correct focus when an `AppFocus`\n        happens.\n        \"\"\"\n\n        # Size of previous inline update\n        self._previous_inline_height: int | None = None\n\n    def validate_title(self, title: Any) -> str:\n        \"\"\"Make sure the title is set to a string.\"\"\"\n        return str(title)\n\n    def validate_sub_title(self, sub_title: Any) -> str:\n        \"\"\"Make sure the sub-title is set to a string.\"\"\"\n        return str(sub_title)\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The [worker](/guide/workers/) manager.\n\n        Returns:\n            An object to manage workers.\n        \"\"\"\n        return self._workers\n\n    @property\n    def return_value(self) -> ReturnType | None:\n        \"\"\"The return value of the app, or `None` if it has not yet been set.\n\n        The return value is set when calling [exit][textual.app.App.exit].\n        \"\"\"\n        return self._return_value\n\n    @property\n    def return_code(self) -> int | None:\n        \"\"\"The return code with which the app exited.\n\n        Non-zero codes indicate errors.\n        A value of 1 means the app exited with a fatal error.\n        If the app wasn't exited yet, this will be `None`.\n\n        Example:\n            The return code can be used to exit the process via `sys.exit`.\n            ```py\n            my_app.run()\n            sys.exit(my_app.return_code)\n            ```\n        \"\"\"\n        return self._return\n```\n# Function Signature: def switch_mode(self, mode: str) -> AwaitMount:\n# Natural Language Annotations: \"\"\"Switch to a given mode.\n\n        Args:\n            mode: The mode to switch to.\n\n        Returns:\n            An optionally awaitable object which waits for the screen associated\n                with the mode to be mounted.\n\n        Raises:\n            UnknownModeError: If trying to switch to an unknown mode.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef switch_mode(self, mode: str) -> A", "  ```\ndef switch_mode(self, mode: str) -> A", "  ```python\ndef switch_mode(self, mode: str) ->"]}, "textual-src/textual/app.py-push_screen": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/app.py:\n```\n\"\"\"\n\nHere you will find the [App][textual.app.App] class, which is the base class for Textual apps.\n\nSee [app basics](/guide/app) for how to build Textual apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport inspect\nimport io\nimport os\nimport signal\nimport sys\nimport threading\nimport warnings\nfrom asyncio import Task, create_task\nfrom concurrent.futures import Future\nfrom contextlib import (\n    asynccontextmanager,\n    contextmanager,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom datetime import datetime\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    Sequence,\n    Type,\n    TypeVar,\n    overload,\n)\nfrom weakref import WeakKeyDictionary, WeakSet\n\nimport rich\nimport rich.repr\nfrom rich.console import Console, RenderableType\nfrom rich.control import Control\nfrom rich.protocol import is_renderable\nfrom rich.segment import Segment, Segments\nfrom rich.terminal_theme import TerminalTheme\n\nfrom . import (\n    Logger,\n    LogGroup,\n    LogVerbosity,\n    actions,\n    constants,\n    events,\n    log,\n    messages,\n    on,\n)\nfrom ._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction\nfrom ._ansi_sequences import SYNC_END, SYNC_START\nfrom ._ansi_theme import ALABASTER, MONOKAI\nfrom ._callback import invoke\nfrom ._compose import compose\nfrom ._compositor import CompositorUpdate\nfrom ._context import active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._event_broker import NoHandler, extract_handler_actions\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import AnimationLevel\nfrom ._wait import wait_for_idle\nfrom ._worker_manager import WorkerManager\nfrom .actions import ActionParseResult, SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .binding import Binding, BindingType, _Bindings\nfrom .command import CommandPalette, Provider\nfrom .css.errors import StylesheetError\nfrom .css.query import NoMatches\nfrom .css.stylesheet import RulesMap, Stylesheet\nfrom .design import ColorSystem\nfrom .dom import DOMNode, NoScreen\nfrom .driver import Driver\nfrom .errors import NoWidget\nfrom .features import FeatureFlag, parse_features\nfrom .file_monitor import FileMonitor\nfrom .filter import ANSIToTruecolor, DimFilter, Monochrome\nfrom .geometry import Offset, Region, Size\nfrom .keys import (\n    REPLACED_KEYS,\n    _character_to_key,\n    _get_key_display,\n    _get_unicode_name_from_key,\n)\nfrom .messages import CallbackType\nfrom .notifications import Notification, Notifications, Notify, SeverityLevel\nfrom .reactive import Reactive\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .screen import (\n    ActiveBinding,\n    Screen,\n    ScreenResultCallbackType,\n    ScreenResultType,\n    SystemModalScreen,\n)\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import AwaitMount, Widget\nfrom .widgets._toast import ToastRack\nfrom .worker import NoActiveWorker, get_current_worker\n\nif TYPE_CHECKING:\n    from textual_dev.client import DevtoolsClient\n    from typing_extensions import Coroutine, Literal, Self, TypeAlias\n\n    from ._system_commands import SystemCommands\n    from ._types import MessageTarget\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import WrongType  # type: ignore  # noqa: F401\n    from .filter import LineFilter\n    from .message import Message\n    from .pilot import Pilot\n    from .widget import MountError  # type: ignore  # noqa: F401\n\nWINDOWS = sys.platform == \"win32\"\n\n# asyncio will warn against resources not being cleared\nif constants.DEBUG:\n    warnings.simplefilter(\"always\", ResourceWarning)\n\n# `asyncio.get_event_loop()` is deprecated since Python 3.10:\n_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)\n\nLayoutDefinition = \"dict[str, Any]\"\n\nDEFAULT_COLORS = {\n    \"dark\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=True,\n    ),\n    \"light\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=False,\n    ),\n}\n\nComposeResult = Iterable[Widget]\nRenderResult = RenderableType\n\nAutopilotCallbackType: TypeAlias = (\n    \"Callable[[Pilot[object]], Coroutine[Any, Any, None]]\"\n)\n\"\"\"Signature for valid callbacks that can be used to control apps.\"\"\"\n\n\ndef get_system_commands() -> type[SystemCommands]:\n    \"\"\"Callable to lazy load the system commands.\n\n    Returns:\n        System commands class.\n    \"\"\"\n    from ._system_commands import SystemCommands\n\n    return SystemCommands\n\n\nclass AppError(Exception):\n    \"\"\"Base class for general App related exceptions.\"\"\"\n\n\nclass ActionError(Exception):\n    \"\"\"Base class for exceptions relating to actions.\"\"\"\n\n\nclass ScreenError(Exception):\n    \"\"\"Base class for exceptions that relate to screens.\"\"\"\n\n\nclass ScreenStackError(ScreenError):\n    \"\"\"Raised when trying to manipulate the screen stack incorrectly.\"\"\"\n\n\nclass ModeError(Exception):\n    \"\"\"Base class for exceptions related to modes.\"\"\"\n\n\nclass InvalidModeError(ModeError):\n    \"\"\"Raised if there is an issue with a mode name.\"\"\"\n\n\nclass UnknownModeError(ModeError):\n    \"\"\"Raised when attempting to use a mode that is not known.\"\"\"\n\n\nclass ActiveModeError(ModeError):\n    \"\"\"Raised when attempting to remove the currently active mode.\"\"\"\n\n\nclass SuspendNotSupported(Exception):\n    \"\"\"Raised if suspending the application is not supported.\n\n    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while\n    the application is running in an environment where this isn't supported.\n    \"\"\"\n\n\nReturnType = TypeVar(\"ReturnType\")\nCallThreadReturnType = TypeVar(\"CallThreadReturnType\")\n\n\nclass _NullFile:\n    \"\"\"A file-like where writes go nowhere.\"\"\"\n\n    def write(self, text: str) -> None:\n        pass\n\n    def flush(self) -> None:\n        pass\n\n    def isatty(self) -> bool:\n        return True\n\n\nclass _PrintCapture:\n    \"\"\"A file-like which captures output.\"\"\"\n\n    def __init__(self, app: App, stderr: bool = False) -> None:\n        \"\"\"\n\n        Args:\n            app: App instance.\n            stderr: Write from stderr.\n        \"\"\"\n        self.app = app\n        self.stderr = stderr\n\n    def write(self, text: str) -> None:\n        \"\"\"Called when writing to stdout or stderr.\n\n        Args:\n            text: Text that was \"printed\".\n        \"\"\"\n        self.app._print(text, stderr=self.stderr)\n\n    def flush(self) -> None:\n        \"\"\"Called when stdout or stderr was flushed.\"\"\"\n        self.app._flush(stderr=self.stderr)\n\n    def isatty(self) -> bool:\n        \"\"\"Pretend we're a terminal.\"\"\"\n        # TODO: should this be configurable?\n        return True\n\n    def fileno(self) -> int:\n        \"\"\"Return invalid fileno.\"\"\"\n        return -1\n\n\n@rich.repr.auto\nclass App(Generic[ReturnType], DOMNode):\n    \"\"\"The base class for Textual Applications.\"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,\n    and therefore takes priority in the event of a specificity clash.\"\"\"\n\n    # Default (the lowest priority) CSS\n    DEFAULT_CSS: ClassVar[str]\n    DEFAULT_CSS = \"\"\"\n    App {\n        background: $background;\n        color: $text;\n    }\n    *:disabled:can-focus {\n        opacity: 0.7;\n    }\n    \"\"\"\n\n    MODES: ClassVar[dict[str, str | Screen | Callable[[], Screen]]] = {}\n    \"\"\"Modes associated with the app and their base screens.\n\n    The base screen is the screen at the bottom of the mode stack. You can think of\n    it as the default screen for that stack.\n    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],\n    [`Screen`][textual.screen.Screen] instances, or callables that return screens.\n\n    Example:\n        ```py\n        class HelpScreen(Screen[None]):\n            ...\n\n        class MainAppScreen(Screen[None]):\n            ...\n\n        class MyApp(App[None]):\n            MODES = {\n                \"default\": \"main\",\n                \"help\": HelpScreen,\n            }\n\n            SCREENS = {\n                \"main\": MainAppScreen,\n            }\n\n            ...\n        ```\n    \"\"\"\n    SCREENS: ClassVar[dict[str, Screen[Any] | Callable[[], Screen[Any]]]] = {}\n    \"\"\"Screens associated with the app for the lifetime of the app.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = \"*\"\n    \"\"\"A selector to determine what to focus automatically when a screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Setting to `None` or `\"\"` disables auto focus.\n    \"\"\"\n\n    _BASE_PATH: str | None = None\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\"\"\"\n\n    TITLE: str | None = None\n    \"\"\"A class variable to set the *default* title for the application.\n\n    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.\n    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].\n    \"\"\"\n\n    SUB_TITLE: str | None = None\n    \"\"\"A class variable to set the default sub-title for the application.\n\n    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.\n    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True\n    \"\"\"Should the [command palette][textual.command.CommandPalette] be enabled for the application?\"\"\"\n\n    NOTIFICATION_TIMEOUT: ClassVar[float] = 5\n    \"\"\"Default number of seconds to show notifications before removing them.\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {\n        get_system_commands\n    }\n    \"\"\"Command providers used by the [command palette](/guide/command_palette).\n\n    Should be a set of [command.Provider][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS: ClassVar[list[BindingType]] = [\n        Binding(\"ctrl+c\", \"quit\", \"Quit\", show=False, priority=True),\n        Binding(\"ctrl+backslash\", \"command_palette\", show=False, priority=True),\n    ]\n\n    CLOSE_TIMEOUT: float | None = 5.0\n    \"\"\"Timeout waiting for widget's to close, or `None` for no timeout.\"\"\"\n\n    title: Reactive[str] = Reactive(\"\", compute=False)\n    sub_title: Reactive[str] = Reactive(\"\", compute=False)\n\n    dark: Reactive[bool] = Reactive(True, compute=False)\n    \"\"\"Use a dark theme if `True`, otherwise use a light theme.\n\n    Modify this attribute to switch between light and dark themes.\n\n    Example:\n        ```python\n        self.app.dark = not self.app.dark  # Toggle dark mode\n        ```\n    \"\"\"\n    app_focus = Reactive(True, compute=False)\n    \"\"\"Indicates if the app has focus.\n\n    When run in the terminal, the app always has focus. When run in the web, the app will\n    get focus when the terminal widget has focus.\n    \"\"\"\n\n    ansi_theme_dark = Reactive(MONOKAI, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in dark mode.\"\"\"\n\n    ansi_theme_light = Reactive(ALABASTER, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in light mode.\"\"\"\n\n    def __init__(\n        self,\n        driver_class: Type[Driver] | None = None,\n        css_path: CSSPathType | None = None,\n        watch_css: bool = False,\n    ):\n        \"\"\"Create an instance of an app.\n\n        Args:\n            driver_class: Driver class or `None` to auto-detect.\n                This will be used by some Textual tools.\n            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.\n                To load multiple CSS files, pass a list of strings or paths which\n                will be loaded in order.\n            watch_css: Reload CSS if the files changed. This is set automatically if\n                you are using `textual run` with the `dev` switch.\n\n        Raises:\n            CssPathError: When the supplied CSS path(s) are an unexpected type.\n        \"\"\"\n        self._start_time = perf_counter()\n        super().__init__()\n        self.features: frozenset[FeatureFlag] = parse_features(os.getenv(\"TEXTUAL\", \"\"))\n\n        ansi_theme = self.ansi_theme_dark if self.dark else self.ansi_theme_light\n        self._filters: list[LineFilter] = [ANSIToTruecolor(ansi_theme)]\n\n        environ = dict(os.environ)\n        no_color = environ.pop(\"NO_COLOR\", None)\n        if no_color is not None:\n            self._filters.append(Monochrome())\n\n        for filter_name in constants.FILTERS.split(\",\"):\n            filter = filter_name.lower().strip()\n            if filter == \"dim\":\n                self._filters.append(DimFilter())\n\n        self.console = Console(\n            color_system=constants.COLOR_SYSTEM,\n            file=_NullFile(),\n            markup=True,\n            highlight=False,\n            emoji=False,\n            legacy_windows=False,\n            _environ=environ,\n            force_terminal=True,\n            safe_box=False,\n            soft_wrap=False,\n        )\n        self._workers = WorkerManager(self)\n        self.error_console = Console(markup=False, highlight=False, stderr=True)\n        self.driver_class = driver_class or self.get_driver_class()\n        self._screen_stacks: dict[str, list[Screen[Any]]] = {\"_default\": []}\n        \"\"\"A stack of screens per mode.\"\"\"\n        self._current_mode: str = \"_default\"\n        \"\"\"The current mode the app is in.\"\"\"\n        self._sync_available = False\n\n        self.mouse_over: Widget | None = None\n        self.mouse_captured: Widget | None = None\n        self._driver: Driver | None = None\n        self._exit_renderables: list[RenderableType] = []\n\n        self._action_targets = {\"app\", \"screen\", \"focused\"}\n        self._animator = Animator(self)\n        self._animate = self._animator.bind(self)\n        self.mouse_position = Offset(0, 0)\n\n        self._mouse_down_widget: Widget | None = None\n        \"\"\"The widget that was most recently mouse downed (used to create click events).\"\"\"\n\n        self._previous_cursor_position = Offset(0, 0)\n        \"\"\"The previous cursor position\"\"\"\n\n        self.cursor_position = Offset(0, 0)\n        \"\"\"The position of the terminal cursor in screen-space.\n\n        This can be set by widgets and is useful for controlling the\n        positioning of OS IME and emoji popup menus.\"\"\"\n\n        self._exception: Exception | None = None\n        \"\"\"The unhandled exception which is leading to the app shutting down,\n        or None if the app is still running with no unhandled exceptions.\"\"\"\n\n        self._exception_event: asyncio.Event = asyncio.Event()\n        \"\"\"An event that will be set when the first exception is encountered.\"\"\"\n\n        self.title = (\n            self.TITLE if self.TITLE is not None else f\"{self.__class__.__name__}\"\n        )\n        \"\"\"The title for the application.\n\n        The initial value for `title` will be set to the `TITLE` class variable if it exists, or\n        the name of the app if it doesn't.\n\n        Assign a new value to this attribute to change the title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else \"\"\n        \"\"\"The sub-title for the application.\n\n        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or\n        an empty string if it doesn't.\n\n        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to\n        the file being worked on.\n\n        Assign a new value to this attribute to change the sub-title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE\n        \"\"\"A flag to say if the application should use the command palette.\n\n        If set to `False` any call to\n        [`action_command_palette`][textual.app.App.action_command_palette]\n        will be ignored.\n        \"\"\"\n\n        self._logger = Logger(self._log)\n\n        self._refresh_required = False\n\n        self.design = DEFAULT_COLORS\n\n        self._css_has_errors = False\n        self.stylesheet = Stylesheet(variables=self.get_css_variables())\n\n        css_path = css_path or self.CSS_PATH\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(css_path) if css_path is not None else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self._registry: WeakSet[DOMNode] = WeakSet()\n\n        # Sensitivity on X is double the sensitivity on Y to account for\n        # cells being twice as tall as wide\n        self.scroll_sensitivity_x: float = 4.0\n        \"\"\"Number of columns to scroll in the X direction with wheel or trackpad.\"\"\"\n        self.scroll_sensitivity_y: float = 2.0\n        \"\"\"Number of lines to scroll in the Y direction with wheel or trackpad.\"\"\"\n\n        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}\n        self._installed_screens.update(**self.SCREENS)\n\n        self._compose_stacks: list[list[Widget]] = []\n        self._composed: list[list[Widget]] = []\n        self._recompose_required = False\n\n        self.devtools: DevtoolsClient | None = None\n        self._devtools_redirector: StdoutRedirector | None = None\n        if \"devtools\" in self.features:\n            try:\n                from textual_dev.client import DevtoolsClient\n                from textual_dev.redirect_output import StdoutRedirector\n            except ImportError:\n                # Dev dependencies not installed\n                pass\n            else:\n                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)\n                self._devtools_redirector = StdoutRedirector(self.devtools)\n\n        self._loop: asyncio.AbstractEventLoop | None = None\n        self._return_value: ReturnType | None = None\n        \"\"\"Internal attribute used to set the return value for the app.\"\"\"\n        self._return_code: int | None = None\n        \"\"\"Internal attribute used to set the return code for the app.\"\"\"\n        self._exit = False\n        self._disable_tooltips = False\n        self._disable_notifications = False\n\n        self.css_monitor = (\n            FileMonitor(self.css_path, self._on_css_change)\n            if watch_css or self.debug\n            else None\n        )\n        self._screenshot: str | None = None\n        self._dom_lock = RLock()\n        self._dom_ready = False\n        self._batch_count = 0\n        self._notifications = Notifications()\n\n        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (\n            WeakKeyDictionary()\n        )\n        \"\"\"Registry of the MessageTargets which are capturing output at any given time.\"\"\"\n        self._capture_stdout = _PrintCapture(self, stderr=False)\n        \"\"\"File-like object capturing data written to stdout.\"\"\"\n        self._capture_stderr = _PrintCapture(self, stderr=True)\n        \"\"\"File-like object capturing data written to stderr.\"\"\"\n        self._original_stdout = sys.__stdout__\n        \"\"\"The original stdout stream (before redirection etc).\"\"\"\n        self._original_stderr = sys.__stderr__\n        \"\"\"The original stderr stream (before redirection etc).\"\"\"\n\n        self.app_suspend_signal: Signal[App] = Signal(self, \"app-suspend\")\n        \"\"\"The signal that is published when the app is suspended.\n\n        When [`App.suspend`][textual.app.App.suspend] is called this signal\n        will be [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work before the suspension takes place.\n        \"\"\"\n        self.app_resume_signal: Signal[App] = Signal(self, \"app-resume\")\n        \"\"\"The signal that is published when the app is resumed after a suspend.\n\n        When the app is resumed after a\n        [`App.suspend`][textual.app.App.suspend] call this signal will be\n        [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work after the app has resumed.\n        \"\"\"\n\n        self.set_class(self.dark, \"-dark-mode\")\n        self.set_class(not self.dark, \"-light-mode\")\n\n        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS\n        \"\"\"Determines what type of animations the app will display.\n\n        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].\n        \"\"\"\n\n        self._last_focused_on_app_blur: Widget | None = None\n        \"\"\"The widget that had focus when the last `AppBlur` happened.\n\n        This will be used to restore correct focus when an `AppFocus`\n        happens.\n        \"\"\"\n\n        # Size of previous inline update\n        self._previous_inline_height: int | None = None\n\n    def validate_title(self, title: Any) -> str:\n        \"\"\"Make sure the title is set to a string.\"\"\"\n        return str(title)\n\n    def validate_sub_title(self, sub_title: Any) -> str:\n        \"\"\"Make sure the sub-title is set to a string.\"\"\"\n        return str(sub_title)\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The [worker](/guide/workers/) manager.\n\n        Returns:\n            An object to manage workers.\n        \"\"\"\n        return self._workers\n\n    @property\n    def return_value(self) -> ReturnType | None:\n        \"\"\"The return value of the app, or `None` if it has not yet been set.\n\n        The return value is set when calling [exit][textual.app.App.exit].\n        \"\"\"\n        return self._return_value\n\n    @property\n    def return_code(self) -> int | None:\n        \"\"\"The return code with which the app exited.\n\n        Non-zero codes indicate errors.\n        A value of 1 means the app exited with a fatal error.\n        If the app wasn't exited yet, this will be `None`.\n\n        Example:\n            The return code can be used to exit the process via `sys.exit`.\n            ```py\n            my_app.run()\n            sys.exit(my_app.return_code)\n            ```\n        \"\"\"\n        return self._return\n```\n# Function Signature: def push_screen(\n# Natural Language Annotations: \"\"\"Push a new [screen](/guide/screens) on the screen stack, making it the current screen.\n\n        Args:\n            screen: A Screen instance or the name of an installed screen.\n            callback: An optional callback function that will be called if the screen is [dismissed][textual.screen.Screen.dismiss] with a result.\n            wait_for_dismiss: If `True`, awaiting this method will return the dismiss value from the screen. When set to `False`, awaiting\n                this method will wait for the screen to be mounted. Note that `wait_for_dismiss` should only be set to `True` when running in a worker.\n\n        Raises:\n            NoActiveWorker: If using `wait_for_dismiss` outside of a worker.\n\n        Returns:\n            An optional awaitable that awaits the mounting of the screen and its children, or an asyncio Future\n                to await the result of the screen.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef push_screen(\n        screen: Screen | str,", "  ```py\ndef push_screen(\n    self, screen: Screen", "  ```\npush_screen(self, screen: Screen[ReturnType],"]}, "textual-src/textual/app.py-switch_screen": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/app.py:\n```\n\"\"\"\n\nHere you will find the [App][textual.app.App] class, which is the base class for Textual apps.\n\nSee [app basics](/guide/app) for how to build Textual apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport inspect\nimport io\nimport os\nimport signal\nimport sys\nimport threading\nimport warnings\nfrom asyncio import Task, create_task\nfrom concurrent.futures import Future\nfrom contextlib import (\n    asynccontextmanager,\n    contextmanager,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom datetime import datetime\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    Sequence,\n    Type,\n    TypeVar,\n    overload,\n)\nfrom weakref import WeakKeyDictionary, WeakSet\n\nimport rich\nimport rich.repr\nfrom rich.console import Console, RenderableType\nfrom rich.control import Control\nfrom rich.protocol import is_renderable\nfrom rich.segment import Segment, Segments\nfrom rich.terminal_theme import TerminalTheme\n\nfrom . import (\n    Logger,\n    LogGroup,\n    LogVerbosity,\n    actions,\n    constants,\n    events,\n    log,\n    messages,\n    on,\n)\nfrom ._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction\nfrom ._ansi_sequences import SYNC_END, SYNC_START\nfrom ._ansi_theme import ALABASTER, MONOKAI\nfrom ._callback import invoke\nfrom ._compose import compose\nfrom ._compositor import CompositorUpdate\nfrom ._context import active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._event_broker import NoHandler, extract_handler_actions\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import AnimationLevel\nfrom ._wait import wait_for_idle\nfrom ._worker_manager import WorkerManager\nfrom .actions import ActionParseResult, SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .binding import Binding, BindingType, _Bindings\nfrom .command import CommandPalette, Provider\nfrom .css.errors import StylesheetError\nfrom .css.query import NoMatches\nfrom .css.stylesheet import RulesMap, Stylesheet\nfrom .design import ColorSystem\nfrom .dom import DOMNode, NoScreen\nfrom .driver import Driver\nfrom .errors import NoWidget\nfrom .features import FeatureFlag, parse_features\nfrom .file_monitor import FileMonitor\nfrom .filter import ANSIToTruecolor, DimFilter, Monochrome\nfrom .geometry import Offset, Region, Size\nfrom .keys import (\n    REPLACED_KEYS,\n    _character_to_key,\n    _get_key_display,\n    _get_unicode_name_from_key,\n)\nfrom .messages import CallbackType\nfrom .notifications import Notification, Notifications, Notify, SeverityLevel\nfrom .reactive import Reactive\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .screen import (\n    ActiveBinding,\n    Screen,\n    ScreenResultCallbackType,\n    ScreenResultType,\n    SystemModalScreen,\n)\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import AwaitMount, Widget\nfrom .widgets._toast import ToastRack\nfrom .worker import NoActiveWorker, get_current_worker\n\nif TYPE_CHECKING:\n    from textual_dev.client import DevtoolsClient\n    from typing_extensions import Coroutine, Literal, Self, TypeAlias\n\n    from ._system_commands import SystemCommands\n    from ._types import MessageTarget\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import WrongType  # type: ignore  # noqa: F401\n    from .filter import LineFilter\n    from .message import Message\n    from .pilot import Pilot\n    from .widget import MountError  # type: ignore  # noqa: F401\n\nWINDOWS = sys.platform == \"win32\"\n\n# asyncio will warn against resources not being cleared\nif constants.DEBUG:\n    warnings.simplefilter(\"always\", ResourceWarning)\n\n# `asyncio.get_event_loop()` is deprecated since Python 3.10:\n_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)\n\nLayoutDefinition = \"dict[str, Any]\"\n\nDEFAULT_COLORS = {\n    \"dark\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=True,\n    ),\n    \"light\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=False,\n    ),\n}\n\nComposeResult = Iterable[Widget]\nRenderResult = RenderableType\n\nAutopilotCallbackType: TypeAlias = (\n    \"Callable[[Pilot[object]], Coroutine[Any, Any, None]]\"\n)\n\"\"\"Signature for valid callbacks that can be used to control apps.\"\"\"\n\n\ndef get_system_commands() -> type[SystemCommands]:\n    \"\"\"Callable to lazy load the system commands.\n\n    Returns:\n        System commands class.\n    \"\"\"\n    from ._system_commands import SystemCommands\n\n    return SystemCommands\n\n\nclass AppError(Exception):\n    \"\"\"Base class for general App related exceptions.\"\"\"\n\n\nclass ActionError(Exception):\n    \"\"\"Base class for exceptions relating to actions.\"\"\"\n\n\nclass ScreenError(Exception):\n    \"\"\"Base class for exceptions that relate to screens.\"\"\"\n\n\nclass ScreenStackError(ScreenError):\n    \"\"\"Raised when trying to manipulate the screen stack incorrectly.\"\"\"\n\n\nclass ModeError(Exception):\n    \"\"\"Base class for exceptions related to modes.\"\"\"\n\n\nclass InvalidModeError(ModeError):\n    \"\"\"Raised if there is an issue with a mode name.\"\"\"\n\n\nclass UnknownModeError(ModeError):\n    \"\"\"Raised when attempting to use a mode that is not known.\"\"\"\n\n\nclass ActiveModeError(ModeError):\n    \"\"\"Raised when attempting to remove the currently active mode.\"\"\"\n\n\nclass SuspendNotSupported(Exception):\n    \"\"\"Raised if suspending the application is not supported.\n\n    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while\n    the application is running in an environment where this isn't supported.\n    \"\"\"\n\n\nReturnType = TypeVar(\"ReturnType\")\nCallThreadReturnType = TypeVar(\"CallThreadReturnType\")\n\n\nclass _NullFile:\n    \"\"\"A file-like where writes go nowhere.\"\"\"\n\n    def write(self, text: str) -> None:\n        pass\n\n    def flush(self) -> None:\n        pass\n\n    def isatty(self) -> bool:\n        return True\n\n\nclass _PrintCapture:\n    \"\"\"A file-like which captures output.\"\"\"\n\n    def __init__(self, app: App, stderr: bool = False) -> None:\n        \"\"\"\n\n        Args:\n            app: App instance.\n            stderr: Write from stderr.\n        \"\"\"\n        self.app = app\n        self.stderr = stderr\n\n    def write(self, text: str) -> None:\n        \"\"\"Called when writing to stdout or stderr.\n\n        Args:\n            text: Text that was \"printed\".\n        \"\"\"\n        self.app._print(text, stderr=self.stderr)\n\n    def flush(self) -> None:\n        \"\"\"Called when stdout or stderr was flushed.\"\"\"\n        self.app._flush(stderr=self.stderr)\n\n    def isatty(self) -> bool:\n        \"\"\"Pretend we're a terminal.\"\"\"\n        # TODO: should this be configurable?\n        return True\n\n    def fileno(self) -> int:\n        \"\"\"Return invalid fileno.\"\"\"\n        return -1\n\n\n@rich.repr.auto\nclass App(Generic[ReturnType], DOMNode):\n    \"\"\"The base class for Textual Applications.\"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,\n    and therefore takes priority in the event of a specificity clash.\"\"\"\n\n    # Default (the lowest priority) CSS\n    DEFAULT_CSS: ClassVar[str]\n    DEFAULT_CSS = \"\"\"\n    App {\n        background: $background;\n        color: $text;\n    }\n    *:disabled:can-focus {\n        opacity: 0.7;\n    }\n    \"\"\"\n\n    MODES: ClassVar[dict[str, str | Screen | Callable[[], Screen]]] = {}\n    \"\"\"Modes associated with the app and their base screens.\n\n    The base screen is the screen at the bottom of the mode stack. You can think of\n    it as the default screen for that stack.\n    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],\n    [`Screen`][textual.screen.Screen] instances, or callables that return screens.\n\n    Example:\n        ```py\n        class HelpScreen(Screen[None]):\n            ...\n\n        class MainAppScreen(Screen[None]):\n            ...\n\n        class MyApp(App[None]):\n            MODES = {\n                \"default\": \"main\",\n                \"help\": HelpScreen,\n            }\n\n            SCREENS = {\n                \"main\": MainAppScreen,\n            }\n\n            ...\n        ```\n    \"\"\"\n    SCREENS: ClassVar[dict[str, Screen[Any] | Callable[[], Screen[Any]]]] = {}\n    \"\"\"Screens associated with the app for the lifetime of the app.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = \"*\"\n    \"\"\"A selector to determine what to focus automatically when a screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Setting to `None` or `\"\"` disables auto focus.\n    \"\"\"\n\n    _BASE_PATH: str | None = None\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\"\"\"\n\n    TITLE: str | None = None\n    \"\"\"A class variable to set the *default* title for the application.\n\n    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.\n    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].\n    \"\"\"\n\n    SUB_TITLE: str | None = None\n    \"\"\"A class variable to set the default sub-title for the application.\n\n    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.\n    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True\n    \"\"\"Should the [command palette][textual.command.CommandPalette] be enabled for the application?\"\"\"\n\n    NOTIFICATION_TIMEOUT: ClassVar[float] = 5\n    \"\"\"Default number of seconds to show notifications before removing them.\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {\n        get_system_commands\n    }\n    \"\"\"Command providers used by the [command palette](/guide/command_palette).\n\n    Should be a set of [command.Provider][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS: ClassVar[list[BindingType]] = [\n        Binding(\"ctrl+c\", \"quit\", \"Quit\", show=False, priority=True),\n        Binding(\"ctrl+backslash\", \"command_palette\", show=False, priority=True),\n    ]\n\n    CLOSE_TIMEOUT: float | None = 5.0\n    \"\"\"Timeout waiting for widget's to close, or `None` for no timeout.\"\"\"\n\n    title: Reactive[str] = Reactive(\"\", compute=False)\n    sub_title: Reactive[str] = Reactive(\"\", compute=False)\n\n    dark: Reactive[bool] = Reactive(True, compute=False)\n    \"\"\"Use a dark theme if `True`, otherwise use a light theme.\n\n    Modify this attribute to switch between light and dark themes.\n\n    Example:\n        ```python\n        self.app.dark = not self.app.dark  # Toggle dark mode\n        ```\n    \"\"\"\n    app_focus = Reactive(True, compute=False)\n    \"\"\"Indicates if the app has focus.\n\n    When run in the terminal, the app always has focus. When run in the web, the app will\n    get focus when the terminal widget has focus.\n    \"\"\"\n\n    ansi_theme_dark = Reactive(MONOKAI, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in dark mode.\"\"\"\n\n    ansi_theme_light = Reactive(ALABASTER, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in light mode.\"\"\"\n\n    def __init__(\n        self,\n        driver_class: Type[Driver] | None = None,\n        css_path: CSSPathType | None = None,\n        watch_css: bool = False,\n    ):\n        \"\"\"Create an instance of an app.\n\n        Args:\n            driver_class: Driver class or `None` to auto-detect.\n                This will be used by some Textual tools.\n            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.\n                To load multiple CSS files, pass a list of strings or paths which\n                will be loaded in order.\n            watch_css: Reload CSS if the files changed. This is set automatically if\n                you are using `textual run` with the `dev` switch.\n\n        Raises:\n            CssPathError: When the supplied CSS path(s) are an unexpected type.\n        \"\"\"\n        self._start_time = perf_counter()\n        super().__init__()\n        self.features: frozenset[FeatureFlag] = parse_features(os.getenv(\"TEXTUAL\", \"\"))\n\n        ansi_theme = self.ansi_theme_dark if self.dark else self.ansi_theme_light\n        self._filters: list[LineFilter] = [ANSIToTruecolor(ansi_theme)]\n\n        environ = dict(os.environ)\n        no_color = environ.pop(\"NO_COLOR\", None)\n        if no_color is not None:\n            self._filters.append(Monochrome())\n\n        for filter_name in constants.FILTERS.split(\",\"):\n            filter = filter_name.lower().strip()\n            if filter == \"dim\":\n                self._filters.append(DimFilter())\n\n        self.console = Console(\n            color_system=constants.COLOR_SYSTEM,\n            file=_NullFile(),\n            markup=True,\n            highlight=False,\n            emoji=False,\n            legacy_windows=False,\n            _environ=environ,\n            force_terminal=True,\n            safe_box=False,\n            soft_wrap=False,\n        )\n        self._workers = WorkerManager(self)\n        self.error_console = Console(markup=False, highlight=False, stderr=True)\n        self.driver_class = driver_class or self.get_driver_class()\n        self._screen_stacks: dict[str, list[Screen[Any]]] = {\"_default\": []}\n        \"\"\"A stack of screens per mode.\"\"\"\n        self._current_mode: str = \"_default\"\n        \"\"\"The current mode the app is in.\"\"\"\n        self._sync_available = False\n\n        self.mouse_over: Widget | None = None\n        self.mouse_captured: Widget | None = None\n        self._driver: Driver | None = None\n        self._exit_renderables: list[RenderableType] = []\n\n        self._action_targets = {\"app\", \"screen\", \"focused\"}\n        self._animator = Animator(self)\n        self._animate = self._animator.bind(self)\n        self.mouse_position = Offset(0, 0)\n\n        self._mouse_down_widget: Widget | None = None\n        \"\"\"The widget that was most recently mouse downed (used to create click events).\"\"\"\n\n        self._previous_cursor_position = Offset(0, 0)\n        \"\"\"The previous cursor position\"\"\"\n\n        self.cursor_position = Offset(0, 0)\n        \"\"\"The position of the terminal cursor in screen-space.\n\n        This can be set by widgets and is useful for controlling the\n        positioning of OS IME and emoji popup menus.\"\"\"\n\n        self._exception: Exception | None = None\n        \"\"\"The unhandled exception which is leading to the app shutting down,\n        or None if the app is still running with no unhandled exceptions.\"\"\"\n\n        self._exception_event: asyncio.Event = asyncio.Event()\n        \"\"\"An event that will be set when the first exception is encountered.\"\"\"\n\n        self.title = (\n            self.TITLE if self.TITLE is not None else f\"{self.__class__.__name__}\"\n        )\n        \"\"\"The title for the application.\n\n        The initial value for `title` will be set to the `TITLE` class variable if it exists, or\n        the name of the app if it doesn't.\n\n        Assign a new value to this attribute to change the title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else \"\"\n        \"\"\"The sub-title for the application.\n\n        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or\n        an empty string if it doesn't.\n\n        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to\n        the file being worked on.\n\n        Assign a new value to this attribute to change the sub-title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE\n        \"\"\"A flag to say if the application should use the command palette.\n\n        If set to `False` any call to\n        [`action_command_palette`][textual.app.App.action_command_palette]\n        will be ignored.\n        \"\"\"\n\n        self._logger = Logger(self._log)\n\n        self._refresh_required = False\n\n        self.design = DEFAULT_COLORS\n\n        self._css_has_errors = False\n        self.stylesheet = Stylesheet(variables=self.get_css_variables())\n\n        css_path = css_path or self.CSS_PATH\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(css_path) if css_path is not None else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self._registry: WeakSet[DOMNode] = WeakSet()\n\n        # Sensitivity on X is double the sensitivity on Y to account for\n        # cells being twice as tall as wide\n        self.scroll_sensitivity_x: float = 4.0\n        \"\"\"Number of columns to scroll in the X direction with wheel or trackpad.\"\"\"\n        self.scroll_sensitivity_y: float = 2.0\n        \"\"\"Number of lines to scroll in the Y direction with wheel or trackpad.\"\"\"\n\n        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}\n        self._installed_screens.update(**self.SCREENS)\n\n        self._compose_stacks: list[list[Widget]] = []\n        self._composed: list[list[Widget]] = []\n        self._recompose_required = False\n\n        self.devtools: DevtoolsClient | None = None\n        self._devtools_redirector: StdoutRedirector | None = None\n        if \"devtools\" in self.features:\n            try:\n                from textual_dev.client import DevtoolsClient\n                from textual_dev.redirect_output import StdoutRedirector\n            except ImportError:\n                # Dev dependencies not installed\n                pass\n            else:\n                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)\n                self._devtools_redirector = StdoutRedirector(self.devtools)\n\n        self._loop: asyncio.AbstractEventLoop | None = None\n        self._return_value: ReturnType | None = None\n        \"\"\"Internal attribute used to set the return value for the app.\"\"\"\n        self._return_code: int | None = None\n        \"\"\"Internal attribute used to set the return code for the app.\"\"\"\n        self._exit = False\n        self._disable_tooltips = False\n        self._disable_notifications = False\n\n        self.css_monitor = (\n            FileMonitor(self.css_path, self._on_css_change)\n            if watch_css or self.debug\n            else None\n        )\n        self._screenshot: str | None = None\n        self._dom_lock = RLock()\n        self._dom_ready = False\n        self._batch_count = 0\n        self._notifications = Notifications()\n\n        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (\n            WeakKeyDictionary()\n        )\n        \"\"\"Registry of the MessageTargets which are capturing output at any given time.\"\"\"\n        self._capture_stdout = _PrintCapture(self, stderr=False)\n        \"\"\"File-like object capturing data written to stdout.\"\"\"\n        self._capture_stderr = _PrintCapture(self, stderr=True)\n        \"\"\"File-like object capturing data written to stderr.\"\"\"\n        self._original_stdout = sys.__stdout__\n        \"\"\"The original stdout stream (before redirection etc).\"\"\"\n        self._original_stderr = sys.__stderr__\n        \"\"\"The original stderr stream (before redirection etc).\"\"\"\n\n        self.app_suspend_signal: Signal[App] = Signal(self, \"app-suspend\")\n        \"\"\"The signal that is published when the app is suspended.\n\n        When [`App.suspend`][textual.app.App.suspend] is called this signal\n        will be [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work before the suspension takes place.\n        \"\"\"\n        self.app_resume_signal: Signal[App] = Signal(self, \"app-resume\")\n        \"\"\"The signal that is published when the app is resumed after a suspend.\n\n        When the app is resumed after a\n        [`App.suspend`][textual.app.App.suspend] call this signal will be\n        [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work after the app has resumed.\n        \"\"\"\n\n        self.set_class(self.dark, \"-dark-mode\")\n        self.set_class(not self.dark, \"-light-mode\")\n\n        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS\n        \"\"\"Determines what type of animations the app will display.\n\n        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].\n        \"\"\"\n\n        self._last_focused_on_app_blur: Widget | None = None\n        \"\"\"The widget that had focus when the last `AppBlur` happened.\n\n        This will be used to restore correct focus when an `AppFocus`\n        happens.\n        \"\"\"\n\n        # Size of previous inline update\n        self._previous_inline_height: int | None = None\n\n    def validate_title(self, title: Any) -> str:\n        \"\"\"Make sure the title is set to a string.\"\"\"\n        return str(title)\n\n    def validate_sub_title(self, sub_title: Any) -> str:\n        \"\"\"Make sure the sub-title is set to a string.\"\"\"\n        return str(sub_title)\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The [worker](/guide/workers/) manager.\n\n        Returns:\n            An object to manage workers.\n        \"\"\"\n        return self._workers\n\n    @property\n    def return_value(self) -> ReturnType | None:\n        \"\"\"The return value of the app, or `None` if it has not yet been set.\n\n        The return value is set when calling [exit][textual.app.App.exit].\n        \"\"\"\n        return self._return_value\n\n    @property\n    def return_code(self) -> int | None:\n        \"\"\"The return code with which the app exited.\n\n        Non-zero codes indicate errors.\n        A value of 1 means the app exited with a fatal error.\n        If the app wasn't exited yet, this will be `None`.\n\n        Example:\n            The return code can be used to exit the process via `sys.exit`.\n            ```py\n            my_app.run()\n            sys.exit(my_app.return_code)\n            ```\n        \"\"\"\n        return self._return\n```\n# Function Signature: def switch_screen(self, screen: Screen | str) -> AwaitComplete:\n# Natural Language Annotations: \"\"\"Switch to another [screen](/guide/screens) by replacing the top of the screen stack with a new screen.\n\n        Args:\n            screen: Either a Screen object or screen name (the `name` argument when installed).\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```py\ndef switch_screen(self, screen: Screen | str", "  ```python\ndef switch_screen(self, screen: \"Screen |", "  ```\ndef switch_screen(self, screen: (Screen | str"]}, "textual-src/textual/app.py-pop_screen": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/app.py:\n```\n\"\"\"\n\nHere you will find the [App][textual.app.App] class, which is the base class for Textual apps.\n\nSee [app basics](/guide/app) for how to build Textual apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport inspect\nimport io\nimport os\nimport signal\nimport sys\nimport threading\nimport warnings\nfrom asyncio import Task, create_task\nfrom concurrent.futures import Future\nfrom contextlib import (\n    asynccontextmanager,\n    contextmanager,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom datetime import datetime\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    Sequence,\n    Type,\n    TypeVar,\n    overload,\n)\nfrom weakref import WeakKeyDictionary, WeakSet\n\nimport rich\nimport rich.repr\nfrom rich.console import Console, RenderableType\nfrom rich.control import Control\nfrom rich.protocol import is_renderable\nfrom rich.segment import Segment, Segments\nfrom rich.terminal_theme import TerminalTheme\n\nfrom . import (\n    Logger,\n    LogGroup,\n    LogVerbosity,\n    actions,\n    constants,\n    events,\n    log,\n    messages,\n    on,\n)\nfrom ._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction\nfrom ._ansi_sequences import SYNC_END, SYNC_START\nfrom ._ansi_theme import ALABASTER, MONOKAI\nfrom ._callback import invoke\nfrom ._compose import compose\nfrom ._compositor import CompositorUpdate\nfrom ._context import active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._event_broker import NoHandler, extract_handler_actions\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import AnimationLevel\nfrom ._wait import wait_for_idle\nfrom ._worker_manager import WorkerManager\nfrom .actions import ActionParseResult, SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .binding import Binding, BindingType, _Bindings\nfrom .command import CommandPalette, Provider\nfrom .css.errors import StylesheetError\nfrom .css.query import NoMatches\nfrom .css.stylesheet import RulesMap, Stylesheet\nfrom .design import ColorSystem\nfrom .dom import DOMNode, NoScreen\nfrom .driver import Driver\nfrom .errors import NoWidget\nfrom .features import FeatureFlag, parse_features\nfrom .file_monitor import FileMonitor\nfrom .filter import ANSIToTruecolor, DimFilter, Monochrome\nfrom .geometry import Offset, Region, Size\nfrom .keys import (\n    REPLACED_KEYS,\n    _character_to_key,\n    _get_key_display,\n    _get_unicode_name_from_key,\n)\nfrom .messages import CallbackType\nfrom .notifications import Notification, Notifications, Notify, SeverityLevel\nfrom .reactive import Reactive\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .screen import (\n    ActiveBinding,\n    Screen,\n    ScreenResultCallbackType,\n    ScreenResultType,\n    SystemModalScreen,\n)\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import AwaitMount, Widget\nfrom .widgets._toast import ToastRack\nfrom .worker import NoActiveWorker, get_current_worker\n\nif TYPE_CHECKING:\n    from textual_dev.client import DevtoolsClient\n    from typing_extensions import Coroutine, Literal, Self, TypeAlias\n\n    from ._system_commands import SystemCommands\n    from ._types import MessageTarget\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import WrongType  # type: ignore  # noqa: F401\n    from .filter import LineFilter\n    from .message import Message\n    from .pilot import Pilot\n    from .widget import MountError  # type: ignore  # noqa: F401\n\nWINDOWS = sys.platform == \"win32\"\n\n# asyncio will warn against resources not being cleared\nif constants.DEBUG:\n    warnings.simplefilter(\"always\", ResourceWarning)\n\n# `asyncio.get_event_loop()` is deprecated since Python 3.10:\n_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)\n\nLayoutDefinition = \"dict[str, Any]\"\n\nDEFAULT_COLORS = {\n    \"dark\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=True,\n    ),\n    \"light\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=False,\n    ),\n}\n\nComposeResult = Iterable[Widget]\nRenderResult = RenderableType\n\nAutopilotCallbackType: TypeAlias = (\n    \"Callable[[Pilot[object]], Coroutine[Any, Any, None]]\"\n)\n\"\"\"Signature for valid callbacks that can be used to control apps.\"\"\"\n\n\ndef get_system_commands() -> type[SystemCommands]:\n    \"\"\"Callable to lazy load the system commands.\n\n    Returns:\n        System commands class.\n    \"\"\"\n    from ._system_commands import SystemCommands\n\n    return SystemCommands\n\n\nclass AppError(Exception):\n    \"\"\"Base class for general App related exceptions.\"\"\"\n\n\nclass ActionError(Exception):\n    \"\"\"Base class for exceptions relating to actions.\"\"\"\n\n\nclass ScreenError(Exception):\n    \"\"\"Base class for exceptions that relate to screens.\"\"\"\n\n\nclass ScreenStackError(ScreenError):\n    \"\"\"Raised when trying to manipulate the screen stack incorrectly.\"\"\"\n\n\nclass ModeError(Exception):\n    \"\"\"Base class for exceptions related to modes.\"\"\"\n\n\nclass InvalidModeError(ModeError):\n    \"\"\"Raised if there is an issue with a mode name.\"\"\"\n\n\nclass UnknownModeError(ModeError):\n    \"\"\"Raised when attempting to use a mode that is not known.\"\"\"\n\n\nclass ActiveModeError(ModeError):\n    \"\"\"Raised when attempting to remove the currently active mode.\"\"\"\n\n\nclass SuspendNotSupported(Exception):\n    \"\"\"Raised if suspending the application is not supported.\n\n    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while\n    the application is running in an environment where this isn't supported.\n    \"\"\"\n\n\nReturnType = TypeVar(\"ReturnType\")\nCallThreadReturnType = TypeVar(\"CallThreadReturnType\")\n\n\nclass _NullFile:\n    \"\"\"A file-like where writes go nowhere.\"\"\"\n\n    def write(self, text: str) -> None:\n        pass\n\n    def flush(self) -> None:\n        pass\n\n    def isatty(self) -> bool:\n        return True\n\n\nclass _PrintCapture:\n    \"\"\"A file-like which captures output.\"\"\"\n\n    def __init__(self, app: App, stderr: bool = False) -> None:\n        \"\"\"\n\n        Args:\n            app: App instance.\n            stderr: Write from stderr.\n        \"\"\"\n        self.app = app\n        self.stderr = stderr\n\n    def write(self, text: str) -> None:\n        \"\"\"Called when writing to stdout or stderr.\n\n        Args:\n            text: Text that was \"printed\".\n        \"\"\"\n        self.app._print(text, stderr=self.stderr)\n\n    def flush(self) -> None:\n        \"\"\"Called when stdout or stderr was flushed.\"\"\"\n        self.app._flush(stderr=self.stderr)\n\n    def isatty(self) -> bool:\n        \"\"\"Pretend we're a terminal.\"\"\"\n        # TODO: should this be configurable?\n        return True\n\n    def fileno(self) -> int:\n        \"\"\"Return invalid fileno.\"\"\"\n        return -1\n\n\n@rich.repr.auto\nclass App(Generic[ReturnType], DOMNode):\n    \"\"\"The base class for Textual Applications.\"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,\n    and therefore takes priority in the event of a specificity clash.\"\"\"\n\n    # Default (the lowest priority) CSS\n    DEFAULT_CSS: ClassVar[str]\n    DEFAULT_CSS = \"\"\"\n    App {\n        background: $background;\n        color: $text;\n    }\n    *:disabled:can-focus {\n        opacity: 0.7;\n    }\n    \"\"\"\n\n    MODES: ClassVar[dict[str, str | Screen | Callable[[], Screen]]] = {}\n    \"\"\"Modes associated with the app and their base screens.\n\n    The base screen is the screen at the bottom of the mode stack. You can think of\n    it as the default screen for that stack.\n    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],\n    [`Screen`][textual.screen.Screen] instances, or callables that return screens.\n\n    Example:\n        ```py\n        class HelpScreen(Screen[None]):\n            ...\n\n        class MainAppScreen(Screen[None]):\n            ...\n\n        class MyApp(App[None]):\n            MODES = {\n                \"default\": \"main\",\n                \"help\": HelpScreen,\n            }\n\n            SCREENS = {\n                \"main\": MainAppScreen,\n            }\n\n            ...\n        ```\n    \"\"\"\n    SCREENS: ClassVar[dict[str, Screen[Any] | Callable[[], Screen[Any]]]] = {}\n    \"\"\"Screens associated with the app for the lifetime of the app.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = \"*\"\n    \"\"\"A selector to determine what to focus automatically when a screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Setting to `None` or `\"\"` disables auto focus.\n    \"\"\"\n\n    _BASE_PATH: str | None = None\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\"\"\"\n\n    TITLE: str | None = None\n    \"\"\"A class variable to set the *default* title for the application.\n\n    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.\n    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].\n    \"\"\"\n\n    SUB_TITLE: str | None = None\n    \"\"\"A class variable to set the default sub-title for the application.\n\n    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.\n    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True\n    \"\"\"Should the [command palette][textual.command.CommandPalette] be enabled for the application?\"\"\"\n\n    NOTIFICATION_TIMEOUT: ClassVar[float] = 5\n    \"\"\"Default number of seconds to show notifications before removing them.\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {\n        get_system_commands\n    }\n    \"\"\"Command providers used by the [command palette](/guide/command_palette).\n\n    Should be a set of [command.Provider][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS: ClassVar[list[BindingType]] = [\n        Binding(\"ctrl+c\", \"quit\", \"Quit\", show=False, priority=True),\n        Binding(\"ctrl+backslash\", \"command_palette\", show=False, priority=True),\n    ]\n\n    CLOSE_TIMEOUT: float | None = 5.0\n    \"\"\"Timeout waiting for widget's to close, or `None` for no timeout.\"\"\"\n\n    title: Reactive[str] = Reactive(\"\", compute=False)\n    sub_title: Reactive[str] = Reactive(\"\", compute=False)\n\n    dark: Reactive[bool] = Reactive(True, compute=False)\n    \"\"\"Use a dark theme if `True`, otherwise use a light theme.\n\n    Modify this attribute to switch between light and dark themes.\n\n    Example:\n        ```python\n        self.app.dark = not self.app.dark  # Toggle dark mode\n        ```\n    \"\"\"\n    app_focus = Reactive(True, compute=False)\n    \"\"\"Indicates if the app has focus.\n\n    When run in the terminal, the app always has focus. When run in the web, the app will\n    get focus when the terminal widget has focus.\n    \"\"\"\n\n    ansi_theme_dark = Reactive(MONOKAI, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in dark mode.\"\"\"\n\n    ansi_theme_light = Reactive(ALABASTER, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in light mode.\"\"\"\n\n    def __init__(\n        self,\n        driver_class: Type[Driver] | None = None,\n        css_path: CSSPathType | None = None,\n        watch_css: bool = False,\n    ):\n        \"\"\"Create an instance of an app.\n\n        Args:\n            driver_class: Driver class or `None` to auto-detect.\n                This will be used by some Textual tools.\n            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.\n                To load multiple CSS files, pass a list of strings or paths which\n                will be loaded in order.\n            watch_css: Reload CSS if the files changed. This is set automatically if\n                you are using `textual run` with the `dev` switch.\n\n        Raises:\n            CssPathError: When the supplied CSS path(s) are an unexpected type.\n        \"\"\"\n        self._start_time = perf_counter()\n        super().__init__()\n        self.features: frozenset[FeatureFlag] = parse_features(os.getenv(\"TEXTUAL\", \"\"))\n\n        ansi_theme = self.ansi_theme_dark if self.dark else self.ansi_theme_light\n        self._filters: list[LineFilter] = [ANSIToTruecolor(ansi_theme)]\n\n        environ = dict(os.environ)\n        no_color = environ.pop(\"NO_COLOR\", None)\n        if no_color is not None:\n            self._filters.append(Monochrome())\n\n        for filter_name in constants.FILTERS.split(\",\"):\n            filter = filter_name.lower().strip()\n            if filter == \"dim\":\n                self._filters.append(DimFilter())\n\n        self.console = Console(\n            color_system=constants.COLOR_SYSTEM,\n            file=_NullFile(),\n            markup=True,\n            highlight=False,\n            emoji=False,\n            legacy_windows=False,\n            _environ=environ,\n            force_terminal=True,\n            safe_box=False,\n            soft_wrap=False,\n        )\n        self._workers = WorkerManager(self)\n        self.error_console = Console(markup=False, highlight=False, stderr=True)\n        self.driver_class = driver_class or self.get_driver_class()\n        self._screen_stacks: dict[str, list[Screen[Any]]] = {\"_default\": []}\n        \"\"\"A stack of screens per mode.\"\"\"\n        self._current_mode: str = \"_default\"\n        \"\"\"The current mode the app is in.\"\"\"\n        self._sync_available = False\n\n        self.mouse_over: Widget | None = None\n        self.mouse_captured: Widget | None = None\n        self._driver: Driver | None = None\n        self._exit_renderables: list[RenderableType] = []\n\n        self._action_targets = {\"app\", \"screen\", \"focused\"}\n        self._animator = Animator(self)\n        self._animate = self._animator.bind(self)\n        self.mouse_position = Offset(0, 0)\n\n        self._mouse_down_widget: Widget | None = None\n        \"\"\"The widget that was most recently mouse downed (used to create click events).\"\"\"\n\n        self._previous_cursor_position = Offset(0, 0)\n        \"\"\"The previous cursor position\"\"\"\n\n        self.cursor_position = Offset(0, 0)\n        \"\"\"The position of the terminal cursor in screen-space.\n\n        This can be set by widgets and is useful for controlling the\n        positioning of OS IME and emoji popup menus.\"\"\"\n\n        self._exception: Exception | None = None\n        \"\"\"The unhandled exception which is leading to the app shutting down,\n        or None if the app is still running with no unhandled exceptions.\"\"\"\n\n        self._exception_event: asyncio.Event = asyncio.Event()\n        \"\"\"An event that will be set when the first exception is encountered.\"\"\"\n\n        self.title = (\n            self.TITLE if self.TITLE is not None else f\"{self.__class__.__name__}\"\n        )\n        \"\"\"The title for the application.\n\n        The initial value for `title` will be set to the `TITLE` class variable if it exists, or\n        the name of the app if it doesn't.\n\n        Assign a new value to this attribute to change the title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else \"\"\n        \"\"\"The sub-title for the application.\n\n        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or\n        an empty string if it doesn't.\n\n        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to\n        the file being worked on.\n\n        Assign a new value to this attribute to change the sub-title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE\n        \"\"\"A flag to say if the application should use the command palette.\n\n        If set to `False` any call to\n        [`action_command_palette`][textual.app.App.action_command_palette]\n        will be ignored.\n        \"\"\"\n\n        self._logger = Logger(self._log)\n\n        self._refresh_required = False\n\n        self.design = DEFAULT_COLORS\n\n        self._css_has_errors = False\n        self.stylesheet = Stylesheet(variables=self.get_css_variables())\n\n        css_path = css_path or self.CSS_PATH\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(css_path) if css_path is not None else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self._registry: WeakSet[DOMNode] = WeakSet()\n\n        # Sensitivity on X is double the sensitivity on Y to account for\n        # cells being twice as tall as wide\n        self.scroll_sensitivity_x: float = 4.0\n        \"\"\"Number of columns to scroll in the X direction with wheel or trackpad.\"\"\"\n        self.scroll_sensitivity_y: float = 2.0\n        \"\"\"Number of lines to scroll in the Y direction with wheel or trackpad.\"\"\"\n\n        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}\n        self._installed_screens.update(**self.SCREENS)\n\n        self._compose_stacks: list[list[Widget]] = []\n        self._composed: list[list[Widget]] = []\n        self._recompose_required = False\n\n        self.devtools: DevtoolsClient | None = None\n        self._devtools_redirector: StdoutRedirector | None = None\n        if \"devtools\" in self.features:\n            try:\n                from textual_dev.client import DevtoolsClient\n                from textual_dev.redirect_output import StdoutRedirector\n            except ImportError:\n                # Dev dependencies not installed\n                pass\n            else:\n                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)\n                self._devtools_redirector = StdoutRedirector(self.devtools)\n\n        self._loop: asyncio.AbstractEventLoop | None = None\n        self._return_value: ReturnType | None = None\n        \"\"\"Internal attribute used to set the return value for the app.\"\"\"\n        self._return_code: int | None = None\n        \"\"\"Internal attribute used to set the return code for the app.\"\"\"\n        self._exit = False\n        self._disable_tooltips = False\n        self._disable_notifications = False\n\n        self.css_monitor = (\n            FileMonitor(self.css_path, self._on_css_change)\n            if watch_css or self.debug\n            else None\n        )\n        self._screenshot: str | None = None\n        self._dom_lock = RLock()\n        self._dom_ready = False\n        self._batch_count = 0\n        self._notifications = Notifications()\n\n        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (\n            WeakKeyDictionary()\n        )\n        \"\"\"Registry of the MessageTargets which are capturing output at any given time.\"\"\"\n        self._capture_stdout = _PrintCapture(self, stderr=False)\n        \"\"\"File-like object capturing data written to stdout.\"\"\"\n        self._capture_stderr = _PrintCapture(self, stderr=True)\n        \"\"\"File-like object capturing data written to stderr.\"\"\"\n        self._original_stdout = sys.__stdout__\n        \"\"\"The original stdout stream (before redirection etc).\"\"\"\n        self._original_stderr = sys.__stderr__\n        \"\"\"The original stderr stream (before redirection etc).\"\"\"\n\n        self.app_suspend_signal: Signal[App] = Signal(self, \"app-suspend\")\n        \"\"\"The signal that is published when the app is suspended.\n\n        When [`App.suspend`][textual.app.App.suspend] is called this signal\n        will be [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work before the suspension takes place.\n        \"\"\"\n        self.app_resume_signal: Signal[App] = Signal(self, \"app-resume\")\n        \"\"\"The signal that is published when the app is resumed after a suspend.\n\n        When the app is resumed after a\n        [`App.suspend`][textual.app.App.suspend] call this signal will be\n        [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work after the app has resumed.\n        \"\"\"\n\n        self.set_class(self.dark, \"-dark-mode\")\n        self.set_class(not self.dark, \"-light-mode\")\n\n        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS\n        \"\"\"Determines what type of animations the app will display.\n\n        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].\n        \"\"\"\n\n        self._last_focused_on_app_blur: Widget | None = None\n        \"\"\"The widget that had focus when the last `AppBlur` happened.\n\n        This will be used to restore correct focus when an `AppFocus`\n        happens.\n        \"\"\"\n\n        # Size of previous inline update\n        self._previous_inline_height: int | None = None\n\n    def validate_title(self, title: Any) -> str:\n        \"\"\"Make sure the title is set to a string.\"\"\"\n        return str(title)\n\n    def validate_sub_title(self, sub_title: Any) -> str:\n        \"\"\"Make sure the sub-title is set to a string.\"\"\"\n        return str(sub_title)\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The [worker](/guide/workers/) manager.\n\n        Returns:\n            An object to manage workers.\n        \"\"\"\n        return self._workers\n\n    @property\n    def return_value(self) -> ReturnType | None:\n        \"\"\"The return value of the app, or `None` if it has not yet been set.\n\n        The return value is set when calling [exit][textual.app.App.exit].\n        \"\"\"\n        return self._return_value\n\n    @property\n    def return_code(self) -> int | None:\n        \"\"\"The return code with which the app exited.\n\n        Non-zero codes indicate errors.\n        A value of 1 means the app exited with a fatal error.\n        If the app wasn't exited yet, this will be `None`.\n\n        Example:\n            The return code can be used to exit the process via `sys.exit`.\n            ```py\n            my_app.run()\n            sys.exit(my_app.return_code)\n            ```\n        \"\"\"\n        return self._return\n```\n# Function Signature: def pop_screen(self) -> AwaitComplete:\n# Natural Language Annotations: \"\"\"Pop the current [screen](/guide/screens) from the stack, and switch to the previous screen.\n\n        Returns:\n            The screen that was replaced.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```python\ndef pop_screen(self) -> AwaitComplete:", "  ```\ndef pop_screen(self) -> AwaitComplete:\n", "  ```python\ndef pop_screen(self) -> AwaitComplete:"]}, "textual-src/textual/app.py-run_action": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/app.py:\n```\n\"\"\"\n\nHere you will find the [App][textual.app.App] class, which is the base class for Textual apps.\n\nSee [app basics](/guide/app) for how to build Textual apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport inspect\nimport io\nimport os\nimport signal\nimport sys\nimport threading\nimport warnings\nfrom asyncio import Task, create_task\nfrom concurrent.futures import Future\nfrom contextlib import (\n    asynccontextmanager,\n    contextmanager,\n    redirect_stderr,\n    redirect_stdout,\n)\nfrom datetime import datetime\nfrom functools import partial\nfrom time import perf_counter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncGenerator,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generator,\n    Generic,\n    Iterable,\n    Iterator,\n    Sequence,\n    Type,\n    TypeVar,\n    overload,\n)\nfrom weakref import WeakKeyDictionary, WeakSet\n\nimport rich\nimport rich.repr\nfrom rich.console import Console, RenderableType\nfrom rich.control import Control\nfrom rich.protocol import is_renderable\nfrom rich.segment import Segment, Segments\nfrom rich.terminal_theme import TerminalTheme\n\nfrom . import (\n    Logger,\n    LogGroup,\n    LogVerbosity,\n    actions,\n    constants,\n    events,\n    log,\n    messages,\n    on,\n)\nfrom ._animator import DEFAULT_EASING, Animatable, Animator, EasingFunction\nfrom ._ansi_sequences import SYNC_END, SYNC_START\nfrom ._ansi_theme import ALABASTER, MONOKAI\nfrom ._callback import invoke\nfrom ._compose import compose\nfrom ._compositor import CompositorUpdate\nfrom ._context import active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._event_broker import NoHandler, extract_handler_actions\nfrom ._path import CSSPathType, _css_path_type_as_list, _make_path_object_relative\nfrom ._types import AnimationLevel\nfrom ._wait import wait_for_idle\nfrom ._worker_manager import WorkerManager\nfrom .actions import ActionParseResult, SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .binding import Binding, BindingType, _Bindings\nfrom .command import CommandPalette, Provider\nfrom .css.errors import StylesheetError\nfrom .css.query import NoMatches\nfrom .css.stylesheet import RulesMap, Stylesheet\nfrom .design import ColorSystem\nfrom .dom import DOMNode, NoScreen\nfrom .driver import Driver\nfrom .errors import NoWidget\nfrom .features import FeatureFlag, parse_features\nfrom .file_monitor import FileMonitor\nfrom .filter import ANSIToTruecolor, DimFilter, Monochrome\nfrom .geometry import Offset, Region, Size\nfrom .keys import (\n    REPLACED_KEYS,\n    _character_to_key,\n    _get_key_display,\n    _get_unicode_name_from_key,\n)\nfrom .messages import CallbackType\nfrom .notifications import Notification, Notifications, Notify, SeverityLevel\nfrom .reactive import Reactive\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .screen import (\n    ActiveBinding,\n    Screen,\n    ScreenResultCallbackType,\n    ScreenResultType,\n    SystemModalScreen,\n)\nfrom .signal import Signal\nfrom .timer import Timer\nfrom .widget import AwaitMount, Widget\nfrom .widgets._toast import ToastRack\nfrom .worker import NoActiveWorker, get_current_worker\n\nif TYPE_CHECKING:\n    from textual_dev.client import DevtoolsClient\n    from typing_extensions import Coroutine, Literal, Self, TypeAlias\n\n    from ._system_commands import SystemCommands\n    from ._types import MessageTarget\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import WrongType  # type: ignore  # noqa: F401\n    from .filter import LineFilter\n    from .message import Message\n    from .pilot import Pilot\n    from .widget import MountError  # type: ignore  # noqa: F401\n\nWINDOWS = sys.platform == \"win32\"\n\n# asyncio will warn against resources not being cleared\nif constants.DEBUG:\n    warnings.simplefilter(\"always\", ResourceWarning)\n\n# `asyncio.get_event_loop()` is deprecated since Python 3.10:\n_ASYNCIO_GET_EVENT_LOOP_IS_DEPRECATED = sys.version_info >= (3, 10, 0)\n\nLayoutDefinition = \"dict[str, Any]\"\n\nDEFAULT_COLORS = {\n    \"dark\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=True,\n    ),\n    \"light\": ColorSystem(\n        primary=\"#004578\",\n        secondary=\"#ffa62b\",\n        warning=\"#ffa62b\",\n        error=\"#ba3c5b\",\n        success=\"#4EBF71\",\n        accent=\"#0178D4\",\n        dark=False,\n    ),\n}\n\nComposeResult = Iterable[Widget]\nRenderResult = RenderableType\n\nAutopilotCallbackType: TypeAlias = (\n    \"Callable[[Pilot[object]], Coroutine[Any, Any, None]]\"\n)\n\"\"\"Signature for valid callbacks that can be used to control apps.\"\"\"\n\n\ndef get_system_commands() -> type[SystemCommands]:\n    \"\"\"Callable to lazy load the system commands.\n\n    Returns:\n        System commands class.\n    \"\"\"\n    from ._system_commands import SystemCommands\n\n    return SystemCommands\n\n\nclass AppError(Exception):\n    \"\"\"Base class for general App related exceptions.\"\"\"\n\n\nclass ActionError(Exception):\n    \"\"\"Base class for exceptions relating to actions.\"\"\"\n\n\nclass ScreenError(Exception):\n    \"\"\"Base class for exceptions that relate to screens.\"\"\"\n\n\nclass ScreenStackError(ScreenError):\n    \"\"\"Raised when trying to manipulate the screen stack incorrectly.\"\"\"\n\n\nclass ModeError(Exception):\n    \"\"\"Base class for exceptions related to modes.\"\"\"\n\n\nclass InvalidModeError(ModeError):\n    \"\"\"Raised if there is an issue with a mode name.\"\"\"\n\n\nclass UnknownModeError(ModeError):\n    \"\"\"Raised when attempting to use a mode that is not known.\"\"\"\n\n\nclass ActiveModeError(ModeError):\n    \"\"\"Raised when attempting to remove the currently active mode.\"\"\"\n\n\nclass SuspendNotSupported(Exception):\n    \"\"\"Raised if suspending the application is not supported.\n\n    This exception is raised if [`App.suspend`][textual.app.App.suspend] is called while\n    the application is running in an environment where this isn't supported.\n    \"\"\"\n\n\nReturnType = TypeVar(\"ReturnType\")\nCallThreadReturnType = TypeVar(\"CallThreadReturnType\")\n\n\nclass _NullFile:\n    \"\"\"A file-like where writes go nowhere.\"\"\"\n\n    def write(self, text: str) -> None:\n        pass\n\n    def flush(self) -> None:\n        pass\n\n    def isatty(self) -> bool:\n        return True\n\n\nclass _PrintCapture:\n    \"\"\"A file-like which captures output.\"\"\"\n\n    def __init__(self, app: App, stderr: bool = False) -> None:\n        \"\"\"\n\n        Args:\n            app: App instance.\n            stderr: Write from stderr.\n        \"\"\"\n        self.app = app\n        self.stderr = stderr\n\n    def write(self, text: str) -> None:\n        \"\"\"Called when writing to stdout or stderr.\n\n        Args:\n            text: Text that was \"printed\".\n        \"\"\"\n        self.app._print(text, stderr=self.stderr)\n\n    def flush(self) -> None:\n        \"\"\"Called when stdout or stderr was flushed.\"\"\"\n        self.app._flush(stderr=self.stderr)\n\n    def isatty(self) -> bool:\n        \"\"\"Pretend we're a terminal.\"\"\"\n        # TODO: should this be configurable?\n        return True\n\n    def fileno(self) -> int:\n        \"\"\"Return invalid fileno.\"\"\"\n        return -1\n\n\n@rich.repr.auto\nclass App(Generic[ReturnType], DOMNode):\n    \"\"\"The base class for Textual Applications.\"\"\"\n\n    CSS: ClassVar[str] = \"\"\n    \"\"\"Inline CSS, useful for quick scripts. This is loaded after CSS_PATH,\n    and therefore takes priority in the event of a specificity clash.\"\"\"\n\n    # Default (the lowest priority) CSS\n    DEFAULT_CSS: ClassVar[str]\n    DEFAULT_CSS = \"\"\"\n    App {\n        background: $background;\n        color: $text;\n    }\n    *:disabled:can-focus {\n        opacity: 0.7;\n    }\n    \"\"\"\n\n    MODES: ClassVar[dict[str, str | Screen | Callable[[], Screen]]] = {}\n    \"\"\"Modes associated with the app and their base screens.\n\n    The base screen is the screen at the bottom of the mode stack. You can think of\n    it as the default screen for that stack.\n    The base screens can be names of screens listed in [SCREENS][textual.app.App.SCREENS],\n    [`Screen`][textual.screen.Screen] instances, or callables that return screens.\n\n    Example:\n        ```py\n        class HelpScreen(Screen[None]):\n            ...\n\n        class MainAppScreen(Screen[None]):\n            ...\n\n        class MyApp(App[None]):\n            MODES = {\n                \"default\": \"main\",\n                \"help\": HelpScreen,\n            }\n\n            SCREENS = {\n                \"main\": MainAppScreen,\n            }\n\n            ...\n        ```\n    \"\"\"\n    SCREENS: ClassVar[dict[str, Screen[Any] | Callable[[], Screen[Any]]]] = {}\n    \"\"\"Screens associated with the app for the lifetime of the app.\"\"\"\n\n    AUTO_FOCUS: ClassVar[str | None] = \"*\"\n    \"\"\"A selector to determine what to focus automatically when a screen is activated.\n\n    The widget focused is the first that matches the given [CSS selector](/guide/queries/#query-selectors).\n    Setting to `None` or `\"\"` disables auto focus.\n    \"\"\"\n\n    _BASE_PATH: str | None = None\n    CSS_PATH: ClassVar[CSSPathType | None] = None\n    \"\"\"File paths to load CSS from.\"\"\"\n\n    TITLE: str | None = None\n    \"\"\"A class variable to set the *default* title for the application.\n\n    To update the title while the app is running, you can set the [title][textual.app.App.title] attribute.\n    See also [the `Screen.TITLE` attribute][textual.screen.Screen.TITLE].\n    \"\"\"\n\n    SUB_TITLE: str | None = None\n    \"\"\"A class variable to set the default sub-title for the application.\n\n    To update the sub-title while the app is running, you can set the [sub_title][textual.app.App.sub_title] attribute.\n    See also [the `Screen.SUB_TITLE` attribute][textual.screen.Screen.SUB_TITLE].\n    \"\"\"\n\n    ENABLE_COMMAND_PALETTE: ClassVar[bool] = True\n    \"\"\"Should the [command palette][textual.command.CommandPalette] be enabled for the application?\"\"\"\n\n    NOTIFICATION_TIMEOUT: ClassVar[float] = 5\n    \"\"\"Default number of seconds to show notifications before removing them.\"\"\"\n\n    COMMANDS: ClassVar[set[type[Provider] | Callable[[], type[Provider]]]] = {\n        get_system_commands\n    }\n    \"\"\"Command providers used by the [command palette](/guide/command_palette).\n\n    Should be a set of [command.Provider][textual.command.Provider] classes.\n    \"\"\"\n\n    BINDINGS: ClassVar[list[BindingType]] = [\n        Binding(\"ctrl+c\", \"quit\", \"Quit\", show=False, priority=True),\n        Binding(\"ctrl+backslash\", \"command_palette\", show=False, priority=True),\n    ]\n\n    CLOSE_TIMEOUT: float | None = 5.0\n    \"\"\"Timeout waiting for widget's to close, or `None` for no timeout.\"\"\"\n\n    title: Reactive[str] = Reactive(\"\", compute=False)\n    sub_title: Reactive[str] = Reactive(\"\", compute=False)\n\n    dark: Reactive[bool] = Reactive(True, compute=False)\n    \"\"\"Use a dark theme if `True`, otherwise use a light theme.\n\n    Modify this attribute to switch between light and dark themes.\n\n    Example:\n        ```python\n        self.app.dark = not self.app.dark  # Toggle dark mode\n        ```\n    \"\"\"\n    app_focus = Reactive(True, compute=False)\n    \"\"\"Indicates if the app has focus.\n\n    When run in the terminal, the app always has focus. When run in the web, the app will\n    get focus when the terminal widget has focus.\n    \"\"\"\n\n    ansi_theme_dark = Reactive(MONOKAI, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in dark mode.\"\"\"\n\n    ansi_theme_light = Reactive(ALABASTER, init=False)\n    \"\"\"Maps ANSI colors to hex colors using a Rich TerminalTheme object while in light mode.\"\"\"\n\n    def __init__(\n        self,\n        driver_class: Type[Driver] | None = None,\n        css_path: CSSPathType | None = None,\n        watch_css: bool = False,\n    ):\n        \"\"\"Create an instance of an app.\n\n        Args:\n            driver_class: Driver class or `None` to auto-detect.\n                This will be used by some Textual tools.\n            css_path: Path to CSS or `None` to use the `CSS_PATH` class variable.\n                To load multiple CSS files, pass a list of strings or paths which\n                will be loaded in order.\n            watch_css: Reload CSS if the files changed. This is set automatically if\n                you are using `textual run` with the `dev` switch.\n\n        Raises:\n            CssPathError: When the supplied CSS path(s) are an unexpected type.\n        \"\"\"\n        self._start_time = perf_counter()\n        super().__init__()\n        self.features: frozenset[FeatureFlag] = parse_features(os.getenv(\"TEXTUAL\", \"\"))\n\n        ansi_theme = self.ansi_theme_dark if self.dark else self.ansi_theme_light\n        self._filters: list[LineFilter] = [ANSIToTruecolor(ansi_theme)]\n\n        environ = dict(os.environ)\n        no_color = environ.pop(\"NO_COLOR\", None)\n        if no_color is not None:\n            self._filters.append(Monochrome())\n\n        for filter_name in constants.FILTERS.split(\",\"):\n            filter = filter_name.lower().strip()\n            if filter == \"dim\":\n                self._filters.append(DimFilter())\n\n        self.console = Console(\n            color_system=constants.COLOR_SYSTEM,\n            file=_NullFile(),\n            markup=True,\n            highlight=False,\n            emoji=False,\n            legacy_windows=False,\n            _environ=environ,\n            force_terminal=True,\n            safe_box=False,\n            soft_wrap=False,\n        )\n        self._workers = WorkerManager(self)\n        self.error_console = Console(markup=False, highlight=False, stderr=True)\n        self.driver_class = driver_class or self.get_driver_class()\n        self._screen_stacks: dict[str, list[Screen[Any]]] = {\"_default\": []}\n        \"\"\"A stack of screens per mode.\"\"\"\n        self._current_mode: str = \"_default\"\n        \"\"\"The current mode the app is in.\"\"\"\n        self._sync_available = False\n\n        self.mouse_over: Widget | None = None\n        self.mouse_captured: Widget | None = None\n        self._driver: Driver | None = None\n        self._exit_renderables: list[RenderableType] = []\n\n        self._action_targets = {\"app\", \"screen\", \"focused\"}\n        self._animator = Animator(self)\n        self._animate = self._animator.bind(self)\n        self.mouse_position = Offset(0, 0)\n\n        self._mouse_down_widget: Widget | None = None\n        \"\"\"The widget that was most recently mouse downed (used to create click events).\"\"\"\n\n        self._previous_cursor_position = Offset(0, 0)\n        \"\"\"The previous cursor position\"\"\"\n\n        self.cursor_position = Offset(0, 0)\n        \"\"\"The position of the terminal cursor in screen-space.\n\n        This can be set by widgets and is useful for controlling the\n        positioning of OS IME and emoji popup menus.\"\"\"\n\n        self._exception: Exception | None = None\n        \"\"\"The unhandled exception which is leading to the app shutting down,\n        or None if the app is still running with no unhandled exceptions.\"\"\"\n\n        self._exception_event: asyncio.Event = asyncio.Event()\n        \"\"\"An event that will be set when the first exception is encountered.\"\"\"\n\n        self.title = (\n            self.TITLE if self.TITLE is not None else f\"{self.__class__.__name__}\"\n        )\n        \"\"\"The title for the application.\n\n        The initial value for `title` will be set to the `TITLE` class variable if it exists, or\n        the name of the app if it doesn't.\n\n        Assign a new value to this attribute to change the title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.sub_title = self.SUB_TITLE if self.SUB_TITLE is not None else \"\"\n        \"\"\"The sub-title for the application.\n\n        The initial value for `sub_title` will be set to the `SUB_TITLE` class variable if it exists, or\n        an empty string if it doesn't.\n\n        Sub-titles are typically used to show the high-level state of the app, such as the current mode, or path to\n        the file being worked on.\n\n        Assign a new value to this attribute to change the sub-title.\n        The new value is always converted to string.\n        \"\"\"\n\n        self.use_command_palette: bool = self.ENABLE_COMMAND_PALETTE\n        \"\"\"A flag to say if the application should use the command palette.\n\n        If set to `False` any call to\n        [`action_command_palette`][textual.app.App.action_command_palette]\n        will be ignored.\n        \"\"\"\n\n        self._logger = Logger(self._log)\n\n        self._refresh_required = False\n\n        self.design = DEFAULT_COLORS\n\n        self._css_has_errors = False\n        self.stylesheet = Stylesheet(variables=self.get_css_variables())\n\n        css_path = css_path or self.CSS_PATH\n        css_paths = [\n            _make_path_object_relative(css_path, self)\n            for css_path in (\n                _css_path_type_as_list(css_path) if css_path is not None else []\n            )\n        ]\n        self.css_path = css_paths\n\n        self._registry: WeakSet[DOMNode] = WeakSet()\n\n        # Sensitivity on X is double the sensitivity on Y to account for\n        # cells being twice as tall as wide\n        self.scroll_sensitivity_x: float = 4.0\n        \"\"\"Number of columns to scroll in the X direction with wheel or trackpad.\"\"\"\n        self.scroll_sensitivity_y: float = 2.0\n        \"\"\"Number of lines to scroll in the Y direction with wheel or trackpad.\"\"\"\n\n        self._installed_screens: dict[str, Screen | Callable[[], Screen]] = {}\n        self._installed_screens.update(**self.SCREENS)\n\n        self._compose_stacks: list[list[Widget]] = []\n        self._composed: list[list[Widget]] = []\n        self._recompose_required = False\n\n        self.devtools: DevtoolsClient | None = None\n        self._devtools_redirector: StdoutRedirector | None = None\n        if \"devtools\" in self.features:\n            try:\n                from textual_dev.client import DevtoolsClient\n                from textual_dev.redirect_output import StdoutRedirector\n            except ImportError:\n                # Dev dependencies not installed\n                pass\n            else:\n                self.devtools = DevtoolsClient(constants.DEVTOOLS_HOST)\n                self._devtools_redirector = StdoutRedirector(self.devtools)\n\n        self._loop: asyncio.AbstractEventLoop | None = None\n        self._return_value: ReturnType | None = None\n        \"\"\"Internal attribute used to set the return value for the app.\"\"\"\n        self._return_code: int | None = None\n        \"\"\"Internal attribute used to set the return code for the app.\"\"\"\n        self._exit = False\n        self._disable_tooltips = False\n        self._disable_notifications = False\n\n        self.css_monitor = (\n            FileMonitor(self.css_path, self._on_css_change)\n            if watch_css or self.debug\n            else None\n        )\n        self._screenshot: str | None = None\n        self._dom_lock = RLock()\n        self._dom_ready = False\n        self._batch_count = 0\n        self._notifications = Notifications()\n\n        self._capture_print: WeakKeyDictionary[MessageTarget, tuple[bool, bool]] = (\n            WeakKeyDictionary()\n        )\n        \"\"\"Registry of the MessageTargets which are capturing output at any given time.\"\"\"\n        self._capture_stdout = _PrintCapture(self, stderr=False)\n        \"\"\"File-like object capturing data written to stdout.\"\"\"\n        self._capture_stderr = _PrintCapture(self, stderr=True)\n        \"\"\"File-like object capturing data written to stderr.\"\"\"\n        self._original_stdout = sys.__stdout__\n        \"\"\"The original stdout stream (before redirection etc).\"\"\"\n        self._original_stderr = sys.__stderr__\n        \"\"\"The original stderr stream (before redirection etc).\"\"\"\n\n        self.app_suspend_signal: Signal[App] = Signal(self, \"app-suspend\")\n        \"\"\"The signal that is published when the app is suspended.\n\n        When [`App.suspend`][textual.app.App.suspend] is called this signal\n        will be [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work before the suspension takes place.\n        \"\"\"\n        self.app_resume_signal: Signal[App] = Signal(self, \"app-resume\")\n        \"\"\"The signal that is published when the app is resumed after a suspend.\n\n        When the app is resumed after a\n        [`App.suspend`][textual.app.App.suspend] call this signal will be\n        [published][textual.signal.Signal.publish];\n        [subscribe][textual.signal.Signal.subscribe] to this signal to\n        perform work after the app has resumed.\n        \"\"\"\n\n        self.set_class(self.dark, \"-dark-mode\")\n        self.set_class(not self.dark, \"-light-mode\")\n\n        self.animation_level: AnimationLevel = constants.TEXTUAL_ANIMATIONS\n        \"\"\"Determines what type of animations the app will display.\n\n        See [`textual.constants.TEXTUAL_ANIMATIONS`][textual.constants.TEXTUAL_ANIMATIONS].\n        \"\"\"\n\n        self._last_focused_on_app_blur: Widget | None = None\n        \"\"\"The widget that had focus when the last `AppBlur` happened.\n\n        This will be used to restore correct focus when an `AppFocus`\n        happens.\n        \"\"\"\n\n        # Size of previous inline update\n        self._previous_inline_height: int | None = None\n\n    def validate_title(self, title: Any) -> str:\n        \"\"\"Make sure the title is set to a string.\"\"\"\n        return str(title)\n\n    def validate_sub_title(self, sub_title: Any) -> str:\n        \"\"\"Make sure the sub-title is set to a string.\"\"\"\n        return str(sub_title)\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The [worker](/guide/workers/) manager.\n\n        Returns:\n            An object to manage workers.\n        \"\"\"\n        return self._workers\n\n    @property\n    def return_value(self) -> ReturnType | None:\n        \"\"\"The return value of the app, or `None` if it has not yet been set.\n\n        The return value is set when calling [exit][textual.app.App.exit].\n        \"\"\"\n        return self._return_value\n\n    @property\n    def return_code(self) -> int | None:\n        \"\"\"The return code with which the app exited.\n\n        Non-zero codes indicate errors.\n        A value of 1 means the app exited with a fatal error.\n        If the app wasn't exited yet, this will be `None`.\n\n        Example:\n            The return code can be used to exit the process via `sys.exit`.\n            ```py\n            my_app.run()\n            sys.exit(my_app.return_code)\n            ```\n        \"\"\"\n        return self._return\n```\n# Function Signature: async def run_action(\n# Natural Language Annotations: \"\"\"Perform an [action](/guide/actions).\n\n        Actions are typically associated with key bindings, where you wouldn't need to call this method manually.\n\n        Args:\n            action: Action encoded in a string.\n            default_namespace: Namespace to use if not provided in the action,\n                or None to use app.\n\n        Returns:\n            True if the event has been handled.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  \n[PYTHON]\n```\nasync def run_action(", "  ```python\nasync def run_action(self, action: str,", "  ```py\nasync def run_action(\n    self,\n   "]}, "textual-src/textual/file_monitor.py-check": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/file_monitor.py:\n```\nfrom __future__ import annotations\n\nimport os\nfrom pathlib import Path\nfrom typing import Callable, Iterable, Sequence\n\nimport rich.repr\n\nfrom ._callback import invoke\n\n\n@rich.repr.auto\nclass FileMonitor:\n    \"\"\"Monitors files for changes and invokes a callback when it does.\"\"\"\n\n    _paths: set[Path]\n\n    def __init__(self, paths: Sequence[Path], callback: Callable[[], None]) -> None:\n        \"\"\"Monitor the given file paths for changes.\n\n        Args:\n            paths: Paths to monitor.\n            callback: Callback to invoke if any of the paths change.\n        \"\"\"\n        self._paths = set(paths)\n        self.callback = callback\n        self._modified = self._get_last_modified_time()\n\n    def __rich_repr__(self) -> rich.repr.Result:\n        yield self._paths\n\n    def _get_last_modified_time(self) -> float:\n        \"\"\"Get the most recent modified time out of all files being watched.\"\"\"\n        modified_times = []\n        for path in self._paths:\n            try:\n                modified_time = os.stat(path).st_mtime\n            except FileNotFoundError:\n                modified_time = 0\n            modified_times.append(modified_time)\n        return max(modified_times, default=0)\n\n\n\n\n\n\n\n\n    def add_paths(self, paths: Iterable[Path]) -> None:\n        \"\"\"Adds paths to start being monitored.\n\n        Args:\n            paths: The paths to be monitored.\n        \"\"\"\n        self._paths.update(paths)\n\n    async def __call__(self) -> None:\n        if self.check():\n            await self.on_change()\n\n    async def on_change(self) -> None:\n        \"\"\"Called when any of the monitored files change.\"\"\"\n        await invoke(self.callback)\n\n```\n# Function Signature: def check(self) -> bool:\n# Natural Language Annotations: \"\"\"Check the monitored files. Return True if any were changed since the last modification time.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n    def check(self) -> bool:\n        \"\"\"Check", "  ```\ndef check(self) -> bool:\n    modified = self", "  ```\ndef check(self) -> bool:\n    \"\"\"Check the"]}, "textual-src/textual/expand_tabs.py-expand_tabs_inline": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/expand_tabs.py:\n```\nfrom __future__ import annotations\n\nimport re\n\nfrom rich.cells import cell_len\nfrom rich.text import Text\n\n_TABS_SPLITTER_RE = re.compile(r\"(.*?\\t|.+?$)\")\n\n\ndef get_tab_widths(line: str, tab_size: int = 4) -> list[tuple[str, int]]:\n    \"\"\"Splits a string line into tuples (str, int).\n\n    Each tuple represents a section of the line which precedes a tab character.\n    The string is the string text that appears before the tab character (excluding the tab).\n    The integer is the width that the tab character is expanded to.\n\n    Args:\n        line: The text to expand tabs in.\n        tab_size: Number of cells in a tab.\n\n    Returns:\n        A list of tuples representing the line split on tab characters,\n            and the widths of the tabs after tab expansion is applied.\n    \"\"\"\n\n    parts: list[tuple[str, int]] = []\n    add_part = parts.append\n    cell_position = 0\n    matches = _TABS_SPLITTER_RE.findall(line)\n\n    for match in matches:\n        expansion_width = 0\n        if match.endswith(\"\\t\"):\n            # Remove the tab, and check the width of the rest of the line.\n            match = match[:-1]\n            cell_position += cell_len(match)\n\n            # Now move along the line by the width of the tab.\n            tab_remainder = cell_position % tab_size\n            expansion_width = tab_size - tab_remainder\n            cell_position += expansion_width\n\n        add_part((match, expansion_width))\n\n    return parts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef expand_text_tabs_from_widths(line: Text, tab_widths: list[int]) -> Text:\n    \"\"\"Expand tabs to the widths defined in the `tab_widths` list.\n\n    This will return a new Text instance with tab characters expanded into a\n    number of spaces. Each time a tab is encountered, it's expanded into the\n    next integer encountered in the `tab_widths` list. Consequently, the length\n    of `tab_widths` should match the number of tab characters in `line`.\n\n    Args:\n        line: The `Text` instance to expand tabs in.\n        tab_widths: The widths to expand tabs to.\n\n    Returns:\n        A new text instance with tab characters converted to spaces.\n    \"\"\"\n    if \"\\t\" not in line.plain:\n        return line\n\n    parts = line.split(\"\\t\", include_separator=True)\n    tab_widths_iter = iter(tab_widths)\n\n    new_parts: list[Text] = []\n    append_part = new_parts.append\n    for part in parts:\n        if part.plain.endswith(\"\\t\"):\n            part._text[-1] = part._text[-1][:-1] + \" \"\n            spaces = next(tab_widths_iter)\n            part.extend_style(spaces - 1)\n        append_part(part)\n\n    return Text(\"\", end=\"\").join(new_parts)\n\n\nif __name__ == \"__main__\":\n    print(expand_tabs_inline(\"\\tbar\"))\n    print(expand_tabs_inline(\"\\tbar\\t\"))\n    print(expand_tabs_inline(\"1\\tbar\"))\n    print(expand_tabs_inline(\"12\\tbar\"))\n    print(expand_tabs_inline(\"123\\tbar\"))\n    print(expand_tabs_inline(\"1234\\tbar\"))\n    print(expand_tabs_inline(\"\ud83d\udca9\\tbar\"))\n    print(expand_tabs_inline(\"\ud83d\udca9\ud83d\udca9\\tbar\"))\n    print(expand_tabs_inline(\"\ud83d\udca9\ud83d\udca9\ud83d\udca9\\tbar\"))\n    print(expand_tabs_inline(\"F\ud83d\udca9\\tbar\"))\n    print(expand_tabs_inline(\"F\ud83d\udca9O\\tbar\"))\n\n```\n# Function Signature: def expand_tabs_inline(line: str, tab_size: int = 4) -> str:\n# Natural Language Annotations: \"\"\"Expands tabs, taking into account double cell characters.\n\n    Args:\n        line: The text to expand tabs in.\n        tab_size: Number of cells in a tab.\n    Returns:\n        New string with tabs replaced with spaces.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef expand_tabs_inline(line: str, tab_", "  ```\ndef expand_tabs_inline(line: str, tab_", "  ```\ndef expand_tabs_inline(line: str, tab_"]}, "textual-src/textual/_resolve.py-resolve": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/_resolve.py:\n```\nfrom __future__ import annotations\n\nfrom fractions import Fraction\nfrom itertools import accumulate\nfrom typing import TYPE_CHECKING, Iterable, Sequence, cast\n\nfrom typing_extensions import Literal\n\nfrom .box_model import BoxModel\nfrom .css.scalar import Scalar\nfrom .css.styles import RenderStyles\nfrom .geometry import Size\n\nif TYPE_CHECKING:\n    from .widget import Widget\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef resolve_fraction_unit(\n    widget_styles: Iterable[RenderStyles],\n    size: Size,\n    viewport_size: Size,\n    remaining_space: Fraction,\n    resolve_dimension: Literal[\"width\", \"height\"] = \"width\",\n) -> Fraction:\n    \"\"\"Calculate the fraction.\n\n    Args:\n        widget_styles: Styles for widgets with fraction units.\n        size: Container size.\n        viewport_size: Viewport size.\n        remaining_space: Remaining space for fr units.\n        resolve_dimension: Which dimension to resolve.\n\n    Returns:\n        The value of 1fr.\n    \"\"\"\n    if not remaining_space or not widget_styles:\n        return Fraction(1)\n\n    initial_space = remaining_space\n\n    def resolve_scalar(\n        scalar: Scalar | None, fraction_unit: Fraction = Fraction(1)\n    ) -> Fraction | None:\n        \"\"\"Resolve a scalar if it is not None.\n\n        Args:\n            scalar: Optional scalar to resolve.\n            fraction_unit: Size of 1fr.\n\n        Returns:\n            Fraction if resolved, otherwise None.\n        \"\"\"\n        return (\n            None\n            if scalar is None\n            else scalar.resolve(size, viewport_size, fraction_unit)\n        )\n\n    resolve: list[tuple[Scalar, Fraction | None, Fraction | None]] = []\n\n    if resolve_dimension == \"width\":\n        resolve = [\n            (\n                cast(Scalar, styles.width),\n                resolve_scalar(styles.min_width),\n                resolve_scalar(styles.max_width),\n            )\n            for styles in widget_styles\n            if styles.overlay != \"screen\"\n        ]\n    else:\n        resolve = [\n            (\n                cast(Scalar, styles.height),\n                resolve_scalar(styles.min_height),\n                resolve_scalar(styles.max_height),\n            )\n            for styles in widget_styles\n            if styles.overlay != \"screen\"\n        ]\n\n    resolved: list[Fraction | None] = [None] * len(resolve)\n    remaining_fraction = Fraction(sum(scalar.value for scalar, _, _ in resolve))\n\n    while remaining_fraction > 0:\n        remaining_space_changed = False\n        resolve_fraction = Fraction(remaining_space, remaining_fraction)\n        for index, (scalar, min_value, max_value) in enumerate(resolve):\n            value = resolved[index]\n            if value is None:\n                resolved_scalar = scalar.resolve(size, viewport_size, resolve_fraction)\n                if min_value is not None and resolved_scalar < min_value:\n                    remaining_space -= min_value\n                    remaining_fraction -= Fraction(scalar.value)\n                    resolved[index] = min_value\n                    remaining_space_changed = True\n                elif max_value is not None and resolved_scalar > max_value:\n                    remaining_space -= max_value\n                    remaining_fraction -= Fraction(scalar.value)\n                    resolved[index] = max_value\n                    remaining_space_changed = True\n\n        if not remaining_space_changed:\n            break\n\n    return (\n        Fraction(remaining_space, remaining_fraction)\n        if remaining_fraction > 0\n        else initial_space\n    )\n\n\ndef resolve_box_models(\n    dimensions: list[Scalar | None],\n    widgets: list[Widget],\n    size: Size,\n    viewport_size: Size,\n    margin: Size,\n    resolve_dimension: Literal[\"width\", \"height\"] = \"width\",\n) -> list[BoxModel]:\n    \"\"\"Resolve box models for a list of dimensions\n\n    Args:\n        dimensions: A list of Scalars or Nones for each dimension.\n        widgets: Widgets in resolve.\n        size: Size of container.\n        viewport_size: Viewport size.\n        margin: Total space occupied by margin\n        resolve_dimension: Which dimension to resolve.\n\n    Returns:\n        List of resolved box models.\n    \"\"\"\n    margin_width, margin_height = margin\n\n    fraction_width = Fraction(max(0, size.width - margin_width))\n    fraction_height = Fraction(max(0, size.height - margin_height))\n\n    margin_size = size - margin\n\n    # Fixed box models\n    box_models: list[BoxModel | None] = [\n        (\n            None\n            if _dimension is not None and _dimension.is_fraction\n            else widget._get_box_model(\n                size, viewport_size, fraction_width, fraction_height\n            )\n        )\n        for (_dimension, widget) in zip(dimensions, widgets)\n    ]\n\n    if None not in box_models:\n        # No fr units, so we're done\n        return cast(\"list[BoxModel]\", box_models)\n\n    # If all box models have been calculated\n    widget_styles = [widget.styles for widget in widgets]\n    if resolve_dimension == \"width\":\n        total_remaining = int(\n            sum(\n                [\n                    box_model.width\n                    for widget, box_model in zip(widgets, box_models)\n                    if (box_model is not None and widget.styles.overlay != \"screen\")\n                ]\n            )\n        )\n\n        remaining_space = int(max(0, size.width - total_remaining - margin_width))\n        fraction_unit = resolve_fraction_unit(\n            [\n                styles\n                for styles in widget_styles\n                if styles.width is not None\n                and styles.width.is_fraction\n                and styles.overlay != \"screen\"\n            ],\n            size,\n            viewport_size,\n            Fraction(remaining_space),\n            resolve_dimension,\n        )\n        width_fraction = fraction_unit\n        height_fraction = Fraction(margin_size.height)\n    else:\n        total_remaining = int(\n            sum(\n                [\n                    box_model.height\n                    for widget, box_model in zip(widgets, box_models)\n                    if (box_model is not None and widget.styles.overlay != \"screen\")\n                ]\n            )\n        )\n\n        remaining_space = int(max(0, size.height - total_remaining - margin_height))\n        fraction_unit = resolve_fraction_unit(\n            [\n                styles\n                for styles in widget_styles\n                if styles.height is not None\n                and styles.height.is_fraction\n                and styles.overlay != \"screen\"\n            ],\n            size,\n            viewport_size,\n            Fraction(remaining_space),\n            resolve_dimension,\n        )\n        width_fraction = Fraction(margin_size.width)\n        height_fraction = fraction_unit\n\n    box_models = [\n        box_model\n        or widget._get_box_model(\n            size,\n            viewport_size,\n            width_fraction,\n            height_fraction,\n        )\n        for widget, box_model in zip(widgets, box_models)\n    ]\n\n    return cast(\"list[BoxModel]\", box_models)\n\n```\n# Function Signature: def resolve(\n# Natural Language Annotations: \"\"\"Resolve a list of dimensions.\n\n    Args:\n        dimensions: Scalars for column / row sizes.\n        total: Total space to divide.\n        gutter: Gutter between rows / columns.\n        size: Size of container.\n        viewport: Size of viewport.\n\n    Returns:\n        List of (<OFFSET>, <LENGTH>)\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef resolve(dimensions: tuple[Scalar, ...],", "  ```\ndef resolve(dimensions: list[Scalar | None],", "  ```\ndef resolve(dimensions: list[Scalar], total:"]}, "textual-src/textual/widget.py-mount": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/widget.py:\n```\n\"\"\"\nThe base class for widgets.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom asyncio import create_task, wait\nfrom collections import Counter\nfrom contextlib import asynccontextmanager\nfrom fractions import Fraction\nfrom itertools import islice\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    AsyncGenerator,\n    Awaitable,\n    ClassVar,\n    Collection,\n    Generator,\n    Iterable,\n    NamedTuple,\n    Sequence,\n    TypeVar,\n    cast,\n    overload,\n)\n\nimport rich.repr\nfrom rich.console import (\n    Console,\n    ConsoleOptions,\n    ConsoleRenderable,\n    JustifyMethod,\n    RenderableType,\n)\nfrom rich.console import RenderResult as RichRenderResult\nfrom rich.console import RichCast\nfrom rich.measure import Measurement\nfrom rich.segment import Segment\nfrom rich.style import Style\nfrom rich.text import Text\nfrom typing_extensions import Self\n\nif TYPE_CHECKING:\n    from .app import RenderResult\n\nfrom . import constants, errors, events, messages\nfrom ._animator import DEFAULT_EASING, Animatable, BoundAnimator, EasingFunction\nfrom ._arrange import DockArrangeResult, arrange\nfrom ._compose import compose\nfrom ._context import NoActiveAppError, active_app\nfrom ._easing import DEFAULT_SCROLL_EASING\nfrom ._layout import Layout\nfrom ._segment_tools import align_lines\nfrom ._styles_cache import StylesCache\nfrom ._types import AnimationLevel\nfrom .actions import SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .box_model import BoxModel\nfrom .cache import FIFOCache\nfrom .css.match import match\nfrom .css.parse import parse_selectors\nfrom .css.query import NoMatches, WrongType\nfrom .css.scalar import ScalarOffset\nfrom .dom import DOMNode, NoScreen\nfrom .geometry import (\n    NULL_REGION,\n    NULL_SIZE,\n    NULL_SPACING,\n    Offset,\n    Region,\n    Size,\n    Spacing,\n    clamp,\n)\nfrom .layouts.vertical import VerticalLayout\nfrom .message import Message\nfrom .messages import CallbackType\nfrom .notifications import SeverityLevel\nfrom .reactive import Reactive\nfrom .render import measure\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .strip import Strip\nfrom .walk import walk_depth_first\n\nif TYPE_CHECKING:\n    from .app import App, ComposeResult\n    from .css.query import QueryType\n    from .message_pump import MessagePump\n    from .scrollbar import (\n        ScrollBar,\n        ScrollBarCorner,\n        ScrollDown,\n        ScrollLeft,\n        ScrollRight,\n        ScrollTo,\n        ScrollUp,\n    )\n\n_JUSTIFY_MAP: dict[str, JustifyMethod] = {\n    \"start\": \"left\",\n    \"end\": \"right\",\n    \"justify\": \"full\",\n}\n\n\n_NULL_STYLE = Style()\n\n\nclass AwaitMount:\n    \"\"\"An *optional* awaitable returned by [mount][textual.widget.Widget.mount] and [mount_all][textual.widget.Widget.mount_all].\n\n    Example:\n        ```python\n        await self.mount(Static(\"foo\"))\n        ```\n    \"\"\"\n\n    def __init__(self, parent: Widget, widgets: Sequence[Widget]) -> None:\n        self._parent = parent\n        self._widgets = widgets\n\n    async def __call__(self) -> None:\n        \"\"\"Allows awaiting via a call operation.\"\"\"\n        await self\n\n    def __await__(self) -> Generator[None, None, None]:\n        async def await_mount() -> None:\n            if self._widgets:\n                aws = [\n                    create_task(widget._mounted_event.wait(), name=\"await mount\")\n                    for widget in self._widgets\n                ]\n                if aws:\n                    await wait(aws)\n                    self._parent.refresh(layout=True)\n                    try:\n                        self._parent.app._update_mouse_over(self._parent.screen)\n                    except NoScreen:\n                        pass\n\n        return await_mount().__await__()\n\n\nclass _Styled:\n    \"\"\"Apply a style to a renderable.\n\n    Args:\n        renderable: Any renderable.\n        style: A style to apply across the entire renderable.\n    \"\"\"\n\n    def __init__(\n        self, renderable: \"ConsoleRenderable\", style: Style, link_style: Style | None\n    ) -> None:\n        self.renderable = renderable\n        self.style = style\n        self.link_style = link_style\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> \"RichRenderResult\":\n        style = console.get_style(self.style)\n        result_segments = console.render(self.renderable, options)\n\n        _Segment = Segment\n        if style:\n            apply = style.__add__\n            result_segments = (\n                _Segment(text, apply(_style), None)\n                for text, _style, control in result_segments\n            )\n        link_style = self.link_style\n        if link_style:\n            result_segments = (\n                _Segment(\n                    text,\n                    (\n                        style\n                        if style._meta is None\n                        else (style + link_style if \"@click\" in style.meta else style)\n                    ),\n                    control,\n                )\n                for text, style, control in result_segments\n                if style is not None\n            )\n        return result_segments\n\n    def __rich_measure__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> Measurement:\n        return Measurement.get(console, options, self.renderable)\n\n\nclass _RenderCache(NamedTuple):\n    \"\"\"Stores results of a previous render.\"\"\"\n\n    size: Size\n    \"\"\"The size of the render.\"\"\"\n    lines: list[Strip]\n    \"\"\"Contents of the render.\"\"\"\n\n\nclass WidgetError(Exception):\n    \"\"\"Base widget error.\"\"\"\n\n\nclass MountError(WidgetError):\n    \"\"\"Error raised when there was a problem with the mount request.\"\"\"\n\n\nclass PseudoClasses(NamedTuple):\n    \"\"\"Used for render/render_line based widgets that use caching. This structure can be used as a\n    cache-key.\"\"\"\n\n    enabled: bool\n    \"\"\"Is 'enabled' applied?\"\"\"\n    focus: bool\n    \"\"\"Is 'focus' applied?\"\"\"\n    hover: bool\n    \"\"\"Is 'hover' applied?\"\"\"\n\n\nclass _BorderTitle:\n    \"\"\"Descriptor to set border titles.\"\"\"\n\n    def __set_name__(self, owner: Widget, name: str) -> None:\n        # The private name where we store the real data.\n        self._internal_name = f\"_{name}\"\n\n    def __set__(self, obj: Widget, title: str | Text | None) -> None:\n        \"\"\"Setting a title accepts a str, Text, or None.\"\"\"\n        if title is None:\n            setattr(obj, self._internal_name, None)\n        else:\n            # We store the title as Text\n            new_title = obj.render_str(title)\n            new_title.expand_tabs(4)\n            new_title = new_title.split()[0]\n            setattr(obj, self._internal_name, new_title)\n        obj.refresh()\n\n    def __get__(self, obj: Widget, objtype: type[Widget] | None = None) -> str | None:\n        \"\"\"Getting a title will return None or a str as console markup.\"\"\"\n        title: Text | None = getattr(obj, self._internal_name, None)\n        if title is None:\n            return None\n        # If we have a title, convert from Text to console markup\n        return title.markup\n\n\nclass BadWidgetName(Exception):\n    \"\"\"Raised when widget class names do not satisfy the required restrictions.\"\"\"\n\n\n@rich.repr.auto\nclass Widget(DOMNode):\n    \"\"\"\n    A Widget is the base class for Textual widgets.\n\n    See also [static][textual.widgets._static.Static] for starting point for your own widgets.\n    \"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    Widget{\n        scrollbar-background: $panel-darken-1;\n        scrollbar-background-hover: $panel-darken-2;\n        scrollbar-background-active: $panel-darken-3;\n        scrollbar-color: $primary-lighten-1;\n        scrollbar-color-active: $warning-darken-1;\n        scrollbar-color-hover: $primary-lighten-1;\n        scrollbar-corner-color: $panel-darken-1;\n        scrollbar-size-vertical: 2;\n        scrollbar-size-horizontal: 1;\n        link-background: initial;\n        link-color: $text;\n        link-style: underline;\n        link-background-hover: $accent;\n        link-color-hover: $text;\n        link-style-hover: bold not underline;\n    }\n    \"\"\"\n    COMPONENT_CLASSES: ClassVar[set[str]] = set()\n\n    BORDER_TITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_title attribute.\"\"\"\n\n    BORDER_SUBTITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_subtitle attribute.\"\"\"\n\n    can_focus: bool = False\n    \"\"\"Widget may receive focus.\"\"\"\n    can_focus_children: bool = True\n    \"\"\"Widget's children may receive focus.\"\"\"\n    expand: Reactive[bool] = Reactive(False)\n    \"\"\"Rich renderable may expand beyond optimal size.\"\"\"\n    shrink: Reactive[bool] = Reactive(True)\n    \"\"\"Rich renderable may shrink below optimal size.\"\"\"\n    auto_links: Reactive[bool] = Reactive(True)\n    \"\"\"Widget will highlight links automatically.\"\"\"\n    disabled: Reactive[bool] = Reactive(False)\n    \"\"\"Is the widget disabled? Disabled widgets can not be interacted with, and are typically styled to look dimmer.\"\"\"\n\n    hover_style: Reactive[Style] = Reactive(Style, repaint=False)\n    \"\"\"The current hover style (style under the mouse cursor). Read only.\"\"\"\n    highlight_link_id: Reactive[str] = Reactive(\"\")\n    \"\"\"The currently highlighted link id. Read only.\"\"\"\n    loading: Reactive[bool] = Reactive(False)\n    \"\"\"If set to `True` this widget will temporarily be replaced with a loading indicator.\"\"\"\n\n    # Default sort order, incremented by constructor\n    _sort_order: ClassVar[int] = 0\n\n    def __init__(\n        self,\n        *children: Widget,\n        name: str | None = None,\n        id: str | None = None,\n        classes: str | None = None,\n        disabled: bool = False,\n    ) -> None:\n        \"\"\"Initialize a Widget.\n\n        Args:\n            *children: Child widgets.\n            name: The name of the widget.\n            id: The ID of the widget in the DOM.\n            classes: The CSS classes for the widget.\n            disabled: Whether the widget is disabled or not.\n        \"\"\"\n        _null_size = NULL_SIZE\n        self._size = _null_size\n        self._container_size = _null_size\n        self._layout_required = False\n        self._repaint_required = False\n        self._scroll_required = False\n        self._recompose_required = False\n        self._default_layout = VerticalLayout()\n        self._animate: BoundAnimator | None = None\n        Widget._sort_order += 1\n        self.sort_order = Widget._sort_order\n        self.highlight_style: Style | None = None\n\n        self._vertical_scrollbar: ScrollBar | None = None\n        self._horizontal_scrollbar: ScrollBar | None = None\n        self._scrollbar_corner: ScrollBarCorner | None = None\n\n        self._border_title: Text | None = None\n        self._border_subtitle: Text | None = None\n\n        self._render_cache = _RenderCache(_null_size, [])\n        # Regions which need to be updated (in Widget)\n        self._dirty_regions: set[Region] = set()\n        # Regions which need to be transferred from cache to screen\n        self._repaint_regions: set[Region] = set()\n\n        # Cache the auto content dimensions\n        self._content_width_cache: tuple[object, int] = (None, 0)\n        self._content_height_cache: tuple[object, int] = (None, 0)\n\n        self._arrangement_cache: FIFOCache[tuple[Size, int], DockArrangeResult] = (\n            FIFOCache(4)\n        )\n\n        self._styles_cache = StylesCache()\n        self._rich_style_cache: dict[tuple[str, ...], tuple[Style, Style]] = {}\n\n        self._tooltip: RenderableType | None = None\n        \"\"\"The tooltip content.\"\"\"\n        self._absolute_offset: Offset | None = None\n        \"\"\"Force an absolute offset for the widget (used by tooltips).\"\"\"\n\n        self._scrollbar_changes: set[tuple[bool, bool]] = set()\n        \"\"\"Used to stabilize scrollbars.\"\"\"\n\n        super().__init__(\n            name=name,\n            id=id,\n            classes=self.DEFAULT_CLASSES if classes is None else classes,\n        )\n\n        if self in children:\n            raise WidgetError(\"A widget can't be its own parent\")\n\n        for child in children:\n            if not isinstance(child, Widget):\n                raise TypeError(\n                    f\"Widget positional arguments must be Widget subclasses; not {child!r}\"\n                )\n        self._pending_children = list(children)\n        self.disabled = disabled\n        if self.BORDER_TITLE:\n            self.border_title = self.BORDER_TITLE\n        if self.BORDER_SUBTITLE:\n            self.border_subtitle = self.BORDER_SUBTITLE\n\n        self.lock = RLock()\n        \"\"\"`asyncio` lock to be used to synchronize the state of the widget.\n\n        Two different tasks might call methods on a widget at the same time, which\n        might result in a race condition.\n        This can be fixed by adding `async with widget.lock:` around the method calls.\n        \"\"\"\n        self._anchored: Widget | None = None\n        \"\"\"An anchored child widget, or `None` if no child is anchored.\"\"\"\n        self._anchor_animate: bool = False\n        \"\"\"Flag to enable animation when scrolling anchored widgets.\"\"\"\n\n    virtual_size: Reactive[Size] = Reactive(Size(0, 0), layout=True)\n    \"\"\"The virtual (scrollable) [size][textual.geometry.Size] of the widget.\"\"\"\n\n    has_focus: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Does this widget have focus? Read only.\"\"\"\n\n    mouse_over: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Is the mouse over this widget? Read only.\"\"\"\n\n    scroll_x: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the X axis.\"\"\"\n\n    scroll_y: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the Y axis.\"\"\"\n\n    scroll_target_x = Reactive(0.0, repaint=False)\n    scroll_target_y = Reactive(0.0, repaint=False)\n\n    show_vertical_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a vertical scrollbar?\"\"\"\n\n    show_horizontal_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a horizontal scrollbar?\"\"\"\n\n    border_title: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the top border (if there is one).\"\"\"\n    border_subtitle: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the bottom border (if there is one).\"\"\"\n\n    @property\n    def is_mounted(self) -> bool:\n        \"\"\"Check if this widget is mounted.\"\"\"\n        return self._is_mounted\n\n    @property\n    def siblings(self) -> list[Widget]:\n        \"\"\"Get the widget's siblings (self is removed from the return list).\n\n        Returns:\n            A list of siblings.\n        \"\"\"\n        parent = self.parent\n        if parent is not None:\n            siblings = list(parent._nodes)\n            siblings.remove(self)\n            return siblings\n        else:\n            return []\n\n    @property\n    def visible_siblings(self) -> list[Widget]:\n        \"\"\"A list of siblings which will be shown.\n\n        Returns:\n            List of siblings.\n        \"\"\"\n        siblings = [\n            widget for widget in self.siblings if widget.visible and widget.display\n        ]\n        return siblings\n\n    @property\n    def allow_vertical_scroll(self) -> bool:\n        \"\"\"Check if vertical scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_vertical_scrollbar\n\n    @property\n    def allow_horizontal_scroll(self) -> bool:\n        \"\"\"Check if horizontal scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_horizontal_scrollbar\n\n    @property\n    def _allow_scroll(self) -> bool:\n        \"\"\"Check if both axis may be scrolled.\n\n        Returns:\n            True if horizontal and vertical scrolling is enabled.\n        \"\"\"\n        return self.is_scrollable and (\n            self.allow_horizontal_scroll or self.allow_vertical_scroll\n        )\n\n    @property\n    def offset(self) -> Offset:\n        \"\"\"Widget offset from origin.\n\n        Returns:\n            Relative offset.\n        \"\"\"\n        return self.styles.offset.resolve(self.size, self.app.size)\n\n    @offset.setter\n    def offset(self, offset: tuple[int, int]) -> None:\n        self.styles.offset = ScalarOffset.from_offset(offset)\n\n    @property\n    def opacity(self) -> float:\n        \"\"\"Total opacity of widget.\"\"\"\n        opacity = 1.0\n        for node in reversed(self.ancestors_with_self):\n            opacity *= node.styles.opacity\n            if not opacity:\n                break\n        return opacity\n\n    @property\n    def is_anchored(self) -> bool:\n        \"\"\"Is this widget anchored?\"\"\"\n        return self._parent is not None and self._parent is self\n\n    def anchor(self, *, animate: bool = False) -> None:\n        \"\"\"Anchor the widget, which scrolls it into view (like [scroll_visible][textual.widget.Widget.scroll_visible]),\n        but also keeps it in view if the widget's size changes, or the size of its container changes.\n\n        !!! note\n\n            Anchored widgets will be un-anchored if the users scrolls the container.\n\n        Args:\n            animate: `True` if the scroll should animate, or `False` if it shouldn't.\n        \"\"\"\n        if self._parent is not None and isinstance(self._parent, Widget):\n            self._parent._anchored = self\n            self._parent._anchor_animate = animate\n            self.check_idle()\n\n    def clear_anchor(self) -> None:\n        \"\"\"Stop anchoring this widget (a no-op if this widget is not anchored).\"\"\"\n        if (\n            self._parent is not None\n            and isinstance(self._parent, Widget)\n            and self._parent._anchored is self\n        ):\n            self._parent._anchored = None\n\n    def _clear_anchor(self) -> None:\n        \"\"\"Clear an anchored child.\"\"\"\n        self._anchored = None\n\n    def _check_disabled(self) -> bool:\n        \"\"\"Check if the widget is disabled either explicitly by setting `disabled`,\n        or implicitly by setting `loading`.\n\n        Returns:\n            True if the widget should be disabled.\n        \"\"\"\n        return self.disabled or self.loading\n\n    @property\n    def tooltip(self) -> RenderableType | None:\n        \"\"\"Tooltip for the widget, or `None` for no tooltip.\"\"\"\n        return self._tooltip\n\n    @tooltip.setter\n    def tooltip(self, tooltip: RenderableType | None):\n        self._tooltip = tooltip\n        try:\n            self.screen._update_tooltip(self)\n        except NoScreen:\n            pass\n\n    def allow_focus(self) -> bool:\n        \"\"\"Check if the widget is permitted to focus.\n\n        The base class returns [`can_focus`][textual.widget.Widget.can_focus].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget may be focused, or `False` if it may not be focused.\n        \"\"\"\n        return self.can_focus\n\n    def allow_focus_children(self) -> bool:\n        \"\"\"Check if a widget's children may be focused.\n\n        The base class returns [`can_focus_children`][textual.widget.Widget.can_focus_children].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget's children may be focused, or `False` if the widget's children may not be focused.\n        \"\"\"\n        return self.can_focus_children\n\n    def compose_add_child(self, widget: Widget) -> None:\n        \"\"\"Add a node to children.\n\n        This is used by the compose process when it adds children.\n        There is no need to use it directly, but you may want to override it in a subclass\n        if you want children to be attached to a different node.\n\n        Args:\n            widget: A Widget to add.\n        \"\"\"\n        _rich_traceback_omit = True\n        self._pending_children.append(widget)\n\n    def __enter__(self) -> Self:\n        \"\"\"Use as context manager when composing.\"\"\"\n        self.app._compose_stacks[-1].append(self)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        \"\"\"Exit compose context manager.\"\"\"\n        compose_stack = self.app._compose_stacks[-1]\n        composed = compose_stack.pop()\n        if compose_stack:\n            compose_stack[-1].compose_add_child(composed)\n        else:\n            self.app._composed[-1].append(composed)\n\n    def clear_cached_dimensions(self) -> None:\n        \"\"\"Clear cached results of `get_content_width` and `get_content_height`.\n\n        Call if the widget's renderable changes size after the widget has been created.\n\n        !!! note\n\n            This is not required if you are extending [`Static`][textual.widgets.Static].\n\n        \"\"\"\n        self._content_width_cache = (None, 0)\n        self._content_height_cache = (None, 0)\n\n    def get_loading_widget(self) -> Widget:\n        \"\"\"Get a widget to display a loading indicator.\n\n        The default implementation will defer to App.get_loading_widget.\n\n        Returns:\n            A widget in place of this widget to indicate a loading.\n        \"\"\"\n        loading_widget = self.app.get_loading_widget()\n        return loading_widget\n\n    def set_loading(self, loading: bool) -> Awaitable:\n        \"\"\"Set or reset the loading state of this widget.\n\n        A widget in a loading state will display a LoadingIndicator that obscures the widget.\n\n        Args:\n            loading: `True` to put the widget into a loading state, or `False` to reset the loading state.\n\n        Returns:\n            An optional awaitable.\n        \"\"\"\n        LOADING_INDICATOR_CLASS = \"-textual-loading-indicator\"\n        LOADING_INDICATOR_QUERY = f\".{LOADING_INDICATOR_CLASS}\"\n        remove_indicator = self.query_children(LOADING_INDICATOR_QUERY).remove()\n        if loading:\n            loading_indicator = self.get_loading_widget()\n            loading_indicator.add_class(LOADING_INDICATOR_CLASS)\n            await_mount = self.mount(loading_indicator)\n            return AwaitComplete(remove_indicator, await_mount).call_next(self)\n        else:\n            return remove_indicator\n\n    async def _watch_loading(self, loading: bool) -> None:\n        \"\"\"Called when the 'loading' reactive is changed.\"\"\"\n        await self.set_loading(loading)\n\n    ExpectType = TypeVar(\"ExpectType\", bound=\"Widget\")\n\n    if TYPE_CHECKING:\n\n        @overload\n        def get_child_by_id(self, id: str) -> Widget: ...\n\n        @overload\n        def get_child_by_id(\n            self, id: str, expect_type: type[ExpectType]\n        ) -> ExpectType: ...\n\n    def get_child_by_id(\n        self, id: str, expect_type: type[ExpectType] | None = None\n    ) -> ExpectType | Widget:\n        \"\"\"Return the first child (immediate descendent) of this node with the given ID.\n\n       \n```\n# Function Signature: def mount(\n# Natural Language Annotations: \"\"\"Mount widgets below this widget (making this widget a container).\n\n        Args:\n            *widgets: The widget(s) to mount.\n            before: Optional location to mount before. An `int` is the index\n                of the child to mount before, a `str` is a `query_one` query to\n                find the widget to mount before.\n            after: Optional location to mount after. An `int` is the index\n                of the child to mount after, a `str` is a `query_one` query to\n                find the widget to mount after.\n\n        Returns:\n            An awaitable object that waits for widgets to be mounted.\n\n        Raises:\n            MountError: If there is a problem with the mount request.\n\n        Note:\n            Only one of ``before`` or ``after`` can be provided. If both are\n            provided a ``MountError`` will be raised.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  def mount(\n        self: Widget,\n        *widgets:", "  ```python\ndef mount(\n    self,\n    *widgets", "  if TYPE_CHECKING:\n\n            @overload\n"]}, "textual-src/textual/widget.py-move_child": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/widget.py:\n```\n\"\"\"\nThe base class for widgets.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom asyncio import create_task, wait\nfrom collections import Counter\nfrom contextlib import asynccontextmanager\nfrom fractions import Fraction\nfrom itertools import islice\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    AsyncGenerator,\n    Awaitable,\n    ClassVar,\n    Collection,\n    Generator,\n    Iterable,\n    NamedTuple,\n    Sequence,\n    TypeVar,\n    cast,\n    overload,\n)\n\nimport rich.repr\nfrom rich.console import (\n    Console,\n    ConsoleOptions,\n    ConsoleRenderable,\n    JustifyMethod,\n    RenderableType,\n)\nfrom rich.console import RenderResult as RichRenderResult\nfrom rich.console import RichCast\nfrom rich.measure import Measurement\nfrom rich.segment import Segment\nfrom rich.style import Style\nfrom rich.text import Text\nfrom typing_extensions import Self\n\nif TYPE_CHECKING:\n    from .app import RenderResult\n\nfrom . import constants, errors, events, messages\nfrom ._animator import DEFAULT_EASING, Animatable, BoundAnimator, EasingFunction\nfrom ._arrange import DockArrangeResult, arrange\nfrom ._compose import compose\nfrom ._context import NoActiveAppError, active_app\nfrom ._easing import DEFAULT_SCROLL_EASING\nfrom ._layout import Layout\nfrom ._segment_tools import align_lines\nfrom ._styles_cache import StylesCache\nfrom ._types import AnimationLevel\nfrom .actions import SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .box_model import BoxModel\nfrom .cache import FIFOCache\nfrom .css.match import match\nfrom .css.parse import parse_selectors\nfrom .css.query import NoMatches, WrongType\nfrom .css.scalar import ScalarOffset\nfrom .dom import DOMNode, NoScreen\nfrom .geometry import (\n    NULL_REGION,\n    NULL_SIZE,\n    NULL_SPACING,\n    Offset,\n    Region,\n    Size,\n    Spacing,\n    clamp,\n)\nfrom .layouts.vertical import VerticalLayout\nfrom .message import Message\nfrom .messages import CallbackType\nfrom .notifications import SeverityLevel\nfrom .reactive import Reactive\nfrom .render import measure\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .strip import Strip\nfrom .walk import walk_depth_first\n\nif TYPE_CHECKING:\n    from .app import App, ComposeResult\n    from .css.query import QueryType\n    from .message_pump import MessagePump\n    from .scrollbar import (\n        ScrollBar,\n        ScrollBarCorner,\n        ScrollDown,\n        ScrollLeft,\n        ScrollRight,\n        ScrollTo,\n        ScrollUp,\n    )\n\n_JUSTIFY_MAP: dict[str, JustifyMethod] = {\n    \"start\": \"left\",\n    \"end\": \"right\",\n    \"justify\": \"full\",\n}\n\n\n_NULL_STYLE = Style()\n\n\nclass AwaitMount:\n    \"\"\"An *optional* awaitable returned by [mount][textual.widget.Widget.mount] and [mount_all][textual.widget.Widget.mount_all].\n\n    Example:\n        ```python\n        await self.mount(Static(\"foo\"))\n        ```\n    \"\"\"\n\n    def __init__(self, parent: Widget, widgets: Sequence[Widget]) -> None:\n        self._parent = parent\n        self._widgets = widgets\n\n    async def __call__(self) -> None:\n        \"\"\"Allows awaiting via a call operation.\"\"\"\n        await self\n\n    def __await__(self) -> Generator[None, None, None]:\n        async def await_mount() -> None:\n            if self._widgets:\n                aws = [\n                    create_task(widget._mounted_event.wait(), name=\"await mount\")\n                    for widget in self._widgets\n                ]\n                if aws:\n                    await wait(aws)\n                    self._parent.refresh(layout=True)\n                    try:\n                        self._parent.app._update_mouse_over(self._parent.screen)\n                    except NoScreen:\n                        pass\n\n        return await_mount().__await__()\n\n\nclass _Styled:\n    \"\"\"Apply a style to a renderable.\n\n    Args:\n        renderable: Any renderable.\n        style: A style to apply across the entire renderable.\n    \"\"\"\n\n    def __init__(\n        self, renderable: \"ConsoleRenderable\", style: Style, link_style: Style | None\n    ) -> None:\n        self.renderable = renderable\n        self.style = style\n        self.link_style = link_style\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> \"RichRenderResult\":\n        style = console.get_style(self.style)\n        result_segments = console.render(self.renderable, options)\n\n        _Segment = Segment\n        if style:\n            apply = style.__add__\n            result_segments = (\n                _Segment(text, apply(_style), None)\n                for text, _style, control in result_segments\n            )\n        link_style = self.link_style\n        if link_style:\n            result_segments = (\n                _Segment(\n                    text,\n                    (\n                        style\n                        if style._meta is None\n                        else (style + link_style if \"@click\" in style.meta else style)\n                    ),\n                    control,\n                )\n                for text, style, control in result_segments\n                if style is not None\n            )\n        return result_segments\n\n    def __rich_measure__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> Measurement:\n        return Measurement.get(console, options, self.renderable)\n\n\nclass _RenderCache(NamedTuple):\n    \"\"\"Stores results of a previous render.\"\"\"\n\n    size: Size\n    \"\"\"The size of the render.\"\"\"\n    lines: list[Strip]\n    \"\"\"Contents of the render.\"\"\"\n\n\nclass WidgetError(Exception):\n    \"\"\"Base widget error.\"\"\"\n\n\nclass MountError(WidgetError):\n    \"\"\"Error raised when there was a problem with the mount request.\"\"\"\n\n\nclass PseudoClasses(NamedTuple):\n    \"\"\"Used for render/render_line based widgets that use caching. This structure can be used as a\n    cache-key.\"\"\"\n\n    enabled: bool\n    \"\"\"Is 'enabled' applied?\"\"\"\n    focus: bool\n    \"\"\"Is 'focus' applied?\"\"\"\n    hover: bool\n    \"\"\"Is 'hover' applied?\"\"\"\n\n\nclass _BorderTitle:\n    \"\"\"Descriptor to set border titles.\"\"\"\n\n    def __set_name__(self, owner: Widget, name: str) -> None:\n        # The private name where we store the real data.\n        self._internal_name = f\"_{name}\"\n\n    def __set__(self, obj: Widget, title: str | Text | None) -> None:\n        \"\"\"Setting a title accepts a str, Text, or None.\"\"\"\n        if title is None:\n            setattr(obj, self._internal_name, None)\n        else:\n            # We store the title as Text\n            new_title = obj.render_str(title)\n            new_title.expand_tabs(4)\n            new_title = new_title.split()[0]\n            setattr(obj, self._internal_name, new_title)\n        obj.refresh()\n\n    def __get__(self, obj: Widget, objtype: type[Widget] | None = None) -> str | None:\n        \"\"\"Getting a title will return None or a str as console markup.\"\"\"\n        title: Text | None = getattr(obj, self._internal_name, None)\n        if title is None:\n            return None\n        # If we have a title, convert from Text to console markup\n        return title.markup\n\n\nclass BadWidgetName(Exception):\n    \"\"\"Raised when widget class names do not satisfy the required restrictions.\"\"\"\n\n\n@rich.repr.auto\nclass Widget(DOMNode):\n    \"\"\"\n    A Widget is the base class for Textual widgets.\n\n    See also [static][textual.widgets._static.Static] for starting point for your own widgets.\n    \"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    Widget{\n        scrollbar-background: $panel-darken-1;\n        scrollbar-background-hover: $panel-darken-2;\n        scrollbar-background-active: $panel-darken-3;\n        scrollbar-color: $primary-lighten-1;\n        scrollbar-color-active: $warning-darken-1;\n        scrollbar-color-hover: $primary-lighten-1;\n        scrollbar-corner-color: $panel-darken-1;\n        scrollbar-size-vertical: 2;\n        scrollbar-size-horizontal: 1;\n        link-background: initial;\n        link-color: $text;\n        link-style: underline;\n        link-background-hover: $accent;\n        link-color-hover: $text;\n        link-style-hover: bold not underline;\n    }\n    \"\"\"\n    COMPONENT_CLASSES: ClassVar[set[str]] = set()\n\n    BORDER_TITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_title attribute.\"\"\"\n\n    BORDER_SUBTITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_subtitle attribute.\"\"\"\n\n    can_focus: bool = False\n    \"\"\"Widget may receive focus.\"\"\"\n    can_focus_children: bool = True\n    \"\"\"Widget's children may receive focus.\"\"\"\n    expand: Reactive[bool] = Reactive(False)\n    \"\"\"Rich renderable may expand beyond optimal size.\"\"\"\n    shrink: Reactive[bool] = Reactive(True)\n    \"\"\"Rich renderable may shrink below optimal size.\"\"\"\n    auto_links: Reactive[bool] = Reactive(True)\n    \"\"\"Widget will highlight links automatically.\"\"\"\n    disabled: Reactive[bool] = Reactive(False)\n    \"\"\"Is the widget disabled? Disabled widgets can not be interacted with, and are typically styled to look dimmer.\"\"\"\n\n    hover_style: Reactive[Style] = Reactive(Style, repaint=False)\n    \"\"\"The current hover style (style under the mouse cursor). Read only.\"\"\"\n    highlight_link_id: Reactive[str] = Reactive(\"\")\n    \"\"\"The currently highlighted link id. Read only.\"\"\"\n    loading: Reactive[bool] = Reactive(False)\n    \"\"\"If set to `True` this widget will temporarily be replaced with a loading indicator.\"\"\"\n\n    # Default sort order, incremented by constructor\n    _sort_order: ClassVar[int] = 0\n\n    def __init__(\n        self,\n        *children: Widget,\n        name: str | None = None,\n        id: str | None = None,\n        classes: str | None = None,\n        disabled: bool = False,\n    ) -> None:\n        \"\"\"Initialize a Widget.\n\n        Args:\n            *children: Child widgets.\n            name: The name of the widget.\n            id: The ID of the widget in the DOM.\n            classes: The CSS classes for the widget.\n            disabled: Whether the widget is disabled or not.\n        \"\"\"\n        _null_size = NULL_SIZE\n        self._size = _null_size\n        self._container_size = _null_size\n        self._layout_required = False\n        self._repaint_required = False\n        self._scroll_required = False\n        self._recompose_required = False\n        self._default_layout = VerticalLayout()\n        self._animate: BoundAnimator | None = None\n        Widget._sort_order += 1\n        self.sort_order = Widget._sort_order\n        self.highlight_style: Style | None = None\n\n        self._vertical_scrollbar: ScrollBar | None = None\n        self._horizontal_scrollbar: ScrollBar | None = None\n        self._scrollbar_corner: ScrollBarCorner | None = None\n\n        self._border_title: Text | None = None\n        self._border_subtitle: Text | None = None\n\n        self._render_cache = _RenderCache(_null_size, [])\n        # Regions which need to be updated (in Widget)\n        self._dirty_regions: set[Region] = set()\n        # Regions which need to be transferred from cache to screen\n        self._repaint_regions: set[Region] = set()\n\n        # Cache the auto content dimensions\n        self._content_width_cache: tuple[object, int] = (None, 0)\n        self._content_height_cache: tuple[object, int] = (None, 0)\n\n        self._arrangement_cache: FIFOCache[tuple[Size, int], DockArrangeResult] = (\n            FIFOCache(4)\n        )\n\n        self._styles_cache = StylesCache()\n        self._rich_style_cache: dict[tuple[str, ...], tuple[Style, Style]] = {}\n\n        self._tooltip: RenderableType | None = None\n        \"\"\"The tooltip content.\"\"\"\n        self._absolute_offset: Offset | None = None\n        \"\"\"Force an absolute offset for the widget (used by tooltips).\"\"\"\n\n        self._scrollbar_changes: set[tuple[bool, bool]] = set()\n        \"\"\"Used to stabilize scrollbars.\"\"\"\n\n        super().__init__(\n            name=name,\n            id=id,\n            classes=self.DEFAULT_CLASSES if classes is None else classes,\n        )\n\n        if self in children:\n            raise WidgetError(\"A widget can't be its own parent\")\n\n        for child in children:\n            if not isinstance(child, Widget):\n                raise TypeError(\n                    f\"Widget positional arguments must be Widget subclasses; not {child!r}\"\n                )\n        self._pending_children = list(children)\n        self.disabled = disabled\n        if self.BORDER_TITLE:\n            self.border_title = self.BORDER_TITLE\n        if self.BORDER_SUBTITLE:\n            self.border_subtitle = self.BORDER_SUBTITLE\n\n        self.lock = RLock()\n        \"\"\"`asyncio` lock to be used to synchronize the state of the widget.\n\n        Two different tasks might call methods on a widget at the same time, which\n        might result in a race condition.\n        This can be fixed by adding `async with widget.lock:` around the method calls.\n        \"\"\"\n        self._anchored: Widget | None = None\n        \"\"\"An anchored child widget, or `None` if no child is anchored.\"\"\"\n        self._anchor_animate: bool = False\n        \"\"\"Flag to enable animation when scrolling anchored widgets.\"\"\"\n\n    virtual_size: Reactive[Size] = Reactive(Size(0, 0), layout=True)\n    \"\"\"The virtual (scrollable) [size][textual.geometry.Size] of the widget.\"\"\"\n\n    has_focus: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Does this widget have focus? Read only.\"\"\"\n\n    mouse_over: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Is the mouse over this widget? Read only.\"\"\"\n\n    scroll_x: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the X axis.\"\"\"\n\n    scroll_y: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the Y axis.\"\"\"\n\n    scroll_target_x = Reactive(0.0, repaint=False)\n    scroll_target_y = Reactive(0.0, repaint=False)\n\n    show_vertical_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a vertical scrollbar?\"\"\"\n\n    show_horizontal_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a horizontal scrollbar?\"\"\"\n\n    border_title: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the top border (if there is one).\"\"\"\n    border_subtitle: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the bottom border (if there is one).\"\"\"\n\n    @property\n    def is_mounted(self) -> bool:\n        \"\"\"Check if this widget is mounted.\"\"\"\n        return self._is_mounted\n\n    @property\n    def siblings(self) -> list[Widget]:\n        \"\"\"Get the widget's siblings (self is removed from the return list).\n\n        Returns:\n            A list of siblings.\n        \"\"\"\n        parent = self.parent\n        if parent is not None:\n            siblings = list(parent._nodes)\n            siblings.remove(self)\n            return siblings\n        else:\n            return []\n\n    @property\n    def visible_siblings(self) -> list[Widget]:\n        \"\"\"A list of siblings which will be shown.\n\n        Returns:\n            List of siblings.\n        \"\"\"\n        siblings = [\n            widget for widget in self.siblings if widget.visible and widget.display\n        ]\n        return siblings\n\n    @property\n    def allow_vertical_scroll(self) -> bool:\n        \"\"\"Check if vertical scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_vertical_scrollbar\n\n    @property\n    def allow_horizontal_scroll(self) -> bool:\n        \"\"\"Check if horizontal scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_horizontal_scrollbar\n\n    @property\n    def _allow_scroll(self) -> bool:\n        \"\"\"Check if both axis may be scrolled.\n\n        Returns:\n            True if horizontal and vertical scrolling is enabled.\n        \"\"\"\n        return self.is_scrollable and (\n            self.allow_horizontal_scroll or self.allow_vertical_scroll\n        )\n\n    @property\n    def offset(self) -> Offset:\n        \"\"\"Widget offset from origin.\n\n        Returns:\n            Relative offset.\n        \"\"\"\n        return self.styles.offset.resolve(self.size, self.app.size)\n\n    @offset.setter\n    def offset(self, offset: tuple[int, int]) -> None:\n        self.styles.offset = ScalarOffset.from_offset(offset)\n\n    @property\n    def opacity(self) -> float:\n        \"\"\"Total opacity of widget.\"\"\"\n        opacity = 1.0\n        for node in reversed(self.ancestors_with_self):\n            opacity *= node.styles.opacity\n            if not opacity:\n                break\n        return opacity\n\n    @property\n    def is_anchored(self) -> bool:\n        \"\"\"Is this widget anchored?\"\"\"\n        return self._parent is not None and self._parent is self\n\n    def anchor(self, *, animate: bool = False) -> None:\n        \"\"\"Anchor the widget, which scrolls it into view (like [scroll_visible][textual.widget.Widget.scroll_visible]),\n        but also keeps it in view if the widget's size changes, or the size of its container changes.\n\n        !!! note\n\n            Anchored widgets will be un-anchored if the users scrolls the container.\n\n        Args:\n            animate: `True` if the scroll should animate, or `False` if it shouldn't.\n        \"\"\"\n        if self._parent is not None and isinstance(self._parent, Widget):\n            self._parent._anchored = self\n            self._parent._anchor_animate = animate\n            self.check_idle()\n\n    def clear_anchor(self) -> None:\n        \"\"\"Stop anchoring this widget (a no-op if this widget is not anchored).\"\"\"\n        if (\n            self._parent is not None\n            and isinstance(self._parent, Widget)\n            and self._parent._anchored is self\n        ):\n            self._parent._anchored = None\n\n    def _clear_anchor(self) -> None:\n        \"\"\"Clear an anchored child.\"\"\"\n        self._anchored = None\n\n    def _check_disabled(self) -> bool:\n        \"\"\"Check if the widget is disabled either explicitly by setting `disabled`,\n        or implicitly by setting `loading`.\n\n        Returns:\n            True if the widget should be disabled.\n        \"\"\"\n        return self.disabled or self.loading\n\n    @property\n    def tooltip(self) -> RenderableType | None:\n        \"\"\"Tooltip for the widget, or `None` for no tooltip.\"\"\"\n        return self._tooltip\n\n    @tooltip.setter\n    def tooltip(self, tooltip: RenderableType | None):\n        self._tooltip = tooltip\n        try:\n            self.screen._update_tooltip(self)\n        except NoScreen:\n            pass\n\n    def allow_focus(self) -> bool:\n        \"\"\"Check if the widget is permitted to focus.\n\n        The base class returns [`can_focus`][textual.widget.Widget.can_focus].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget may be focused, or `False` if it may not be focused.\n        \"\"\"\n        return self.can_focus\n\n    def allow_focus_children(self) -> bool:\n        \"\"\"Check if a widget's children may be focused.\n\n        The base class returns [`can_focus_children`][textual.widget.Widget.can_focus_children].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget's children may be focused, or `False` if the widget's children may not be focused.\n        \"\"\"\n        return self.can_focus_children\n\n    def compose_add_child(self, widget: Widget) -> None:\n        \"\"\"Add a node to children.\n\n        This is used by the compose process when it adds children.\n        There is no need to use it directly, but you may want to override it in a subclass\n        if you want children to be attached to a different node.\n\n        Args:\n            widget: A Widget to add.\n        \"\"\"\n        _rich_traceback_omit = True\n        self._pending_children.append(widget)\n\n    def __enter__(self) -> Self:\n        \"\"\"Use as context manager when composing.\"\"\"\n        self.app._compose_stacks[-1].append(self)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        \"\"\"Exit compose context manager.\"\"\"\n        compose_stack = self.app._compose_stacks[-1]\n        composed = compose_stack.pop()\n        if compose_stack:\n            compose_stack[-1].compose_add_child(composed)\n        else:\n            self.app._composed[-1].append(composed)\n\n    def clear_cached_dimensions(self) -> None:\n        \"\"\"Clear cached results of `get_content_width` and `get_content_height`.\n\n        Call if the widget's renderable changes size after the widget has been created.\n\n        !!! note\n\n            This is not required if you are extending [`Static`][textual.widgets.Static].\n\n        \"\"\"\n        self._content_width_cache = (None, 0)\n        self._content_height_cache = (None, 0)\n\n    def get_loading_widget(self) -> Widget:\n        \"\"\"Get a widget to display a loading indicator.\n\n        The default implementation will defer to App.get_loading_widget.\n\n        Returns:\n            A widget in place of this widget to indicate a loading.\n        \"\"\"\n        loading_widget = self.app.get_loading_widget()\n        return loading_widget\n\n    def set_loading(self, loading: bool) -> Awaitable:\n        \"\"\"Set or reset the loading state of this widget.\n\n        A widget in a loading state will display a LoadingIndicator that obscures the widget.\n\n        Args:\n            loading: `True` to put the widget into a loading state, or `False` to reset the loading state.\n\n        Returns:\n            An optional awaitable.\n        \"\"\"\n        LOADING_INDICATOR_CLASS = \"-textual-loading-indicator\"\n        LOADING_INDICATOR_QUERY = f\".{LOADING_INDICATOR_CLASS}\"\n        remove_indicator = self.query_children(LOADING_INDICATOR_QUERY).remove()\n        if loading:\n            loading_indicator = self.get_loading_widget()\n            loading_indicator.add_class(LOADING_INDICATOR_CLASS)\n            await_mount = self.mount(loading_indicator)\n            return AwaitComplete(remove_indicator, await_mount).call_next(self)\n        else:\n            return remove_indicator\n\n    async def _watch_loading(self, loading: bool) -> None:\n        \"\"\"Called when the 'loading' reactive is changed.\"\"\"\n        await self.set_loading(loading)\n\n    ExpectType = TypeVar(\"ExpectType\", bound=\"Widget\")\n\n    if TYPE_CHECKING:\n\n        @overload\n        def get_child_by_id(self, id: str) -> Widget: ...\n\n        @overload\n        def get_child_by_id(\n            self, id: str, expect_type: type[ExpectType]\n        ) -> ExpectType: ...\n\n    def get_child_by_id(\n        self, id: str, expect_type: type[ExpectType] | None = None\n    ) -> ExpectType | Widget:\n        \"\"\"Return the first child (immediate descendent) of this node with the given ID.\n\n       \n```\n# Function Signature: def move_child(\n# Natural Language Annotations: \"\"\"Move a child widget within its parent's list of children.\n\n        Args:\n            child: The child widget to move.\n            before: Child widget or location index to move before.\n            after: Child widget or location index to move after.\n\n        Raises:\n            WidgetError: If there is a problem with the child or target.\n\n        Note:\n            Only one of `before` or `after` can be provided. If neither\n            or both are provided a `WidgetError` will be raised.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef move_child(\n    child: Widget, before:", "  ```py\ndef move_child(\n    self: Widget,\n", "  raise NotImplementedError()"]}, "textual-src/textual/widget.py-_get_box_model": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/widget.py:\n```\n\"\"\"\nThe base class for widgets.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom asyncio import create_task, wait\nfrom collections import Counter\nfrom contextlib import asynccontextmanager\nfrom fractions import Fraction\nfrom itertools import islice\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    AsyncGenerator,\n    Awaitable,\n    ClassVar,\n    Collection,\n    Generator,\n    Iterable,\n    NamedTuple,\n    Sequence,\n    TypeVar,\n    cast,\n    overload,\n)\n\nimport rich.repr\nfrom rich.console import (\n    Console,\n    ConsoleOptions,\n    ConsoleRenderable,\n    JustifyMethod,\n    RenderableType,\n)\nfrom rich.console import RenderResult as RichRenderResult\nfrom rich.console import RichCast\nfrom rich.measure import Measurement\nfrom rich.segment import Segment\nfrom rich.style import Style\nfrom rich.text import Text\nfrom typing_extensions import Self\n\nif TYPE_CHECKING:\n    from .app import RenderResult\n\nfrom . import constants, errors, events, messages\nfrom ._animator import DEFAULT_EASING, Animatable, BoundAnimator, EasingFunction\nfrom ._arrange import DockArrangeResult, arrange\nfrom ._compose import compose\nfrom ._context import NoActiveAppError, active_app\nfrom ._easing import DEFAULT_SCROLL_EASING\nfrom ._layout import Layout\nfrom ._segment_tools import align_lines\nfrom ._styles_cache import StylesCache\nfrom ._types import AnimationLevel\nfrom .actions import SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .box_model import BoxModel\nfrom .cache import FIFOCache\nfrom .css.match import match\nfrom .css.parse import parse_selectors\nfrom .css.query import NoMatches, WrongType\nfrom .css.scalar import ScalarOffset\nfrom .dom import DOMNode, NoScreen\nfrom .geometry import (\n    NULL_REGION,\n    NULL_SIZE,\n    NULL_SPACING,\n    Offset,\n    Region,\n    Size,\n    Spacing,\n    clamp,\n)\nfrom .layouts.vertical import VerticalLayout\nfrom .message import Message\nfrom .messages import CallbackType\nfrom .notifications import SeverityLevel\nfrom .reactive import Reactive\nfrom .render import measure\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .strip import Strip\nfrom .walk import walk_depth_first\n\nif TYPE_CHECKING:\n    from .app import App, ComposeResult\n    from .css.query import QueryType\n    from .message_pump import MessagePump\n    from .scrollbar import (\n        ScrollBar,\n        ScrollBarCorner,\n        ScrollDown,\n        ScrollLeft,\n        ScrollRight,\n        ScrollTo,\n        ScrollUp,\n    )\n\n_JUSTIFY_MAP: dict[str, JustifyMethod] = {\n    \"start\": \"left\",\n    \"end\": \"right\",\n    \"justify\": \"full\",\n}\n\n\n_NULL_STYLE = Style()\n\n\nclass AwaitMount:\n    \"\"\"An *optional* awaitable returned by [mount][textual.widget.Widget.mount] and [mount_all][textual.widget.Widget.mount_all].\n\n    Example:\n        ```python\n        await self.mount(Static(\"foo\"))\n        ```\n    \"\"\"\n\n    def __init__(self, parent: Widget, widgets: Sequence[Widget]) -> None:\n        self._parent = parent\n        self._widgets = widgets\n\n    async def __call__(self) -> None:\n        \"\"\"Allows awaiting via a call operation.\"\"\"\n        await self\n\n    def __await__(self) -> Generator[None, None, None]:\n        async def await_mount() -> None:\n            if self._widgets:\n                aws = [\n                    create_task(widget._mounted_event.wait(), name=\"await mount\")\n                    for widget in self._widgets\n                ]\n                if aws:\n                    await wait(aws)\n                    self._parent.refresh(layout=True)\n                    try:\n                        self._parent.app._update_mouse_over(self._parent.screen)\n                    except NoScreen:\n                        pass\n\n        return await_mount().__await__()\n\n\nclass _Styled:\n    \"\"\"Apply a style to a renderable.\n\n    Args:\n        renderable: Any renderable.\n        style: A style to apply across the entire renderable.\n    \"\"\"\n\n    def __init__(\n        self, renderable: \"ConsoleRenderable\", style: Style, link_style: Style | None\n    ) -> None:\n        self.renderable = renderable\n        self.style = style\n        self.link_style = link_style\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> \"RichRenderResult\":\n        style = console.get_style(self.style)\n        result_segments = console.render(self.renderable, options)\n\n        _Segment = Segment\n        if style:\n            apply = style.__add__\n            result_segments = (\n                _Segment(text, apply(_style), None)\n                for text, _style, control in result_segments\n            )\n        link_style = self.link_style\n        if link_style:\n            result_segments = (\n                _Segment(\n                    text,\n                    (\n                        style\n                        if style._meta is None\n                        else (style + link_style if \"@click\" in style.meta else style)\n                    ),\n                    control,\n                )\n                for text, style, control in result_segments\n                if style is not None\n            )\n        return result_segments\n\n    def __rich_measure__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> Measurement:\n        return Measurement.get(console, options, self.renderable)\n\n\nclass _RenderCache(NamedTuple):\n    \"\"\"Stores results of a previous render.\"\"\"\n\n    size: Size\n    \"\"\"The size of the render.\"\"\"\n    lines: list[Strip]\n    \"\"\"Contents of the render.\"\"\"\n\n\nclass WidgetError(Exception):\n    \"\"\"Base widget error.\"\"\"\n\n\nclass MountError(WidgetError):\n    \"\"\"Error raised when there was a problem with the mount request.\"\"\"\n\n\nclass PseudoClasses(NamedTuple):\n    \"\"\"Used for render/render_line based widgets that use caching. This structure can be used as a\n    cache-key.\"\"\"\n\n    enabled: bool\n    \"\"\"Is 'enabled' applied?\"\"\"\n    focus: bool\n    \"\"\"Is 'focus' applied?\"\"\"\n    hover: bool\n    \"\"\"Is 'hover' applied?\"\"\"\n\n\nclass _BorderTitle:\n    \"\"\"Descriptor to set border titles.\"\"\"\n\n    def __set_name__(self, owner: Widget, name: str) -> None:\n        # The private name where we store the real data.\n        self._internal_name = f\"_{name}\"\n\n    def __set__(self, obj: Widget, title: str | Text | None) -> None:\n        \"\"\"Setting a title accepts a str, Text, or None.\"\"\"\n        if title is None:\n            setattr(obj, self._internal_name, None)\n        else:\n            # We store the title as Text\n            new_title = obj.render_str(title)\n            new_title.expand_tabs(4)\n            new_title = new_title.split()[0]\n            setattr(obj, self._internal_name, new_title)\n        obj.refresh()\n\n    def __get__(self, obj: Widget, objtype: type[Widget] | None = None) -> str | None:\n        \"\"\"Getting a title will return None or a str as console markup.\"\"\"\n        title: Text | None = getattr(obj, self._internal_name, None)\n        if title is None:\n            return None\n        # If we have a title, convert from Text to console markup\n        return title.markup\n\n\nclass BadWidgetName(Exception):\n    \"\"\"Raised when widget class names do not satisfy the required restrictions.\"\"\"\n\n\n@rich.repr.auto\nclass Widget(DOMNode):\n    \"\"\"\n    A Widget is the base class for Textual widgets.\n\n    See also [static][textual.widgets._static.Static] for starting point for your own widgets.\n    \"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    Widget{\n        scrollbar-background: $panel-darken-1;\n        scrollbar-background-hover: $panel-darken-2;\n        scrollbar-background-active: $panel-darken-3;\n        scrollbar-color: $primary-lighten-1;\n        scrollbar-color-active: $warning-darken-1;\n        scrollbar-color-hover: $primary-lighten-1;\n        scrollbar-corner-color: $panel-darken-1;\n        scrollbar-size-vertical: 2;\n        scrollbar-size-horizontal: 1;\n        link-background: initial;\n        link-color: $text;\n        link-style: underline;\n        link-background-hover: $accent;\n        link-color-hover: $text;\n        link-style-hover: bold not underline;\n    }\n    \"\"\"\n    COMPONENT_CLASSES: ClassVar[set[str]] = set()\n\n    BORDER_TITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_title attribute.\"\"\"\n\n    BORDER_SUBTITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_subtitle attribute.\"\"\"\n\n    can_focus: bool = False\n    \"\"\"Widget may receive focus.\"\"\"\n    can_focus_children: bool = True\n    \"\"\"Widget's children may receive focus.\"\"\"\n    expand: Reactive[bool] = Reactive(False)\n    \"\"\"Rich renderable may expand beyond optimal size.\"\"\"\n    shrink: Reactive[bool] = Reactive(True)\n    \"\"\"Rich renderable may shrink below optimal size.\"\"\"\n    auto_links: Reactive[bool] = Reactive(True)\n    \"\"\"Widget will highlight links automatically.\"\"\"\n    disabled: Reactive[bool] = Reactive(False)\n    \"\"\"Is the widget disabled? Disabled widgets can not be interacted with, and are typically styled to look dimmer.\"\"\"\n\n    hover_style: Reactive[Style] = Reactive(Style, repaint=False)\n    \"\"\"The current hover style (style under the mouse cursor). Read only.\"\"\"\n    highlight_link_id: Reactive[str] = Reactive(\"\")\n    \"\"\"The currently highlighted link id. Read only.\"\"\"\n    loading: Reactive[bool] = Reactive(False)\n    \"\"\"If set to `True` this widget will temporarily be replaced with a loading indicator.\"\"\"\n\n    # Default sort order, incremented by constructor\n    _sort_order: ClassVar[int] = 0\n\n    def __init__(\n        self,\n        *children: Widget,\n        name: str | None = None,\n        id: str | None = None,\n        classes: str | None = None,\n        disabled: bool = False,\n    ) -> None:\n        \"\"\"Initialize a Widget.\n\n        Args:\n            *children: Child widgets.\n            name: The name of the widget.\n            id: The ID of the widget in the DOM.\n            classes: The CSS classes for the widget.\n            disabled: Whether the widget is disabled or not.\n        \"\"\"\n        _null_size = NULL_SIZE\n        self._size = _null_size\n        self._container_size = _null_size\n        self._layout_required = False\n        self._repaint_required = False\n        self._scroll_required = False\n        self._recompose_required = False\n        self._default_layout = VerticalLayout()\n        self._animate: BoundAnimator | None = None\n        Widget._sort_order += 1\n        self.sort_order = Widget._sort_order\n        self.highlight_style: Style | None = None\n\n        self._vertical_scrollbar: ScrollBar | None = None\n        self._horizontal_scrollbar: ScrollBar | None = None\n        self._scrollbar_corner: ScrollBarCorner | None = None\n\n        self._border_title: Text | None = None\n        self._border_subtitle: Text | None = None\n\n        self._render_cache = _RenderCache(_null_size, [])\n        # Regions which need to be updated (in Widget)\n        self._dirty_regions: set[Region] = set()\n        # Regions which need to be transferred from cache to screen\n        self._repaint_regions: set[Region] = set()\n\n        # Cache the auto content dimensions\n        self._content_width_cache: tuple[object, int] = (None, 0)\n        self._content_height_cache: tuple[object, int] = (None, 0)\n\n        self._arrangement_cache: FIFOCache[tuple[Size, int], DockArrangeResult] = (\n            FIFOCache(4)\n        )\n\n        self._styles_cache = StylesCache()\n        self._rich_style_cache: dict[tuple[str, ...], tuple[Style, Style]] = {}\n\n        self._tooltip: RenderableType | None = None\n        \"\"\"The tooltip content.\"\"\"\n        self._absolute_offset: Offset | None = None\n        \"\"\"Force an absolute offset for the widget (used by tooltips).\"\"\"\n\n        self._scrollbar_changes: set[tuple[bool, bool]] = set()\n        \"\"\"Used to stabilize scrollbars.\"\"\"\n\n        super().__init__(\n            name=name,\n            id=id,\n            classes=self.DEFAULT_CLASSES if classes is None else classes,\n        )\n\n        if self in children:\n            raise WidgetError(\"A widget can't be its own parent\")\n\n        for child in children:\n            if not isinstance(child, Widget):\n                raise TypeError(\n                    f\"Widget positional arguments must be Widget subclasses; not {child!r}\"\n                )\n        self._pending_children = list(children)\n        self.disabled = disabled\n        if self.BORDER_TITLE:\n            self.border_title = self.BORDER_TITLE\n        if self.BORDER_SUBTITLE:\n            self.border_subtitle = self.BORDER_SUBTITLE\n\n        self.lock = RLock()\n        \"\"\"`asyncio` lock to be used to synchronize the state of the widget.\n\n        Two different tasks might call methods on a widget at the same time, which\n        might result in a race condition.\n        This can be fixed by adding `async with widget.lock:` around the method calls.\n        \"\"\"\n        self._anchored: Widget | None = None\n        \"\"\"An anchored child widget, or `None` if no child is anchored.\"\"\"\n        self._anchor_animate: bool = False\n        \"\"\"Flag to enable animation when scrolling anchored widgets.\"\"\"\n\n    virtual_size: Reactive[Size] = Reactive(Size(0, 0), layout=True)\n    \"\"\"The virtual (scrollable) [size][textual.geometry.Size] of the widget.\"\"\"\n\n    has_focus: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Does this widget have focus? Read only.\"\"\"\n\n    mouse_over: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Is the mouse over this widget? Read only.\"\"\"\n\n    scroll_x: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the X axis.\"\"\"\n\n    scroll_y: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the Y axis.\"\"\"\n\n    scroll_target_x = Reactive(0.0, repaint=False)\n    scroll_target_y = Reactive(0.0, repaint=False)\n\n    show_vertical_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a vertical scrollbar?\"\"\"\n\n    show_horizontal_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a horizontal scrollbar?\"\"\"\n\n    border_title: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the top border (if there is one).\"\"\"\n    border_subtitle: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the bottom border (if there is one).\"\"\"\n\n    @property\n    def is_mounted(self) -> bool:\n        \"\"\"Check if this widget is mounted.\"\"\"\n        return self._is_mounted\n\n    @property\n    def siblings(self) -> list[Widget]:\n        \"\"\"Get the widget's siblings (self is removed from the return list).\n\n        Returns:\n            A list of siblings.\n        \"\"\"\n        parent = self.parent\n        if parent is not None:\n            siblings = list(parent._nodes)\n            siblings.remove(self)\n            return siblings\n        else:\n            return []\n\n    @property\n    def visible_siblings(self) -> list[Widget]:\n        \"\"\"A list of siblings which will be shown.\n\n        Returns:\n            List of siblings.\n        \"\"\"\n        siblings = [\n            widget for widget in self.siblings if widget.visible and widget.display\n        ]\n        return siblings\n\n    @property\n    def allow_vertical_scroll(self) -> bool:\n        \"\"\"Check if vertical scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_vertical_scrollbar\n\n    @property\n    def allow_horizontal_scroll(self) -> bool:\n        \"\"\"Check if horizontal scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_horizontal_scrollbar\n\n    @property\n    def _allow_scroll(self) -> bool:\n        \"\"\"Check if both axis may be scrolled.\n\n        Returns:\n            True if horizontal and vertical scrolling is enabled.\n        \"\"\"\n        return self.is_scrollable and (\n            self.allow_horizontal_scroll or self.allow_vertical_scroll\n        )\n\n    @property\n    def offset(self) -> Offset:\n        \"\"\"Widget offset from origin.\n\n        Returns:\n            Relative offset.\n        \"\"\"\n        return self.styles.offset.resolve(self.size, self.app.size)\n\n    @offset.setter\n    def offset(self, offset: tuple[int, int]) -> None:\n        self.styles.offset = ScalarOffset.from_offset(offset)\n\n    @property\n    def opacity(self) -> float:\n        \"\"\"Total opacity of widget.\"\"\"\n        opacity = 1.0\n        for node in reversed(self.ancestors_with_self):\n            opacity *= node.styles.opacity\n            if not opacity:\n                break\n        return opacity\n\n    @property\n    def is_anchored(self) -> bool:\n        \"\"\"Is this widget anchored?\"\"\"\n        return self._parent is not None and self._parent is self\n\n    def anchor(self, *, animate: bool = False) -> None:\n        \"\"\"Anchor the widget, which scrolls it into view (like [scroll_visible][textual.widget.Widget.scroll_visible]),\n        but also keeps it in view if the widget's size changes, or the size of its container changes.\n\n        !!! note\n\n            Anchored widgets will be un-anchored if the users scrolls the container.\n\n        Args:\n            animate: `True` if the scroll should animate, or `False` if it shouldn't.\n        \"\"\"\n        if self._parent is not None and isinstance(self._parent, Widget):\n            self._parent._anchored = self\n            self._parent._anchor_animate = animate\n            self.check_idle()\n\n    def clear_anchor(self) -> None:\n        \"\"\"Stop anchoring this widget (a no-op if this widget is not anchored).\"\"\"\n        if (\n            self._parent is not None\n            and isinstance(self._parent, Widget)\n            and self._parent._anchored is self\n        ):\n            self._parent._anchored = None\n\n    def _clear_anchor(self) -> None:\n        \"\"\"Clear an anchored child.\"\"\"\n        self._anchored = None\n\n    def _check_disabled(self) -> bool:\n        \"\"\"Check if the widget is disabled either explicitly by setting `disabled`,\n        or implicitly by setting `loading`.\n\n        Returns:\n            True if the widget should be disabled.\n        \"\"\"\n        return self.disabled or self.loading\n\n    @property\n    def tooltip(self) -> RenderableType | None:\n        \"\"\"Tooltip for the widget, or `None` for no tooltip.\"\"\"\n        return self._tooltip\n\n    @tooltip.setter\n    def tooltip(self, tooltip: RenderableType | None):\n        self._tooltip = tooltip\n        try:\n            self.screen._update_tooltip(self)\n        except NoScreen:\n            pass\n\n    def allow_focus(self) -> bool:\n        \"\"\"Check if the widget is permitted to focus.\n\n        The base class returns [`can_focus`][textual.widget.Widget.can_focus].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget may be focused, or `False` if it may not be focused.\n        \"\"\"\n        return self.can_focus\n\n    def allow_focus_children(self) -> bool:\n        \"\"\"Check if a widget's children may be focused.\n\n        The base class returns [`can_focus_children`][textual.widget.Widget.can_focus_children].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget's children may be focused, or `False` if the widget's children may not be focused.\n        \"\"\"\n        return self.can_focus_children\n\n    def compose_add_child(self, widget: Widget) -> None:\n        \"\"\"Add a node to children.\n\n        This is used by the compose process when it adds children.\n        There is no need to use it directly, but you may want to override it in a subclass\n        if you want children to be attached to a different node.\n\n        Args:\n            widget: A Widget to add.\n        \"\"\"\n        _rich_traceback_omit = True\n        self._pending_children.append(widget)\n\n    def __enter__(self) -> Self:\n        \"\"\"Use as context manager when composing.\"\"\"\n        self.app._compose_stacks[-1].append(self)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        \"\"\"Exit compose context manager.\"\"\"\n        compose_stack = self.app._compose_stacks[-1]\n        composed = compose_stack.pop()\n        if compose_stack:\n            compose_stack[-1].compose_add_child(composed)\n        else:\n            self.app._composed[-1].append(composed)\n\n    def clear_cached_dimensions(self) -> None:\n        \"\"\"Clear cached results of `get_content_width` and `get_content_height`.\n\n        Call if the widget's renderable changes size after the widget has been created.\n\n        !!! note\n\n            This is not required if you are extending [`Static`][textual.widgets.Static].\n\n        \"\"\"\n        self._content_width_cache = (None, 0)\n        self._content_height_cache = (None, 0)\n\n    def get_loading_widget(self) -> Widget:\n        \"\"\"Get a widget to display a loading indicator.\n\n        The default implementation will defer to App.get_loading_widget.\n\n        Returns:\n            A widget in place of this widget to indicate a loading.\n        \"\"\"\n        loading_widget = self.app.get_loading_widget()\n        return loading_widget\n\n    def set_loading(self, loading: bool) -> Awaitable:\n        \"\"\"Set or reset the loading state of this widget.\n\n        A widget in a loading state will display a LoadingIndicator that obscures the widget.\n\n        Args:\n            loading: `True` to put the widget into a loading state, or `False` to reset the loading state.\n\n        Returns:\n            An optional awaitable.\n        \"\"\"\n        LOADING_INDICATOR_CLASS = \"-textual-loading-indicator\"\n        LOADING_INDICATOR_QUERY = f\".{LOADING_INDICATOR_CLASS}\"\n        remove_indicator = self.query_children(LOADING_INDICATOR_QUERY).remove()\n        if loading:\n            loading_indicator = self.get_loading_widget()\n            loading_indicator.add_class(LOADING_INDICATOR_CLASS)\n            await_mount = self.mount(loading_indicator)\n            return AwaitComplete(remove_indicator, await_mount).call_next(self)\n        else:\n            return remove_indicator\n\n    async def _watch_loading(self, loading: bool) -> None:\n        \"\"\"Called when the 'loading' reactive is changed.\"\"\"\n        await self.set_loading(loading)\n\n    ExpectType = TypeVar(\"ExpectType\", bound=\"Widget\")\n\n    if TYPE_CHECKING:\n\n        @overload\n        def get_child_by_id(self, id: str) -> Widget: ...\n\n        @overload\n        def get_child_by_id(\n            self, id: str, expect_type: type[ExpectType]\n        ) -> ExpectType: ...\n\n    def get_child_by_id(\n        self, id: str, expect_type: type[ExpectType] | None = None\n    ) -> ExpectType | Widget:\n        \"\"\"Return the first child (immediate descendent) of this node with the given ID.\n\n       \n```\n# Function Signature: def _get_box_model(\n# Natural Language Annotations: \"\"\"Process the box model for this widget.\n\n        Args:\n            container: The size of the container widget (with a layout)\n            viewport: The viewport size.\n            width_fraction: A fraction used for 1 `fr` unit on the width dimension.\n            height_fraction: A fraction used for 1 `fr` unit on the height dimension.\n\n        Returns:\n            The size and margin for this widget.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  def _get_box_model(\n    self,\n    container", "  ```\ndef _get_box_model(\n    self,\n", "  [PYTHON]\ndef _get_box_model(\n"]}, "textual-src/textual/widget.py-get_content_width": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/widget.py:\n```\n\"\"\"\nThe base class for widgets.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom asyncio import create_task, wait\nfrom collections import Counter\nfrom contextlib import asynccontextmanager\nfrom fractions import Fraction\nfrom itertools import islice\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    AsyncGenerator,\n    Awaitable,\n    ClassVar,\n    Collection,\n    Generator,\n    Iterable,\n    NamedTuple,\n    Sequence,\n    TypeVar,\n    cast,\n    overload,\n)\n\nimport rich.repr\nfrom rich.console import (\n    Console,\n    ConsoleOptions,\n    ConsoleRenderable,\n    JustifyMethod,\n    RenderableType,\n)\nfrom rich.console import RenderResult as RichRenderResult\nfrom rich.console import RichCast\nfrom rich.measure import Measurement\nfrom rich.segment import Segment\nfrom rich.style import Style\nfrom rich.text import Text\nfrom typing_extensions import Self\n\nif TYPE_CHECKING:\n    from .app import RenderResult\n\nfrom . import constants, errors, events, messages\nfrom ._animator import DEFAULT_EASING, Animatable, BoundAnimator, EasingFunction\nfrom ._arrange import DockArrangeResult, arrange\nfrom ._compose import compose\nfrom ._context import NoActiveAppError, active_app\nfrom ._easing import DEFAULT_SCROLL_EASING\nfrom ._layout import Layout\nfrom ._segment_tools import align_lines\nfrom ._styles_cache import StylesCache\nfrom ._types import AnimationLevel\nfrom .actions import SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .box_model import BoxModel\nfrom .cache import FIFOCache\nfrom .css.match import match\nfrom .css.parse import parse_selectors\nfrom .css.query import NoMatches, WrongType\nfrom .css.scalar import ScalarOffset\nfrom .dom import DOMNode, NoScreen\nfrom .geometry import (\n    NULL_REGION,\n    NULL_SIZE,\n    NULL_SPACING,\n    Offset,\n    Region,\n    Size,\n    Spacing,\n    clamp,\n)\nfrom .layouts.vertical import VerticalLayout\nfrom .message import Message\nfrom .messages import CallbackType\nfrom .notifications import SeverityLevel\nfrom .reactive import Reactive\nfrom .render import measure\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .strip import Strip\nfrom .walk import walk_depth_first\n\nif TYPE_CHECKING:\n    from .app import App, ComposeResult\n    from .css.query import QueryType\n    from .message_pump import MessagePump\n    from .scrollbar import (\n        ScrollBar,\n        ScrollBarCorner,\n        ScrollDown,\n        ScrollLeft,\n        ScrollRight,\n        ScrollTo,\n        ScrollUp,\n    )\n\n_JUSTIFY_MAP: dict[str, JustifyMethod] = {\n    \"start\": \"left\",\n    \"end\": \"right\",\n    \"justify\": \"full\",\n}\n\n\n_NULL_STYLE = Style()\n\n\nclass AwaitMount:\n    \"\"\"An *optional* awaitable returned by [mount][textual.widget.Widget.mount] and [mount_all][textual.widget.Widget.mount_all].\n\n    Example:\n        ```python\n        await self.mount(Static(\"foo\"))\n        ```\n    \"\"\"\n\n    def __init__(self, parent: Widget, widgets: Sequence[Widget]) -> None:\n        self._parent = parent\n        self._widgets = widgets\n\n    async def __call__(self) -> None:\n        \"\"\"Allows awaiting via a call operation.\"\"\"\n        await self\n\n    def __await__(self) -> Generator[None, None, None]:\n        async def await_mount() -> None:\n            if self._widgets:\n                aws = [\n                    create_task(widget._mounted_event.wait(), name=\"await mount\")\n                    for widget in self._widgets\n                ]\n                if aws:\n                    await wait(aws)\n                    self._parent.refresh(layout=True)\n                    try:\n                        self._parent.app._update_mouse_over(self._parent.screen)\n                    except NoScreen:\n                        pass\n\n        return await_mount().__await__()\n\n\nclass _Styled:\n    \"\"\"Apply a style to a renderable.\n\n    Args:\n        renderable: Any renderable.\n        style: A style to apply across the entire renderable.\n    \"\"\"\n\n    def __init__(\n        self, renderable: \"ConsoleRenderable\", style: Style, link_style: Style | None\n    ) -> None:\n        self.renderable = renderable\n        self.style = style\n        self.link_style = link_style\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> \"RichRenderResult\":\n        style = console.get_style(self.style)\n        result_segments = console.render(self.renderable, options)\n\n        _Segment = Segment\n        if style:\n            apply = style.__add__\n            result_segments = (\n                _Segment(text, apply(_style), None)\n                for text, _style, control in result_segments\n            )\n        link_style = self.link_style\n        if link_style:\n            result_segments = (\n                _Segment(\n                    text,\n                    (\n                        style\n                        if style._meta is None\n                        else (style + link_style if \"@click\" in style.meta else style)\n                    ),\n                    control,\n                )\n                for text, style, control in result_segments\n                if style is not None\n            )\n        return result_segments\n\n    def __rich_measure__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> Measurement:\n        return Measurement.get(console, options, self.renderable)\n\n\nclass _RenderCache(NamedTuple):\n    \"\"\"Stores results of a previous render.\"\"\"\n\n    size: Size\n    \"\"\"The size of the render.\"\"\"\n    lines: list[Strip]\n    \"\"\"Contents of the render.\"\"\"\n\n\nclass WidgetError(Exception):\n    \"\"\"Base widget error.\"\"\"\n\n\nclass MountError(WidgetError):\n    \"\"\"Error raised when there was a problem with the mount request.\"\"\"\n\n\nclass PseudoClasses(NamedTuple):\n    \"\"\"Used for render/render_line based widgets that use caching. This structure can be used as a\n    cache-key.\"\"\"\n\n    enabled: bool\n    \"\"\"Is 'enabled' applied?\"\"\"\n    focus: bool\n    \"\"\"Is 'focus' applied?\"\"\"\n    hover: bool\n    \"\"\"Is 'hover' applied?\"\"\"\n\n\nclass _BorderTitle:\n    \"\"\"Descriptor to set border titles.\"\"\"\n\n    def __set_name__(self, owner: Widget, name: str) -> None:\n        # The private name where we store the real data.\n        self._internal_name = f\"_{name}\"\n\n    def __set__(self, obj: Widget, title: str | Text | None) -> None:\n        \"\"\"Setting a title accepts a str, Text, or None.\"\"\"\n        if title is None:\n            setattr(obj, self._internal_name, None)\n        else:\n            # We store the title as Text\n            new_title = obj.render_str(title)\n            new_title.expand_tabs(4)\n            new_title = new_title.split()[0]\n            setattr(obj, self._internal_name, new_title)\n        obj.refresh()\n\n    def __get__(self, obj: Widget, objtype: type[Widget] | None = None) -> str | None:\n        \"\"\"Getting a title will return None or a str as console markup.\"\"\"\n        title: Text | None = getattr(obj, self._internal_name, None)\n        if title is None:\n            return None\n        # If we have a title, convert from Text to console markup\n        return title.markup\n\n\nclass BadWidgetName(Exception):\n    \"\"\"Raised when widget class names do not satisfy the required restrictions.\"\"\"\n\n\n@rich.repr.auto\nclass Widget(DOMNode):\n    \"\"\"\n    A Widget is the base class for Textual widgets.\n\n    See also [static][textual.widgets._static.Static] for starting point for your own widgets.\n    \"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    Widget{\n        scrollbar-background: $panel-darken-1;\n        scrollbar-background-hover: $panel-darken-2;\n        scrollbar-background-active: $panel-darken-3;\n        scrollbar-color: $primary-lighten-1;\n        scrollbar-color-active: $warning-darken-1;\n        scrollbar-color-hover: $primary-lighten-1;\n        scrollbar-corner-color: $panel-darken-1;\n        scrollbar-size-vertical: 2;\n        scrollbar-size-horizontal: 1;\n        link-background: initial;\n        link-color: $text;\n        link-style: underline;\n        link-background-hover: $accent;\n        link-color-hover: $text;\n        link-style-hover: bold not underline;\n    }\n    \"\"\"\n    COMPONENT_CLASSES: ClassVar[set[str]] = set()\n\n    BORDER_TITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_title attribute.\"\"\"\n\n    BORDER_SUBTITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_subtitle attribute.\"\"\"\n\n    can_focus: bool = False\n    \"\"\"Widget may receive focus.\"\"\"\n    can_focus_children: bool = True\n    \"\"\"Widget's children may receive focus.\"\"\"\n    expand: Reactive[bool] = Reactive(False)\n    \"\"\"Rich renderable may expand beyond optimal size.\"\"\"\n    shrink: Reactive[bool] = Reactive(True)\n    \"\"\"Rich renderable may shrink below optimal size.\"\"\"\n    auto_links: Reactive[bool] = Reactive(True)\n    \"\"\"Widget will highlight links automatically.\"\"\"\n    disabled: Reactive[bool] = Reactive(False)\n    \"\"\"Is the widget disabled? Disabled widgets can not be interacted with, and are typically styled to look dimmer.\"\"\"\n\n    hover_style: Reactive[Style] = Reactive(Style, repaint=False)\n    \"\"\"The current hover style (style under the mouse cursor). Read only.\"\"\"\n    highlight_link_id: Reactive[str] = Reactive(\"\")\n    \"\"\"The currently highlighted link id. Read only.\"\"\"\n    loading: Reactive[bool] = Reactive(False)\n    \"\"\"If set to `True` this widget will temporarily be replaced with a loading indicator.\"\"\"\n\n    # Default sort order, incremented by constructor\n    _sort_order: ClassVar[int] = 0\n\n    def __init__(\n        self,\n        *children: Widget,\n        name: str | None = None,\n        id: str | None = None,\n        classes: str | None = None,\n        disabled: bool = False,\n    ) -> None:\n        \"\"\"Initialize a Widget.\n\n        Args:\n            *children: Child widgets.\n            name: The name of the widget.\n            id: The ID of the widget in the DOM.\n            classes: The CSS classes for the widget.\n            disabled: Whether the widget is disabled or not.\n        \"\"\"\n        _null_size = NULL_SIZE\n        self._size = _null_size\n        self._container_size = _null_size\n        self._layout_required = False\n        self._repaint_required = False\n        self._scroll_required = False\n        self._recompose_required = False\n        self._default_layout = VerticalLayout()\n        self._animate: BoundAnimator | None = None\n        Widget._sort_order += 1\n        self.sort_order = Widget._sort_order\n        self.highlight_style: Style | None = None\n\n        self._vertical_scrollbar: ScrollBar | None = None\n        self._horizontal_scrollbar: ScrollBar | None = None\n        self._scrollbar_corner: ScrollBarCorner | None = None\n\n        self._border_title: Text | None = None\n        self._border_subtitle: Text | None = None\n\n        self._render_cache = _RenderCache(_null_size, [])\n        # Regions which need to be updated (in Widget)\n        self._dirty_regions: set[Region] = set()\n        # Regions which need to be transferred from cache to screen\n        self._repaint_regions: set[Region] = set()\n\n        # Cache the auto content dimensions\n        self._content_width_cache: tuple[object, int] = (None, 0)\n        self._content_height_cache: tuple[object, int] = (None, 0)\n\n        self._arrangement_cache: FIFOCache[tuple[Size, int], DockArrangeResult] = (\n            FIFOCache(4)\n        )\n\n        self._styles_cache = StylesCache()\n        self._rich_style_cache: dict[tuple[str, ...], tuple[Style, Style]] = {}\n\n        self._tooltip: RenderableType | None = None\n        \"\"\"The tooltip content.\"\"\"\n        self._absolute_offset: Offset | None = None\n        \"\"\"Force an absolute offset for the widget (used by tooltips).\"\"\"\n\n        self._scrollbar_changes: set[tuple[bool, bool]] = set()\n        \"\"\"Used to stabilize scrollbars.\"\"\"\n\n        super().__init__(\n            name=name,\n            id=id,\n            classes=self.DEFAULT_CLASSES if classes is None else classes,\n        )\n\n        if self in children:\n            raise WidgetError(\"A widget can't be its own parent\")\n\n        for child in children:\n            if not isinstance(child, Widget):\n                raise TypeError(\n                    f\"Widget positional arguments must be Widget subclasses; not {child!r}\"\n                )\n        self._pending_children = list(children)\n        self.disabled = disabled\n        if self.BORDER_TITLE:\n            self.border_title = self.BORDER_TITLE\n        if self.BORDER_SUBTITLE:\n            self.border_subtitle = self.BORDER_SUBTITLE\n\n        self.lock = RLock()\n        \"\"\"`asyncio` lock to be used to synchronize the state of the widget.\n\n        Two different tasks might call methods on a widget at the same time, which\n        might result in a race condition.\n        This can be fixed by adding `async with widget.lock:` around the method calls.\n        \"\"\"\n        self._anchored: Widget | None = None\n        \"\"\"An anchored child widget, or `None` if no child is anchored.\"\"\"\n        self._anchor_animate: bool = False\n        \"\"\"Flag to enable animation when scrolling anchored widgets.\"\"\"\n\n    virtual_size: Reactive[Size] = Reactive(Size(0, 0), layout=True)\n    \"\"\"The virtual (scrollable) [size][textual.geometry.Size] of the widget.\"\"\"\n\n    has_focus: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Does this widget have focus? Read only.\"\"\"\n\n    mouse_over: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Is the mouse over this widget? Read only.\"\"\"\n\n    scroll_x: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the X axis.\"\"\"\n\n    scroll_y: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the Y axis.\"\"\"\n\n    scroll_target_x = Reactive(0.0, repaint=False)\n    scroll_target_y = Reactive(0.0, repaint=False)\n\n    show_vertical_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a vertical scrollbar?\"\"\"\n\n    show_horizontal_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a horizontal scrollbar?\"\"\"\n\n    border_title: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the top border (if there is one).\"\"\"\n    border_subtitle: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the bottom border (if there is one).\"\"\"\n\n    @property\n    def is_mounted(self) -> bool:\n        \"\"\"Check if this widget is mounted.\"\"\"\n        return self._is_mounted\n\n    @property\n    def siblings(self) -> list[Widget]:\n        \"\"\"Get the widget's siblings (self is removed from the return list).\n\n        Returns:\n            A list of siblings.\n        \"\"\"\n        parent = self.parent\n        if parent is not None:\n            siblings = list(parent._nodes)\n            siblings.remove(self)\n            return siblings\n        else:\n            return []\n\n    @property\n    def visible_siblings(self) -> list[Widget]:\n        \"\"\"A list of siblings which will be shown.\n\n        Returns:\n            List of siblings.\n        \"\"\"\n        siblings = [\n            widget for widget in self.siblings if widget.visible and widget.display\n        ]\n        return siblings\n\n    @property\n    def allow_vertical_scroll(self) -> bool:\n        \"\"\"Check if vertical scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_vertical_scrollbar\n\n    @property\n    def allow_horizontal_scroll(self) -> bool:\n        \"\"\"Check if horizontal scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_horizontal_scrollbar\n\n    @property\n    def _allow_scroll(self) -> bool:\n        \"\"\"Check if both axis may be scrolled.\n\n        Returns:\n            True if horizontal and vertical scrolling is enabled.\n        \"\"\"\n        return self.is_scrollable and (\n            self.allow_horizontal_scroll or self.allow_vertical_scroll\n        )\n\n    @property\n    def offset(self) -> Offset:\n        \"\"\"Widget offset from origin.\n\n        Returns:\n            Relative offset.\n        \"\"\"\n        return self.styles.offset.resolve(self.size, self.app.size)\n\n    @offset.setter\n    def offset(self, offset: tuple[int, int]) -> None:\n        self.styles.offset = ScalarOffset.from_offset(offset)\n\n    @property\n    def opacity(self) -> float:\n        \"\"\"Total opacity of widget.\"\"\"\n        opacity = 1.0\n        for node in reversed(self.ancestors_with_self):\n            opacity *= node.styles.opacity\n            if not opacity:\n                break\n        return opacity\n\n    @property\n    def is_anchored(self) -> bool:\n        \"\"\"Is this widget anchored?\"\"\"\n        return self._parent is not None and self._parent is self\n\n    def anchor(self, *, animate: bool = False) -> None:\n        \"\"\"Anchor the widget, which scrolls it into view (like [scroll_visible][textual.widget.Widget.scroll_visible]),\n        but also keeps it in view if the widget's size changes, or the size of its container changes.\n\n        !!! note\n\n            Anchored widgets will be un-anchored if the users scrolls the container.\n\n        Args:\n            animate: `True` if the scroll should animate, or `False` if it shouldn't.\n        \"\"\"\n        if self._parent is not None and isinstance(self._parent, Widget):\n            self._parent._anchored = self\n            self._parent._anchor_animate = animate\n            self.check_idle()\n\n    def clear_anchor(self) -> None:\n        \"\"\"Stop anchoring this widget (a no-op if this widget is not anchored).\"\"\"\n        if (\n            self._parent is not None\n            and isinstance(self._parent, Widget)\n            and self._parent._anchored is self\n        ):\n            self._parent._anchored = None\n\n    def _clear_anchor(self) -> None:\n        \"\"\"Clear an anchored child.\"\"\"\n        self._anchored = None\n\n    def _check_disabled(self) -> bool:\n        \"\"\"Check if the widget is disabled either explicitly by setting `disabled`,\n        or implicitly by setting `loading`.\n\n        Returns:\n            True if the widget should be disabled.\n        \"\"\"\n        return self.disabled or self.loading\n\n    @property\n    def tooltip(self) -> RenderableType | None:\n        \"\"\"Tooltip for the widget, or `None` for no tooltip.\"\"\"\n        return self._tooltip\n\n    @tooltip.setter\n    def tooltip(self, tooltip: RenderableType | None):\n        self._tooltip = tooltip\n        try:\n            self.screen._update_tooltip(self)\n        except NoScreen:\n            pass\n\n    def allow_focus(self) -> bool:\n        \"\"\"Check if the widget is permitted to focus.\n\n        The base class returns [`can_focus`][textual.widget.Widget.can_focus].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget may be focused, or `False` if it may not be focused.\n        \"\"\"\n        return self.can_focus\n\n    def allow_focus_children(self) -> bool:\n        \"\"\"Check if a widget's children may be focused.\n\n        The base class returns [`can_focus_children`][textual.widget.Widget.can_focus_children].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget's children may be focused, or `False` if the widget's children may not be focused.\n        \"\"\"\n        return self.can_focus_children\n\n    def compose_add_child(self, widget: Widget) -> None:\n        \"\"\"Add a node to children.\n\n        This is used by the compose process when it adds children.\n        There is no need to use it directly, but you may want to override it in a subclass\n        if you want children to be attached to a different node.\n\n        Args:\n            widget: A Widget to add.\n        \"\"\"\n        _rich_traceback_omit = True\n        self._pending_children.append(widget)\n\n    def __enter__(self) -> Self:\n        \"\"\"Use as context manager when composing.\"\"\"\n        self.app._compose_stacks[-1].append(self)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        \"\"\"Exit compose context manager.\"\"\"\n        compose_stack = self.app._compose_stacks[-1]\n        composed = compose_stack.pop()\n        if compose_stack:\n            compose_stack[-1].compose_add_child(composed)\n        else:\n            self.app._composed[-1].append(composed)\n\n    def clear_cached_dimensions(self) -> None:\n        \"\"\"Clear cached results of `get_content_width` and `get_content_height`.\n\n        Call if the widget's renderable changes size after the widget has been created.\n\n        !!! note\n\n            This is not required if you are extending [`Static`][textual.widgets.Static].\n\n        \"\"\"\n        self._content_width_cache = (None, 0)\n        self._content_height_cache = (None, 0)\n\n    def get_loading_widget(self) -> Widget:\n        \"\"\"Get a widget to display a loading indicator.\n\n        The default implementation will defer to App.get_loading_widget.\n\n        Returns:\n            A widget in place of this widget to indicate a loading.\n        \"\"\"\n        loading_widget = self.app.get_loading_widget()\n        return loading_widget\n\n    def set_loading(self, loading: bool) -> Awaitable:\n        \"\"\"Set or reset the loading state of this widget.\n\n        A widget in a loading state will display a LoadingIndicator that obscures the widget.\n\n        Args:\n            loading: `True` to put the widget into a loading state, or `False` to reset the loading state.\n\n        Returns:\n            An optional awaitable.\n        \"\"\"\n        LOADING_INDICATOR_CLASS = \"-textual-loading-indicator\"\n        LOADING_INDICATOR_QUERY = f\".{LOADING_INDICATOR_CLASS}\"\n        remove_indicator = self.query_children(LOADING_INDICATOR_QUERY).remove()\n        if loading:\n            loading_indicator = self.get_loading_widget()\n            loading_indicator.add_class(LOADING_INDICATOR_CLASS)\n            await_mount = self.mount(loading_indicator)\n            return AwaitComplete(remove_indicator, await_mount).call_next(self)\n        else:\n            return remove_indicator\n\n    async def _watch_loading(self, loading: bool) -> None:\n        \"\"\"Called when the 'loading' reactive is changed.\"\"\"\n        await self.set_loading(loading)\n\n    ExpectType = TypeVar(\"ExpectType\", bound=\"Widget\")\n\n    if TYPE_CHECKING:\n\n        @overload\n        def get_child_by_id(self, id: str) -> Widget: ...\n\n        @overload\n        def get_child_by_id(\n            self, id: str, expect_type: type[ExpectType]\n        ) -> ExpectType: ...\n\n    def get_child_by_id(\n        self, id: str, expect_type: type[ExpectType] | None = None\n    ) -> ExpectType | Widget:\n        \"\"\"Return the first child (immediate descendent) of this node with the given ID.\n\n       \n```\n# Function Signature: def get_content_width(self, container: Size, viewport: Size) -> int:\n# Natural Language Annotations: \"\"\"Called by textual to get the width of the content area. May be overridden in a subclass.\n\n        Args:\n            container: Size of the container (immediate parent) widget.\n            viewport: Size of the viewport.\n\n        Returns:\n            The optimal width of the content.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  Please provide more information on the specific task and any additional details you can provide", "  ```\ndef get_content_width(self, container: Size,", "  ```\ndef get_content_width(self, container: Size,"]}, "textual-src/textual/widget.py-get_content_height": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/widget.py:\n```\n\"\"\"\nThe base class for widgets.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom asyncio import create_task, wait\nfrom collections import Counter\nfrom contextlib import asynccontextmanager\nfrom fractions import Fraction\nfrom itertools import islice\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    AsyncGenerator,\n    Awaitable,\n    ClassVar,\n    Collection,\n    Generator,\n    Iterable,\n    NamedTuple,\n    Sequence,\n    TypeVar,\n    cast,\n    overload,\n)\n\nimport rich.repr\nfrom rich.console import (\n    Console,\n    ConsoleOptions,\n    ConsoleRenderable,\n    JustifyMethod,\n    RenderableType,\n)\nfrom rich.console import RenderResult as RichRenderResult\nfrom rich.console import RichCast\nfrom rich.measure import Measurement\nfrom rich.segment import Segment\nfrom rich.style import Style\nfrom rich.text import Text\nfrom typing_extensions import Self\n\nif TYPE_CHECKING:\n    from .app import RenderResult\n\nfrom . import constants, errors, events, messages\nfrom ._animator import DEFAULT_EASING, Animatable, BoundAnimator, EasingFunction\nfrom ._arrange import DockArrangeResult, arrange\nfrom ._compose import compose\nfrom ._context import NoActiveAppError, active_app\nfrom ._easing import DEFAULT_SCROLL_EASING\nfrom ._layout import Layout\nfrom ._segment_tools import align_lines\nfrom ._styles_cache import StylesCache\nfrom ._types import AnimationLevel\nfrom .actions import SkipAction\nfrom .await_complete import AwaitComplete\nfrom .await_remove import AwaitRemove\nfrom .box_model import BoxModel\nfrom .cache import FIFOCache\nfrom .css.match import match\nfrom .css.parse import parse_selectors\nfrom .css.query import NoMatches, WrongType\nfrom .css.scalar import ScalarOffset\nfrom .dom import DOMNode, NoScreen\nfrom .geometry import (\n    NULL_REGION,\n    NULL_SIZE,\n    NULL_SPACING,\n    Offset,\n    Region,\n    Size,\n    Spacing,\n    clamp,\n)\nfrom .layouts.vertical import VerticalLayout\nfrom .message import Message\nfrom .messages import CallbackType\nfrom .notifications import SeverityLevel\nfrom .reactive import Reactive\nfrom .render import measure\nfrom .renderables.blank import Blank\nfrom .rlock import RLock\nfrom .strip import Strip\nfrom .walk import walk_depth_first\n\nif TYPE_CHECKING:\n    from .app import App, ComposeResult\n    from .css.query import QueryType\n    from .message_pump import MessagePump\n    from .scrollbar import (\n        ScrollBar,\n        ScrollBarCorner,\n        ScrollDown,\n        ScrollLeft,\n        ScrollRight,\n        ScrollTo,\n        ScrollUp,\n    )\n\n_JUSTIFY_MAP: dict[str, JustifyMethod] = {\n    \"start\": \"left\",\n    \"end\": \"right\",\n    \"justify\": \"full\",\n}\n\n\n_NULL_STYLE = Style()\n\n\nclass AwaitMount:\n    \"\"\"An *optional* awaitable returned by [mount][textual.widget.Widget.mount] and [mount_all][textual.widget.Widget.mount_all].\n\n    Example:\n        ```python\n        await self.mount(Static(\"foo\"))\n        ```\n    \"\"\"\n\n    def __init__(self, parent: Widget, widgets: Sequence[Widget]) -> None:\n        self._parent = parent\n        self._widgets = widgets\n\n    async def __call__(self) -> None:\n        \"\"\"Allows awaiting via a call operation.\"\"\"\n        await self\n\n    def __await__(self) -> Generator[None, None, None]:\n        async def await_mount() -> None:\n            if self._widgets:\n                aws = [\n                    create_task(widget._mounted_event.wait(), name=\"await mount\")\n                    for widget in self._widgets\n                ]\n                if aws:\n                    await wait(aws)\n                    self._parent.refresh(layout=True)\n                    try:\n                        self._parent.app._update_mouse_over(self._parent.screen)\n                    except NoScreen:\n                        pass\n\n        return await_mount().__await__()\n\n\nclass _Styled:\n    \"\"\"Apply a style to a renderable.\n\n    Args:\n        renderable: Any renderable.\n        style: A style to apply across the entire renderable.\n    \"\"\"\n\n    def __init__(\n        self, renderable: \"ConsoleRenderable\", style: Style, link_style: Style | None\n    ) -> None:\n        self.renderable = renderable\n        self.style = style\n        self.link_style = link_style\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> \"RichRenderResult\":\n        style = console.get_style(self.style)\n        result_segments = console.render(self.renderable, options)\n\n        _Segment = Segment\n        if style:\n            apply = style.__add__\n            result_segments = (\n                _Segment(text, apply(_style), None)\n                for text, _style, control in result_segments\n            )\n        link_style = self.link_style\n        if link_style:\n            result_segments = (\n                _Segment(\n                    text,\n                    (\n                        style\n                        if style._meta is None\n                        else (style + link_style if \"@click\" in style.meta else style)\n                    ),\n                    control,\n                )\n                for text, style, control in result_segments\n                if style is not None\n            )\n        return result_segments\n\n    def __rich_measure__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> Measurement:\n        return Measurement.get(console, options, self.renderable)\n\n\nclass _RenderCache(NamedTuple):\n    \"\"\"Stores results of a previous render.\"\"\"\n\n    size: Size\n    \"\"\"The size of the render.\"\"\"\n    lines: list[Strip]\n    \"\"\"Contents of the render.\"\"\"\n\n\nclass WidgetError(Exception):\n    \"\"\"Base widget error.\"\"\"\n\n\nclass MountError(WidgetError):\n    \"\"\"Error raised when there was a problem with the mount request.\"\"\"\n\n\nclass PseudoClasses(NamedTuple):\n    \"\"\"Used for render/render_line based widgets that use caching. This structure can be used as a\n    cache-key.\"\"\"\n\n    enabled: bool\n    \"\"\"Is 'enabled' applied?\"\"\"\n    focus: bool\n    \"\"\"Is 'focus' applied?\"\"\"\n    hover: bool\n    \"\"\"Is 'hover' applied?\"\"\"\n\n\nclass _BorderTitle:\n    \"\"\"Descriptor to set border titles.\"\"\"\n\n    def __set_name__(self, owner: Widget, name: str) -> None:\n        # The private name where we store the real data.\n        self._internal_name = f\"_{name}\"\n\n    def __set__(self, obj: Widget, title: str | Text | None) -> None:\n        \"\"\"Setting a title accepts a str, Text, or None.\"\"\"\n        if title is None:\n            setattr(obj, self._internal_name, None)\n        else:\n            # We store the title as Text\n            new_title = obj.render_str(title)\n            new_title.expand_tabs(4)\n            new_title = new_title.split()[0]\n            setattr(obj, self._internal_name, new_title)\n        obj.refresh()\n\n    def __get__(self, obj: Widget, objtype: type[Widget] | None = None) -> str | None:\n        \"\"\"Getting a title will return None or a str as console markup.\"\"\"\n        title: Text | None = getattr(obj, self._internal_name, None)\n        if title is None:\n            return None\n        # If we have a title, convert from Text to console markup\n        return title.markup\n\n\nclass BadWidgetName(Exception):\n    \"\"\"Raised when widget class names do not satisfy the required restrictions.\"\"\"\n\n\n@rich.repr.auto\nclass Widget(DOMNode):\n    \"\"\"\n    A Widget is the base class for Textual widgets.\n\n    See also [static][textual.widgets._static.Static] for starting point for your own widgets.\n    \"\"\"\n\n    DEFAULT_CSS = \"\"\"\n    Widget{\n        scrollbar-background: $panel-darken-1;\n        scrollbar-background-hover: $panel-darken-2;\n        scrollbar-background-active: $panel-darken-3;\n        scrollbar-color: $primary-lighten-1;\n        scrollbar-color-active: $warning-darken-1;\n        scrollbar-color-hover: $primary-lighten-1;\n        scrollbar-corner-color: $panel-darken-1;\n        scrollbar-size-vertical: 2;\n        scrollbar-size-horizontal: 1;\n        link-background: initial;\n        link-color: $text;\n        link-style: underline;\n        link-background-hover: $accent;\n        link-color-hover: $text;\n        link-style-hover: bold not underline;\n    }\n    \"\"\"\n    COMPONENT_CLASSES: ClassVar[set[str]] = set()\n\n    BORDER_TITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_title attribute.\"\"\"\n\n    BORDER_SUBTITLE: ClassVar[str] = \"\"\n    \"\"\"Initial value for border_subtitle attribute.\"\"\"\n\n    can_focus: bool = False\n    \"\"\"Widget may receive focus.\"\"\"\n    can_focus_children: bool = True\n    \"\"\"Widget's children may receive focus.\"\"\"\n    expand: Reactive[bool] = Reactive(False)\n    \"\"\"Rich renderable may expand beyond optimal size.\"\"\"\n    shrink: Reactive[bool] = Reactive(True)\n    \"\"\"Rich renderable may shrink below optimal size.\"\"\"\n    auto_links: Reactive[bool] = Reactive(True)\n    \"\"\"Widget will highlight links automatically.\"\"\"\n    disabled: Reactive[bool] = Reactive(False)\n    \"\"\"Is the widget disabled? Disabled widgets can not be interacted with, and are typically styled to look dimmer.\"\"\"\n\n    hover_style: Reactive[Style] = Reactive(Style, repaint=False)\n    \"\"\"The current hover style (style under the mouse cursor). Read only.\"\"\"\n    highlight_link_id: Reactive[str] = Reactive(\"\")\n    \"\"\"The currently highlighted link id. Read only.\"\"\"\n    loading: Reactive[bool] = Reactive(False)\n    \"\"\"If set to `True` this widget will temporarily be replaced with a loading indicator.\"\"\"\n\n    # Default sort order, incremented by constructor\n    _sort_order: ClassVar[int] = 0\n\n    def __init__(\n        self,\n        *children: Widget,\n        name: str | None = None,\n        id: str | None = None,\n        classes: str | None = None,\n        disabled: bool = False,\n    ) -> None:\n        \"\"\"Initialize a Widget.\n\n        Args:\n            *children: Child widgets.\n            name: The name of the widget.\n            id: The ID of the widget in the DOM.\n            classes: The CSS classes for the widget.\n            disabled: Whether the widget is disabled or not.\n        \"\"\"\n        _null_size = NULL_SIZE\n        self._size = _null_size\n        self._container_size = _null_size\n        self._layout_required = False\n        self._repaint_required = False\n        self._scroll_required = False\n        self._recompose_required = False\n        self._default_layout = VerticalLayout()\n        self._animate: BoundAnimator | None = None\n        Widget._sort_order += 1\n        self.sort_order = Widget._sort_order\n        self.highlight_style: Style | None = None\n\n        self._vertical_scrollbar: ScrollBar | None = None\n        self._horizontal_scrollbar: ScrollBar | None = None\n        self._scrollbar_corner: ScrollBarCorner | None = None\n\n        self._border_title: Text | None = None\n        self._border_subtitle: Text | None = None\n\n        self._render_cache = _RenderCache(_null_size, [])\n        # Regions which need to be updated (in Widget)\n        self._dirty_regions: set[Region] = set()\n        # Regions which need to be transferred from cache to screen\n        self._repaint_regions: set[Region] = set()\n\n        # Cache the auto content dimensions\n        self._content_width_cache: tuple[object, int] = (None, 0)\n        self._content_height_cache: tuple[object, int] = (None, 0)\n\n        self._arrangement_cache: FIFOCache[tuple[Size, int], DockArrangeResult] = (\n            FIFOCache(4)\n        )\n\n        self._styles_cache = StylesCache()\n        self._rich_style_cache: dict[tuple[str, ...], tuple[Style, Style]] = {}\n\n        self._tooltip: RenderableType | None = None\n        \"\"\"The tooltip content.\"\"\"\n        self._absolute_offset: Offset | None = None\n        \"\"\"Force an absolute offset for the widget (used by tooltips).\"\"\"\n\n        self._scrollbar_changes: set[tuple[bool, bool]] = set()\n        \"\"\"Used to stabilize scrollbars.\"\"\"\n\n        super().__init__(\n            name=name,\n            id=id,\n            classes=self.DEFAULT_CLASSES if classes is None else classes,\n        )\n\n        if self in children:\n            raise WidgetError(\"A widget can't be its own parent\")\n\n        for child in children:\n            if not isinstance(child, Widget):\n                raise TypeError(\n                    f\"Widget positional arguments must be Widget subclasses; not {child!r}\"\n                )\n        self._pending_children = list(children)\n        self.disabled = disabled\n        if self.BORDER_TITLE:\n            self.border_title = self.BORDER_TITLE\n        if self.BORDER_SUBTITLE:\n            self.border_subtitle = self.BORDER_SUBTITLE\n\n        self.lock = RLock()\n        \"\"\"`asyncio` lock to be used to synchronize the state of the widget.\n\n        Two different tasks might call methods on a widget at the same time, which\n        might result in a race condition.\n        This can be fixed by adding `async with widget.lock:` around the method calls.\n        \"\"\"\n        self._anchored: Widget | None = None\n        \"\"\"An anchored child widget, or `None` if no child is anchored.\"\"\"\n        self._anchor_animate: bool = False\n        \"\"\"Flag to enable animation when scrolling anchored widgets.\"\"\"\n\n    virtual_size: Reactive[Size] = Reactive(Size(0, 0), layout=True)\n    \"\"\"The virtual (scrollable) [size][textual.geometry.Size] of the widget.\"\"\"\n\n    has_focus: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Does this widget have focus? Read only.\"\"\"\n\n    mouse_over: Reactive[bool] = Reactive(False, repaint=False)\n    \"\"\"Is the mouse over this widget? Read only.\"\"\"\n\n    scroll_x: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the X axis.\"\"\"\n\n    scroll_y: Reactive[float] = Reactive(0.0, repaint=False, layout=False)\n    \"\"\"The scroll position on the Y axis.\"\"\"\n\n    scroll_target_x = Reactive(0.0, repaint=False)\n    scroll_target_y = Reactive(0.0, repaint=False)\n\n    show_vertical_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a vertical scrollbar?\"\"\"\n\n    show_horizontal_scrollbar: Reactive[bool] = Reactive(False, layout=True)\n    \"\"\"Show a horizontal scrollbar?\"\"\"\n\n    border_title: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the top border (if there is one).\"\"\"\n    border_subtitle: str | Text | None = _BorderTitle()  # type: ignore\n    \"\"\"A title to show in the bottom border (if there is one).\"\"\"\n\n    @property\n    def is_mounted(self) -> bool:\n        \"\"\"Check if this widget is mounted.\"\"\"\n        return self._is_mounted\n\n    @property\n    def siblings(self) -> list[Widget]:\n        \"\"\"Get the widget's siblings (self is removed from the return list).\n\n        Returns:\n            A list of siblings.\n        \"\"\"\n        parent = self.parent\n        if parent is not None:\n            siblings = list(parent._nodes)\n            siblings.remove(self)\n            return siblings\n        else:\n            return []\n\n    @property\n    def visible_siblings(self) -> list[Widget]:\n        \"\"\"A list of siblings which will be shown.\n\n        Returns:\n            List of siblings.\n        \"\"\"\n        siblings = [\n            widget for widget in self.siblings if widget.visible and widget.display\n        ]\n        return siblings\n\n    @property\n    def allow_vertical_scroll(self) -> bool:\n        \"\"\"Check if vertical scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_vertical_scrollbar\n\n    @property\n    def allow_horizontal_scroll(self) -> bool:\n        \"\"\"Check if horizontal scroll is permitted.\n\n        May be overridden if you want different logic regarding allowing scrolling.\n        \"\"\"\n        if self._check_disabled():\n            return False\n        return self.is_scrollable and self.show_horizontal_scrollbar\n\n    @property\n    def _allow_scroll(self) -> bool:\n        \"\"\"Check if both axis may be scrolled.\n\n        Returns:\n            True if horizontal and vertical scrolling is enabled.\n        \"\"\"\n        return self.is_scrollable and (\n            self.allow_horizontal_scroll or self.allow_vertical_scroll\n        )\n\n    @property\n    def offset(self) -> Offset:\n        \"\"\"Widget offset from origin.\n\n        Returns:\n            Relative offset.\n        \"\"\"\n        return self.styles.offset.resolve(self.size, self.app.size)\n\n    @offset.setter\n    def offset(self, offset: tuple[int, int]) -> None:\n        self.styles.offset = ScalarOffset.from_offset(offset)\n\n    @property\n    def opacity(self) -> float:\n        \"\"\"Total opacity of widget.\"\"\"\n        opacity = 1.0\n        for node in reversed(self.ancestors_with_self):\n            opacity *= node.styles.opacity\n            if not opacity:\n                break\n        return opacity\n\n    @property\n    def is_anchored(self) -> bool:\n        \"\"\"Is this widget anchored?\"\"\"\n        return self._parent is not None and self._parent is self\n\n    def anchor(self, *, animate: bool = False) -> None:\n        \"\"\"Anchor the widget, which scrolls it into view (like [scroll_visible][textual.widget.Widget.scroll_visible]),\n        but also keeps it in view if the widget's size changes, or the size of its container changes.\n\n        !!! note\n\n            Anchored widgets will be un-anchored if the users scrolls the container.\n\n        Args:\n            animate: `True` if the scroll should animate, or `False` if it shouldn't.\n        \"\"\"\n        if self._parent is not None and isinstance(self._parent, Widget):\n            self._parent._anchored = self\n            self._parent._anchor_animate = animate\n            self.check_idle()\n\n    def clear_anchor(self) -> None:\n        \"\"\"Stop anchoring this widget (a no-op if this widget is not anchored).\"\"\"\n        if (\n            self._parent is not None\n            and isinstance(self._parent, Widget)\n            and self._parent._anchored is self\n        ):\n            self._parent._anchored = None\n\n    def _clear_anchor(self) -> None:\n        \"\"\"Clear an anchored child.\"\"\"\n        self._anchored = None\n\n    def _check_disabled(self) -> bool:\n        \"\"\"Check if the widget is disabled either explicitly by setting `disabled`,\n        or implicitly by setting `loading`.\n\n        Returns:\n            True if the widget should be disabled.\n        \"\"\"\n        return self.disabled or self.loading\n\n    @property\n    def tooltip(self) -> RenderableType | None:\n        \"\"\"Tooltip for the widget, or `None` for no tooltip.\"\"\"\n        return self._tooltip\n\n    @tooltip.setter\n    def tooltip(self, tooltip: RenderableType | None):\n        self._tooltip = tooltip\n        try:\n            self.screen._update_tooltip(self)\n        except NoScreen:\n            pass\n\n    def allow_focus(self) -> bool:\n        \"\"\"Check if the widget is permitted to focus.\n\n        The base class returns [`can_focus`][textual.widget.Widget.can_focus].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget may be focused, or `False` if it may not be focused.\n        \"\"\"\n        return self.can_focus\n\n    def allow_focus_children(self) -> bool:\n        \"\"\"Check if a widget's children may be focused.\n\n        The base class returns [`can_focus_children`][textual.widget.Widget.can_focus_children].\n        This method maybe overridden if additional logic is required.\n\n        Returns:\n            `True` if the widget's children may be focused, or `False` if the widget's children may not be focused.\n        \"\"\"\n        return self.can_focus_children\n\n    def compose_add_child(self, widget: Widget) -> None:\n        \"\"\"Add a node to children.\n\n        This is used by the compose process when it adds children.\n        There is no need to use it directly, but you may want to override it in a subclass\n        if you want children to be attached to a different node.\n\n        Args:\n            widget: A Widget to add.\n        \"\"\"\n        _rich_traceback_omit = True\n        self._pending_children.append(widget)\n\n    def __enter__(self) -> Self:\n        \"\"\"Use as context manager when composing.\"\"\"\n        self.app._compose_stacks[-1].append(self)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        \"\"\"Exit compose context manager.\"\"\"\n        compose_stack = self.app._compose_stacks[-1]\n        composed = compose_stack.pop()\n        if compose_stack:\n            compose_stack[-1].compose_add_child(composed)\n        else:\n            self.app._composed[-1].append(composed)\n\n    def clear_cached_dimensions(self) -> None:\n        \"\"\"Clear cached results of `get_content_width` and `get_content_height`.\n\n        Call if the widget's renderable changes size after the widget has been created.\n\n        !!! note\n\n            This is not required if you are extending [`Static`][textual.widgets.Static].\n\n        \"\"\"\n        self._content_width_cache = (None, 0)\n        self._content_height_cache = (None, 0)\n\n    def get_loading_widget(self) -> Widget:\n        \"\"\"Get a widget to display a loading indicator.\n\n        The default implementation will defer to App.get_loading_widget.\n\n        Returns:\n            A widget in place of this widget to indicate a loading.\n        \"\"\"\n        loading_widget = self.app.get_loading_widget()\n        return loading_widget\n\n    def set_loading(self, loading: bool) -> Awaitable:\n        \"\"\"Set or reset the loading state of this widget.\n\n        A widget in a loading state will display a LoadingIndicator that obscures the widget.\n\n        Args:\n            loading: `True` to put the widget into a loading state, or `False` to reset the loading state.\n\n        Returns:\n            An optional awaitable.\n        \"\"\"\n        LOADING_INDICATOR_CLASS = \"-textual-loading-indicator\"\n        LOADING_INDICATOR_QUERY = f\".{LOADING_INDICATOR_CLASS}\"\n        remove_indicator = self.query_children(LOADING_INDICATOR_QUERY).remove()\n        if loading:\n            loading_indicator = self.get_loading_widget()\n            loading_indicator.add_class(LOADING_INDICATOR_CLASS)\n            await_mount = self.mount(loading_indicator)\n            return AwaitComplete(remove_indicator, await_mount).call_next(self)\n        else:\n            return remove_indicator\n\n    async def _watch_loading(self, loading: bool) -> None:\n        \"\"\"Called when the 'loading' reactive is changed.\"\"\"\n        await self.set_loading(loading)\n\n    ExpectType = TypeVar(\"ExpectType\", bound=\"Widget\")\n\n    if TYPE_CHECKING:\n\n        @overload\n        def get_child_by_id(self, id: str) -> Widget: ...\n\n        @overload\n        def get_child_by_id(\n            self, id: str, expect_type: type[ExpectType]\n        ) -> ExpectType: ...\n\n    def get_child_by_id(\n        self, id: str, expect_type: type[ExpectType] | None = None\n    ) -> ExpectType | Widget:\n        \"\"\"Return the first child (immediate descendent) of this node with the given ID.\n\n       \n```\n# Function Signature: def get_content_height(self, container: Size, viewport: Size, width: int) -> int:\n# Natural Language Annotations: \"\"\"Called by Textual to get the height of the content area. May be overridden in a subclass.\n\n        Args:\n            container: Size of the container (immediate parent) widget.\n            viewport: Size of the viewport.\n            width: Width of renderable.\n\n        Returns:\n            The height of the content.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_content_height(self, container: Size,", "  ```\ndef get_content_height(self, container: Size,", "  ```\ndef get_content_height(self, container: Size,"]}, "textual-src/textual/suggester.py-_get_suggestion": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/suggester.py:\n```\n\"\"\"\n\nThe `Suggester` class is used by the [Input](/widgets/input) widget.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Iterable\n\nfrom .cache import LRUCache\nfrom .dom import DOMNode\nfrom .message import Message\n\n\n@dataclass\nclass SuggestionReady(Message):\n    \"\"\"Sent when a completion suggestion is ready.\"\"\"\n\n    value: str\n    \"\"\"The value to which the suggestion is for.\"\"\"\n    suggestion: str\n    \"\"\"The string suggestion.\"\"\"\n\n\nclass Suggester(ABC):\n    \"\"\"Defines how widgets generate completion suggestions.\n\n    To define a custom suggester, subclass `Suggester` and implement the async method\n    `get_suggestion`.\n    See [`SuggestFromList`][textual.suggester.SuggestFromList] for an example.\n    \"\"\"\n\n    cache: LRUCache[str, str | None] | None\n    \"\"\"Suggestion cache, if used.\"\"\"\n\n    def __init__(self, *, use_cache: bool = True, case_sensitive: bool = False) -> None:\n        \"\"\"Create a suggester object.\n\n        Args:\n            use_cache: Whether to cache suggestion results.\n            case_sensitive: Whether suggestions are case sensitive or not.\n                If they are not, incoming values are casefolded before generating\n                the suggestion.\n        \"\"\"\n        self.cache = LRUCache(1024) if use_cache else None\n        self.case_sensitive = case_sensitive\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @abstractmethod\n    async def get_suggestion(self, value: str) -> str | None:\n        \"\"\"Try to get a completion suggestion for the given input value.\n\n        Custom suggesters should implement this method.\n\n        Note:\n            The value argument will be casefolded if `self.case_sensitive` is `False`.\n\n        Note:\n            If your implementation is not deterministic, you may need to disable caching.\n\n        Args:\n            value: The current value of the requester widget.\n\n        Returns:\n            A valid suggestion or `None`.\n        \"\"\"\n        pass\n\n\nclass SuggestFromList(Suggester):\n    \"\"\"Give completion suggestions based on a fixed list of options.\n\n    Example:\n        ```py\n        countries = [\"England\", \"Scotland\", \"Portugal\", \"Spain\", \"France\"]\n\n        class MyApp(App[None]):\n            def compose(self) -> ComposeResult:\n                yield Input(suggester=SuggestFromList(countries, case_sensitive=False))\n        ```\n\n        If the user types ++p++ inside the input widget, a completion suggestion\n        for `\"Portugal\"` appears.\n    \"\"\"\n\n    def __init__(\n        self, suggestions: Iterable[str], *, case_sensitive: bool = True\n    ) -> None:\n        \"\"\"Creates a suggester based off of a given iterable of possibilities.\n\n        Args:\n            suggestions: Valid suggestions sorted by decreasing priority.\n            case_sensitive: Whether suggestions are computed in a case sensitive manner\n                or not. The values provided in the argument `suggestions` represent the\n                canonical representation of the completions and they will be suggested\n                with that same casing.\n        \"\"\"\n        super().__init__(case_sensitive=case_sensitive)\n        self._suggestions = list(suggestions)\n        self._for_comparison = (\n            self._suggestions\n            if self.case_sensitive\n            else [suggestion.casefold() for suggestion in self._suggestions]\n        )\n\n    async def get_suggestion(self, value: str) -> str | None:\n        \"\"\"Gets a completion from the given possibilities.\n\n        Args:\n            value: The current value.\n\n        Returns:\n            A valid completion suggestion or `None`.\n        \"\"\"\n        for idx, suggestion in enumerate(self._for_comparison):\n            if suggestion.startswith(value):\n                return self._suggestions[idx]\n        return None\n\n```\n# Function Signature: async def _get_suggestion(self, requester: DOMNode, value: str) -> None:\n# Natural Language Annotations: \"\"\"Used by widgets to get completion suggestions.\n\n        Note:\n            When implementing custom suggesters, this method does not need to be\n            overridden.\n\n        Args:\n            requester: The message target that requested a suggestion.\n            value: The current value to complete.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nasync def _get_suggestion(self, requester", "  ```\nasync def _get_suggestion(self, requester", "  ```\nasync def _get_suggestion(self, requester"]}, "textual-src/textual/dom.py-add_class": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/dom.py:\n```\n\"\"\"\nA DOMNode is a base class for any object within the Textual Document Object Model,\nwhich includes all Widgets, Screens, and Apps.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nimport threading\nfrom functools import lru_cache, partial\nfrom inspect import getfile\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Iterable,\n    Sequence,\n    Type,\n    TypeVar,\n    cast,\n    overload,\n)\n\nimport rich.repr\nfrom rich.highlighter import ReprHighlighter\nfrom rich.style import Style\nfrom rich.text import Text\nfrom rich.tree import Tree\n\nfrom ._context import NoActiveAppError, active_message_pump\nfrom ._node_list import NodeList\nfrom ._types import WatchCallbackType\nfrom ._worker_manager import WorkerManager\nfrom .binding import Binding, BindingType, _Bindings\nfrom .color import BLACK, WHITE, Color\nfrom .css._error_tools import friendly_list\nfrom .css.constants import VALID_DISPLAY, VALID_VISIBILITY\nfrom .css.errors import DeclarationError, StyleValueError\nfrom .css.parse import parse_declarations\nfrom .css.styles import RenderStyles, Styles\nfrom .css.tokenize import IDENTIFIER\nfrom .message_pump import MessagePump\nfrom .reactive import Reactive, ReactiveError, _watch\nfrom .timer import Timer\nfrom .walk import walk_breadth_first, walk_depth_first\n\nif TYPE_CHECKING:\n    from typing_extensions import Self, TypeAlias\n    from _typeshed import SupportsRichComparison\n\n    from rich.console import RenderableType\n    from .app import App\n    from .css.query import DOMQuery, QueryType\n    from .css.types import CSSLocation\n    from .message import Message\n    from .screen import Screen\n    from .widget import Widget\n    from .worker import Worker, WorkType, ResultType\n\n    # Unused & ignored imports are needed for the docs to link to these objects:\n    from .css.query import NoMatches, TooManyMatches, WrongType  # type: ignore  # noqa: F401\n\nfrom typing_extensions import Literal\n\n_re_identifier = re.compile(IDENTIFIER)\n\n\nWalkMethod: TypeAlias = Literal[\"depth\", \"breadth\"]\n\"\"\"Valid walking methods for the [`DOMNode.walk_children` method][textual.dom.DOMNode.walk_children].\"\"\"\n\n\nReactiveType = TypeVar(\"ReactiveType\")\n\n\nclass BadIdentifier(Exception):\n    \"\"\"Exception raised if you supply a `id` attribute or class name in the wrong format.\"\"\"\n\n\ndef check_identifiers(description: str, *names: str) -> None:\n    \"\"\"Validate identifier and raise an error if it fails.\n\n    Args:\n        description: Description of where identifier is used for error message.\n        *names: Identifiers to check.\n    \"\"\"\n    match = _re_identifier.fullmatch\n    for name in names:\n        if match(name) is None:\n            raise BadIdentifier(\n                f\"{name!r} is an invalid {description}; \"\n                \"identifiers must contain only letters, numbers, underscores, or hyphens, and must not begin with a number.\"\n            )\n\n\nclass DOMError(Exception):\n    \"\"\"Base exception class for errors relating to the DOM.\"\"\"\n\n\nclass NoScreen(DOMError):\n    \"\"\"Raised when the node has no associated screen.\"\"\"\n\n\nclass _ClassesDescriptor:\n    \"\"\"A descriptor to manage the `classes` property.\"\"\"\n\n    def __get__(\n        self, obj: DOMNode, objtype: type[DOMNode] | None = None\n    ) -> frozenset[str]:\n        \"\"\"A frozenset of the current classes on the widget.\"\"\"\n        return frozenset(obj._classes)\n\n    def __set__(self, obj: DOMNode, classes: str | Iterable[str]) -> None:\n        \"\"\"Replaces classes entirely.\"\"\"\n        if isinstance(classes, str):\n            class_names = set(classes.split())\n        else:\n            class_names = set(classes)\n        check_identifiers(\"class name\", *class_names)\n        obj._classes = class_names\n        obj._update_styles()\n\n\n@rich.repr.auto\nclass DOMNode(MessagePump):\n    \"\"\"The base class for object that can be in the Textual DOM (App and Widget)\"\"\"\n\n    # CSS defaults\n    DEFAULT_CSS: ClassVar[str] = \"\"\n\n    # Default classes argument if not supplied\n    DEFAULT_CLASSES: str = \"\"\n\n    # Virtual DOM nodes\n    COMPONENT_CLASSES: ClassVar[set[str]] = set()\n\n    # Mapping of key bindings\n    BINDINGS: ClassVar[list[BindingType]] = []\n\n    # Indicates if the CSS should be automatically scoped\n    SCOPED_CSS: ClassVar[bool] = True\n    \"\"\"Should default css be limited to the widget type?\"\"\"\n\n    # True if this node inherits the CSS from the base class.\n    _inherit_css: ClassVar[bool] = True\n\n    # True if this node inherits the component classes from the base class.\n    _inherit_component_classes: ClassVar[bool] = True\n\n    # True to inherit bindings from base class\n    _inherit_bindings: ClassVar[bool] = True\n\n    # List of names of base classes that inherit CSS\n    _css_type_names: ClassVar[frozenset[str]] = frozenset()\n\n    # Name of the widget in CSS\n    _css_type_name: str = \"\"\n\n    # Generated list of bindings\n    _merged_bindings: ClassVar[_Bindings | None] = None\n\n    _reactives: ClassVar[dict[str, Reactive]]\n\n    _decorated_handlers: dict[type[Message], list[tuple[Callable, str | None]]]\n\n    # Names of potential computed reactives\n    _computes: ClassVar[frozenset[str]]\n\n    def __init__(\n        self,\n        *,\n        name: str | None = None,\n        id: str | None = None,\n        classes: str | None = None,\n    ) -> None:\n        self._classes: set[str] = set()\n        self._name = name\n        self._id = None\n        if id is not None:\n            self.id = id\n\n        _classes = classes.split() if classes else []\n        check_identifiers(\"class name\", *_classes)\n        self._classes.update(_classes)\n\n        self._nodes: NodeList = NodeList()\n        self._css_styles: Styles = Styles(self)\n        self._inline_styles: Styles = Styles(self)\n        self.styles: RenderStyles = RenderStyles(\n            self, self._css_styles, self._inline_styles\n        )\n        # A mapping of class names to Styles set in COMPONENT_CLASSES\n        self._component_styles: dict[str, RenderStyles] = {}\n\n        self._auto_refresh: float | None = None\n        self._auto_refresh_timer: Timer | None = None\n        self._css_types = {cls.__name__ for cls in self._css_bases(self.__class__)}\n        self._bindings = (\n            _Bindings()\n            if self._merged_bindings is None\n            else self._merged_bindings.copy()\n        )\n        self._has_hover_style: bool = False\n        self._has_focus_within: bool = False\n        self._reactive_connect: (\n            dict[str, tuple[MessagePump, Reactive | object]] | None\n        ) = None\n\n        super().__init__()\n\n    def set_reactive(\n        self, reactive: Reactive[ReactiveType], value: ReactiveType\n    ) -> None:\n        \"\"\"Sets a reactive value *without* invoking validators or watchers.\n\n        Example:\n            ```python\n            self.set_reactive(App.dark_mode, True)\n            ```\n\n        Args:\n            name: Name of reactive attribute.\n            value: New value of reactive.\n\n        Raises:\n            AttributeError: If the first argument is not a reactive.\n        \"\"\"\n        if not isinstance(reactive, Reactive):\n            raise TypeError(\n                \"A Reactive class is required; for example: MyApp.dark_mode\"\n            )\n        if reactive.name not in self._reactives:\n            raise AttributeError(\n                \"No reactive called {name!r}; Have you called super().__init__(...) in the {self.__class__.__name__} constructor?\"\n            )\n        setattr(self, f\"_reactive_{reactive.name}\", value)\n\n    def data_bind(\n        self,\n        *reactives: Reactive[Any],\n        **bind_vars: Reactive[Any] | object,\n    ) -> Self:\n        \"\"\"Bind reactive data so that changes to a reactive automatically change the reactive on another widget.\n\n        Reactives may be given as positional arguments or keyword arguments.\n        See the [guide on data binding](/guide/reactivity#data-binding).\n\n        Example:\n            ```python\n            def compose(self) -> ComposeResult:\n                yield WorldClock(\"Europe/London\").data_bind(WorldClockApp.time)\n                yield WorldClock(\"Europe/Paris\").data_bind(WorldClockApp.time)\n                yield WorldClock(\"Asia/Tokyo\").data_bind(WorldClockApp.time)\n            ```\n\n        Raises:\n            ReactiveError: If the data wasn't bound.\n\n        Returns:\n            Self.\n        \"\"\"\n        _rich_traceback_omit = True\n\n        parent = active_message_pump.get()\n\n        if self._reactive_connect is None:\n            self._reactive_connect = {}\n        bind_vars = {**{reactive.name: reactive for reactive in reactives}, **bind_vars}\n        for name, reactive in bind_vars.items():\n            if name not in self._reactives:\n                raise ReactiveError(\n                    f\"Unable to bind non-reactive attribute {name!r} on {self}\"\n                )\n            if isinstance(reactive, Reactive) and not isinstance(\n                parent, reactive.owner\n            ):\n                raise ReactiveError(\n                    f\"Unable to bind data; {reactive.owner.__name__} is not defined on {parent.__class__.__name__}.\"\n                )\n            self._reactive_connect[name] = (parent, reactive)\n        if self._is_mounted:\n            self._initialize_data_bind()\n        else:\n            self.call_later(self._initialize_data_bind)\n        return self\n\n    def _initialize_data_bind(self) -> None:\n        \"\"\"initialize a data binding.\n\n        Args:\n            compose_parent: The node doing the binding.\n        \"\"\"\n        if not self._reactive_connect:\n            return\n        for variable_name, (compose_parent, reactive) in self._reactive_connect.items():\n\n            def make_setter(variable_name: str) -> Callable[[object], None]:\n                \"\"\"Make a setter for the given variable name.\n\n                Args:\n                    variable_name: Name of variable being set.\n\n                Returns:\n                    A callable which takes the value to set.\n                \"\"\"\n\n                def setter(value: object) -> None:\n                    \"\"\"Set bound data.\"\"\"\n                    _rich_traceback_omit = True\n                    Reactive._initialize_object(self)\n                    setattr(self, variable_name, value)\n\n                return setter\n\n            assert isinstance(compose_parent, DOMNode)\n            setter = make_setter(variable_name)\n            if isinstance(reactive, Reactive):\n                self.watch(\n                    compose_parent,\n                    reactive.name,\n                    setter,\n                    init=True,\n                )\n            else:\n                self.call_later(partial(setter, reactive))\n        self._reactive_connect = None\n\n    def compose_add_child(self, widget: Widget) -> None:\n        \"\"\"Add a node to children.\n\n        This is used by the compose process when it adds children.\n        There is no need to use it directly, but you may want to override it in a subclass\n        if you want children to be attached to a different node.\n\n        Args:\n            widget: A Widget to add.\n        \"\"\"\n        self._nodes._append(widget)\n\n    @property\n    def children(self) -> Sequence[\"Widget\"]:\n        \"\"\"A view on to the children.\n\n        Returns:\n            The node's children.\n        \"\"\"\n        return self._nodes\n\n    def sort_children(\n        self,\n        *,\n        key: Callable[[Widget], SupportsRichComparison] | None = None,\n        reverse: bool = False,\n    ) -> None:\n        \"\"\"Sort child widgets with an optional key function.\n\n        If `key` is not provided then widgets will be sorted in the order they are constructed.\n\n        Example:\n            ```python\n            # Sort widgets by name\n            screen.sort_children(key=lambda widget: widget.name or \"\")\n            ```\n\n        Args:\n            key: A callable which accepts a widget and returns something that can be sorted,\n                or `None` to sort without a key function.\n            reverse: Sort in descending order.\n        \"\"\"\n        self._nodes._sort(key=key, reverse=reverse)\n        self.refresh(layout=True)\n\n    @property\n    def auto_refresh(self) -> float | None:\n        \"\"\"Number of seconds between automatic refresh, or `None` for no automatic refresh.\"\"\"\n        return self._auto_refresh\n\n    @auto_refresh.setter\n    def auto_refresh(self, interval: float | None) -> None:\n        if self._auto_refresh_timer is not None:\n            self._auto_refresh_timer.stop()\n            self._auto_refresh_timer = None\n        if interval is not None:\n            self._auto_refresh_timer = self.set_interval(\n                interval, self._automatic_refresh, name=f\"auto refresh {self!r}\"\n            )\n        self._auto_refresh = interval\n\n    @property\n    def workers(self) -> WorkerManager:\n        \"\"\"The app's worker manager. Shortcut for `self.app.workers`.\"\"\"\n        return self.app.workers\n\n    def run_worker(\n        self,\n        work: WorkType[ResultType],\n        name: str | None = \"\",\n        group: str = \"default\",\n        description: str = \"\",\n        exit_on_error: bool = True,\n        start: bool = True,\n        exclusive: bool = False,\n        thread: bool = False,\n    ) -> Worker[ResultType]:\n        \"\"\"Run work in a worker.\n\n        A worker runs a function, coroutine, or awaitable, in the *background* as an async task or as a thread.\n\n        Args:\n            work: A function, async function, or an awaitable object to run in a worker.\n            name: A short string to identify the worker (in logs and debugging).\n            group: A short string to identify a group of workers.\n            description: A longer string to store longer information on the worker.\n            exit_on_error: Exit the app if the worker raises an error. Set to `False` to suppress exceptions.\n            start: Start the worker immediately.\n            exclusive: Cancel all workers in the same group.\n            thread: Mark the worker as a thread worker.\n\n        Returns:\n            New Worker instance.\n        \"\"\"\n\n        # If we're running a worker from inside a secondary thread,\n        # do so in a thread-safe way.\n        if self.app._thread_id != threading.get_ident():\n            creator = partial(self.app.call_from_thread, self.workers._new_worker)\n        else:\n            creator = self.workers._new_worker\n        worker: Worker[ResultType] = creator(\n            work,\n            self,\n            name=name,\n            group=group,\n            description=description,\n            exit_on_error=exit_on_error,\n            start=start,\n            exclusive=exclusive,\n            thread=thread,\n        )\n        return worker\n\n    @property\n    def is_modal(self) -> bool:\n        \"\"\"Is the node a modal?\"\"\"\n        return False\n\n    def _automatic_refresh(self) -> None:\n        \"\"\"Perform an automatic refresh (set with auto_refresh property).\"\"\"\n        self.refresh()\n\n    def __init_subclass__(\n        cls,\n        inherit_css: bool = True,\n        inherit_bindings: bool = True,\n        inherit_component_classes: bool = True,\n    ) -> None:\n        super().__init_subclass__()\n\n        reactives = cls._reactives = {}\n        for base in reversed(cls.__mro__):\n            reactives.update(\n                {\n                    name: reactive\n                    for name, reactive in base.__dict__.items()\n                    if isinstance(reactive, Reactive)\n                }\n            )\n\n        cls._inherit_css = inherit_css\n        cls._inherit_bindings = inherit_bindings\n        cls._inherit_component_classes = inherit_component_classes\n        css_type_names: set[str] = set()\n        bases = cls._css_bases(cls)\n        cls._css_type_name = bases[0].__name__\n        for base in bases:\n            css_type_names.add(base.__name__)\n        cls._merged_bindings = cls._merge_bindings()\n        cls._css_type_names = frozenset(css_type_names)\n        cls._computes = frozenset(\n            [\n                name.lstrip(\"_\")[8:]\n                for name in dir(cls)\n                if name.startswith((\"_compute_\", \"compute_\"))\n            ]\n        )\n\n    def get_component_styles(self, *names: str) -> RenderStyles:\n        \"\"\"Get a \"component\" styles object (must be defined in COMPONENT_CLASSES classvar).\n\n        Args:\n            name: Name of the component.\n\n        Raises:\n            KeyError: If the component class doesn't exist.\n\n        Returns:\n            A Styles object.\n        \"\"\"\n        styles = RenderStyles(self, Styles(), Styles())\n        for name in names:\n            if name not in self._component_styles:\n                raise KeyError(f\"No {name!r} key in COMPONENT_CLASSES\")\n            component_styles = self._component_styles[name]\n            styles.node = component_styles.node\n            styles.base.merge(component_styles.base)\n            styles.inline.merge(component_styles.inline)\n            styles._updates += 1\n\n        return styles\n\n    def _post_mount(self):\n        \"\"\"Called after the object has been mounted.\"\"\"\n        _rich_traceback_omit = True\n        Reactive._initialize_object(self)\n\n    def notify_style_update(self) -> None:\n        \"\"\"Called after styles are updated.\n\n        Implement this in a subclass if you want to clear any cached data when the CSS is reloaded.\n        \"\"\"\n\n    @property\n    def _node_bases(self) -> Sequence[Type[DOMNode]]:\n        \"\"\"The DOMNode bases classes (including self.__class__)\"\"\"\n        # Node bases are in reversed order so that the base class is lower priority\n        return self._css_bases(self.__class__)\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def _css_bases(cls, base: Type[DOMNode]) -> Sequence[Type[DOMNode]]:\n        \"\"\"Get the DOMNode base classes, which inherit CSS.\n\n        Args:\n            base: A DOMNode class\n\n        Returns:\n            An iterable of DOMNode classes.\n        \"\"\"\n        classes: list[type[DOMNode]] = []\n        _class = base\n        while True:\n            classes.append(_class)\n            if not _class._inherit_css:\n                break\n            for _base in _class.__bases__:\n                if issubclass(_base, DOMNode):\n                    _class = _base\n                    break\n            else:\n                break\n        return classes\n\n    @classmethod\n    def _merge_bindings(cls) -> _Bindings:\n        \"\"\"Merge bindings from base classes.\n\n        Returns:\n            Merged bindings.\n        \"\"\"\n        bindings: list[_Bindings] = []\n\n        for base in reversed(cls.__mro__):\n            if issubclass(base, DOMNode):\n                if not base._inherit_bindings:\n                    bindings.clear()\n                bindings.append(\n                    _Bindings(\n                        base.__dict__.get(\"BINDINGS\", []),\n                    )\n                )\n        keys: dict[str, Binding] = {}\n        for bindings_ in bindings:\n            keys.update(bindings_.keys)\n        return _Bindings(keys.values())\n\n    def _post_register(self, app: App) -> None:\n        \"\"\"Called when the widget is registered\n\n        Args:\n            app: Parent application.\n        \"\"\"\n\n    def __rich_repr__(self) -> rich.repr.Result:\n        # Being a bit defensive here to guard against errors when calling repr before initialization\n        if hasattr(self, \"_name\"):\n            yield \"name\", self._name, None\n        if hasattr(self, \"_id\"):\n            yield \"id\", self._id, None\n        if hasattr(self, \"_classes\") and self._classes:\n            yield \"classes\", \" \".join(self._classes)\n\n    def _get_default_css(self) -> list[tuple[CSSLocation, str, int, str]]:\n        \"\"\"Gets the CSS for this class and inherited from bases.\n\n        Default CSS is inherited from base classes, unless `inherit_css` is set to\n        `False` when subclassing.\n\n        Returns:\n            A list of tuples containing (LOCATION, SOURCE, SPECIFICITY, SCOPE) for this\n                class and inherited from base classes.\n        \"\"\"\n\n        css_stack: list[tuple[CSSLocation, str, int, str]] = []\n\n        def get_location(base: Type[DOMNode]) -> CSSLocation:\n            \"\"\"Get the original location of this DEFAULT_CSS.\n\n            Args:\n                base: The class from which the default css was extracted.\n\n            Returns:\n                The filename where the class was defined (if possible) and the class\n                    variable the CSS was extracted from.\n            \"\"\"\n            try:\n                return (getfile(base), f\"{base.__name__}.DEFAULT_CSS\")\n            except (TypeError, OSError):\n                return (\"\", f\"{base.__name__}.DEFAULT_CSS\")\n\n        for tie_breaker, base in enumerate(self._node_bases):\n            css: str = base.__dict__.get(\"DEFAULT_CSS\", \"\")\n            if css:\n                scoped: bool = base.__dict__.get(\"SCOPED_CSS\", True)\n                css_stack.append(\n                    (\n                        get_location(base),\n                        css,\n                        -tie_breaker,\n                        base._css_type_name if scoped else \"\",\n                    )\n                )\n        return css_stack\n\n    @classmethod\n    @lru_cache(maxsize=None)\n    def _get_component_classes(cls) -> frozenset[str]:\n        \"\"\"Gets the component classes for this class and inherited from bases.\n\n        Component classes are inherited from base classes, unless\n        `inherit_component_classes` is set to `False` when subclassing.\n\n        Returns:\n            A set with all the component classes available.\n        \"\"\"\n\n        component_classes: set[str] = set()\n        for base in cls._css_bases(cls):\n            component_classes.update(base.__dict__.get(\"COMPONENT_CLASSES\", set()))\n            if not base.__dict__.get(\"_inherit_component_classes\", True):\n                break\n\n        return frozenset(component_classes)\n\n    @property\n    def parent(self) -> DOMNode | None:\n        \"\"\"The parent node.\n\n        All nodes have parent once added to the DOM, with the exception of the App which is the *root* node.\n        \"\"\"\n        return cast(\"DOMNode | None\", self._parent)\n\n    @property\n    def screen(self) -> \"Screen[object]\":\n        \"\"\"The screen containing this node.\n\n        Returns:\n            A screen object.\n\n        Raises:\n            NoScreen: If this node isn't mounted (and has no screen).\n        \"\"\"\n        # Get the node by looking up a chain of parents\n        # Note that self.screen may not be the same as self.app.screen\n        from .screen import Screen\n\n        node: MessagePump | None = self\n        while node is not None and not isinstance(node, Screen):\n            node = node._parent\n        if not isinstance(node, Screen):\n            raise NoScreen(\"node has no screen\")\n        return node\n\n    @property\n    def id(self) -> str | None:\n        \"\"\"The ID of this node, or None if the node has no ID.\"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, new_id: str) -> str:\n        \"\"\"Sets the ID (may only be done once).\n\n        Args:\n            new_id: ID for this node.\n\n        Raises:\n            ValueError: If the ID has already been set.\n        \"\"\"\n        check_identifiers(\"id\", new_id)\n\n        if self._id is not None:\n            raise ValueError(\n                f\"Node 'id' attribute may not be changed once set (current id={self._id!r})\"\n            )\n        self._id = new_id\n        return new_id\n\n    @property\n    def name(self) -> str | None:\n        \"\"\"The name of the node.\"\"\"\n        return\n```\n# Function Signature: def add_class(self, *class_names: str, update: bool = True) -> Self:\n# Natural Language Annotations: \"\"\"Add class names to this Node.\n\n        Args:\n            *class_names: CSS class names to add.\n            update: Also update styles.\n\n        Returns:\n            Self.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef add_class(self, *class_names: str", "  \n[PYTHON]\ndef add_class(self, *", "  ```\ndef add_class(self, *class_names: str"]}, "textual-src/textual/reactive.py-_initialize_object": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/reactive.py:\n```\n\"\"\"\n\nThe `Reactive` class implements [reactivity](/guide/reactivity/).\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom functools import partial\nfrom inspect import isawaitable\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Generic,\n    Type,\n    TypeVar,\n    cast,\n    overload,\n)\n\nimport rich.repr\n\nfrom . import events\nfrom ._callback import count_parameters\nfrom ._context import active_message_pump\nfrom ._types import (\n    MessageTarget,\n    WatchCallbackBothValuesType,\n    WatchCallbackNewValueType,\n    WatchCallbackNoArgsType,\n    WatchCallbackType,\n)\n\nif TYPE_CHECKING:\n    from .dom import DOMNode\n\n    Reactable = DOMNode\n\nReactiveType = TypeVar(\"ReactiveType\")\nReactableType = TypeVar(\"ReactableType\", bound=\"DOMNode\")\n\n\nclass ReactiveError(Exception):\n    \"\"\"Base class for reactive errors.\"\"\"\n\n\nclass TooManyComputesError(ReactiveError):\n    \"\"\"Raised when an attribute has public and private compute methods.\"\"\"\n\n\nasync def await_watcher(obj: Reactable, awaitable: Awaitable[object]) -> None:\n    \"\"\"Coroutine to await an awaitable returned from a watcher\"\"\"\n    _rich_traceback_omit = True\n    await awaitable\n    # Watcher may have changed the state, so run compute again\n    obj.post_message(events.Callback(callback=partial(Reactive._compute, obj)))\n\n\ndef invoke_watcher(\n    watcher_object: Reactable,\n    watch_function: WatchCallbackType,\n    old_value: object,\n    value: object,\n) -> None:\n    \"\"\"Invoke a watch function.\n\n    Args:\n        watcher_object: The object watching for the changes.\n        watch_function: A watch function, which may be sync or async.\n        old_value: The old value of the attribute.\n        value: The new value of the attribute.\n    \"\"\"\n    _rich_traceback_omit = True\n\n    param_count = count_parameters(watch_function)\n    reset_token = active_message_pump.set(watcher_object)\n    try:\n        if param_count == 2:\n            watch_result = cast(WatchCallbackBothValuesType, watch_function)(\n                old_value, value\n            )\n        elif param_count == 1:\n            watch_result = cast(WatchCallbackNewValueType, watch_function)(value)\n        else:\n            watch_result = cast(WatchCallbackNoArgsType, watch_function)()\n        if isawaitable(watch_result):\n            # Result is awaitable, so we need to await it within an async context\n            watcher_object.call_next(\n                partial(await_watcher, watcher_object, watch_result)\n            )\n    finally:\n        active_message_pump.reset(reset_token)\n\n\n@rich.repr.auto\nclass Reactive(Generic[ReactiveType]):\n    \"\"\"Reactive descriptor.\n\n    Args:\n        default: A default value or callable that returns a default.\n        layout: Perform a layout on change.\n        repaint: Perform a repaint on change.\n        init: Call watchers on initialize (post mount).\n        always_update: Call watchers even when the new value equals the old value.\n        compute: Run compute methods when attribute is changed.\n        recompose: Compose the widget again when the attribute changes.\n        bindings: Refresh bindings when the reactive changes.\n    \"\"\"\n\n    _reactives: ClassVar[dict[str, object]] = {}\n\n    def __init__(\n        self,\n        default: ReactiveType | Callable[[], ReactiveType],\n        *,\n        layout: bool = False,\n        repaint: bool = True,\n        init: bool = False,\n        always_update: bool = False,\n        compute: bool = True,\n        recompose: bool = False,\n        bindings: bool = False,\n    ) -> None:\n        self._default = default\n        self._layout = layout\n        self._repaint = repaint\n        self._init = init\n        self._always_update = always_update\n        self._run_compute = compute\n        self._recompose = recompose\n        self._bindings = bindings\n        self._owner: Type[MessageTarget] | None = None\n\n    def __rich_repr__(self) -> rich.repr.Result:\n        yield self._default\n        yield \"layout\", self._layout\n        yield \"repaint\", self._repaint\n        yield \"init\", self._init\n        yield \"always_update\", self._always_update\n        yield \"compute\", self._run_compute\n        yield \"recompose\", self._recompose\n\n    @property\n    def owner(self) -> Type[MessageTarget]:\n        \"\"\"The owner (class) where the reactive was declared.\"\"\"\n        assert self._owner is not None\n        return self._owner\n\n    def _initialize_reactive(self, obj: Reactable, name: str) -> None:\n        \"\"\"Initialized a reactive attribute on an object.\n\n        Args:\n            obj: An object with reactive attributes.\n            name: Name of attribute.\n        \"\"\"\n        _rich_traceback_omit = True\n        internal_name = f\"_reactive_{name}\"\n        if hasattr(obj, internal_name):\n            # Attribute already has a value\n            return\n\n        compute_method = getattr(obj, self.compute_name, None)\n        if compute_method is not None and self._init:\n            default = compute_method()\n        else:\n            default_or_callable = self._default\n            default = (\n                default_or_callable()\n                if callable(default_or_callable)\n                else default_or_callable\n            )\n        setattr(obj, internal_name, default)\n        if self._init:\n            self._check_watchers(obj, name, default)\n\n    @classmethod\n\n\n\n\n\n\n\n\n\n\n    @classmethod\n    def _reset_object(cls, obj: object) -> None:\n        \"\"\"Reset reactive structures on object (to avoid reference cycles).\n\n        Args:\n            obj: A reactive object.\n        \"\"\"\n        getattr(obj, \"__watchers\", {}).clear()\n        getattr(obj, \"__computes\", []).clear()\n\n    def __set_name__(self, owner: Type[MessageTarget], name: str) -> None:\n        # Check for compute method\n        self._owner = owner\n        public_compute = f\"compute_{name}\"\n        private_compute = f\"_compute_{name}\"\n        compute_name = (\n            private_compute if hasattr(owner, private_compute) else public_compute\n        )\n        if hasattr(owner, compute_name):\n            # Compute methods are stored in a list called `__computes`\n            try:\n                computes = getattr(owner, \"__computes\")\n            except AttributeError:\n                computes = []\n                setattr(owner, \"__computes\", computes)\n            computes.append(name)\n\n        # The name of the attribute\n        self.name = name\n        # The internal name where the attribute's value is stored\n        self.internal_name = f\"_reactive_{name}\"\n        self.compute_name = compute_name\n        default = self._default\n        setattr(owner, f\"_default_{name}\", default)\n\n    if TYPE_CHECKING:\n\n        @overload\n        def __get__(\n            self: Reactive[ReactiveType],\n            obj: ReactableType,\n            obj_type: type[ReactableType],\n        ) -> ReactiveType: ...\n\n        @overload\n        def __get__(\n            self: Reactive[ReactiveType], obj: None, obj_type: type[ReactableType]\n        ) -> Reactive[ReactiveType]: ...\n\n    def __get__(\n        self: Reactive[ReactiveType],\n        obj: Reactable | None,\n        obj_type: type[ReactableType],\n    ) -> Reactive[ReactiveType] | ReactiveType:\n        _rich_traceback_omit = True\n        if obj is None:\n            # obj is None means we are invoking the descriptor via the class, and not the instance\n            return self\n        if not hasattr(obj, \"id\"):\n            raise ReactiveError(\n                f\"Node is missing data; Check you are calling super().__init__(...) in the {obj.__class__.__name__}() constructor, before getting reactives.\"\n            )\n        internal_name = self.internal_name\n        if not hasattr(obj, internal_name):\n            self._initialize_reactive(obj, self.name)\n\n        if hasattr(obj, self.compute_name):\n            value: ReactiveType\n            old_value = getattr(obj, internal_name)\n            value = getattr(obj, self.compute_name)()\n            setattr(obj, internal_name, value)\n            self._check_watchers(obj, self.name, old_value)\n            return value\n        else:\n            return getattr(obj, internal_name)\n\n    def __set__(self, obj: Reactable, value: ReactiveType) -> None:\n        _rich_traceback_omit = True\n\n        if not hasattr(obj, \"_id\"):\n            raise ReactiveError(\n                f\"Node is missing data; Check you are calling super().__init__(...) in the {obj.__class__.__name__}() constructor, before setting reactives.\"\n            )\n\n        self._initialize_reactive(obj, self.name)\n\n        if hasattr(obj, self.compute_name):\n            raise AttributeError(\n                f\"Can't set {obj}.{self.name!r}; reactive attributes with a compute method are read-only\"\n            )\n\n        name = self.name\n        current_value = getattr(obj, name)\n        # Check for private and public validate functions.\n        private_validate_function = getattr(obj, f\"_validate_{name}\", None)\n        if callable(private_validate_function):\n            value = private_validate_function(value)\n        public_validate_function = getattr(obj, f\"validate_{name}\", None)\n        if callable(public_validate_function):\n            value = public_validate_function(value)\n        # If the value has changed, or this is the first time setting the value\n        if current_value != value or self._always_update:\n            # Store the internal value\n            setattr(obj, self.internal_name, value)\n\n            # Check all watchers\n            self._check_watchers(obj, name, current_value)\n\n            if self._run_compute:\n                self._compute(obj)\n\n            if self._bindings:\n                obj.refresh_bindings()\n\n            # Refresh according to descriptor flags\n            if self._layout or self._repaint or self._recompose:\n                obj.refresh(\n                    repaint=self._repaint,\n                    layout=self._layout,\n                    recompose=self._recompose,\n                )\n\n    @classmethod\n    def _check_watchers(cls, obj: Reactable, name: str, old_value: Any) -> None:\n        \"\"\"Check watchers, and call watch methods / computes\n\n        Args:\n            obj: The reactable object.\n            name: Attribute name.\n            old_value: The old (previous) value of the attribute.\n        \"\"\"\n        _rich_traceback_omit = True\n        # Get the current value.\n        internal_name = f\"_reactive_{name}\"\n        value = getattr(obj, internal_name)\n\n        private_watch_function = getattr(obj, f\"_watch_{name}\", None)\n        if callable(private_watch_function):\n            invoke_watcher(obj, private_watch_function, old_value, value)\n\n        public_watch_function = getattr(obj, f\"watch_{name}\", None)\n        if callable(public_watch_function):\n            invoke_watcher(obj, public_watch_function, old_value, value)\n\n        # Process \"global\" watchers\n        watchers: list[tuple[Reactable, WatchCallbackType]]\n        watchers = getattr(obj, \"__watchers\", {}).get(name, [])\n        # Remove any watchers for reactables that have since closed\n        if watchers:\n            watchers[:] = [\n                (reactable, callback)\n                for reactable, callback in watchers\n                if not reactable._closing\n            ]\n            for reactable, callback in watchers:\n                with reactable.prevent(*obj._prevent_message_types_stack[-1]):\n                    invoke_watcher(reactable, callback, old_value, value)\n\n    @classmethod\n    def _compute(cls, obj: Reactable) -> None:\n        \"\"\"Invoke all computes.\n\n        Args:\n            obj: Reactable object.\n        \"\"\"\n        _rich_traceback_guard = True\n        for compute in obj._reactives.keys() & obj._computes:\n            try:\n                compute_method = getattr(obj, f\"compute_{compute}\")\n            except AttributeError:\n                try:\n                    compute_method = getattr(obj, f\"_compute_{compute}\")\n                except AttributeError:\n                    continue\n            current_value = getattr(\n                obj, f\"_reactive_{compute}\", getattr(obj, f\"_default_{compute}\", None)\n            )\n            value = compute_method()\n            setattr(obj, f\"_reactive_{compute}\", value)\n            if value != current_value:\n                cls._check_watchers(obj, compute, current_value)\n\n\nclass reactive(Reactive[ReactiveType]):\n    \"\"\"Create a reactive attribute.\n\n    Args:\n        default: A default value or callable that returns a default.\n        layout: Perform a layout on change.\n        repaint: Perform a repaint on change.\n        init: Call watchers on initialize (post mount).\n        always_update: Call watchers even when the new value equals the old value.\n        bindings: Refresh bindings when the reactive changes.\n    \"\"\"\n\n    def __init__(\n        self,\n        default: ReactiveType | Callable[[], ReactiveType],\n        *,\n        layout: bool = False,\n        repaint: bool = True,\n        init: bool = True,\n        always_update: bool = False,\n        recompose: bool = False,\n        bindings: bool = False,\n    ) -> None:\n        super().__init__(\n            default,\n            layout=layout,\n            repaint=repaint,\n            init=init,\n            always_update=always_update,\n            recompose=recompose,\n            bindings=bindings,\n        )\n\n\nclass var(Reactive[ReactiveType]):\n    \"\"\"Create a reactive attribute (with no auto-refresh).\n\n    Args:\n        default: A default value or callable that returns a default.\n        init: Call watchers on initialize (post mount).\n        always_update: Call watchers even when the new value equals the old value.\n        bindings: Refresh bindings when the reactive changes.\n    \"\"\"\n\n    def __init__(\n        self,\n        default: ReactiveType | Callable[[], ReactiveType],\n        init: bool = True,\n        always_update: bool = False,\n        bindings: bool = False,\n    ) -> None:\n        super().__init__(\n            default,\n            layout=False,\n            repaint=False,\n            init=init,\n            always_update=always_update,\n            bindings=bindings,\n        )\n\n\ndef _watch(\n    node: DOMNode,\n    obj: Reactable,\n    attribute_name: str,\n    callback: WatchCallbackType,\n    *,\n    init: bool = True,\n) -> None:\n    \"\"\"Watch a reactive variable on an object.\n\n    Args:\n        node: The node that created the watcher.\n        obj: The parent object.\n        attribute_name: The attribute to watch.\n        callback: A callable to call when the attribute changes.\n        init: True to call watcher initialization.\n    \"\"\"\n    if not hasattr(obj, \"__watchers\"):\n        setattr(obj, \"__watchers\", {})\n    watchers: dict[str, list[tuple[Reactable, WatchCallbackType]]] = getattr(\n        obj, \"__watchers\"\n    )\n    watcher_list = watchers.setdefault(attribute_name, [])\n    if any(callback == callback_from_list for _, callback_from_list in watcher_list):\n        return\n    if init:\n        current_value = getattr(obj, attribute_name, None)\n        invoke_watcher(obj, callback, current_value, current_value)\n    watcher_list.append((node, callback))\n\n```\n# Function Signature: def _initialize_object(cls, obj: Reactable) -> None:\n# Natural Language Annotations: \"\"\"Set defaults and call any watchers / computes for the first time.\n\n        Args:\n            obj: An object with Reactive descriptors\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _initialize_object(cls, obj):\n    \"\"\"", "  ```\ndef _initialize_object(cls, obj: Reactable)", "  ```\ndef _initialize_object(cls, obj: Reactable)"]}, "textual-src/textual/message_pump.py-call_after_refresh": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/message_pump.py:\n```\n\"\"\"\n\nA `MessagePump` is a base class for any object which processes messages, which includes Widget, Screen, and App.\n\n!!! tip\n\n    Most of the method here are useful in general app development.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport inspect\nimport threading\nfrom asyncio import CancelledError, Queue, QueueEmpty, Task, create_task\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Generator,\n    Iterable,\n    Type,\n    TypeVar,\n    cast,\n)\nfrom weakref import WeakSet\n\nfrom . import Logger, events, log, messages\nfrom ._callback import invoke\nfrom ._context import NoActiveAppError, active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._context import prevent_message_types_stack\nfrom ._on import OnNoWidget\nfrom ._time import time\nfrom .case import camel_to_snake\nfrom .css.match import match\nfrom .errors import DuplicateKeyHandlers\nfrom .events import Event\nfrom .message import Message\nfrom .reactive import Reactive, TooManyComputesError\nfrom .signal import Signal\nfrom .timer import Timer, TimerCallback\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n    from .app import App\n    from .css.model import SelectorSet\n\n\nCallback: TypeAlias = \"Callable[..., Any] | Callable[..., Awaitable[Any]]\"\n\n\nclass CallbackError(Exception):\n    pass\n\n\nclass MessagePumpClosed(Exception):\n    pass\n\n\n_MessagePumpMetaSub = TypeVar(\"_MessagePumpMetaSub\", bound=\"_MessagePumpMeta\")\n\n\nclass _MessagePumpMeta(type):\n    \"\"\"Metaclass for message pump. This exists to populate a Message inner class of a Widget with the\n    parent classes' name.\n    \"\"\"\n\n    def __new__(\n        cls: Type[_MessagePumpMetaSub],\n        name: str,\n        bases: tuple[type, ...],\n        class_dict: dict[str, Any],\n        **kwargs: Any,\n    ) -> _MessagePumpMetaSub:\n        namespace = camel_to_snake(name)\n        isclass = inspect.isclass\n        handlers: dict[\n            type[Message], list[tuple[Callable, dict[str, tuple[SelectorSet, ...]]]]\n        ] = class_dict.get(\"_decorated_handlers\", {})\n\n        class_dict[\"_decorated_handlers\"] = handlers\n\n        for value in class_dict.values():\n            if callable(value) and hasattr(value, \"_textual_on\"):\n                textual_on: list[\n                    tuple[type[Message], dict[str, tuple[SelectorSet, ...]]]\n                ] = getattr(value, \"_textual_on\")\n                for message_type, selectors in textual_on:\n                    handlers.setdefault(message_type, []).append((value, selectors))\n            if isclass(value) and issubclass(value, Message):\n                if \"namespace\" in value.__dict__:\n                    value.handler_name = f\"on_{value.__dict__['namespace']}_{camel_to_snake(value.__name__)}\"\n                else:\n                    value.handler_name = (\n                        f\"on_{namespace}_{camel_to_snake(value.__name__)}\"\n                    )\n\n        # Look for reactives with public AND private compute methods.\n        prefix = \"compute_\"\n        prefix_len = len(prefix)\n        for attr_name, value in class_dict.items():\n            if attr_name.startswith(prefix) and callable(value):\n                reactive_name = attr_name[prefix_len:]\n                if (\n                    reactive_name in class_dict\n                    and isinstance(class_dict[reactive_name], Reactive)\n                    and f\"_{attr_name}\" in class_dict\n                ):\n                    raise TooManyComputesError(\n                        f\"reactive {reactive_name!r} can't have two computes.\"\n                    )\n\n        class_obj = super().__new__(cls, name, bases, class_dict, **kwargs)\n        return class_obj\n\n\nclass MessagePump(metaclass=_MessagePumpMeta):\n    \"\"\"Base class which supplies a message pump.\"\"\"\n\n    def __init__(self, parent: MessagePump | None = None) -> None:\n        self._message_queue: Queue[Message | None] = Queue()\n        self._parent = parent\n        self._running: bool = False\n        self._closing: bool = False\n        self._closed: bool = False\n        self._disabled_messages: set[type[Message]] = set()\n        self._pending_message: Message | None = None\n        self._task: Task | None = None\n        self._timers: WeakSet[Timer] = WeakSet()\n        self._last_idle: float = time()\n        self._max_idle: float | None = None\n        self._mounted_event = asyncio.Event()\n        self._is_mounted = False\n        \"\"\"Having this explicit Boolean is an optimization.\n\n        The same information could be retrieved from `self._mounted_event.is_set()`, but\n        we need to access this frequently in the compositor and the attribute with the\n        explicit Boolean value is faster than the two lookups and the function call.\n        \"\"\"\n        self._next_callbacks: list[events.Callback] = []\n        self._thread_id: int = threading.get_ident()\n        self._prevented_messages_on_mount = self._prevent_message_types_stack[-1]\n        self.message_signal: Signal[Message] = Signal(self, \"messages\")\n        \"\"\"Subscribe to this signal to be notified of all messages sent to this widget.\n        \n        This is a fairly low-level mechanism, and shouldn't replace regular message handling.\n        \n        \"\"\"\n\n    @property\n    def _prevent_message_types_stack(self) -> list[set[type[Message]]]:\n        \"\"\"The stack that manages prevented messages.\"\"\"\n        try:\n            stack = prevent_message_types_stack.get()\n        except LookupError:\n            stack = [set()]\n            prevent_message_types_stack.set(stack)\n        return stack\n\n    def _get_prevented_messages(self) -> set[type[Message]]:\n        \"\"\"A set of all the prevented message types.\"\"\"\n        return self._prevent_message_types_stack[-1]\n\n    def _is_prevented(self, message_type: type[Message]) -> bool:\n        \"\"\"Check if a message type has been prevented via the\n        [prevent][textual.message_pump.MessagePump.prevent] context manager.\n\n        Args:\n            message_type: A message type.\n\n        Returns:\n            `True` if the message has been prevented from sending, or `False` if it will be sent as normal.\n        \"\"\"\n        return message_type in self._prevent_message_types_stack[-1]\n\n    @contextmanager\n    def prevent(self, *message_types: type[Message]) -> Generator[None, None, None]:\n        \"\"\"A context manager to *temporarily* prevent the given message types from being posted.\n\n        Example:\n            ```python\n            input = self.query_one(Input)\n            with self.prevent(Input.Changed):\n                input.value = \"foo\"\n            ```\n        \"\"\"\n        if message_types:\n            prevent_stack = self._prevent_message_types_stack\n            prevent_stack.append(prevent_stack[-1].union(message_types))\n            try:\n                yield\n            finally:\n                prevent_stack.pop()\n        else:\n            yield\n\n    @property\n    def task(self) -> Task:\n        assert self._task is not None\n        return self._task\n\n    @property\n    def has_parent(self) -> bool:\n        \"\"\"Does this object have a parent?\"\"\"\n        return self._parent is not None\n\n    @property\n    def message_queue_size(self) -> int:\n        \"\"\"The current size of the message queue.\"\"\"\n        return self._message_queue.qsize()\n\n    @property\n    def is_dom_root(self):\n        \"\"\"Is this a root node (i.e. the App)?\"\"\"\n        return False\n\n    @property\n    def app(self) -> \"App[object]\":\n        \"\"\"\n        Get the current app.\n\n        Returns:\n            The current app.\n\n        Raises:\n            NoActiveAppError: if no active app could be found for the current asyncio context\n        \"\"\"\n        try:\n            return active_app.get()\n        except LookupError:\n            from .app import App\n\n            node: MessagePump | None = self\n            while not isinstance(node, App):\n                if node is None:\n                    raise NoActiveAppError()\n                node = node._parent\n            active_app.set(node)\n            return node\n\n    @property\n    def is_attached(self) -> bool:\n        \"\"\"Is this node linked to the app through the DOM?\"\"\"\n        if self.app._exit:\n            return False\n        node: MessagePump | None = self\n        while (node := node._parent) is not None:\n            if node.is_dom_root:\n                return True\n        return False\n\n    @property\n    def is_parent_active(self) -> bool:\n        \"\"\"Is the parent active?\"\"\"\n        return bool(\n            self._parent and not self._parent._closed and not self._parent._closing\n        )\n\n    @property\n    def is_running(self) -> bool:\n        \"\"\"Is the message pump running (potentially processing messages)?\"\"\"\n        return self._running\n\n    @property\n    def log(self) -> Logger:\n        \"\"\"Get a logger for this object.\n\n        Returns:\n            A logger.\n        \"\"\"\n        return self.app._logger\n\n    def _attach(self, parent: MessagePump) -> None:\n        \"\"\"Set the parent, and therefore attach this node to the tree.\n\n        Args:\n            parent: Parent node.\n        \"\"\"\n        self._parent = parent\n\n    def _detach(self) -> None:\n        \"\"\"Set the parent to None to remove the node from the tree.\"\"\"\n        self._parent = None\n\n    def check_message_enabled(self, message: Message) -> bool:\n        \"\"\"Check if a given message is enabled (allowed to be sent).\n\n        Args:\n            message: A message object.\n\n        Returns:\n            `True` if the message will be sent, or `False` if it is disabled.\n        \"\"\"\n        return type(message) not in self._disabled_messages\n\n    def disable_messages(self, *messages: type[Message]) -> None:\n        \"\"\"Disable message types from being processed.\"\"\"\n        self._disabled_messages.update(messages)\n\n    def enable_messages(self, *messages: type[Message]) -> None:\n        \"\"\"Enable processing of messages types.\"\"\"\n        self._disabled_messages.difference_update(messages)\n\n    async def _get_message(self) -> Message:\n        \"\"\"Get the next event on the queue, or None if queue is closed.\n\n        Returns:\n            Event object or None.\n        \"\"\"\n        if self._closed:\n            raise MessagePumpClosed(\"The message pump is closed\")\n        if self._pending_message is not None:\n            try:\n                return self._pending_message\n            finally:\n                self._pending_message = None\n\n        message = await self._message_queue.get()\n\n        if message is None:\n            self._closed = True\n            raise MessagePumpClosed(\"The message pump is now closed\")\n        return message\n\n    def _peek_message(self) -> Message | None:\n        \"\"\"Peek the message at the head of the queue (does not remove it from the queue),\n        or return None if the queue is empty.\n\n        Returns:\n            The message or None.\n        \"\"\"\n        if self._pending_message is None:\n            try:\n                message = self._message_queue.get_nowait()\n            except QueueEmpty:\n                pass\n            else:\n                if message is None:\n                    self._closed = True\n                    raise MessagePumpClosed(\"The message pump is now closed\")\n                self._pending_message = message\n\n        if self._pending_message is not None:\n            return self._pending_message\n        return None\n\n    def set_timer(\n        self,\n        delay: float,\n        callback: TimerCallback | None = None,\n        *,\n        name: str | None = None,\n        pause: bool = False,\n    ) -> Timer:\n        \"\"\"Make a function call after a delay.\n\n        Args:\n            delay: Time (in seconds) to wait before invoking callback.\n            callback: Callback to call after time has expired.\n            name: Name of the timer (for debug).\n            pause: Start timer paused.\n\n        Returns:\n            A timer object.\n        \"\"\"\n        timer = Timer(\n            self,\n            delay,\n            name=name or f\"set_timer#{Timer._timer_count}\",\n            callback=callback,\n            repeat=0,\n            pause=pause,\n        )\n        timer._start()\n        self._timers.add(timer)\n        return timer\n\n    def set_interval(\n        self,\n        interval: float,\n        callback: TimerCallback | None = None,\n        *,\n        name: str | None = None,\n        repeat: int = 0,\n        pause: bool = False,\n    ) -> Timer:\n        \"\"\"Call a function at periodic intervals.\n\n        Args:\n            interval: Time (in seconds) between calls.\n            callback: Function to call.\n            name: Name of the timer object.\n            repeat: Number of times to repeat the call or 0 for continuous.\n            pause: Start the timer paused.\n\n        Returns:\n            A timer object.\n        \"\"\"\n        timer = Timer(\n            self,\n            interval,\n            name=name or f\"set_interval#{Timer._timer_count}\",\n            callback=callback,\n            repeat=repeat or None,\n            pause=pause,\n        )\n        timer._start()\n        self._timers.add(timer)\n        return timer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def call_next(self, callback: Callback, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Schedule a callback to run immediately after processing the current message.\n\n        Args:\n            callback: Callable to run after current event.\n            *args: Positional arguments to pass to the callable.\n            **kwargs: Keyword arguments to pass to the callable.\n        \"\"\"\n        callback_message = events.Callback(callback=partial(callback, *args, **kwargs))\n        callback_message._prevent.update(self._get_prevented_messages())\n        self._next_callbacks.append(callback_message)\n        self.check_idle()\n\n    def _on_invoke_later(self, message: messages.InvokeLater) -> None:\n        # Forward InvokeLater message to the Screen\n        self.app.screen._invoke_later(\n            message.callback, message._sender or active_message_pump.get()\n        )\n\n    def _close_messages_no_wait(self) -> None:\n        \"\"\"Request the message queue to immediately exit.\"\"\"\n        self._message_queue.put_nowait(messages.CloseMessages())\n\n    async def _on_close_messages(self, message: messages.CloseMessages) -> None:\n        await self._close_messages()\n\n    async def _close_messages(self, wait: bool = True) -> None:\n        \"\"\"Close message queue, and optionally wait for queue to finish processing.\"\"\"\n        if self._closed or self._closing:\n            return\n        self._closing = True\n        if self._timers:\n            await Timer._stop_all(self._timers)\n            self._timers.clear()\n        self._message_queue.put_nowait(events.Unmount())\n        Reactive._reset_object(self)\n        self._message_queue.put_nowait(None)\n        if wait and self._task is not None and asyncio.current_task() != self._task:\n            try:\n                running_widget = active_message_pump.get()\n            except LookupError:\n                running_widget = None\n\n            if running_widget is None or running_widget is not self:\n                try:\n                    await self._task\n                except CancelledError:\n                    pass\n\n    def _start_messages(self) -> None:\n        \"\"\"Start messages task.\"\"\"\n        if self.app._running:\n            self._task = create_task(\n                self._process_messages(), name=f\"message pump {self}\"\n            )\n        else:\n            self._closing = True\n            self._closed = True\n\n    async def _process_messages(self) -> None:\n        self._running = True\n        active_message_pump.set(self)\n\n        if not await self._pre_process():\n            self._running = False\n            return\n\n        try:\n            await self._process_messages_loop()\n        except CancelledError:\n            pass\n        finally:\n            self._running = False\n            if self._timers:\n                await Timer._stop_all(self._timers)\n                self._timers.clear()\n\n    async def _pre_process(self) -> bool:\n        \"\"\"Procedure to run before processing messages.\n\n        Returns:\n            `True` if successful, or `False` if any exception occurred.\n\n        \"\"\"\n        # Dispatch compose and mount messages without going through loop\n        # These events must occur in this order, and at the start.\n\n        try:\n            await self._dispatch_message(events.Compose())\n            if self._prevented_messages_on_mount:\n                with self.prevent(*self._prevented_messages_on_mount):\n                    await self._dispatch_message(events.Mount())\n            else:\n                await self._dispatch_message(events.Mount())\n            self.check_idle()\n            self._post_mount()\n        except Exception as error:\n            self.app._handle_exception(error)\n            return False\n        finally:\n            # This is critical, mount may be waiting\n            self._mounted_event.set()\n            self._is_mounted = True\n        return True\n\n    def _post_mount(self):\n        \"\"\"Called after the object has been mounted.\"\"\"\n\n    async def _process_messages_loop(self) -> None:\n        \"\"\"Process messages until the queue is closed.\"\"\"\n        _rich_traceback_guard = True\n        self._thread_id = threading.get_ident()\n\n        while not self._closed:\n            try:\n                message = await self._get_message()\n            except MessagePumpClosed:\n                break\n            except CancelledError:\n                raise\n            except Exception as error:\n                raise error from None\n\n            # Combine any pending messages that may supersede this one\n            while not (self._closed or self._closing):\n                try:\n                    pending = self._peek_message()\n                except MessagePumpClosed:\n                    break\n                if pending is None or not message.can_replace(pending):\n                    break\n                try:\n                    message = await self._get_message()\n                except MessagePumpClosed:\n                    break\n\n            try:\n                await self._dispatch_message(message)\n            except CancelledError:\n                raise\n            except Exception as error:\n                self._mounted_event.set()\n                self._is_mounted = True\n                self.app._handle_exception(error)\n                break\n            finally:\n                self.message_signal.publish(message)\n                self._message_queue.task_done()\n\n                current_time = time()\n\n                # Insert idle events\n                if self._message_queue.empty() or (\n                    self._max_idle is not None\n                    and current_time - self._last_idle > self._max_idle\n                ):\n                    self._last_idle = current_time\n                    if not self._closed:\n                        event = events.Idle()\n                        for _cls, method in self._get_dispatch_methods(\n                            \"on_idle\", event\n                        ):\n                            try:\n                                await invoke(method, event)\n                            except Exception as error:\n                                self.app._handle_exception(error)\n                                break\n                    await self._flush_next_callbacks()\n\n    async def _flush_next_callbacks(self) -> None:\n        \"\"\"Invoke pending callbacks in next callbacks queue.\"\"\"\n        callbacks = self._next_callbacks.copy()\n        self._next_callbacks.clear()\n        for callback in callbacks:\n            try:\n                with self.prevent(*callback._prevent):\n                    await invoke(callback.callback)\n            except Exception as error:\n                self.app._handle_exception(error)\n                break\n\n    async def _dispatch_message(self, message: Message) -> None:\n        \"\"\"Dispatch a message received from the message queue.\n\n        Args:\n            message: A message object\n        \"\"\"\n        _rich_traceback_guard = True\n        if message.no_dispatch:\n            return\n\n        try:\n            message_hook = message_hook_context_var.get()\n        except LookupError:\n            pass\n        else:\n            message_hook(message)\n\n        with self.prevent(*message._prevent):\n            # Allow apps to treat events and messages separately\n            if isinstance(message, Event):\n                await self.on_event(message)\n            else:\n                await self._on_message(message)\n            if self._next_callbacks:\n                await self._flush_next_callbacks()\n\n    def _get_dispatch_methods(\n        self, method_name: str, message: Message\n    ) -> Iterable[tuple[type, Callable[[Message], Awaitable]]]:\n        \"\"\"Gets handlers from the MRO\n\n        Args:\n            method_name: Handler method name.\n            message: Message object.\n        \"\"\"\n        from .widget import Widget\n\n        methods_dispatched: set[Callable] = set()\n        message_mro = [\n            _type for _type in message.__class__.__mro__ if issubclass(_type, Message)\n        ]\n        for cls in self.__class__.__mro__:\n            if message._no_default_action:\n                break\n            # Try decorated handlers first\n            decorated_handlers = cast(\n                \"dict[type[Message], list[tuple[Callable, dict[str, tuple[SelectorSet, ...]]]]] | None\",\n                cls.__dict__.get(\"_decorated_handlers\"),\n            )\n\n            if decorated_handlers:\n                for message_class in message_mro:\n                    handlers = decorated_handlers.get(message_class, [])\n\n                    for method, selectors in handlers:\n                        if method in methods_dispatched:\n                            continue\n                        if not selectors:\n                            yield cls, method.__get__(self, cls)\n                            methods_dispatched.add(method)\n                        else:\n                            if not message._sender:\n                                continue\n                            for attribute, selector in selectors.items():\n                                node = getattr(message, attribute)\n                                if not isinstance(node, Widget):\n                                    raise OnNoWidget(\n                                        f\"on decorator can't match against {attribute!r} as it is not a widget.\"\n                                    )\n                                if not match(selector, node):\n                                    break\n                            else:\n                                yield cls, method.__get__(self, cls)\n                                methods_dispatched.add(method)\n\n            # Fall back to the naming convention\n            # But avoid calling the handler if it was decorated\n            method = cls.__dict__.get(f\"_{method_name}\") or cls.__dict__.get(\n                method_name\n            )\n            if method is not None and not getattr(method, \"_textual_on\", None):\n                yield cls, method.__get__(self, cls)\n\n    async def on_event(self, event: events.Event) -> None:\n        \"\"\"Called to process an event.\n\n        Args:\n            event: An Event object.\n        \"\"\"\n        await self._on_message(event)\n\n    async def _on_message(self, message: Message) -> None:\n        \"\"\"Called to process a message.\n\n        Args:\n            message: A Message object.\n        \"\"\"\n        _rich_traceback_guard = True\n        handler_name = message.handler_name\n\n        # Look through the MRO to find a handler\n        dispatched = False\n        for cls, method in self._get_dispatch_methods(handler_name, message):\n            log.event.verbosity(message.verbose)(\n                message,\n                \">>>\",\n                self,\n                f\"method=<{cls.__name__}.{handler_name}>\",\n            )\n            dispatched = True\n            await invoke(method, message)\n        if not dispatched:\n            log.event.verbosity(message.verbose)(message, \">>>\", self, \"method=None\")\n\n        # Bubble messages up the DOM (if enabled on the message)\n        if message.bubble and self._parent and not message._stop_propagation:\n            if message._sender is not None and message._sender == self._parent:\n                # parent is sender, so we stop propagation after parent\n                message.stop()\n            if self.is_parent_active and self.is_attached:\n                message._bubble_to(self._parent)\n\n    def check_idle(self) -> None:\n        \"\"\"Prompt the message pump to call idle if the queue is empty.\"\"\"\n        if self._running and self._message_queue.empty():\n            self.post_message(messages.Prompt())\n\n    async def _post_message(self, message: Message) -> bool:\n        \"\"\"Post a message or an event to this message pump.\n\n        This is an internal method for use where a coroutine is required.\n\n        Args:\n            message: A message object.\n\n        Returns:\n            True if the messages was\n```\n# Function Signature: def call_after_refresh(self, callback: Callback, *args: Any, **kwargs: Any) -> bool:\n# Natural Language Annotations: \"\"\"Schedule a callback to run after all messages are processed and the screen\n        has been refreshed. Positional and keyword arguments are passed to the callable.\n\n        Args:\n            callback: A callable.\n\n        Returns:\n            `True` if the callback was scheduled, or `False` if the callback could not be\n                scheduled (may occur if the message pump was closed or closing).\n\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef call_after_refresh(self, callback: Callback", "  ```\ndef call_after_refresh(self, callback: Callback", "  [CODE]\ndef call_after_refresh(self, callback:"]}, "textual-src/textual/message_pump.py-call_later": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/message_pump.py:\n```\n\"\"\"\n\nA `MessagePump` is a base class for any object which processes messages, which includes Widget, Screen, and App.\n\n!!! tip\n\n    Most of the method here are useful in general app development.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport inspect\nimport threading\nfrom asyncio import CancelledError, Queue, QueueEmpty, Task, create_task\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Generator,\n    Iterable,\n    Type,\n    TypeVar,\n    cast,\n)\nfrom weakref import WeakSet\n\nfrom . import Logger, events, log, messages\nfrom ._callback import invoke\nfrom ._context import NoActiveAppError, active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._context import prevent_message_types_stack\nfrom ._on import OnNoWidget\nfrom ._time import time\nfrom .case import camel_to_snake\nfrom .css.match import match\nfrom .errors import DuplicateKeyHandlers\nfrom .events import Event\nfrom .message import Message\nfrom .reactive import Reactive, TooManyComputesError\nfrom .signal import Signal\nfrom .timer import Timer, TimerCallback\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n    from .app import App\n    from .css.model import SelectorSet\n\n\nCallback: TypeAlias = \"Callable[..., Any] | Callable[..., Awaitable[Any]]\"\n\n\nclass CallbackError(Exception):\n    pass\n\n\nclass MessagePumpClosed(Exception):\n    pass\n\n\n_MessagePumpMetaSub = TypeVar(\"_MessagePumpMetaSub\", bound=\"_MessagePumpMeta\")\n\n\nclass _MessagePumpMeta(type):\n    \"\"\"Metaclass for message pump. This exists to populate a Message inner class of a Widget with the\n    parent classes' name.\n    \"\"\"\n\n    def __new__(\n        cls: Type[_MessagePumpMetaSub],\n        name: str,\n        bases: tuple[type, ...],\n        class_dict: dict[str, Any],\n        **kwargs: Any,\n    ) -> _MessagePumpMetaSub:\n        namespace = camel_to_snake(name)\n        isclass = inspect.isclass\n        handlers: dict[\n            type[Message], list[tuple[Callable, dict[str, tuple[SelectorSet, ...]]]]\n        ] = class_dict.get(\"_decorated_handlers\", {})\n\n        class_dict[\"_decorated_handlers\"] = handlers\n\n        for value in class_dict.values():\n            if callable(value) and hasattr(value, \"_textual_on\"):\n                textual_on: list[\n                    tuple[type[Message], dict[str, tuple[SelectorSet, ...]]]\n                ] = getattr(value, \"_textual_on\")\n                for message_type, selectors in textual_on:\n                    handlers.setdefault(message_type, []).append((value, selectors))\n            if isclass(value) and issubclass(value, Message):\n                if \"namespace\" in value.__dict__:\n                    value.handler_name = f\"on_{value.__dict__['namespace']}_{camel_to_snake(value.__name__)}\"\n                else:\n                    value.handler_name = (\n                        f\"on_{namespace}_{camel_to_snake(value.__name__)}\"\n                    )\n\n        # Look for reactives with public AND private compute methods.\n        prefix = \"compute_\"\n        prefix_len = len(prefix)\n        for attr_name, value in class_dict.items():\n            if attr_name.startswith(prefix) and callable(value):\n                reactive_name = attr_name[prefix_len:]\n                if (\n                    reactive_name in class_dict\n                    and isinstance(class_dict[reactive_name], Reactive)\n                    and f\"_{attr_name}\" in class_dict\n                ):\n                    raise TooManyComputesError(\n                        f\"reactive {reactive_name!r} can't have two computes.\"\n                    )\n\n        class_obj = super().__new__(cls, name, bases, class_dict, **kwargs)\n        return class_obj\n\n\nclass MessagePump(metaclass=_MessagePumpMeta):\n    \"\"\"Base class which supplies a message pump.\"\"\"\n\n    def __init__(self, parent: MessagePump | None = None) -> None:\n        self._message_queue: Queue[Message | None] = Queue()\n        self._parent = parent\n        self._running: bool = False\n        self._closing: bool = False\n        self._closed: bool = False\n        self._disabled_messages: set[type[Message]] = set()\n        self._pending_message: Message | None = None\n        self._task: Task | None = None\n        self._timers: WeakSet[Timer] = WeakSet()\n        self._last_idle: float = time()\n        self._max_idle: float | None = None\n        self._mounted_event = asyncio.Event()\n        self._is_mounted = False\n        \"\"\"Having this explicit Boolean is an optimization.\n\n        The same information could be retrieved from `self._mounted_event.is_set()`, but\n        we need to access this frequently in the compositor and the attribute with the\n        explicit Boolean value is faster than the two lookups and the function call.\n        \"\"\"\n        self._next_callbacks: list[events.Callback] = []\n        self._thread_id: int = threading.get_ident()\n        self._prevented_messages_on_mount = self._prevent_message_types_stack[-1]\n        self.message_signal: Signal[Message] = Signal(self, \"messages\")\n        \"\"\"Subscribe to this signal to be notified of all messages sent to this widget.\n        \n        This is a fairly low-level mechanism, and shouldn't replace regular message handling.\n        \n        \"\"\"\n\n    @property\n    def _prevent_message_types_stack(self) -> list[set[type[Message]]]:\n        \"\"\"The stack that manages prevented messages.\"\"\"\n        try:\n            stack = prevent_message_types_stack.get()\n        except LookupError:\n            stack = [set()]\n            prevent_message_types_stack.set(stack)\n        return stack\n\n    def _get_prevented_messages(self) -> set[type[Message]]:\n        \"\"\"A set of all the prevented message types.\"\"\"\n        return self._prevent_message_types_stack[-1]\n\n    def _is_prevented(self, message_type: type[Message]) -> bool:\n        \"\"\"Check if a message type has been prevented via the\n        [prevent][textual.message_pump.MessagePump.prevent] context manager.\n\n        Args:\n            message_type: A message type.\n\n        Returns:\n            `True` if the message has been prevented from sending, or `False` if it will be sent as normal.\n        \"\"\"\n        return message_type in self._prevent_message_types_stack[-1]\n\n    @contextmanager\n    def prevent(self, *message_types: type[Message]) -> Generator[None, None, None]:\n        \"\"\"A context manager to *temporarily* prevent the given message types from being posted.\n\n        Example:\n            ```python\n            input = self.query_one(Input)\n            with self.prevent(Input.Changed):\n                input.value = \"foo\"\n            ```\n        \"\"\"\n        if message_types:\n            prevent_stack = self._prevent_message_types_stack\n            prevent_stack.append(prevent_stack[-1].union(message_types))\n            try:\n                yield\n            finally:\n                prevent_stack.pop()\n        else:\n            yield\n\n    @property\n    def task(self) -> Task:\n        assert self._task is not None\n        return self._task\n\n    @property\n    def has_parent(self) -> bool:\n        \"\"\"Does this object have a parent?\"\"\"\n        return self._parent is not None\n\n    @property\n    def message_queue_size(self) -> int:\n        \"\"\"The current size of the message queue.\"\"\"\n        return self._message_queue.qsize()\n\n    @property\n    def is_dom_root(self):\n        \"\"\"Is this a root node (i.e. the App)?\"\"\"\n        return False\n\n    @property\n    def app(self) -> \"App[object]\":\n        \"\"\"\n        Get the current app.\n\n        Returns:\n            The current app.\n\n        Raises:\n            NoActiveAppError: if no active app could be found for the current asyncio context\n        \"\"\"\n        try:\n            return active_app.get()\n        except LookupError:\n            from .app import App\n\n            node: MessagePump | None = self\n            while not isinstance(node, App):\n                if node is None:\n                    raise NoActiveAppError()\n                node = node._parent\n            active_app.set(node)\n            return node\n\n    @property\n    def is_attached(self) -> bool:\n        \"\"\"Is this node linked to the app through the DOM?\"\"\"\n        if self.app._exit:\n            return False\n        node: MessagePump | None = self\n        while (node := node._parent) is not None:\n            if node.is_dom_root:\n                return True\n        return False\n\n    @property\n    def is_parent_active(self) -> bool:\n        \"\"\"Is the parent active?\"\"\"\n        return bool(\n            self._parent and not self._parent._closed and not self._parent._closing\n        )\n\n    @property\n    def is_running(self) -> bool:\n        \"\"\"Is the message pump running (potentially processing messages)?\"\"\"\n        return self._running\n\n    @property\n    def log(self) -> Logger:\n        \"\"\"Get a logger for this object.\n\n        Returns:\n            A logger.\n        \"\"\"\n        return self.app._logger\n\n    def _attach(self, parent: MessagePump) -> None:\n        \"\"\"Set the parent, and therefore attach this node to the tree.\n\n        Args:\n            parent: Parent node.\n        \"\"\"\n        self._parent = parent\n\n    def _detach(self) -> None:\n        \"\"\"Set the parent to None to remove the node from the tree.\"\"\"\n        self._parent = None\n\n    def check_message_enabled(self, message: Message) -> bool:\n        \"\"\"Check if a given message is enabled (allowed to be sent).\n\n        Args:\n            message: A message object.\n\n        Returns:\n            `True` if the message will be sent, or `False` if it is disabled.\n        \"\"\"\n        return type(message) not in self._disabled_messages\n\n    def disable_messages(self, *messages: type[Message]) -> None:\n        \"\"\"Disable message types from being processed.\"\"\"\n        self._disabled_messages.update(messages)\n\n    def enable_messages(self, *messages: type[Message]) -> None:\n        \"\"\"Enable processing of messages types.\"\"\"\n        self._disabled_messages.difference_update(messages)\n\n    async def _get_message(self) -> Message:\n        \"\"\"Get the next event on the queue, or None if queue is closed.\n\n        Returns:\n            Event object or None.\n        \"\"\"\n        if self._closed:\n            raise MessagePumpClosed(\"The message pump is closed\")\n        if self._pending_message is not None:\n            try:\n                return self._pending_message\n            finally:\n                self._pending_message = None\n\n        message = await self._message_queue.get()\n\n        if message is None:\n            self._closed = True\n            raise MessagePumpClosed(\"The message pump is now closed\")\n        return message\n\n    def _peek_message(self) -> Message | None:\n        \"\"\"Peek the message at the head of the queue (does not remove it from the queue),\n        or return None if the queue is empty.\n\n        Returns:\n            The message or None.\n        \"\"\"\n        if self._pending_message is None:\n            try:\n                message = self._message_queue.get_nowait()\n            except QueueEmpty:\n                pass\n            else:\n                if message is None:\n                    self._closed = True\n                    raise MessagePumpClosed(\"The message pump is now closed\")\n                self._pending_message = message\n\n        if self._pending_message is not None:\n            return self._pending_message\n        return None\n\n    def set_timer(\n        self,\n        delay: float,\n        callback: TimerCallback | None = None,\n        *,\n        name: str | None = None,\n        pause: bool = False,\n    ) -> Timer:\n        \"\"\"Make a function call after a delay.\n\n        Args:\n            delay: Time (in seconds) to wait before invoking callback.\n            callback: Callback to call after time has expired.\n            name: Name of the timer (for debug).\n            pause: Start timer paused.\n\n        Returns:\n            A timer object.\n        \"\"\"\n        timer = Timer(\n            self,\n            delay,\n            name=name or f\"set_timer#{Timer._timer_count}\",\n            callback=callback,\n            repeat=0,\n            pause=pause,\n        )\n        timer._start()\n        self._timers.add(timer)\n        return timer\n\n    def set_interval(\n        self,\n        interval: float,\n        callback: TimerCallback | None = None,\n        *,\n        name: str | None = None,\n        repeat: int = 0,\n        pause: bool = False,\n    ) -> Timer:\n        \"\"\"Call a function at periodic intervals.\n\n        Args:\n            interval: Time (in seconds) between calls.\n            callback: Function to call.\n            name: Name of the timer object.\n            repeat: Number of times to repeat the call or 0 for continuous.\n            pause: Start the timer paused.\n\n        Returns:\n            A timer object.\n        \"\"\"\n        timer = Timer(\n            self,\n            interval,\n            name=name or f\"set_interval#{Timer._timer_count}\",\n            callback=callback,\n            repeat=repeat or None,\n            pause=pause,\n        )\n        timer._start()\n        self._timers.add(timer)\n        return timer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def call_next(self, callback: Callback, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Schedule a callback to run immediately after processing the current message.\n\n        Args:\n            callback: Callable to run after current event.\n            *args: Positional arguments to pass to the callable.\n            **kwargs: Keyword arguments to pass to the callable.\n        \"\"\"\n        callback_message = events.Callback(callback=partial(callback, *args, **kwargs))\n        callback_message._prevent.update(self._get_prevented_messages())\n        self._next_callbacks.append(callback_message)\n        self.check_idle()\n\n    def _on_invoke_later(self, message: messages.InvokeLater) -> None:\n        # Forward InvokeLater message to the Screen\n        self.app.screen._invoke_later(\n            message.callback, message._sender or active_message_pump.get()\n        )\n\n    def _close_messages_no_wait(self) -> None:\n        \"\"\"Request the message queue to immediately exit.\"\"\"\n        self._message_queue.put_nowait(messages.CloseMessages())\n\n    async def _on_close_messages(self, message: messages.CloseMessages) -> None:\n        await self._close_messages()\n\n    async def _close_messages(self, wait: bool = True) -> None:\n        \"\"\"Close message queue, and optionally wait for queue to finish processing.\"\"\"\n        if self._closed or self._closing:\n            return\n        self._closing = True\n        if self._timers:\n            await Timer._stop_all(self._timers)\n            self._timers.clear()\n        self._message_queue.put_nowait(events.Unmount())\n        Reactive._reset_object(self)\n        self._message_queue.put_nowait(None)\n        if wait and self._task is not None and asyncio.current_task() != self._task:\n            try:\n                running_widget = active_message_pump.get()\n            except LookupError:\n                running_widget = None\n\n            if running_widget is None or running_widget is not self:\n                try:\n                    await self._task\n                except CancelledError:\n                    pass\n\n    def _start_messages(self) -> None:\n        \"\"\"Start messages task.\"\"\"\n        if self.app._running:\n            self._task = create_task(\n                self._process_messages(), name=f\"message pump {self}\"\n            )\n        else:\n            self._closing = True\n            self._closed = True\n\n    async def _process_messages(self) -> None:\n        self._running = True\n        active_message_pump.set(self)\n\n        if not await self._pre_process():\n            self._running = False\n            return\n\n        try:\n            await self._process_messages_loop()\n        except CancelledError:\n            pass\n        finally:\n            self._running = False\n            if self._timers:\n                await Timer._stop_all(self._timers)\n                self._timers.clear()\n\n    async def _pre_process(self) -> bool:\n        \"\"\"Procedure to run before processing messages.\n\n        Returns:\n            `True` if successful, or `False` if any exception occurred.\n\n        \"\"\"\n        # Dispatch compose and mount messages without going through loop\n        # These events must occur in this order, and at the start.\n\n        try:\n            await self._dispatch_message(events.Compose())\n            if self._prevented_messages_on_mount:\n                with self.prevent(*self._prevented_messages_on_mount):\n                    await self._dispatch_message(events.Mount())\n            else:\n                await self._dispatch_message(events.Mount())\n            self.check_idle()\n            self._post_mount()\n        except Exception as error:\n            self.app._handle_exception(error)\n            return False\n        finally:\n            # This is critical, mount may be waiting\n            self._mounted_event.set()\n            self._is_mounted = True\n        return True\n\n    def _post_mount(self):\n        \"\"\"Called after the object has been mounted.\"\"\"\n\n    async def _process_messages_loop(self) -> None:\n        \"\"\"Process messages until the queue is closed.\"\"\"\n        _rich_traceback_guard = True\n        self._thread_id = threading.get_ident()\n\n        while not self._closed:\n            try:\n                message = await self._get_message()\n            except MessagePumpClosed:\n                break\n            except CancelledError:\n                raise\n            except Exception as error:\n                raise error from None\n\n            # Combine any pending messages that may supersede this one\n            while not (self._closed or self._closing):\n                try:\n                    pending = self._peek_message()\n                except MessagePumpClosed:\n                    break\n                if pending is None or not message.can_replace(pending):\n                    break\n                try:\n                    message = await self._get_message()\n                except MessagePumpClosed:\n                    break\n\n            try:\n                await self._dispatch_message(message)\n            except CancelledError:\n                raise\n            except Exception as error:\n                self._mounted_event.set()\n                self._is_mounted = True\n                self.app._handle_exception(error)\n                break\n            finally:\n                self.message_signal.publish(message)\n                self._message_queue.task_done()\n\n                current_time = time()\n\n                # Insert idle events\n                if self._message_queue.empty() or (\n                    self._max_idle is not None\n                    and current_time - self._last_idle > self._max_idle\n                ):\n                    self._last_idle = current_time\n                    if not self._closed:\n                        event = events.Idle()\n                        for _cls, method in self._get_dispatch_methods(\n                            \"on_idle\", event\n                        ):\n                            try:\n                                await invoke(method, event)\n                            except Exception as error:\n                                self.app._handle_exception(error)\n                                break\n                    await self._flush_next_callbacks()\n\n    async def _flush_next_callbacks(self) -> None:\n        \"\"\"Invoke pending callbacks in next callbacks queue.\"\"\"\n        callbacks = self._next_callbacks.copy()\n        self._next_callbacks.clear()\n        for callback in callbacks:\n            try:\n                with self.prevent(*callback._prevent):\n                    await invoke(callback.callback)\n            except Exception as error:\n                self.app._handle_exception(error)\n                break\n\n    async def _dispatch_message(self, message: Message) -> None:\n        \"\"\"Dispatch a message received from the message queue.\n\n        Args:\n            message: A message object\n        \"\"\"\n        _rich_traceback_guard = True\n        if message.no_dispatch:\n            return\n\n        try:\n            message_hook = message_hook_context_var.get()\n        except LookupError:\n            pass\n        else:\n            message_hook(message)\n\n        with self.prevent(*message._prevent):\n            # Allow apps to treat events and messages separately\n            if isinstance(message, Event):\n                await self.on_event(message)\n            else:\n                await self._on_message(message)\n            if self._next_callbacks:\n                await self._flush_next_callbacks()\n\n    def _get_dispatch_methods(\n        self, method_name: str, message: Message\n    ) -> Iterable[tuple[type, Callable[[Message], Awaitable]]]:\n        \"\"\"Gets handlers from the MRO\n\n        Args:\n            method_name: Handler method name.\n            message: Message object.\n        \"\"\"\n        from .widget import Widget\n\n        methods_dispatched: set[Callable] = set()\n        message_mro = [\n            _type for _type in message.__class__.__mro__ if issubclass(_type, Message)\n        ]\n        for cls in self.__class__.__mro__:\n            if message._no_default_action:\n                break\n            # Try decorated handlers first\n            decorated_handlers = cast(\n                \"dict[type[Message], list[tuple[Callable, dict[str, tuple[SelectorSet, ...]]]]] | None\",\n                cls.__dict__.get(\"_decorated_handlers\"),\n            )\n\n            if decorated_handlers:\n                for message_class in message_mro:\n                    handlers = decorated_handlers.get(message_class, [])\n\n                    for method, selectors in handlers:\n                        if method in methods_dispatched:\n                            continue\n                        if not selectors:\n                            yield cls, method.__get__(self, cls)\n                            methods_dispatched.add(method)\n                        else:\n                            if not message._sender:\n                                continue\n                            for attribute, selector in selectors.items():\n                                node = getattr(message, attribute)\n                                if not isinstance(node, Widget):\n                                    raise OnNoWidget(\n                                        f\"on decorator can't match against {attribute!r} as it is not a widget.\"\n                                    )\n                                if not match(selector, node):\n                                    break\n                            else:\n                                yield cls, method.__get__(self, cls)\n                                methods_dispatched.add(method)\n\n            # Fall back to the naming convention\n            # But avoid calling the handler if it was decorated\n            method = cls.__dict__.get(f\"_{method_name}\") or cls.__dict__.get(\n                method_name\n            )\n            if method is not None and not getattr(method, \"_textual_on\", None):\n                yield cls, method.__get__(self, cls)\n\n    async def on_event(self, event: events.Event) -> None:\n        \"\"\"Called to process an event.\n\n        Args:\n            event: An Event object.\n        \"\"\"\n        await self._on_message(event)\n\n    async def _on_message(self, message: Message) -> None:\n        \"\"\"Called to process a message.\n\n        Args:\n            message: A Message object.\n        \"\"\"\n        _rich_traceback_guard = True\n        handler_name = message.handler_name\n\n        # Look through the MRO to find a handler\n        dispatched = False\n        for cls, method in self._get_dispatch_methods(handler_name, message):\n            log.event.verbosity(message.verbose)(\n                message,\n                \">>>\",\n                self,\n                f\"method=<{cls.__name__}.{handler_name}>\",\n            )\n            dispatched = True\n            await invoke(method, message)\n        if not dispatched:\n            log.event.verbosity(message.verbose)(message, \">>>\", self, \"method=None\")\n\n        # Bubble messages up the DOM (if enabled on the message)\n        if message.bubble and self._parent and not message._stop_propagation:\n            if message._sender is not None and message._sender == self._parent:\n                # parent is sender, so we stop propagation after parent\n                message.stop()\n            if self.is_parent_active and self.is_attached:\n                message._bubble_to(self._parent)\n\n    def check_idle(self) -> None:\n        \"\"\"Prompt the message pump to call idle if the queue is empty.\"\"\"\n        if self._running and self._message_queue.empty():\n            self.post_message(messages.Prompt())\n\n    async def _post_message(self, message: Message) -> bool:\n        \"\"\"Post a message or an event to this message pump.\n\n        This is an internal method for use where a coroutine is required.\n\n        Args:\n            message: A message object.\n\n        Returns:\n            True if the messages was\n```\n# Function Signature: def call_later(self, callback: Callback, *args: Any, **kwargs: Any) -> bool:\n# Natural Language Annotations: \"\"\"Schedule a callback to run after all messages are processed in this object.\n        Positional and keywords arguments are passed to the callable.\n\n        Args:\n            callback: Callable to call next.\n            *args: Positional arguments to pass to the callable.\n            **kwargs: Keyword arguments to pass to the callable.\n\n        Returns:\n            `True` if the callback was scheduled, or `False` if the callback could not be\n                scheduled (may occur if the message pump was closed or closing).\n\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef call_later(self, callback: Callback,", "  ```\ndef call_later(self, callback: Callback,", "  ```\ndef call_later(self, callback: Callback,"]}, "textual-src/textual/message_pump.py-post_message": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/message_pump.py:\n```\n\"\"\"\n\nA `MessagePump` is a base class for any object which processes messages, which includes Widget, Screen, and App.\n\n!!! tip\n\n    Most of the method here are useful in general app development.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport inspect\nimport threading\nfrom asyncio import CancelledError, Queue, QueueEmpty, Task, create_task\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Generator,\n    Iterable,\n    Type,\n    TypeVar,\n    cast,\n)\nfrom weakref import WeakSet\n\nfrom . import Logger, events, log, messages\nfrom ._callback import invoke\nfrom ._context import NoActiveAppError, active_app, active_message_pump\nfrom ._context import message_hook as message_hook_context_var\nfrom ._context import prevent_message_types_stack\nfrom ._on import OnNoWidget\nfrom ._time import time\nfrom .case import camel_to_snake\nfrom .css.match import match\nfrom .errors import DuplicateKeyHandlers\nfrom .events import Event\nfrom .message import Message\nfrom .reactive import Reactive, TooManyComputesError\nfrom .signal import Signal\nfrom .timer import Timer, TimerCallback\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n    from .app import App\n    from .css.model import SelectorSet\n\n\nCallback: TypeAlias = \"Callable[..., Any] | Callable[..., Awaitable[Any]]\"\n\n\nclass CallbackError(Exception):\n    pass\n\n\nclass MessagePumpClosed(Exception):\n    pass\n\n\n_MessagePumpMetaSub = TypeVar(\"_MessagePumpMetaSub\", bound=\"_MessagePumpMeta\")\n\n\nclass _MessagePumpMeta(type):\n    \"\"\"Metaclass for message pump. This exists to populate a Message inner class of a Widget with the\n    parent classes' name.\n    \"\"\"\n\n    def __new__(\n        cls: Type[_MessagePumpMetaSub],\n        name: str,\n        bases: tuple[type, ...],\n        class_dict: dict[str, Any],\n        **kwargs: Any,\n    ) -> _MessagePumpMetaSub:\n        namespace = camel_to_snake(name)\n        isclass = inspect.isclass\n        handlers: dict[\n            type[Message], list[tuple[Callable, dict[str, tuple[SelectorSet, ...]]]]\n        ] = class_dict.get(\"_decorated_handlers\", {})\n\n        class_dict[\"_decorated_handlers\"] = handlers\n\n        for value in class_dict.values():\n            if callable(value) and hasattr(value, \"_textual_on\"):\n                textual_on: list[\n                    tuple[type[Message], dict[str, tuple[SelectorSet, ...]]]\n                ] = getattr(value, \"_textual_on\")\n                for message_type, selectors in textual_on:\n                    handlers.setdefault(message_type, []).append((value, selectors))\n            if isclass(value) and issubclass(value, Message):\n                if \"namespace\" in value.__dict__:\n                    value.handler_name = f\"on_{value.__dict__['namespace']}_{camel_to_snake(value.__name__)}\"\n                else:\n                    value.handler_name = (\n                        f\"on_{namespace}_{camel_to_snake(value.__name__)}\"\n                    )\n\n        # Look for reactives with public AND private compute methods.\n        prefix = \"compute_\"\n        prefix_len = len(prefix)\n        for attr_name, value in class_dict.items():\n            if attr_name.startswith(prefix) and callable(value):\n                reactive_name = attr_name[prefix_len:]\n                if (\n                    reactive_name in class_dict\n                    and isinstance(class_dict[reactive_name], Reactive)\n                    and f\"_{attr_name}\" in class_dict\n                ):\n                    raise TooManyComputesError(\n                        f\"reactive {reactive_name!r} can't have two computes.\"\n                    )\n\n        class_obj = super().__new__(cls, name, bases, class_dict, **kwargs)\n        return class_obj\n\n\nclass MessagePump(metaclass=_MessagePumpMeta):\n    \"\"\"Base class which supplies a message pump.\"\"\"\n\n    def __init__(self, parent: MessagePump | None = None) -> None:\n        self._message_queue: Queue[Message | None] = Queue()\n        self._parent = parent\n        self._running: bool = False\n        self._closing: bool = False\n        self._closed: bool = False\n        self._disabled_messages: set[type[Message]] = set()\n        self._pending_message: Message | None = None\n        self._task: Task | None = None\n        self._timers: WeakSet[Timer] = WeakSet()\n        self._last_idle: float = time()\n        self._max_idle: float | None = None\n        self._mounted_event = asyncio.Event()\n        self._is_mounted = False\n        \"\"\"Having this explicit Boolean is an optimization.\n\n        The same information could be retrieved from `self._mounted_event.is_set()`, but\n        we need to access this frequently in the compositor and the attribute with the\n        explicit Boolean value is faster than the two lookups and the function call.\n        \"\"\"\n        self._next_callbacks: list[events.Callback] = []\n        self._thread_id: int = threading.get_ident()\n        self._prevented_messages_on_mount = self._prevent_message_types_stack[-1]\n        self.message_signal: Signal[Message] = Signal(self, \"messages\")\n        \"\"\"Subscribe to this signal to be notified of all messages sent to this widget.\n        \n        This is a fairly low-level mechanism, and shouldn't replace regular message handling.\n        \n        \"\"\"\n\n    @property\n    def _prevent_message_types_stack(self) -> list[set[type[Message]]]:\n        \"\"\"The stack that manages prevented messages.\"\"\"\n        try:\n            stack = prevent_message_types_stack.get()\n        except LookupError:\n            stack = [set()]\n            prevent_message_types_stack.set(stack)\n        return stack\n\n    def _get_prevented_messages(self) -> set[type[Message]]:\n        \"\"\"A set of all the prevented message types.\"\"\"\n        return self._prevent_message_types_stack[-1]\n\n    def _is_prevented(self, message_type: type[Message]) -> bool:\n        \"\"\"Check if a message type has been prevented via the\n        [prevent][textual.message_pump.MessagePump.prevent] context manager.\n\n        Args:\n            message_type: A message type.\n\n        Returns:\n            `True` if the message has been prevented from sending, or `False` if it will be sent as normal.\n        \"\"\"\n        return message_type in self._prevent_message_types_stack[-1]\n\n    @contextmanager\n    def prevent(self, *message_types: type[Message]) -> Generator[None, None, None]:\n        \"\"\"A context manager to *temporarily* prevent the given message types from being posted.\n\n        Example:\n            ```python\n            input = self.query_one(Input)\n            with self.prevent(Input.Changed):\n                input.value = \"foo\"\n            ```\n        \"\"\"\n        if message_types:\n            prevent_stack = self._prevent_message_types_stack\n            prevent_stack.append(prevent_stack[-1].union(message_types))\n            try:\n                yield\n            finally:\n                prevent_stack.pop()\n        else:\n            yield\n\n    @property\n    def task(self) -> Task:\n        assert self._task is not None\n        return self._task\n\n    @property\n    def has_parent(self) -> bool:\n        \"\"\"Does this object have a parent?\"\"\"\n        return self._parent is not None\n\n    @property\n    def message_queue_size(self) -> int:\n        \"\"\"The current size of the message queue.\"\"\"\n        return self._message_queue.qsize()\n\n    @property\n    def is_dom_root(self):\n        \"\"\"Is this a root node (i.e. the App)?\"\"\"\n        return False\n\n    @property\n    def app(self) -> \"App[object]\":\n        \"\"\"\n        Get the current app.\n\n        Returns:\n            The current app.\n\n        Raises:\n            NoActiveAppError: if no active app could be found for the current asyncio context\n        \"\"\"\n        try:\n            return active_app.get()\n        except LookupError:\n            from .app import App\n\n            node: MessagePump | None = self\n            while not isinstance(node, App):\n                if node is None:\n                    raise NoActiveAppError()\n                node = node._parent\n            active_app.set(node)\n            return node\n\n    @property\n    def is_attached(self) -> bool:\n        \"\"\"Is this node linked to the app through the DOM?\"\"\"\n        if self.app._exit:\n            return False\n        node: MessagePump | None = self\n        while (node := node._parent) is not None:\n            if node.is_dom_root:\n                return True\n        return False\n\n    @property\n    def is_parent_active(self) -> bool:\n        \"\"\"Is the parent active?\"\"\"\n        return bool(\n            self._parent and not self._parent._closed and not self._parent._closing\n        )\n\n    @property\n    def is_running(self) -> bool:\n        \"\"\"Is the message pump running (potentially processing messages)?\"\"\"\n        return self._running\n\n    @property\n    def log(self) -> Logger:\n        \"\"\"Get a logger for this object.\n\n        Returns:\n            A logger.\n        \"\"\"\n        return self.app._logger\n\n    def _attach(self, parent: MessagePump) -> None:\n        \"\"\"Set the parent, and therefore attach this node to the tree.\n\n        Args:\n            parent: Parent node.\n        \"\"\"\n        self._parent = parent\n\n    def _detach(self) -> None:\n        \"\"\"Set the parent to None to remove the node from the tree.\"\"\"\n        self._parent = None\n\n    def check_message_enabled(self, message: Message) -> bool:\n        \"\"\"Check if a given message is enabled (allowed to be sent).\n\n        Args:\n            message: A message object.\n\n        Returns:\n            `True` if the message will be sent, or `False` if it is disabled.\n        \"\"\"\n        return type(message) not in self._disabled_messages\n\n    def disable_messages(self, *messages: type[Message]) -> None:\n        \"\"\"Disable message types from being processed.\"\"\"\n        self._disabled_messages.update(messages)\n\n    def enable_messages(self, *messages: type[Message]) -> None:\n        \"\"\"Enable processing of messages types.\"\"\"\n        self._disabled_messages.difference_update(messages)\n\n    async def _get_message(self) -> Message:\n        \"\"\"Get the next event on the queue, or None if queue is closed.\n\n        Returns:\n            Event object or None.\n        \"\"\"\n        if self._closed:\n            raise MessagePumpClosed(\"The message pump is closed\")\n        if self._pending_message is not None:\n            try:\n                return self._pending_message\n            finally:\n                self._pending_message = None\n\n        message = await self._message_queue.get()\n\n        if message is None:\n            self._closed = True\n            raise MessagePumpClosed(\"The message pump is now closed\")\n        return message\n\n    def _peek_message(self) -> Message | None:\n        \"\"\"Peek the message at the head of the queue (does not remove it from the queue),\n        or return None if the queue is empty.\n\n        Returns:\n            The message or None.\n        \"\"\"\n        if self._pending_message is None:\n            try:\n                message = self._message_queue.get_nowait()\n            except QueueEmpty:\n                pass\n            else:\n                if message is None:\n                    self._closed = True\n                    raise MessagePumpClosed(\"The message pump is now closed\")\n                self._pending_message = message\n\n        if self._pending_message is not None:\n            return self._pending_message\n        return None\n\n    def set_timer(\n        self,\n        delay: float,\n        callback: TimerCallback | None = None,\n        *,\n        name: str | None = None,\n        pause: bool = False,\n    ) -> Timer:\n        \"\"\"Make a function call after a delay.\n\n        Args:\n            delay: Time (in seconds) to wait before invoking callback.\n            callback: Callback to call after time has expired.\n            name: Name of the timer (for debug).\n            pause: Start timer paused.\n\n        Returns:\n            A timer object.\n        \"\"\"\n        timer = Timer(\n            self,\n            delay,\n            name=name or f\"set_timer#{Timer._timer_count}\",\n            callback=callback,\n            repeat=0,\n            pause=pause,\n        )\n        timer._start()\n        self._timers.add(timer)\n        return timer\n\n    def set_interval(\n        self,\n        interval: float,\n        callback: TimerCallback | None = None,\n        *,\n        name: str | None = None,\n        repeat: int = 0,\n        pause: bool = False,\n    ) -> Timer:\n        \"\"\"Call a function at periodic intervals.\n\n        Args:\n            interval: Time (in seconds) between calls.\n            callback: Function to call.\n            name: Name of the timer object.\n            repeat: Number of times to repeat the call or 0 for continuous.\n            pause: Start the timer paused.\n\n        Returns:\n            A timer object.\n        \"\"\"\n        timer = Timer(\n            self,\n            interval,\n            name=name or f\"set_interval#{Timer._timer_count}\",\n            callback=callback,\n            repeat=repeat or None,\n            pause=pause,\n        )\n        timer._start()\n        self._timers.add(timer)\n        return timer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def call_next(self, callback: Callback, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Schedule a callback to run immediately after processing the current message.\n\n        Args:\n            callback: Callable to run after current event.\n            *args: Positional arguments to pass to the callable.\n            **kwargs: Keyword arguments to pass to the callable.\n        \"\"\"\n        callback_message = events.Callback(callback=partial(callback, *args, **kwargs))\n        callback_message._prevent.update(self._get_prevented_messages())\n        self._next_callbacks.append(callback_message)\n        self.check_idle()\n\n    def _on_invoke_later(self, message: messages.InvokeLater) -> None:\n        # Forward InvokeLater message to the Screen\n        self.app.screen._invoke_later(\n            message.callback, message._sender or active_message_pump.get()\n        )\n\n    def _close_messages_no_wait(self) -> None:\n        \"\"\"Request the message queue to immediately exit.\"\"\"\n        self._message_queue.put_nowait(messages.CloseMessages())\n\n    async def _on_close_messages(self, message: messages.CloseMessages) -> None:\n        await self._close_messages()\n\n    async def _close_messages(self, wait: bool = True) -> None:\n        \"\"\"Close message queue, and optionally wait for queue to finish processing.\"\"\"\n        if self._closed or self._closing:\n            return\n        self._closing = True\n        if self._timers:\n            await Timer._stop_all(self._timers)\n            self._timers.clear()\n        self._message_queue.put_nowait(events.Unmount())\n        Reactive._reset_object(self)\n        self._message_queue.put_nowait(None)\n        if wait and self._task is not None and asyncio.current_task() != self._task:\n            try:\n                running_widget = active_message_pump.get()\n            except LookupError:\n                running_widget = None\n\n            if running_widget is None or running_widget is not self:\n                try:\n                    await self._task\n                except CancelledError:\n                    pass\n\n    def _start_messages(self) -> None:\n        \"\"\"Start messages task.\"\"\"\n        if self.app._running:\n            self._task = create_task(\n                self._process_messages(), name=f\"message pump {self}\"\n            )\n        else:\n            self._closing = True\n            self._closed = True\n\n    async def _process_messages(self) -> None:\n        self._running = True\n        active_message_pump.set(self)\n\n        if not await self._pre_process():\n            self._running = False\n            return\n\n        try:\n            await self._process_messages_loop()\n        except CancelledError:\n            pass\n        finally:\n            self._running = False\n            if self._timers:\n                await Timer._stop_all(self._timers)\n                self._timers.clear()\n\n    async def _pre_process(self) -> bool:\n        \"\"\"Procedure to run before processing messages.\n\n        Returns:\n            `True` if successful, or `False` if any exception occurred.\n\n        \"\"\"\n        # Dispatch compose and mount messages without going through loop\n        # These events must occur in this order, and at the start.\n\n        try:\n            await self._dispatch_message(events.Compose())\n            if self._prevented_messages_on_mount:\n                with self.prevent(*self._prevented_messages_on_mount):\n                    await self._dispatch_message(events.Mount())\n            else:\n                await self._dispatch_message(events.Mount())\n            self.check_idle()\n            self._post_mount()\n        except Exception as error:\n            self.app._handle_exception(error)\n            return False\n        finally:\n            # This is critical, mount may be waiting\n            self._mounted_event.set()\n            self._is_mounted = True\n        return True\n\n    def _post_mount(self):\n        \"\"\"Called after the object has been mounted.\"\"\"\n\n    async def _process_messages_loop(self) -> None:\n        \"\"\"Process messages until the queue is closed.\"\"\"\n        _rich_traceback_guard = True\n        self._thread_id = threading.get_ident()\n\n        while not self._closed:\n            try:\n                message = await self._get_message()\n            except MessagePumpClosed:\n                break\n            except CancelledError:\n                raise\n            except Exception as error:\n                raise error from None\n\n            # Combine any pending messages that may supersede this one\n            while not (self._closed or self._closing):\n                try:\n                    pending = self._peek_message()\n                except MessagePumpClosed:\n                    break\n                if pending is None or not message.can_replace(pending):\n                    break\n                try:\n                    message = await self._get_message()\n                except MessagePumpClosed:\n                    break\n\n            try:\n                await self._dispatch_message(message)\n            except CancelledError:\n                raise\n            except Exception as error:\n                self._mounted_event.set()\n                self._is_mounted = True\n                self.app._handle_exception(error)\n                break\n            finally:\n                self.message_signal.publish(message)\n                self._message_queue.task_done()\n\n                current_time = time()\n\n                # Insert idle events\n                if self._message_queue.empty() or (\n                    self._max_idle is not None\n                    and current_time - self._last_idle > self._max_idle\n                ):\n                    self._last_idle = current_time\n                    if not self._closed:\n                        event = events.Idle()\n                        for _cls, method in self._get_dispatch_methods(\n                            \"on_idle\", event\n                        ):\n                            try:\n                                await invoke(method, event)\n                            except Exception as error:\n                                self.app._handle_exception(error)\n                                break\n                    await self._flush_next_callbacks()\n\n    async def _flush_next_callbacks(self) -> None:\n        \"\"\"Invoke pending callbacks in next callbacks queue.\"\"\"\n        callbacks = self._next_callbacks.copy()\n        self._next_callbacks.clear()\n        for callback in callbacks:\n            try:\n                with self.prevent(*callback._prevent):\n                    await invoke(callback.callback)\n            except Exception as error:\n                self.app._handle_exception(error)\n                break\n\n    async def _dispatch_message(self, message: Message) -> None:\n        \"\"\"Dispatch a message received from the message queue.\n\n        Args:\n            message: A message object\n        \"\"\"\n        _rich_traceback_guard = True\n        if message.no_dispatch:\n            return\n\n        try:\n            message_hook = message_hook_context_var.get()\n        except LookupError:\n            pass\n        else:\n            message_hook(message)\n\n        with self.prevent(*message._prevent):\n            # Allow apps to treat events and messages separately\n            if isinstance(message, Event):\n                await self.on_event(message)\n            else:\n                await self._on_message(message)\n            if self._next_callbacks:\n                await self._flush_next_callbacks()\n\n    def _get_dispatch_methods(\n        self, method_name: str, message: Message\n    ) -> Iterable[tuple[type, Callable[[Message], Awaitable]]]:\n        \"\"\"Gets handlers from the MRO\n\n        Args:\n            method_name: Handler method name.\n            message: Message object.\n        \"\"\"\n        from .widget import Widget\n\n        methods_dispatched: set[Callable] = set()\n        message_mro = [\n            _type for _type in message.__class__.__mro__ if issubclass(_type, Message)\n        ]\n        for cls in self.__class__.__mro__:\n            if message._no_default_action:\n                break\n            # Try decorated handlers first\n            decorated_handlers = cast(\n                \"dict[type[Message], list[tuple[Callable, dict[str, tuple[SelectorSet, ...]]]]] | None\",\n                cls.__dict__.get(\"_decorated_handlers\"),\n            )\n\n            if decorated_handlers:\n                for message_class in message_mro:\n                    handlers = decorated_handlers.get(message_class, [])\n\n                    for method, selectors in handlers:\n                        if method in methods_dispatched:\n                            continue\n                        if not selectors:\n                            yield cls, method.__get__(self, cls)\n                            methods_dispatched.add(method)\n                        else:\n                            if not message._sender:\n                                continue\n                            for attribute, selector in selectors.items():\n                                node = getattr(message, attribute)\n                                if not isinstance(node, Widget):\n                                    raise OnNoWidget(\n                                        f\"on decorator can't match against {attribute!r} as it is not a widget.\"\n                                    )\n                                if not match(selector, node):\n                                    break\n                            else:\n                                yield cls, method.__get__(self, cls)\n                                methods_dispatched.add(method)\n\n            # Fall back to the naming convention\n            # But avoid calling the handler if it was decorated\n            method = cls.__dict__.get(f\"_{method_name}\") or cls.__dict__.get(\n                method_name\n            )\n            if method is not None and not getattr(method, \"_textual_on\", None):\n                yield cls, method.__get__(self, cls)\n\n    async def on_event(self, event: events.Event) -> None:\n        \"\"\"Called to process an event.\n\n        Args:\n            event: An Event object.\n        \"\"\"\n        await self._on_message(event)\n\n    async def _on_message(self, message: Message) -> None:\n        \"\"\"Called to process a message.\n\n        Args:\n            message: A Message object.\n        \"\"\"\n        _rich_traceback_guard = True\n        handler_name = message.handler_name\n\n        # Look through the MRO to find a handler\n        dispatched = False\n        for cls, method in self._get_dispatch_methods(handler_name, message):\n            log.event.verbosity(message.verbose)(\n                message,\n                \">>>\",\n                self,\n                f\"method=<{cls.__name__}.{handler_name}>\",\n            )\n            dispatched = True\n            await invoke(method, message)\n        if not dispatched:\n            log.event.verbosity(message.verbose)(message, \">>>\", self, \"method=None\")\n\n        # Bubble messages up the DOM (if enabled on the message)\n        if message.bubble and self._parent and not message._stop_propagation:\n            if message._sender is not None and message._sender == self._parent:\n                # parent is sender, so we stop propagation after parent\n                message.stop()\n            if self.is_parent_active and self.is_attached:\n                message._bubble_to(self._parent)\n\n    def check_idle(self) -> None:\n        \"\"\"Prompt the message pump to call idle if the queue is empty.\"\"\"\n        if self._running and self._message_queue.empty():\n            self.post_message(messages.Prompt())\n\n    async def _post_message(self, message: Message) -> bool:\n        \"\"\"Post a message or an event to this message pump.\n\n        This is an internal method for use where a coroutine is required.\n\n        Args:\n            message: A message object.\n\n        Returns:\n            True if the messages was\n```\n# Function Signature: def post_message(self, message: Message) -> bool:\n# Natural Language Annotations: \"\"\"Posts a message on to this widget's queue.\n\n        Args:\n            message: A message (including Event).\n\n        Returns:\n            `True` if the messages was processed, `False` if it wasn't.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef post_message(self, message: Message) -> bool", "  ```\ndef post_message(self, message: Message) -> bool", "  ```\ndef post_message(self, message: Message):\n   "]}, "textual-src/textual/_spatial_map.py-get_values_in_region": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/_spatial_map.py:\n```\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom itertools import product\nfrom typing import Generic, Iterable, TypeVar\n\nfrom typing_extensions import TypeAlias\n\nfrom .geometry import Region\n\nValueType = TypeVar(\"ValueType\")\nGridCoordinate: TypeAlias = \"tuple[int, int]\"\n\n\nclass SpatialMap(Generic[ValueType]):\n    \"\"\"A spatial map allows for data to be associated with rectangular regions\n    in Euclidean space, and efficiently queried.\n\n    When the SpatialMap is populated, a reference to each value is placed into one or\n    more buckets associated with a regular grid that covers 2D space.\n\n    The SpatialMap is able to quickly retrieve the values under a given \"window\" region\n    by combining the values in the grid squares under the visible area.\n    \"\"\"\n\n    def __init__(self, grid_width: int = 100, grid_height: int = 20) -> None:\n        \"\"\"Create a spatial map with the given grid size.\n\n        Args:\n            grid_width: Width of a grid square.\n            grid_height: Height of a grid square.\n        \"\"\"\n        self._grid_size = (grid_width, grid_height)\n        self.total_region = Region()\n        self._map: defaultdict[GridCoordinate, list[ValueType]] = defaultdict(list)\n        self._fixed: list[ValueType] = []\n\n    def _region_to_grid_coordinates(self, region: Region) -> Iterable[GridCoordinate]:\n        \"\"\"Get the grid squares under a region.\n\n        Args:\n            region: A region.\n\n        Returns:\n            Iterable of grid coordinates (tuple of 2 values).\n        \"\"\"\n        # (x1, y1) is the coordinate of the top left cell\n        # (x2, y2) is the coordinate of the bottom right cell\n        x1, y1, width, height = region\n        x2 = x1 + width - 1\n        y2 = y1 + height - 1\n        grid_width, grid_height = self._grid_size\n\n        return product(\n            range(x1 // grid_width, x2 // grid_width + 1),\n            range(y1 // grid_height, y2 // grid_height + 1),\n        )\n\n    def insert(\n        self, regions_and_values: Iterable[tuple[Region, bool, bool, ValueType]]\n    ) -> None:\n        \"\"\"Insert values into the Spatial map.\n\n        Values are associated with their region in Euclidean space, and a boolean that\n        indicates fixed regions. Fixed regions don't scroll and are always visible.\n\n        Args:\n            regions_and_values: An iterable of (REGION, FIXED, OVERLAY, VALUE).\n        \"\"\"\n        append_fixed = self._fixed.append\n        get_grid_list = self._map.__getitem__\n        _region_to_grid = self._region_to_grid_coordinates\n        total_region = self.total_region\n        for region, fixed, overlay, value in regions_and_values:\n            if fixed:\n                append_fixed(value)\n            else:\n                if not overlay:\n                    total_region = total_region.union(region)\n                for grid in _region_to_grid(region):\n                    get_grid_list(grid).append(value)\n        self.total_region = total_region\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def get_values_in_region(self, region: Region) -> list[ValueType]:\n# Natural Language Annotations: \"\"\"Get a superset of all the values that intersect with a given region.\n\n        Note that this may return false positives.\n\n        Args:\n            region: A region.\n\n        Returns:\n            Values under the region.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef get_values_in_region(self, region:", "  ```\ndef get_values_in_region(self, region:", "  ```\ndef get_values_in_region(self, region:"]}, "textual-src/textual/validation.py-validate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/validation.py:\n```\n\"\"\"Framework for validating string values\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nimport re\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Pattern, Sequence\nfrom urllib.parse import urlparse\n\nimport rich.repr\n\n\n@dataclass\nclass ValidationResult:\n    \"\"\"The result of calling a `Validator.validate` method.\"\"\"\n\n    failures: Sequence[Failure] = field(default_factory=list)\n    \"\"\"A list of reasons why the value was invalid. Empty if valid=True\"\"\"\n\n    @staticmethod\n    def merge(results: Sequence[\"ValidationResult\"]) -> \"ValidationResult\":\n        \"\"\"Merge multiple ValidationResult objects into one.\n\n        Args:\n            results: List of ValidationResult objects to merge.\n\n        Returns:\n            Merged ValidationResult object.\n        \"\"\"\n        is_valid = all(result.is_valid for result in results)\n        failures = [failure for result in results for failure in result.failures]\n        if is_valid:\n            return ValidationResult.success()\n        else:\n            return ValidationResult.failure(failures)\n\n    @staticmethod\n    def success() -> ValidationResult:\n        \"\"\"Construct a successful ValidationResult.\n\n        Returns:\n            A successful ValidationResult.\n        \"\"\"\n        return ValidationResult()\n\n    @staticmethod\n    def failure(failures: Sequence[Failure]) -> ValidationResult:\n        \"\"\"Construct a failure ValidationResult.\n\n        Args:\n            failures: The failures.\n\n        Returns:\n            A failure ValidationResult.\n        \"\"\"\n        return ValidationResult(failures)\n\n    @property\n    def failure_descriptions(self) -> list[str]:\n        \"\"\"Utility for extracting failure descriptions as strings.\n\n        Useful if you don't care about the additional metadata included in the `Failure` objects.\n\n        Returns:\n            A list of the string descriptions explaining the failing validations.\n        \"\"\"\n        return [\n            failure.description\n            for failure in self.failures\n            if failure.description is not None\n        ]\n\n    @property\n    def is_valid(self) -> bool:\n        \"\"\"True if the validation was successful.\"\"\"\n        return len(self.failures) == 0\n\n\n@dataclass\nclass Failure:\n    \"\"\"Information about a validation failure.\"\"\"\n\n    validator: Validator\n    \"\"\"The Validator which produced the failure.\"\"\"\n    value: str | None = None\n    \"\"\"The value which resulted in validation failing.\"\"\"\n    description: str | None = None\n    \"\"\"An optional override for describing this failure. Takes precedence over any messages set in the Validator.\"\"\"\n\n    def __post_init__(self) -> None:\n        # If a failure message isn't supplied, try to get it from the Validator.\n        if self.description is None:\n            if self.validator.failure_description is not None:\n                self.description = self.validator.failure_description\n            else:\n                self.description = self.validator.describe_failure(self)\n\n    def __rich_repr__(self) -> rich.repr.Result:  # pragma: no cover\n        yield self.value\n        yield self.validator\n        yield self.description\n\n\nclass Validator(ABC):\n    \"\"\"Base class for the validation of string values.\n\n    Commonly used in conjunction with the `Input` widget, which accepts a\n    list of validators via its constructor. This validation framework can also be used to validate any 'stringly-typed'\n    values (for example raw command line input from `sys.args`).\n\n    To implement your own `Validator`, subclass this class.\n\n    Example:\n        ```python\n        class Palindrome(Validator):\n            def validate(self, value: str) -> ValidationResult:\n                def is_palindrome(value: str) -> bool:\n                    return value == value[::-1]\n                return self.success() if is_palindrome(value) else self.failure(\"Not palindrome!\")\n        ```\n    \"\"\"\n\n    def __init__(self, failure_description: str | None = None) -> None:\n        self.failure_description = failure_description\n        \"\"\"A description of why the validation failed.\n\n        The description (intended to be user-facing) to attached to the Failure if the validation fails.\n        This failure description is ultimately accessible at the time of validation failure  via the `Input.Changed`\n        or `Input.Submitted` event, and you can access it on your message handler (a method called, for example,\n        `on_input_changed` or a method decorated with `@on(Input.Changed)`.\n        \"\"\"\n\n    @abstractmethod\n    def validate(self, value: str) -> ValidationResult:\n        \"\"\"Validate the value and return a ValidationResult describing the outcome of the validation.\n\n        Args:\n            value: The value to validate.\n\n        Returns:\n            The result of the validation.\n        \"\"\"\n\n    def describe_failure(self, failure: Failure) -> str | None:\n        \"\"\"Return a string description of the Failure.\n\n        Used to provide a more fine-grained description of the failure. A Validator could fail for multiple\n        reasons, so this method could be used to provide a different reason for different types of failure.\n\n        !!! warning\n\n            This method is only called if no other description has been supplied. If you supply a description\n            inside a call to `self.failure(description=\"...\")`, or pass a description into the constructor of\n            the validator, those will take priority, and this method won't be called.\n\n        Args:\n            failure: Information about why the validation failed.\n\n        Returns:\n            A string description of the failure.\n        \"\"\"\n        return self.failure_description\n\n    def success(self) -> ValidationResult:\n        \"\"\"Shorthand for `ValidationResult(True)`.\n\n        You can return success() from a `Validator.validate` method implementation to signal\n        that validation has succeeded.\n\n        Returns:\n            A ValidationResult indicating validation succeeded.\n        \"\"\"\n        return ValidationResult()\n\n    def failure(\n        self,\n        description: str | None = None,\n        value: str | None = None,\n        failures: Failure | Sequence[Failure] | None = None,\n    ) -> ValidationResult:\n        \"\"\"Shorthand for signaling validation failure.\n\n        You can return failure(...) from a `Validator.validate` implementation to signal validation succeeded.\n\n        Args:\n            description: The failure description that will be used. When used in conjunction with the Input widget,\n                this is the description that will ultimately be available inside the handler for `Input.Changed`. If not\n                supplied, the `failure_description` from the `Validator` will be used. If that is not supplied either,\n                then the `describe_failure` method on `Validator` will be called.\n            value: The value that was considered invalid. This is optional, and only needs to be supplied if required\n                in your `Input.Changed` handler.\n            failures: The reasons the validator failed. If not supplied, a generic `Failure` will be included in the\n                ValidationResult returned from this function.\n\n        Returns:\n            A ValidationResult representing failed validation, and containing the metadata supplied\n                to this function.\n        \"\"\"\n        if isinstance(failures, Failure):\n            failures = [failures]\n\n        result = ValidationResult(\n            failures or [Failure(validator=self, value=value, description=description)],\n        )\n        return result\n\n\nclass Regex(Validator):\n    \"\"\"A validator that checks the value matches a regex (via `re.fullmatch`).\"\"\"\n\n    def __init__(\n        self,\n        regex: str | Pattern[str],\n        flags: int | re.RegexFlag = 0,\n        failure_description: str | None = None,\n    ) -> None:\n        super().__init__(failure_description=failure_description)\n        self.regex = regex\n        \"\"\"The regex which we'll validate is matched by the value.\"\"\"\n        self.flags = flags\n        \"\"\"The flags to pass to `re.fullmatch`.\"\"\"\n\n    class NoResults(Failure):\n        \"\"\"Indicates validation failed because the regex could not be found within the value string.\"\"\"\n\n    def validate(self, value: str) -> ValidationResult:\n        \"\"\"Ensure that the value matches the regex.\n\n        Args:\n            value: The value that should match the regex.\n\n        Returns:\n            The result of the validation.\n        \"\"\"\n        regex = self.regex\n        has_match = re.fullmatch(regex, value, flags=self.flags) is not None\n        if not has_match:\n            failures = [Regex.NoResults(self, value)]\n            return self.failure(failures=failures)\n        return self.success()\n\n    def describe_failure(self, failure: Failure) -> str | None:\n        \"\"\"Describes why the validator failed.\n\n        Args:\n            failure: Information about why the validation failed.\n\n        Returns:\n            A string description of the failure.\n        \"\"\"\n        return f\"Must match regular expression {self.regex!r} (flags={self.flags}).\"\n\n\nclass Number(Validator):\n    \"\"\"Validator that ensures the value is a number, with an optional range check.\"\"\"\n\n    def __init__(\n        self,\n        minimum: float | None = None,\n        maximum: float | None = None,\n        failure_description: str | None = None,\n    ) -> None:\n        super().__init__(failure_description=failure_description)\n        self.minimum = minimum\n        \"\"\"The minimum value of the number, inclusive. If `None`, the minimum is unbounded.\"\"\"\n        self.maximum = maximum\n        \"\"\"The maximum value of the number, inclusive. If `None`, the maximum is unbounded.\"\"\"\n\n    class NotANumber(Failure):\n        \"\"\"Indicates a failure due to the value not being a valid number (decimal/integer, inc. scientific notation)\"\"\"\n\n    class NotInRange(Failure):\n        \"\"\"Indicates a failure due to the number not being within the range [minimum, maximum].\"\"\"\n\n    def validate(self, value: str) -> ValidationResult:\n        \"\"\"Ensure that `value` is a valid number, optionally within a range.\n\n        Args:\n            value: The value to validate.\n\n        Returns:\n            The result of the validation.\n        \"\"\"\n        try:\n            float_value = float(value)\n        except ValueError:\n            return ValidationResult.failure([Number.NotANumber(self, value)])\n\n        if float_value in {math.nan, math.inf, -math.inf}:\n            return ValidationResult.failure([Number.NotANumber(self, value)])\n\n        if not self._validate_range(float_value):\n            return ValidationResult.failure(\n                [Number.NotInRange(self, value)],\n            )\n        return self.success()\n\n    def _validate_range(self, value: float) -> bool:\n        \"\"\"Return a boolean indicating whether the number is within the range specified in the attributes.\"\"\"\n        if self.minimum is not None and value < self.minimum:\n            return False\n        if self.maximum is not None and value > self.maximum:\n            return False\n        return True\n\n    def describe_failure(self, failure: Failure) -> str | None:\n        \"\"\"Describes why the validator failed.\n\n        Args:\n            failure: Information about why the validation failed.\n\n        Returns:\n            A string description of the failure.\n        \"\"\"\n        if isinstance(failure, Number.NotANumber):\n            return \"Must be a valid number.\"\n        elif isinstance(failure, Number.NotInRange):\n            if self.minimum is None and self.maximum is not None:\n                return f\"Must be less than or equal to {self.maximum}.\"\n            elif self.minimum is not None and self.maximum is None:\n                return f\"Must be greater than or equal to {self.minimum}.\"\n            else:\n                return f\"Must be between {self.minimum} and {self.maximum}.\"\n        else:\n            return None\n\n\nclass Integer(Number):\n    \"\"\"Validator which ensures the value is an integer which falls within a range.\"\"\"\n\n    class NotAnInteger(Failure):\n        \"\"\"Indicates a failure due to the value not being a valid integer.\"\"\"\n\n    def validate(self, value: str) -> ValidationResult:\n        \"\"\"Ensure that `value` is an integer, optionally within a range.\n\n        Args:\n            value: The value to validate.\n\n        Returns:\n            The result of the validation.\n        \"\"\"\n        # First, check that we're dealing with a number in the range.\n        number_validation_result = super().validate(value)\n        if not number_validation_result.is_valid:\n            return number_validation_result\n\n        # We know it's a number, but is that number an integer?\n        is_integer = float(value).is_integer()\n        if not is_integer:\n            return ValidationResult.failure([Integer.NotAnInteger(self, value)])\n\n        return self.success()\n\n    def describe_failure(self, failure: Failure) -> str | None:\n        \"\"\"Describes why the validator failed.\n\n        Args:\n            failure: Information about why the validation failed.\n\n        Returns:\n            A string description of the failure.\n        \"\"\"\n        if isinstance(failure, (Integer.NotANumber, Integer.NotAnInteger)):\n            return \"Must be a valid integer.\"\n        elif isinstance(failure, Integer.NotInRange):\n            if self.minimum is None and self.maximum is not None:\n                return f\"Must be less than or equal to {self.maximum}.\"\n            elif self.minimum is not None and self.maximum is None:\n                return f\"Must be greater than or equal to {self.minimum}.\"\n            else:\n                return f\"Must be between {self.minimum} and {self.maximum}.\"\n        else:\n            return None\n\n\nclass Length(Validator):\n    \"\"\"Validate that a string is within a range (inclusive).\"\"\"\n\n    def __init__(\n        self,\n        minimum: int | None = None,\n        maximum: int | None = None,\n        failure_description: str | None = None,\n    ) -> None:\n        super().__init__(failure_description=failure_description)\n        self.minimum = minimum\n        \"\"\"The inclusive minimum length of the value, or None if unbounded.\"\"\"\n        self.maximum = maximum\n        \"\"\"The inclusive maximum length of the value, or None if unbounded.\"\"\"\n\n    class Incorrect(Failure):\n        \"\"\"Indicates a failure due to the length of the value being outside the range.\"\"\"\n\n    def validate(self, value: str) -> ValidationResult:\n        \"\"\"Ensure that value falls within the maximum and minimum length constraints.\n\n        Args:\n            value: The value to validate.\n\n        Returns:\n            The result of the validation.\n        \"\"\"\n        too_short = self.minimum is not None and len(value) < self.minimum\n        too_long = self.maximum is not None and len(value) > self.maximum\n        if too_short or too_long:\n            return ValidationResult.failure([Length.Incorrect(self, value)])\n        return self.success()\n\n    def describe_failure(self, failure: Failure) -> str | None:\n        \"\"\"Describes why the validator failed.\n\n        Args:\n            failure: Information about why the validation failed.\n\n        Returns:\n            A string description of the failure.\n        \"\"\"\n        if isinstance(failure, Length.Incorrect):\n            if self.minimum is None and self.maximum is not None:\n                return f\"Must be shorter than {self.maximum} characters.\"\n            elif self.minimum is not None and self.maximum is None:\n                return f\"Must be longer than {self.minimum} characters.\"\n            else:\n                return f\"Must be between {self.minimum} and {self.maximum} characters.\"\n        return None\n\n\nclass Function(Validator):\n    \"\"\"A flexible validator which allows you to provide custom validation logic.\"\"\"\n\n    def __init__(\n        self,\n        function: Callable[[str], bool],\n        failure_description: str | None = None,\n    ) -> None:\n        super().__init__(failure_description=failure_description)\n        self.function = function\n        \"\"\"Function which takes the value to validate and returns True if valid, and False otherwise.\"\"\"\n\n    class ReturnedFalse(Failure):\n        \"\"\"Indicates validation failed because the supplied function returned False.\"\"\"\n\n    def validate(self, value: str) -> ValidationResult:\n        \"\"\"Validate that the supplied function returns True.\n\n        Args:\n            value: The value to pass into the supplied function.\n\n        Returns:\n            A ValidationResult indicating success if the function returned True,\n                and failure if the function return False.\n        \"\"\"\n        is_valid = self.function(value)\n        if is_valid:\n            return self.success()\n        return self.failure(failures=Function.ReturnedFalse(self, value))\n\n    def describe_failure(self, failure: Failure) -> str | None:\n        \"\"\"Describes why the validator failed.\n\n        Args:\n            failure: Information about why the validation failed.\n\n        Returns:\n            A string description of the failure.\n        \"\"\"\n        return self.failure_description\n\n\nclass URL(Validator):\n    \"\"\"Validator that checks if a URL is valid (ensuring a scheme is present).\"\"\"\n\n    class InvalidURL(Failure):\n        \"\"\"Indicates that the URL is not valid.\"\"\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def describe_failure(self, failure: Failure) -> str | None:\n        \"\"\"Describes why the validator failed.\n\n        Args:\n            failure: Information about why the validation failed.\n\n        Returns:\n            A string description of the failure.\n        \"\"\"\n        return \"Must be a valid URL.\"\n\n```\n# Function Signature: def validate(self, value: str) -> ValidationResult:\n# Natural Language Annotations: \"\"\"Validates that `value` is a valid URL (contains a scheme).\n\n        Args:\n            value: The value to validate.\n\n        Returns:\n            The result of the validation.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef validate(self, value: str) -> ValidationResult", "  ```\ndef validate(self, value: str) -> ValidationResult", "  ```\ndef validate(self, value: str) -> ValidationResult"]}, "textual-src/textual/geometry.py-clamp": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/geometry.py:\n```\n\"\"\"\n\nFunctions and classes to manage terminal geometry (anything involving coordinates or dimensions).\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom functools import lru_cache\nfrom operator import attrgetter, itemgetter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Collection,\n    NamedTuple,\n    Tuple,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom typing_extensions import Final\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n\nSpacingDimensions: TypeAlias = Union[\n    int, Tuple[int], Tuple[int, int], Tuple[int, int, int, int]\n]\n\"\"\"The valid ways in which you can specify spacing.\"\"\"\n\nT = TypeVar(\"T\", int, float)\n\n\ndef clamp(value: T, minimum: T, maximum: T) -> T:\n    \"\"\"Adjust a value so it is not less than a minimum and not greater\n    than a maximum value.\n\n    Args:\n        value: A value.\n        minimum: Minimum value.\n        maximum: Maximum value.\n\n    Returns:\n        New value that is not less than the minimum or greater than the maximum.\n    \"\"\"\n    if minimum > maximum:\n        maximum, minimum = minimum, maximum\n    if value < minimum:\n        return minimum\n    elif value > maximum:\n        return maximum\n    else:\n        return value\n\n\nclass Offset(NamedTuple):\n    \"\"\"A cell offset defined by x and y coordinates.\n\n    Offsets are typically relative to the top left of the terminal or other container.\n\n    Textual prefers the names `x` and `y`, but you could consider `x` to be the _column_ and `y` to be the _row_.\n\n    Offsets support addition, subtraction, multiplication, and negation.\n\n    Example:\n        ```python\n        >>> from textual.geometry import Offset\n        >>> offset = Offset(3, 2)\n        >>> offset\n        Offset(x=3, y=2)\n        >>> offset += Offset(10, 0)\n        >>> offset\n        Offset(x=13, y=2)\n        >>> -offset\n        Offset(x=-13, y=-2)\n        ```\n    \"\"\"\n\n    x: int = 0\n    \"\"\"Offset in the x-axis (horizontal)\"\"\"\n    y: int = 0\n    \"\"\"Offset in the y-axis (vertical)\"\"\"\n\n    @property\n    def is_origin(self) -> bool:\n        \"\"\"Is the offset at (0, 0)?\"\"\"\n        return self == (0, 0)\n\n    @property\n    def clamped(self) -> Offset:\n        \"\"\"This offset with `x` and `y` restricted to values above zero.\"\"\"\n        x, y = self\n        return Offset(0 if x < 0 else x, 0 if y < 0 else y)\n\n    def __bool__(self) -> bool:\n        return self != (0, 0)\n\n    def __add__(self, other: object) -> Offset:\n        if isinstance(other, tuple):\n            _x, _y = self\n            x, y = other\n            return Offset(_x + x, _y + y)\n        return NotImplemented\n\n    def __sub__(self, other: object) -> Offset:\n        if isinstance(other, tuple):\n            _x, _y = self\n            x, y = other\n            return Offset(_x - x, _y - y)\n        return NotImplemented\n\n    def __mul__(self, other: object) -> Offset:\n        if isinstance(other, (float, int)):\n            x, y = self\n            return Offset(int(x * other), int(y * other))\n        return NotImplemented\n\n    def __neg__(self) -> Offset:\n        x, y = self\n        return Offset(-x, -y)\n\n    def blend(self, destination: Offset, factor: float) -> Offset:\n        \"\"\"Calculate a new offset on a line between this offset and a destination offset.\n\n        Args:\n            destination: Point where factor would be 1.0.\n            factor: A value between 0 and 1.0.\n\n        Returns:\n            A new point on a line between self and destination.\n        \"\"\"\n        x1, y1 = self\n        x2, y2 = destination\n        return Offset(\n            int(x1 + (x2 - x1) * factor),\n            int(y1 + (y2 - y1) * factor),\n        )\n\n    def get_distance_to(self, other: Offset) -> float:\n        \"\"\"Get the distance to another offset.\n\n        Args:\n            other: An offset.\n\n        Returns:\n            Distance to other offset.\n        \"\"\"\n        x1, y1 = self\n        x2, y2 = other\n        distance: float = ((x2 - x1) * (x2 - x1) + (y2 - y1) * (y2 - y1)) ** 0.5\n        return distance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclass Size(NamedTuple):\n    \"\"\"The dimensions (width and height) of a rectangular region.\n\n    Example:\n        ```python\n        >>> from textual.geometry import Size\n        >>> size = Size(2, 3)\n        >>> size\n        Size(width=2, height=3)\n        >>> size.area\n        6\n        >>> size + Size(10, 20)\n        Size(width=12, height=23)\n        ```\n    \"\"\"\n\n    width: int = 0\n    \"\"\"The width in cells.\"\"\"\n\n    height: int = 0\n    \"\"\"The height in cells.\"\"\"\n\n    def __bool__(self) -> bool:\n        \"\"\"A Size is Falsy if it has area 0.\"\"\"\n        return self.width * self.height != 0\n\n    @property\n    def area(self) -> int:\n        \"\"\"The area occupied by a region of this size.\"\"\"\n        return self.width * self.height\n\n    @property\n    def region(self) -> Region:\n        \"\"\"A region of the same size, at the origin.\"\"\"\n        width, height = self\n        return Region(0, 0, width, height)\n\n    @property\n    def line_range(self) -> range:\n        \"\"\"A range object that covers values between 0 and `height`.\"\"\"\n        return range(self.height)\n\n    def with_width(self, width: int) -> Size:\n        \"\"\"Get a new Size with just the width changed.\n\n        Args:\n            width: New width.\n\n        Returns:\n            New Size instance.\n        \"\"\"\n        return Size(width, self.height)\n\n    def with_height(self, height: int) -> Size:\n        \"\"\"Get a new Size with just the height changed.\n\n        Args:\n            width: New height.\n\n        Returns:\n            New Size instance.\n        \"\"\"\n        return Size(self.width, height)\n\n    def __add__(self, other: object) -> Size:\n        if isinstance(other, tuple):\n            width, height = self\n            width2, height2 = other\n            return Size(max(0, width + width2), max(0, height + height2))\n        return NotImplemented\n\n    def __sub__(self, other: object) -> Size:\n        if isinstance(other, tuple):\n            width, height = self\n            width2, height2 = other\n            return Size(max(0, width - width2), max(0, height - height2))\n        return NotImplemented\n\n    def contains(self, x: int, y: int) -> bool:\n        \"\"\"Check if a point is in area defined by the size.\n\n        Args:\n            x: X coordinate.\n            y: Y coordinate.\n\n        Returns:\n            True if the point is within the region.\n        \"\"\"\n        width, height = self\n        return width > x >= 0 and height > y >= 0\n\n    def contains_point(self, point: tuple[int, int]) -> bool:\n        \"\"\"Check if a point is in the area defined by the size.\n\n        Args:\n            point: A tuple of x and y coordinates.\n\n        Returns:\n            True if the point is within the region.\n        \"\"\"\n        x, y = point\n        width, height = self\n        return width > x >= 0 and height > y >= 0\n\n    def __contains__(self, other: Any) -> bool:\n        try:\n            x: int\n            y: int\n            x, y = other\n        except Exception:\n            raise TypeError(\n                \"Dimensions.__contains__ requires an iterable of two integers\"\n            )\n        width, height = self\n        return width > x >= 0 and height > y >= 0\n\n    def clamp_offset(self, offset: Offset) -> Offset:\n        \"\"\"Clamp an offset to fit within the width x height.\n\n        Args:\n            offset: An offset.\n\n        Returns:\n            A new offset that will fit inside the dimensions defined in the Size.\n        \"\"\"\n        return offset.clamp(self.width, self.height)\n\n\nclass Region(NamedTuple):\n    \"\"\"Defines a rectangular region.\n\n    A Region consists of a coordinate (x and y) and dimensions (width and height).\n\n    ```\n      (x, y)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25b2\n        \u2502                    \u2502 \u2502\n        \u2502                    \u2502 \u2502\n        \u2502                    \u2502 height\n        \u2502                    \u2502 \u2502\n        \u2502                    \u2502 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u25bc\n        \u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500 width \u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n    ```\n\n    Example:\n        ```python\n        >>> from textual.geometry import Region\n        >>> region = Region(4, 5, 20, 10)\n        >>> region\n        Region(x=4, y=5, width=20, height=10)\n        >>> region.area\n        200\n        >>> region.size\n        Size(width=20, height=10)\n        >>> region.offset\n        Offset(x=4, y=5)\n        >>> region.contains(1, 2)\n        False\n        >>> region.contains(10, 8)\n        True\n        ```\n    \"\"\"\n\n    x: int = 0\n    \"\"\"Offset in the x-axis (horizontal).\"\"\"\n    y: int = 0\n    \"\"\"Offset in the y-axis (vertical).\"\"\"\n    width: int = 0\n    \"\"\"The width of the region.\"\"\"\n    height: int = 0\n    \"\"\"The height of the region.\"\"\"\n\n    @classmethod\n    def from_union(cls, regions: Collection[Region]) -> Region:\n        \"\"\"Create a Region from the union of other regions.\n\n        Args:\n            regions: One or more regions.\n\n        Returns:\n            A Region that encloses all other regions.\n        \"\"\"\n        if not regions:\n            raise ValueError(\"At least one region expected\")\n        min_x = min(regions, key=itemgetter(0)).x\n        max_x = max(regions, key=attrgetter(\"right\")).right\n        min_y = min(regions, key=itemgetter(1)).y\n        max_y = max(regions, key=attrgetter(\"bottom\")).bottom\n        return cls(min_x, min_y, max_x - min_x, max_y - min_y)\n\n    @classmethod\n    def from_corners(cls, x1: int, y1: int, x2: int, y2: int) -> Region:\n        \"\"\"Construct a Region form the top left and bottom right corners.\n\n        Args:\n            x1: Top left x.\n            y1: Top left y.\n            x2: Bottom right x.\n            y2: Bottom right y.\n\n        Returns:\n            A new region.\n        \"\"\"\n        return cls(x1, y1, x2 - x1, y2 - y1)\n\n    @classmethod\n    def from_offset(cls, offset: tuple[int, int], size: tuple[int, int]) -> Region:\n        \"\"\"Create a region from offset and size.\n\n        Args:\n            offset: Offset (top left point).\n            size: Dimensions of region.\n\n        Returns:\n            A region instance.\n        \"\"\"\n        x, y = offset\n        width, height = size\n        return cls(x, y, width, height)\n\n    @classmethod\n    def get_scroll_to_visible(\n        cls, window_region: Region, region: Region, *, top: bool = False\n    ) -> Offset:\n        \"\"\"Calculate the smallest offset required to translate a window so that it contains\n        another region.\n\n        This method is used to calculate the required offset to scroll something in to view.\n\n        Args:\n            window_region: The window region.\n            region: The region to move inside the window.\n            top: Get offset to top of window.\n\n        Returns:\n            An offset required to add to region to move it inside window_region.\n        \"\"\"\n\n        if region in window_region and not top:\n            # Region is already inside the window, so no need to move it.\n            return NULL_OFFSET\n\n        window_left, window_top, window_right, window_bottom = window_region.corners\n        region = region.crop_size(window_region.size)\n        left, top_, right, bottom = region.corners\n        delta_x = delta_y = 0\n\n        if not (\n            (window_right > left >= window_left)\n            and (window_right > right >= window_left)\n        ):\n            # The region does not fit\n            # The window needs to scroll on the X axis to bring region in to view\n            delta_x = min(\n                left - window_left,\n                left - (window_right - region.width),\n                key=abs,\n            )\n\n        if top:\n            delta_y = top_ - window_top\n\n        elif not (\n            (window_bottom > top_ >= window_top)\n            and (window_bottom > bottom >= window_top)\n        ):\n            # The window needs to scroll on the Y axis to bring region in to view\n            delta_y = min(\n                top_ - window_top,\n                top_ - (window_bottom - region.height),\n                key=abs,\n            )\n        return Offset(delta_x, delta_y)\n\n    def __bool__(self) -> bool:\n        \"\"\"A Region is considered False when it has no area.\"\"\"\n        _, _, width, height = self\n        return width * height > 0\n\n    @property\n    def column_span(self) -> tuple[int, int]:\n        \"\"\"A pair of integers for the start and end columns (x coordinates) in this region.\n\n        The end value is *exclusive*.\n        \"\"\"\n        return (self.x, self.x + self.width)\n\n    @property\n    def line_span(self) -> tuple[int, int]:\n        \"\"\"A pair of integers for the start and end lines (y coordinates) in this region.\n\n        The end value is *exclusive*.\n        \"\"\"\n        return (self.y, self.y + self.height)\n\n    @property\n    def right(self) -> int:\n        \"\"\"Maximum X value (non inclusive).\"\"\"\n        return self.x + self.width\n\n    @property\n    def bottom(self) -> int:\n        \"\"\"Maximum Y value (non inclusive).\"\"\"\n        return self.y + self.height\n\n    @property\n    def area(self) -> int:\n        \"\"\"The area under the region.\"\"\"\n        return self.width * self.height\n\n    @property\n    def offset(self) -> Offset:\n        \"\"\"The top left corner of the region.\n\n        Returns:\n            An offset.\n        \"\"\"\n        return Offset(*self[:2])\n\n    @property\n    def center(self) -> tuple[float, float]:\n        \"\"\"The center of the region.\n\n        Note, that this does *not* return an `Offset`, because the center may not be an integer coordinate.\n\n        Returns:\n            Tuple of floats.\n        \"\"\"\n        x, y, width, height = self\n        return (x + width / 2.0, y + height / 2.0)\n\n    @property\n    def bottom_left(self) -> Offset:\n        \"\"\"Bottom left offset of the region.\n\n        Returns:\n            An offset.\n        \"\"\"\n        x, y, _width, height = self\n        return Offset(x, y + height)\n\n    @property\n    def top_right(self) -> Offset:\n        \"\"\"Top right offset of the region.\n\n        Returns:\n            An offset.\n        \"\"\"\n        x, y, width, _height = self\n        return Offset(x + width, y)\n\n    @property\n    def bottom_right(self) -> Offset:\n        \"\"\"Bottom right offset of the region.\n\n        Returns:\n            An offset.\n        \"\"\"\n        x, y, width, height = self\n        return Offset(x + width, y + height)\n\n    @property\n    def size(self) -> Size:\n        \"\"\"Get the size of the region.\"\"\"\n        return Size(*self[2:])\n\n    @property\n    def corners(self) -> tuple[int, int, int, int]:\n        \"\"\"The top left and bottom right coordinates as a tuple of four integers.\"\"\"\n        x, y, width, height = self\n        return x, y, x + width, y + height\n\n    @property\n    def column_range(self) -> range:\n        \"\"\"A range object for X coordinates.\"\"\"\n        return range(self.x, self.x + self.width)\n\n    @property\n    def line_range(self) -> range:\n        \"\"\"A range object for Y coordinates.\"\"\"\n        return range(self.y, self.y + self.height)\n\n    @property\n    def reset_offset(self) -> Region:\n        \"\"\"An region of the same size at (0, 0).\n\n        Returns:\n            A region at the origin.\n        \"\"\"\n        _, _, width, height = self\n        return Region(0, 0, width, height)\n\n    def __add__(self, other: object) -> Region:\n        if isinstance(other, tuple):\n            ox, oy = other\n            x, y, width, height = self\n            return Region(x + ox, y + oy, width, height)\n        return NotImplemented\n\n    def __sub__(self, other: object) -> Region:\n        if isinstance(other, tuple):\n            ox, oy = other\n            x, y, width, height = self\n            return Region(x - ox, y - oy, width, height)\n        return NotImplemented\n\n    def at_offset(self, offset: tuple[int, int]) -> Region:\n        \"\"\"Get a new Region with the same size at a given offset.\n\n        Args:\n            offset: An offset.\n\n        Returns:\n            New Region with adjusted offset.\n        \"\"\"\n        x, y = offset\n        _x, _y, width, height = self\n        return Region(x, y, width, height)\n\n    def crop_size(self, size: tuple[int, int]) -> Region:\n        \"\"\"Get a region with the same offset, with a size no larger than `size`.\n\n        Args:\n            size: Maximum width and height (WIDTH, HEIGHT).\n\n        Returns:\n            New region that could fit within `size`.\n        \"\"\"\n        x, y, width1, height1 = self\n        width2, height2 = size\n        return Region(x, y, min(width1, width2), min(height1, height2))\n\n    def expand(self, size: tuple[int, int]) -> Region:\n        \"\"\"Increase the size of the region by adding a border.\n\n        Args:\n            size: Additional width and height.\n\n        Returns:\n            A new region.\n        \"\"\"\n        expand_width, expand_height = size\n        x, y, width, height = self\n        return Region(\n            x - expand_width,\n            y - expand_height,\n            width + expand_width * 2,\n            height + expand_height * 2,\n        )\n\n    def clip_size(self, size: tuple[int, int]) -> Region:\n        \"\"\"Clip the size to fit within minimum values.\n\n        Args:\n            size: Maximum width and height.\n\n        Returns:\n            No region, not bigger than size.\n        \"\"\"\n        x, y, width, height = self\n        max_width, max_height = size\n        return Region(x, y, min(width, max_width), min(height, max_height))\n\n    @lru_cache(maxsize=1024)\n    def overlaps(self, other: Region) -> bool:\n        \"\"\"Check if another region overlaps this region.\n\n        Args:\n            other: A Region.\n\n        Returns:\n            True if other region shares any cells with this region.\n        \"\"\"\n        x, y, x2, y2 = self.corners\n        ox, oy, ox2, oy2 = other.corners\n\n        return ((x2 > ox >= x) or (x2 > ox2 > x) or (ox < x and ox2 >= x2)) and (\n            (y2 > oy >= y) or (y2 > oy2 > y) or (oy < y and oy2 >= y2)\n        )\n\n    def contains(self, x: int, y: int) -> bool:\n        \"\"\"Check if a point is in the region.\n\n        Args:\n            x: X coordinate.\n            y: Y coordinate.\n\n        Returns:\n            True if the point is within the region.\n        \"\"\"\n        self_x, self_y, width, height = self\n        return (self_x + width > x >= self_x) and (self_y + height > y >= self_y)\n\n    def contains_point(self, point: tuple[int, int]) -> bool:\n        \"\"\"Check if a point is in the region.\n\n        Args:\n            point: A tuple of x and y coordinates.\n\n        Returns:\n            True if the point is within the region.\n        \"\"\"\n        x1, y1, x2, y2 = self.corners\n        try:\n            ox, oy = point\n        except Exception:\n            raise TypeError(f\"a tuple of two integers is required, not {point!r}\")\n        return (x2 > ox >= x1) and (y2 > oy >= y1)\n\n    @lru_cache(maxsize=1024)\n    def contains_region(self, other: Region) -> bool:\n        \"\"\"Check if a region is entirely contained within this region.\n\n        Args:\n            other: A region.\n\n        Returns:\n            True if the other region fits perfectly within this region.\n        \"\"\"\n        x1, y1, x2, y2 = self.corners\n        ox, oy, ox2, oy2 = other.corners\n        return (\n            (x2 >= ox >= x1)\n            and (y2 >= oy >= y1)\n            and (x2 >= ox2 >= x1)\n            and (y2 >= oy2 >= y1)\n        )\n\n    @lru_cache(maxsize=1024)\n    def translate(self, offset: tuple[int, int]) -> Region:\n        \"\"\"Move the offset of the Region.\n\n        Args:\n            offset: Offset to add to region.\n\n        Returns:\n            A new region shifted by (x, y)\n        \"\"\"\n\n        self_x, self_y, width, height = self\n        offset_x, offset_y = offset\n        return Region(self_x + offset_x, self_y + offset_y, width, height)\n\n    @lru_cache(maxsize=4096)\n    def __contains__(self, other: Any) -> bool:\n        \"\"\"Check if a point is in this region.\"\"\"\n        if isinstance(other, Region):\n            return self.contains_region(other)\n        else:\n            try:\n                return self.contains_point(other)\n            except TypeError:\n                return False\n\n    def clip(self, width: int, height: int) -> Region:\n        \"\"\"Clip this region to fit within width, height.\n\n        Args:\n            width: Width of bounds.\n            height: Height of bounds.\n\n        Returns:\n            Clipped region.\n        \"\"\"\n        x1, y1, x2, y2 = self.corners\n\n        _clamp = clamp\n        new_region = Region.from_corners(\n            _clamp(x1, 0, width),\n            _clamp(y1, 0, height),\n            _clamp(x2, 0, width),\n            _clamp(y2, 0, height),\n        )\n        return new_region\n\n    @lru_cache(maxsize=4096)\n    def grow(self, margin: tuple[int, int, int,\n```\n# Function Signature: def clamp(self, width: int, height: int) -> Offset:\n# Natural Language Annotations: \"\"\"Clamp the offset to fit within a rectangle of width x height.\n\n        Args:\n            width: Width to clamp.\n            height: Height to clamp.\n\n        Returns:\n            A new offset.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef clamp(self, width: int, height: int", "  ```\ndef clamp(self, width: int, height: int", "  ```\ndef clamp(self, width: int, height: int"]}, "textual-src/textual/constants.py-_get_textual_animations": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/constants.py:\n```\n\"\"\"\nConstants that we might want to expose via the public API.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom typing import get_args\n\nfrom typing_extensions import Final, TypeGuard\n\nfrom ._types import AnimationLevel\n\nget_environ = os.environ.get\n\n\ndef _get_environ_bool(name: str) -> bool:\n    \"\"\"Check an environment variable switch.\n\n    Args:\n        name: Name of environment variable.\n\n    Returns:\n        `True` if the env var is \"1\", otherwise `False`.\n    \"\"\"\n    has_environ = get_environ(name) == \"1\"\n    return has_environ\n\n\ndef _get_environ_int(name: str, default: int) -> int:\n    \"\"\"Retrieves an integer environment variable.\n\n    Args:\n        name: Name of environment variable.\n        default: The value to use if the value is not set, or set to something other\n            than a valid integer.\n\n    Returns:\n        The integer associated with the environment variable if it's set to a valid int\n            or the default value otherwise.\n    \"\"\"\n    try:\n        return int(os.environ[name])\n    except KeyError:\n        return default\n    except ValueError:\n        return default\n\n\ndef _is_valid_animation_level(value: str) -> TypeGuard[AnimationLevel]:\n    \"\"\"Checks if a string is a valid animation level.\n\n    Args:\n        value: The string to check.\n\n    Returns:\n        Whether it's a valid level or not.\n    \"\"\"\n    return value in get_args(AnimationLevel)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDEBUG: Final[bool] = _get_environ_bool(\"TEXTUAL_DEBUG\")\n\"\"\"Enable debug mode.\"\"\"\n\nDRIVER: Final[str | None] = get_environ(\"TEXTUAL_DRIVER\", None)\n\"\"\"Import for replacement driver.\"\"\"\n\nFILTERS: Final[str] = get_environ(\"TEXTUAL_FILTERS\", \"\")\n\"\"\"A list of filters to apply to renderables.\"\"\"\n\nLOG_FILE: Final[str | None] = get_environ(\"TEXTUAL_LOG\", None)\n\"\"\"A last resort log file that appends all logs, when devtools isn't working.\"\"\"\n\nDEVTOOLS_HOST: Final[str] = get_environ(\"TEXTUAL_DEVTOOLS_HOST\", \"127.0.0.1\")\n\"\"\"The host where textual console is running.\"\"\"\n\nDEVTOOLS_PORT: Final[int] = _get_environ_int(\"TEXTUAL_DEVTOOLS_PORT\", 8081)\n\"\"\"Constant with the port that the devtools will connect to.\"\"\"\n\nSCREENSHOT_DELAY: Final[int] = _get_environ_int(\"TEXTUAL_SCREENSHOT\", -1)\n\"\"\"Seconds delay before taking screenshot.\"\"\"\n\nSCREENSHOT_LOCATION: Final[str | None] = get_environ(\"TEXTUAL_SCREENSHOT_LOCATION\")\n\"\"\"The location where screenshots should be written.\"\"\"\n\nSCREENSHOT_FILENAME: Final[str | None] = get_environ(\"TEXTUAL_SCREENSHOT_FILENAME\")\n\"\"\"The filename to use for the screenshot.\"\"\"\n\nPRESS: Final[str] = get_environ(\"TEXTUAL_PRESS\", \"\")\n\"\"\"Keys to automatically press.\"\"\"\n\nSHOW_RETURN: Final[bool] = _get_environ_bool(\"TEXTUAL_SHOW_RETURN\")\n\"\"\"Write the return value on exit.\"\"\"\n\nMAX_FPS: Final[int] = _get_environ_int(\"TEXTUAL_FPS\", 60)\n\"\"\"Maximum frames per second for updates.\"\"\"\n\nCOLOR_SYSTEM: Final[str | None] = get_environ(\"TEXTUAL_COLOR_SYSTEM\", \"auto\")\n\"\"\"Force color system override\"\"\"\n\nTEXTUAL_ANIMATIONS: AnimationLevel = _get_textual_animations()\n\"\"\"Determines whether animations run or not.\"\"\"\n\n```\n# Function Signature: def _get_textual_animations() -> AnimationLevel:\n# Natural Language Annotations: \"\"\"Get the value of the environment variable that controls textual animations.\n\n    The variable can be in any of the values defined by [`AnimationLevel`][textual.constants.AnimationLevel].\n\n    Returns:\n        The value that the variable was set to. If the environment variable is set to an\n            invalid value, we default to showing all animations.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _get_textual_animations() -> Animation", "  ```\nimport os\nfrom typing import Final\n\nfrom ._types", "  ```\ndef _get_textual_animations() -> Animation"]}, "textual-src/textual/color.py-parse": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/color.py:\n```\n\"\"\"\nThis module contains a powerful [Color][textual.color.Color] class which Textual uses to manipulate colors.\n\n## Named colors\n\nThe following named colors are used by the [parse][textual.color.Color.parse] method.\n\n\n```{.rich columns=\"80\" title=\"colors\"}\nfrom textual._color_constants import COLOR_NAME_TO_RGB\nfrom textual.color import Color\nfrom rich.table import Table\nfrom rich.text import Text\ntable = Table(\"Name\", \"hex\", \"RGB\", \"Color\", expand=True, highlight=True)\n\nfor name, triplet in sorted(COLOR_NAME_TO_RGB.items()):\n    if len(triplet) != 3:\n        continue\n    color = Color(*triplet)\n    r, g, b = triplet\n    table.add_row(\n        f'\"{name}\"',\n        Text(f\"{color.hex}\", \"bold green\"),\n        f\"rgb({r}, {g}, {b})\",\n        Text(\"                    \", style=f\"on rgb({r},{g},{b})\")\n    )\noutput = table\n```\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nfrom colorsys import hls_to_rgb, rgb_to_hls\nfrom functools import lru_cache\nfrom operator import itemgetter\nfrom typing import Callable, NamedTuple\n\nimport rich.repr\nfrom rich.color import Color as RichColor\nfrom rich.color import ColorType\nfrom rich.color_triplet import ColorTriplet\nfrom typing_extensions import Final\n\nfrom textual.css.scalar import percentage_string_to_float\nfrom textual.css.tokenize import CLOSE_BRACE, COMMA, DECIMAL, OPEN_BRACE, PERCENT\nfrom textual.suggestions import get_suggestion\n\nfrom ._color_constants import COLOR_NAME_TO_RGB\nfrom .geometry import clamp\n\n_TRUECOLOR = ColorType.TRUECOLOR\n\n\nclass HSL(NamedTuple):\n    \"\"\"A color in HLS (Hue, Saturation, Lightness) format.\"\"\"\n\n    h: float\n    \"\"\"Hue in range 0 to 1.\"\"\"\n    s: float\n    \"\"\"Saturation in range 0 to 1.\"\"\"\n    l: float\n    \"\"\"Lightness in range 0 to 1.\"\"\"\n\n    @property\n    def css(self) -> str:\n        \"\"\"HSL in css format.\"\"\"\n        h, s, l = self\n\n        def as_str(number: float) -> str:\n            \"\"\"Format a float.\"\"\"\n            return f\"{number:.1f}\".rstrip(\"0\").rstrip(\".\")\n\n        return f\"hsl({as_str(h*360)},{as_str(s*100)}%,{as_str(l*100)}%)\"\n\n\nclass HSV(NamedTuple):\n    \"\"\"A color in HSV (Hue, Saturation, Value) format.\"\"\"\n\n    h: float\n    \"\"\"Hue in range 0 to 1.\"\"\"\n    s: float\n    \"\"\"Saturation in range 0 to 1.\"\"\"\n    v: float\n    \"\"\"Value un range 0 to 1.\"\"\"\n\n\nclass Lab(NamedTuple):\n    \"\"\"A color in CIE-L*ab format.\"\"\"\n\n    L: float\n    \"\"\"Lightness in range 0 to 100.\"\"\"\n    a: float\n    \"\"\"A axis in range -127 to 128.\"\"\"\n    b: float\n    \"\"\"B axis in range -127 to 128.\"\"\"\n\n\nRE_COLOR = re.compile(\n    rf\"\"\"^\n\\#([0-9a-fA-F]{{3}})$|\n\\#([0-9a-fA-F]{{4}})$|\n\\#([0-9a-fA-F]{{6}})$|\n\\#([0-9a-fA-F]{{8}})$|\nrgb{OPEN_BRACE}({DECIMAL}{COMMA}{DECIMAL}{COMMA}{DECIMAL}){CLOSE_BRACE}$|\nrgba{OPEN_BRACE}({DECIMAL}{COMMA}{DECIMAL}{COMMA}{DECIMAL}{COMMA}{DECIMAL}){CLOSE_BRACE}$|\nhsl{OPEN_BRACE}({DECIMAL}{COMMA}{PERCENT}{COMMA}{PERCENT}){CLOSE_BRACE}$|\nhsla{OPEN_BRACE}({DECIMAL}{COMMA}{PERCENT}{COMMA}{PERCENT}{COMMA}{DECIMAL}){CLOSE_BRACE}$\n\"\"\",\n    re.VERBOSE,\n)\n\n# Fast way to split a string of 6 characters in to 3 pairs of 2 characters\n_split_pairs3: Callable[[str], tuple[str, str, str]] = itemgetter(\n    slice(0, 2), slice(2, 4), slice(4, 6)\n)\n# Fast way to split a string of 8 characters in to 4 pairs of 2 characters\n_split_pairs4: Callable[[str], tuple[str, str, str, str]] = itemgetter(\n    slice(0, 2), slice(2, 4), slice(4, 6), slice(6, 8)\n)\n\n\nclass ColorParseError(Exception):\n    \"\"\"A color failed to parse.\n\n    Args:\n        message: The error message\n        suggested_color: A close color we can suggest.\n    \"\"\"\n\n    def __init__(self, message: str, suggested_color: str | None = None):\n        super().__init__(message)\n        self.suggested_color = suggested_color\n\n\n@rich.repr.auto\nclass Color(NamedTuple):\n    \"\"\"A class to represent a color.\n\n    Colors are stored as three values representing the degree of red, green, and blue in a color, and a\n    fourth \"alpha\" value which defines where the color lies on a gradient of opaque to transparent.\n\n    Example:\n        ```python\n        >>> from textual.color import Color\n        >>> color = Color.parse(\"red\")\n        >>> color\n        Color(255, 0, 0)\n        >>> color.darken(0.5)\n        Color(98, 0, 0)\n        >>> color + Color.parse(\"green\")\n        Color(0, 128, 0)\n        >>> color_with_alpha = Color(100, 50, 25, 0.5)\n        >>> color_with_alpha\n        Color(100, 50, 25, a=0.5)\n        >>> color + color_with_alpha\n        Color(177, 25, 12)\n        ```\n    \"\"\"\n\n    r: int\n    \"\"\"Red component in range 0 to 255.\"\"\"\n    g: int\n    \"\"\"Green component in range 0 to 255.\"\"\"\n    b: int\n    \"\"\"Blue component in range 0 to 255.\"\"\"\n    a: float = 1.0\n    \"\"\"Alpha (opacity) component in range 0 to 1.\"\"\"\n\n    @classmethod\n    def from_rich_color(cls, rich_color: RichColor) -> Color:\n        \"\"\"Create a new color from Rich's Color class.\n\n        Args:\n            rich_color: An instance of [Rich color][rich.color.Color].\n\n        Returns:\n            A new Color instance.\n        \"\"\"\n        r, g, b = rich_color.get_truecolor()\n        return cls(r, g, b)\n\n    @classmethod\n    def from_hsl(cls, h: float, s: float, l: float) -> Color:\n        \"\"\"Create a color from HLS components.\n\n        Args:\n            h: Hue.\n            l: Lightness.\n            s: Saturation.\n\n        Returns:\n            A new color.\n        \"\"\"\n        r, g, b = hls_to_rgb(h, l, s)\n        return cls(int(r * 255 + 0.5), int(g * 255 + 0.5), int(b * 255 + 0.5))\n\n    @property\n    def inverse(self) -> Color:\n        \"\"\"The inverse of this color.\n\n        Returns:\n            Inverse color.\n        \"\"\"\n        r, g, b, a = self\n        return Color(255 - r, 255 - g, 255 - b, a)\n\n    @property\n    def is_transparent(self) -> bool:\n        \"\"\"Is the color transparent (i.e. has 0 alpha)?\"\"\"\n        return self.a == 0\n\n    @property\n    def clamped(self) -> Color:\n        \"\"\"A clamped color (this color with all values in expected range).\"\"\"\n        r, g, b, a = self\n        _clamp = clamp\n        color = Color(\n            _clamp(r, 0, 255),\n            _clamp(g, 0, 255),\n            _clamp(b, 0, 255),\n            _clamp(a, 0.0, 1.0),\n        )\n        return color\n\n    @property\n    def rich_color(self) -> RichColor:\n        \"\"\"This color encoded in Rich's Color class.\n\n        Returns:\n            A color object as used by Rich.\n        \"\"\"\n        r, g, b, _a = self\n        return RichColor(\n            f\"#{r:02x}{g:02x}{b:02x}\", _TRUECOLOR, None, ColorTriplet(r, g, b)\n        )\n\n    @property\n    def normalized(self) -> tuple[float, float, float]:\n        \"\"\"A tuple of the color components normalized to between 0 and 1.\n\n        Returns:\n            Normalized components.\n        \"\"\"\n        r, g, b, _a = self\n        return (r / 255, g / 255, b / 255)\n\n    @property\n    def rgb(self) -> tuple[int, int, int]:\n        \"\"\"The red, green, and blue color components as a tuple of ints.\"\"\"\n        r, g, b, _ = self\n        return (r, g, b)\n\n    @property\n    def hsl(self) -> HSL:\n        \"\"\"This color in HSL format.\n\n        HSL color is an alternative way of representing a color, which can be used in certain color calculations.\n\n        Returns:\n            Color encoded in HSL format.\n        \"\"\"\n        r, g, b = self.normalized\n        h, l, s = rgb_to_hls(r, g, b)\n        return HSL(h, s, l)\n\n    @property\n    def brightness(self) -> float:\n        \"\"\"The human perceptual brightness.\n\n        A value of 1 is returned for pure white, and 0 for pure black.\n        Other colors lie on a gradient between the two extremes.\n        \"\"\"\n        r, g, b = self.normalized\n        brightness = (299 * r + 587 * g + 114 * b) / 1000\n        return brightness\n\n    @property\n    def hex(self) -> str:\n        \"\"\"The color in CSS hex form, with 6 digits for RGB, and 8 digits for RGBA.\n\n        For example, `\"#46b3de\"` for an RGB color, or `\"#3342457f\"` for a color with alpha.\n        \"\"\"\n        r, g, b, a = self.clamped\n        return (\n            f\"#{r:02X}{g:02X}{b:02X}\"\n            if a == 1\n            else f\"#{r:02X}{g:02X}{b:02X}{int(a*255):02X}\"\n        )\n\n    @property\n    def hex6(self) -> str:\n        \"\"\"The color in CSS hex form, with 6 digits for RGB. Alpha is ignored.\n\n        For example, `\"#46b3de\"`.\n        \"\"\"\n        r, g, b, _a = self.clamped\n        return f\"#{r:02X}{g:02X}{b:02X}\"\n\n    @property\n    def css(self) -> str:\n        \"\"\"The color in CSS RGB or RGBA form.\n\n        For example, `\"rgb(10,20,30)\"` for an RGB color, or `\"rgb(50,70,80,0.5)\"` for an RGBA color.\n        \"\"\"\n        r, g, b, a = self\n        return f\"rgb({r},{g},{b})\" if a == 1 else f\"rgba({r},{g},{b},{a})\"\n\n    @property\n    def monochrome(self) -> Color:\n        \"\"\"A monochrome version of this color.\n\n        Returns:\n            The monochrome (black and white) version of this color.\n        \"\"\"\n        r, g, b, a = self\n        gray = round(r * 0.2126 + g * 0.7152 + b * 0.0722)\n        return Color(gray, gray, gray, a)\n\n    def __rich_repr__(self) -> rich.repr.Result:\n        r, g, b, a = self\n        yield r\n        yield g\n        yield b\n        yield \"a\", a, 1.0\n\n    def with_alpha(self, alpha: float) -> Color:\n        \"\"\"Create a new color with the given alpha.\n\n        Args:\n            alpha: New value for alpha.\n\n        Returns:\n            A new color.\n        \"\"\"\n        r, g, b, _ = self\n        return Color(r, g, b, alpha)\n\n    def multiply_alpha(self, alpha: float) -> Color:\n        \"\"\"Create a new color, multiplying the alpha by a constant.\n\n        Args:\n            alpha: A value to multiple the alpha by (expected to be in the range 0 to 1).\n\n        Returns:\n            A new color.\n        \"\"\"\n        r, g, b, a = self\n        return Color(r, g, b, a * alpha)\n\n    @lru_cache(maxsize=1024)\n    def blend(\n        self, destination: Color, factor: float, alpha: float | None = None\n    ) -> Color:\n        \"\"\"Generate a new color between two colors.\n\n        This method calculates a new color on a gradient.\n        The position on the gradient is given by `factor`, which is a float between 0 and 1, where 0 is the original color, and 1 is the `destination` color.\n        A value of `gradient` between the two extremes produces a color somewhere between the two end points.\n\n        Args:\n            destination: Another color.\n            factor: A blend factor, 0 -> 1.\n            alpha: New alpha for result.\n\n        Returns:\n            A new color.\n        \"\"\"\n        if factor <= 0:\n            return self\n        elif factor >= 1:\n            return destination\n        r1, g1, b1, a1 = self\n        r2, g2, b2, a2 = destination\n\n        if alpha is None:\n            new_alpha = a1 + (a2 - a1) * factor\n        else:\n            new_alpha = alpha\n\n        return Color(\n            int(r1 + (r2 - r1) * factor),\n            int(g1 + (g2 - g1) * factor),\n            int(b1 + (b2 - b1) * factor),\n            new_alpha,\n        )\n\n    def __add__(self, other: object) -> Color:\n        if isinstance(other, Color):\n            return self.blend(other, other.a, 1.0)\n        return NotImplemented\n\n    @classmethod\n    @lru_cache(maxsize=1024 * 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @lru_cache(maxsize=1024)\n    def darken(self, amount: float, alpha: float | None = None) -> Color:\n        \"\"\"Darken the color by a given amount.\n\n        Args:\n            amount: Value between 0-1 to reduce luminance by.\n            alpha: Alpha component for new color or None to copy alpha.\n\n        Returns:\n            New color.\n        \"\"\"\n        l, a, b = rgb_to_lab(self)\n        l -= amount * 100\n        return lab_to_rgb(Lab(l, a, b), self.a if alpha is None else alpha).clamped\n\n    def lighten(self, amount: float, alpha: float | None = None) -> Color:\n        \"\"\"Lighten the color by a given amount.\n\n        Args:\n            amount: Value between 0-1 to increase luminance by.\n            alpha: Alpha component for new color or None to copy alpha.\n\n        Returns:\n            New color.\n        \"\"\"\n        return self.darken(-amount, alpha)\n\n    @lru_cache(maxsize=1024)\n    def get_contrast_text(self, alpha: float = 0.95) -> Color:\n        \"\"\"Get a light or dark color that best contrasts this color, for use with text.\n\n        Args:\n            alpha: An alpha value to apply to the result.\n\n        Returns:\n            A new color, either an off-white or off-black.\n        \"\"\"\n        return (WHITE if self.brightness < 0.5 else BLACK).with_alpha(alpha)\n\n\nclass Gradient:\n    \"\"\"Defines a color gradient.\"\"\"\n\n    def __init__(self, *stops: tuple[float, Color]) -> None:\n        \"\"\"Create a color gradient that blends colors to form a spectrum.\n\n        A gradient is defined by a sequence of \"stops\" consisting of a float and a color.\n        The stop indicate the color at that point on a spectrum between 0 and 1.\n\n        Args:\n            stops: A colors stop.\n\n        Raises:\n            ValueError: If any stops are missing (must be at least a stop for 0 and 1).\n        \"\"\"\n        self._stops = sorted(stops)\n        if len(stops) < 2:\n            raise ValueError(\"At least 2 stops required.\")\n        if self._stops[0][0] != 0.0:\n            raise ValueError(\"First stop must be 0.\")\n        if self._stops[-1][0] != 1.0:\n            raise ValueError(\"Last stop must be 1.\")\n\n    def get_color(self, position: float) -> Color:\n        \"\"\"Get a color from the gradient at a position between 0 and 1.\n\n        Positions that are between stops will return a blended color.\n\n        Args:\n            position: A number between 0 and 1, where 0 is the first stop, and 1 is the last.\n\n        Returns:\n            A color.\n        \"\"\"\n        # TODO: consider caching\n        position = clamp(position, 0.0, 1.0)\n        for (stop1, color1), (stop2, color2) in zip(self._stops, self._stops[1:]):\n            if stop2 >= position >= stop1:\n                return color1.blend(\n                    color2,\n                    (position - stop1) / (stop2 - stop1),\n                )\n        raise AssertionError(\"Can't get here if `_stops` is valid\")\n\n\n# Color constants\nWHITE: Final = Color(255, 255, 255)\n\"\"\"A constant for pure white.\"\"\"\nBLACK: Final = Color(0, 0, 0)\n\"\"\"A constant for pure black.\"\"\"\nTRANSPARENT: Final = Color.parse(\"transparent\")\n\"\"\"A constant for transparent.\"\"\"\n\n\ndef rgb_to_lab(rgb: Color) -> Lab:\n    \"\"\"Convert an RGB color to the CIE-L*ab format.\n\n    Uses the standard RGB color space with a D65/2\u2070 standard illuminant.\n    Conversion passes through the XYZ color space.\n    Cf. http://www.easyrgb.com/en/math.php.\n    \"\"\"\n\n    r, g, b = rgb.r / 255, rgb.g / 255, rgb.b / 255\n\n    r = pow((r + 0.055) / 1.055, 2.4) if r > 0.04045 else r / 12.92\n    g = pow((g + 0.055) / 1.055, 2.4) if g > 0.04045 else g / 12.92\n    b = pow((b + 0.055) / 1.055, 2.4) if b > 0.04045 else b / 12.92\n\n    x = (r * 41.24 + g * 35.76 + b * 18.05) / 95.047\n    y = (r * 21.26 + g * 71.52 + b * 7.22) / 100\n    z = (r * 1.93 + g * 11.92 + b * 95.05) / 108.883\n\n    off = 16 / 116\n    x = pow(x, 1 / 3) if x > 0.008856 else 7.787 * x + off\n    y = pow(y, 1 / 3) if y > 0.008856 else 7.787 * y + off\n    z = pow(z, 1 / 3) if z > 0.008856 else 7.787 * z + off\n\n    return Lab(116 * y - 16, 500 * (x - y), 200 * (y - z))\n\n\ndef lab_to_rgb(lab: Lab, alpha: float = 1.0) -> Color:\n    \"\"\"Convert a CIE-L*ab color to RGB.\n\n    Uses the standard RGB color space with a D65/2\u2070 standard illuminant.\n    Conversion passes through the XYZ color space.\n    Cf. http://www.easyrgb.com/en/math.php.\n    \"\"\"\n\n    y = (lab.L + 16) / 116\n    x = lab.a / 500 + y\n    z = y - lab.b / 200\n\n    off = 16 / 116\n    y = pow(y, 3) if y > 0.2068930344 else (y - off) / 7.787\n    x = 0.95047 * pow(x, 3) if x > 0.2068930344 else 0.122059 * (x - off)\n    z = 1.08883 * pow(z, 3) if z > 0.2068930344 else 0.139827 * (z - off)\n\n    r = x * 3.2406 + y * -1.5372 + z * -0.4986\n    g = x * -0.9689 + y * 1.8758 + z * 0.0415\n    b = x * 0.0557 + y * -0.2040 + z * 1.0570\n\n    r = 1.055 * pow(r, 1 / 2.4) - 0.055 if r > 0.0031308 else 12.92 * r\n    g = 1.055 * pow(g, 1 / 2.4) - 0.055 if g > 0.0031308 else 12.92 * g\n    b = 1.055 * pow(b, 1 / 2.4) - 0.055 if b > 0.0031308 else 12.92 * b\n\n    return Color(int(r * 255), int(g * 255), int(b * 255), alpha)\n\n```\n# Function Signature: def parse(cls, color_text: str | Color) -> Color:\n# Natural Language Annotations: \"\"\"Parse a string containing a named color or CSS-style color.\n\n        Colors may be parsed from the following formats:\n\n        - Text beginning with a `#` is parsed as a hexadecimal color code,\n         where R, G, B, and A must be hexadecimal digits (0-9A-F):\n\n            - `#RGB`\n            - `#RGBA`\n            - `#RRGGBB`\n            - `#RRGGBBAA`\n\n        - Alternatively, RGB colors can also be specified in the format\n         that follows, where R, G, and B must be numbers between 0 and 255\n         and A must be a value between 0 and 1:\n\n            - `rgb(R,G,B)`\n            - `rgb(R,G,B,A)`\n\n        - The HSL model can also be used, with a syntax similar to the above,\n         if H is a value between 0 and 360, S and L are percentages, and A\n         is a value between 0 and 1:\n\n            - `hsl(H,S,L)`\n            - `hsla(H,S,L,A)`\n\n        Any other formats will raise a `ColorParseError`.\n\n        Args:\n            color_text: Text with a valid color format. Color objects will\n                be returned unmodified.\n\n        Raises:\n            ColorParseError: If the color is not encoded correctly.\n\n        Returns:\n            Instance encoding the color specified by the argument.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfrom textual import Color \n\nclass Color:\n   ", "  ```\n# def parse(cls, color_text: str | Color", "  ```\ndef parse(cls, color_text: str | Color)"]}, "textual-src/textual/_slug.py-slug": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/_slug.py:\n```\n\"\"\"Provides a utility function and class for creating Markdown-friendly slugs.\n\nThe approach to creating slugs is designed to be as close to\nGitHub-flavoured Markdown as possible. However, because there doesn't appear\nto be any actual documentation for this 'standard', the code here involves\nsome guesswork and also some pragmatic shortcuts.\n\nExpect this to grow over time.\n\nThe main rules used in here at the moment are:\n\n1. Strip all leading and trailing whitespace.\n2. Remove all non-lingual characters (emoji, etc).\n3. Remove all punctuation and whitespace apart from dash and underscore.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections import defaultdict\nfrom re import compile\nfrom string import punctuation\nfrom typing import Pattern\nfrom urllib.parse import quote\n\nfrom typing_extensions import Final\n\nWHITESPACE_REPLACEMENT: Final[str] = \"-\"\n\"\"\"The character to replace undesirable characters with.\"\"\"\n\nREMOVABLE: Final[str] = punctuation.replace(WHITESPACE_REPLACEMENT, \"\").replace(\"_\", \"\")\n\"\"\"The collection of characters that should be removed altogether.\"\"\"\n\nNONLINGUAL: Final[str] = (\n    r\"\\U000024C2-\\U0001F251\"\n    r\"\\U00002702-\\U000027B0\"\n    r\"\\U0001F1E0-\\U0001F1FF\"\n    r\"\\U0001F300-\\U0001F5FF\"\n    r\"\\U0001F600-\\U0001F64F\"\n    r\"\\U0001F680-\\U0001F6FF\"\n    r\"\\U0001f926-\\U0001f937\"\n    r\"\\u200D\"\n    r\"\\u2640-\\u2642\"\n)\n\"\"\"A string that can be used in a regular expression to remove most non-lingual characters.\"\"\"\n\nSTRIP_RE: Final[Pattern] = compile(f\"[{REMOVABLE}{NONLINGUAL}]+\")\n\"\"\"A regular expression for finding all the characters that should be removed.\"\"\"\n\nWHITESPACE_RE: Final[Pattern] = compile(r\"\\s\")\n\"\"\"A regular expression for finding all the whitespace and turning it into `REPLACEMENT`.\"\"\"\n\n\ndef slug(text: str) -> str:\n    \"\"\"Create a Markdown-friendly slug from the given text.\n\n    Args:\n        text: The text to generate a slug from.\n\n    Returns:\n        A slug for the given text.\n\n    The rules used in generating the slug are based on observations of how\n    GitHub-flavoured Markdown works.\n    \"\"\"\n    result = text.strip().lower()\n    for rule, replacement in (\n        (STRIP_RE, \"\"),\n        (WHITESPACE_RE, WHITESPACE_REPLACEMENT),\n    ):\n        result = rule.sub(replacement, result)\n    return quote(result)\n\n\nclass TrackedSlugs:\n    \"\"\"Provides a class for generating tracked slugs.\n\n    While [`slug`][textual._slug.slug] will generate a slug for a given\n    string, it does not guarantee that it is unique for a given context. If\n    you want to ensure that the same string generates unique slugs (perhaps\n    heading slugs within a Markdown document, as an example), use an\n    instance of this class to generate them.\n\n    Example:\n        ```python\n        >>> slug(\"hello world\")\n        'hello-world'\n        >>> slug(\"hello world\")\n        'hello-world'\n        >>> unique = TrackedSlugs()\n        >>> unique.slug(\"hello world\")\n        'hello-world'\n        >>> unique.slug(\"hello world\")\n        'hello-world-1'\n        ```\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialise the tracked slug object.\"\"\"\n        self._used: defaultdict[str, int] = defaultdict(int)\n        \"\"\"Keeps track of how many times a particular slug has been used.\"\"\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def slug(self, text: str) -> str:\n# Natural Language Annotations: \"\"\"Create a Markdown-friendly unique slug from the given text.\n\n        Args:\n            text: The text to generate a slug from.\n\n        Returns:\n            A slug for the given text.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef slug(self, text: str) -> str:", "  ```\ndef slug(self, text: str) -> str:", "  ```\ndef slug(self, text: str) -> str:"]}, "textual-src/textual/_styles_cache.py-render": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/_styles_cache.py:\n```\nfrom __future__ import annotations\n\nfrom functools import lru_cache\nfrom sys import intern\nfrom typing import TYPE_CHECKING, Callable, Iterable, Sequence\n\nfrom rich.console import Console\nfrom rich.segment import Segment\nfrom rich.style import Style\nfrom rich.text import Text\n\nfrom . import log\nfrom ._ansi_theme import DEFAULT_TERMINAL_THEME\nfrom ._border import get_box, render_border_label, render_row\nfrom ._context import active_app\nfrom ._opacity import _apply_opacity\nfrom ._segment_tools import apply_hatch, line_pad, line_trim\nfrom .color import Color\nfrom .constants import DEBUG\nfrom .filter import LineFilter\nfrom .geometry import Region, Size, Spacing\nfrom .renderables.text_opacity import TextOpacity\nfrom .renderables.tint import Tint\nfrom .strip import Strip\n\nif TYPE_CHECKING:\n    from typing_extensions import TypeAlias\n\n    from .css.styles import StylesBase\n    from .widget import Widget\n\nRenderLineCallback: TypeAlias = Callable[[int], Strip]\n\n\n@lru_cache(1024 * 8)\ndef make_blank(width, style: Style) -> Segment:\n    \"\"\"Make a blank segment.\n\n    Args:\n        width: Width of blank.\n        style: Style of blank.\n\n    Returns:\n        A single segment\n    \"\"\"\n    return Segment(intern(\" \" * width), style)\n\n\nclass StylesCache:\n    \"\"\"Responsible for rendering CSS Styles and keeping a cache of rendered lines.\n\n    The render method applies border, outline, and padding set in the Styles object to widget content.\n\n    The diagram below shows content (possibly from a Rich renderable) with padding and border. The\n    labels A. B. and C. indicate the code path (see comments in render_line below) chosen to render\n    the indicated lines.\n\n    ```\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\u25c0\u2500\u2500 A. border\n    \u2503                      \u2503\u25c0\u2510\n    \u2503                      \u2503 \u2514\u2500 B. border + padding +\n    \u2503   Lorem ipsum dolor  \u2503\u25c0\u2510         border\n    \u2503   sit amet,          \u2503 \u2502\n    \u2503   consectetur        \u2503 \u2514\u2500 C. border + padding +\n    \u2503   adipiscing elit,   \u2503     content + padding +\n    \u2503   sed do eiusmod     \u2503           border\n    \u2503   tempor incididunt  \u2503\n    \u2503                      \u2503\n    \u2503                      \u2503\n    \u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n    ```\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._cache: dict[int, Strip] = {}\n        self._dirty_lines: set[int] = set()\n        self._width = 1\n\n    def set_dirty(self, *regions: Region) -> None:\n        \"\"\"Add a dirty regions.\"\"\"\n        if regions:\n            for region in regions:\n                self._dirty_lines.update(region.line_range)\n        else:\n            self.clear()\n\n    def is_dirty(self, y: int) -> bool:\n        \"\"\"Check if a given line is dirty (needs to be rendered again).\n\n        Args:\n            y: Y coordinate of line.\n\n        Returns:\n            True if line requires a render, False if can be cached.\n        \"\"\"\n        return y in self._dirty_lines\n\n    def clear(self) -> None:\n        \"\"\"Clear the styles cache (will cause the content to re-render).\"\"\"\n        self._cache.clear()\n        self._dirty_lines.clear()\n\n    def render_widget(self, widget: Widget, crop: Region) -> list[Strip]:\n        \"\"\"Render the content for a widget.\n\n        Args:\n            widget: A widget.\n            region: A region of the widget to render.\n\n        Returns:\n            Rendered lines.\n        \"\"\"\n\n        border_title = widget._border_title\n        border_subtitle = widget._border_subtitle\n\n        base_background, background = widget._opacity_background_colors\n        styles = widget.styles\n        strips = self.render(\n            styles,\n            widget.region.size,\n            base_background,\n            background,\n            widget.render_line,\n            widget.app.console,\n            (\n                None\n                if border_title is None\n                else (\n                    border_title,\n                    *widget._get_title_style_information(base_background),\n                )\n            ),\n            (\n                None\n                if border_subtitle is None\n                else (\n                    border_subtitle,\n                    *widget._get_subtitle_style_information(base_background),\n                )\n            ),\n            content_size=widget.content_region.size,\n            padding=styles.padding,\n            crop=crop,\n            filters=widget.app._filters,\n            opacity=widget.opacity,\n        )\n        if widget.auto_links:\n            hover_style = widget.hover_style\n            if (\n                hover_style._link_id\n                and hover_style._meta\n                and \"@click\" in hover_style.meta\n            ):\n                link_style_hover = widget.link_style_hover\n                if link_style_hover:\n                    strips = [\n                        strip.style_links(hover_style.link_id, link_style_hover)\n                        for strip in strips\n                    ]\n\n        return strips\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def render_line(\n        self,\n        styles: StylesBase,\n        y: int,\n        size: Size,\n        content_size: Size,\n        padding: Spacing,\n        base_background: Color,\n        background: Color,\n        render_content_line: Callable[[int], Strip],\n        console: Console,\n        border_title: tuple[Text, Color, Color, Style] | None,\n        border_subtitle: tuple[Text, Color, Color, Style] | None,\n        opacity: float,\n    ) -> Strip:\n        \"\"\"Render a styled line.\n\n        Args:\n            styles: Styles object.\n            y: The y coordinate of the line (relative to widget screen offset).\n            size: Size of the widget.\n            content_size: Size of the content area.\n            padding: Padding.\n            base_background: Background color of widget beneath this line.\n            background: Background color of widget.\n            render_content_line: Callback to render a line of content.\n            console: The console in use by the app.\n            border_title: Optional tuple of (title, color, background, style).\n            border_subtitle: Optional tuple of (subtitle, color, background, style).\n            opacity: Opacity of line.\n\n        Returns:\n            A line of segments.\n        \"\"\"\n\n        gutter = styles.gutter\n        width, height = size\n        content_width, content_height = content_size\n\n        pad_top, pad_right, pad_bottom, pad_left = padding\n\n        (\n            (border_top, border_top_color),\n            (border_right, border_right_color),\n            (border_bottom, border_bottom_color),\n            (border_left, border_left_color),\n        ) = styles.border\n\n        (\n            (outline_top, outline_top_color),\n            (outline_right, outline_right_color),\n            (outline_bottom, outline_bottom_color),\n            (outline_left, outline_left_color),\n        ) = styles.outline\n\n        from_color = Style.from_color\n\n        inner = from_color(bgcolor=(base_background + background).rich_color)\n        outer = from_color(bgcolor=base_background.rich_color)\n\n        def line_post(segments: Iterable[Segment]) -> Iterable[Segment]:\n            \"\"\"Apply effects to segments inside the border.\"\"\"\n            if styles.has_rule(\"hatch\"):\n                character, color = styles.hatch\n                if character != \" \" and color.a > 0:\n                    hatch_style = Style.from_color(\n                        (background + color).rich_color, background.rich_color\n                    )\n                    return apply_hatch(segments, character, hatch_style)\n            return segments\n\n        def post(segments: Iterable[Segment]) -> Iterable[Segment]:\n            \"\"\"Post process segments to apply opacity and tint.\n\n            Args:\n                segments: Iterable of segments.\n\n            Returns:\n                New list of segments\n            \"\"\"\n\n            try:\n                app = active_app.get()\n                ansi_theme = app.ansi_theme\n            except LookupError:\n                ansi_theme = DEFAULT_TERMINAL_THEME\n\n            if styles.tint.a:\n                segments = Tint.process_segments(segments, styles.tint, ansi_theme)\n            if opacity != 1.0:\n                segments = _apply_opacity(segments, base_background, opacity)\n            return segments\n\n        line: Iterable[Segment]\n        # Draw top or bottom borders (A)\n        if (border_top and y == 0) or (border_bottom and y == height - 1):\n            is_top = y == 0\n            border_color = base_background + (\n                border_top_color if is_top else border_bottom_color\n            ).multiply_alpha(opacity)\n            border_color_as_style = from_color(color=border_color.rich_color)\n            border_edge_type = border_top if is_top else border_bottom\n            has_left = border_left != \"\"\n            has_right = border_right != \"\"\n            border_label = border_title if is_top else border_subtitle\n            if border_label is None:\n                render_label = None\n            else:\n                label, label_color, label_background, style = border_label\n                base_label_background = base_background + background\n                style += Style.from_color(\n                    (\n                        (base_label_background + label_color).rich_color\n                        if label_color.a\n                        else None\n                    ),\n                    (\n                        (base_label_background + label_background).rich_color\n                        if label_background.a\n                        else None\n                    ),\n                )\n                render_label = (label, style)\n            # Try to save time with expensive call to `render_border_label`:\n            if render_label:\n                label_segments = render_border_label(\n                    render_label,\n                    is_top,\n                    border_edge_type,\n                    width - 2,\n                    inner,\n                    outer,\n                    border_color_as_style,\n                    console,\n                    has_left,\n                    has_right,\n                )\n            else:\n                label_segments = []\n            box_segments = get_box(\n                border_edge_type,\n                inner,\n                outer,\n                border_color_as_style,\n            )\n            label_alignment = (\n                styles.border_title_align if is_top else styles.border_subtitle_align\n            )\n            line = render_row(\n                box_segments[0 if is_top else 2],\n                width,\n                has_left,\n                has_right,\n                label_segments,\n                label_alignment,  # type: ignore\n            )\n\n        # Draw padding (B)\n        elif (pad_top and y < gutter.top) or (\n            pad_bottom and y >= height - gutter.bottom\n        ):\n            background_style = from_color(bgcolor=background.rich_color)\n            left_style = from_color(\n                color=(\n                    base_background + border_left_color.multiply_alpha(opacity)\n                ).rich_color\n            )\n            left = get_box(border_left, inner, outer, left_style)[1][0]\n            right_style = from_color(\n                color=(\n                    base_background + border_right_color.multiply_alpha(opacity)\n                ).rich_color\n            )\n            right = get_box(border_right, inner, outer, right_style)[1][2]\n            if border_left and border_right:\n                line = [left, make_blank(width - 2, background_style), right]\n            elif border_left:\n                line = [left, make_blank(width - 1, background_style)]\n            elif border_right:\n                line = [make_blank(width - 1, background_style), right]\n            else:\n                line = [make_blank(width, background_style)]\n            line = line_post(line)\n        else:\n            # Content with border and padding (C)\n            content_y = y - gutter.top\n            if content_y < content_height:\n                line = render_content_line(y - gutter.top)\n                line = line.adjust_cell_length(content_width)\n            else:\n                line = [make_blank(content_width, inner)]\n            if inner:\n                line = Segment.apply_style(line, inner)\n            if styles.text_opacity != 1.0:\n                line = TextOpacity.process_segments(line, styles.text_opacity)\n            line = line_post(line_pad(line, pad_left, pad_right, inner))\n\n            if border_left or border_right:\n                # Add left / right border\n                left_style = from_color(\n                    (\n                        base_background + border_left_color.multiply_alpha(opacity)\n                    ).rich_color\n                )\n                left = get_box(border_left, inner, outer, left_style)[1][0]\n                right_style = from_color(\n                    (\n                        base_background + border_right_color.multiply_alpha(opacity)\n                    ).rich_color\n                )\n                right = get_box(border_right, inner, outer, right_style)[1][2]\n\n                if border_left and border_right:\n                    line = [left, *line, right]\n                elif border_left:\n                    line = [left, *line]\n                else:\n                    line = [*line, right]\n\n        # Draw any outline\n        if (outline_top and y == 0) or (outline_bottom and y == height - 1):\n            # Top or bottom outlines\n            outline_color = outline_top_color if y == 0 else outline_bottom_color\n            box_segments = get_box(\n                outline_top if y == 0 else outline_bottom,\n                inner,\n                outer,\n                from_color(color=(base_background + outline_color).rich_color),\n            )\n            line = render_row(\n                box_segments[0 if y == 0 else 2],\n                width,\n                outline_left != \"\",\n                outline_right != \"\",\n                (),\n            )\n\n        elif outline_left or outline_right:\n            # Lines in side outline\n            left_style = from_color((base_background + outline_left_color).rich_color)\n            left = get_box(outline_left, inner, outer, left_style)[1][0]\n            right_style = from_color((base_background + outline_right_color).rich_color)\n            right = get_box(outline_right, inner, outer, right_style)[1][2]\n            line = line_trim(list(line), outline_left != \"\", outline_right != \"\")\n            if outline_left and outline_right:\n                line = [left, *line, right]\n            elif outline_left:\n                line = [left, *line]\n            else:\n                line = [*line, right]\n\n        strip = Strip(post(line), width)\n        return strip\n\n```\n# Function Signature: def render(\n# Natural Language Annotations: \"\"\"Render a widget content plus CSS styles.\n\n        Args:\n            styles: CSS Styles object.\n            size: Size of widget.\n            base_background: Background color beneath widget.\n            background: Background color of widget.\n            render_content_line: Callback to render content line.\n            console: The console in use by the app.\n            border_title: Optional tuple of (title, color, background, style).\n            border_subtitle: Optional tuple of (subtitle, color, background, style).\n            content_size: Size of content or None to assume full size.\n            padding: Override padding from Styles, or None to use styles.padding.\n            crop: Region to crop to.\n            filters: Additional post-processing for the segments.\n            opacity: Widget opacity.\n\n        Returns:\n            Rendered lines.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef render(\n    styles: StylesBase,\n   ", "  ```\ndef render(styles: StylesBase, size: Size,", "  ```\ndef render(\n    styles: StylesBase,\n   "]}, "textual-src/textual/document/_document.py-replace_range": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/document/_document.py:\n```\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom functools import lru_cache\nfrom typing import TYPE_CHECKING, NamedTuple, Tuple, overload\n\nfrom typing_extensions import Literal, get_args\n\nif TYPE_CHECKING:\n    from tree_sitter import Node\n    from tree_sitter.binding import Query\n\nfrom textual._cells import cell_len\nfrom textual.geometry import Size\n\nNewline = Literal[\"\\r\\n\", \"\\n\", \"\\r\"]\n\"\"\"The type representing valid line separators.\"\"\"\nVALID_NEWLINES = set(get_args(Newline))\n\"\"\"The set of valid line separator strings.\"\"\"\n\n\n@dataclass\nclass EditResult:\n    \"\"\"Contains information about an edit that has occurred.\"\"\"\n\n    end_location: Location\n    \"\"\"The new end Location after the edit is complete.\"\"\"\n    replaced_text: str\n    \"\"\"The text that was replaced.\"\"\"\n\n\n@lru_cache(maxsize=1024)\ndef _utf8_encode(text: str) -> bytes:\n    \"\"\"Encode the input text as utf-8 bytes.\n\n    The returned encoded bytes may be retrieved from a cache.\n\n    Args:\n        text: The text to encode.\n\n    Returns:\n        The utf-8 bytes representing the input string.\n    \"\"\"\n    return text.encode(\"utf-8\")\n\n\ndef _detect_newline_style(text: str) -> Newline:\n    \"\"\"Return the newline type used in this document.\n\n    Args:\n        text: The text to inspect.\n\n    Returns:\n        The Newline used in the file.\n    \"\"\"\n    if \"\\r\\n\" in text:  # Windows newline\n        return \"\\r\\n\"\n    elif \"\\n\" in text:  # Unix/Linux/MacOS newline\n        return \"\\n\"\n    elif \"\\r\" in text:  # Old MacOS newline\n        return \"\\r\"\n    else:\n        return \"\\n\"  # Default to Unix style newline\n\n\nclass DocumentBase(ABC):\n    \"\"\"Describes the minimum functionality a Document implementation must\n    provide in order to be used by the TextArea widget.\"\"\"\n\n    @abstractmethod\n    def replace_range(self, start: Location, end: Location, text: str) -> EditResult:\n        \"\"\"Replace the text at the given range.\n\n        Args:\n            start: A tuple (row, column) where the edit starts.\n            end: A tuple (row, column) where the edit ends.\n            text: The text to insert between start and end.\n\n        Returns:\n            The new end location after the edit is complete.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def text(self) -> str:\n        \"\"\"The text from the document as a string.\"\"\"\n\n    @property\n    @abstractmethod\n    def newline(self) -> Newline:\n        \"\"\"Return the line separator used in the document.\"\"\"\n\n    @property\n    @abstractmethod\n    def lines(self) -> list[str]:\n        \"\"\"Get the lines of the document as a list of strings.\n\n        The strings should *not* include newline characters. The newline\n        character used for the document can be retrieved via the newline\n        property.\n        \"\"\"\n\n    @abstractmethod\n    def get_line(self, index: int) -> str:\n        \"\"\"Returns the line with the given index from the document.\n\n        This is used in rendering lines, and will be called by the\n        TextArea for each line that is rendered.\n\n        Args:\n            index: The index of the line in the document.\n\n        Returns:\n            The str instance representing the line.\n        \"\"\"\n\n    @abstractmethod\n    def get_text_range(self, start: Location, end: Location) -> str:\n        \"\"\"Get the text that falls between the start and end locations.\n\n        Args:\n            start: The start location of the selection.\n            end: The end location of the selection.\n\n        Returns:\n            The text between start (inclusive) and end (exclusive).\n        \"\"\"\n\n    @abstractmethod\n    def get_size(self, indent_width: int) -> Size:\n        \"\"\"Get the size of the document.\n\n        The height is generally the number of lines, and the width\n        is generally the maximum cell length of all the lines.\n\n        Args:\n            indent_width: The width to use for tab characters.\n\n        Returns:\n            The Size of the document bounding box.\n        \"\"\"\n\n    def query_syntax_tree(\n        self,\n        query: Query,\n        start_point: tuple[int, int] | None = None,\n        end_point: tuple[int, int] | None = None,\n    ) -> list[tuple[Node, str]]:\n        \"\"\"Query the tree-sitter syntax tree.\n\n        The default implementation always returns an empty list.\n\n        To support querying in a subclass, this must be implemented.\n\n        Args:\n            query: The tree-sitter Query to perform.\n            start_point: The (row, column byte) to start the query at.\n            end_point: The (row, column byte) to end the query at.\n\n        Returns:\n            A tuple containing the nodes and text captured by the query.\n        \"\"\"\n        return []\n\n    def prepare_query(self, query: str) -> Query | None:\n        return None\n\n    @property\n    @abstractmethod\n    def line_count(self) -> int:\n        \"\"\"Returns the number of lines in the document.\"\"\"\n\n    @property\n    @abstractmethod\n    def start(self) -> Location:\n        \"\"\"Returns the location of the start of the document (0, 0).\"\"\"\n        return (0, 0)\n\n    @property\n    @abstractmethod\n    def end(self) -> Location:\n        \"\"\"Returns the location of the end of the document.\"\"\"\n\n    if TYPE_CHECKING:\n\n        @overload\n        def __getitem__(self, line_index: int) -> str: ...\n\n        @overload\n        def __getitem__(self, line_index: slice) -> list[str]: ...\n\n    @abstractmethod\n    def __getitem__(self, line_index: int | slice) -> str | list[str]:\n        \"\"\"Return the content of a line as a string, excluding newline characters.\n\n        Args:\n            line_index: The index or slice of the line(s) to retrieve.\n\n        Returns:\n            The line or list of lines requested.\n        \"\"\"\n\n\nclass Document(DocumentBase):\n    \"\"\"A document which can be opened in a TextArea.\"\"\"\n\n    def __init__(self, text: str) -> None:\n        self._newline = _detect_newline_style(text)\n        \"\"\"The type of newline used in the text.\"\"\"\n        self._lines: list[str] = text.splitlines(keepends=False)\n        \"\"\"The lines of the document, excluding newline characters.\n\n        If there's a newline at the end of the file, the final line is an empty string.\n        \"\"\"\n        if text.endswith(tuple(VALID_NEWLINES)) or not text:\n            self._lines.append(\"\")\n\n    @property\n    def lines(self) -> list[str]:\n        \"\"\"Get the document as a list of strings, where each string represents a line.\n\n        Newline characters are not included in at the end of the strings.\n\n        The newline character used in this document can be found via the `Document.newline` property.\n        \"\"\"\n        return self._lines\n\n    @property\n    def text(self) -> str:\n        \"\"\"Get the text from the document.\"\"\"\n        return self._newline.join(self._lines)\n\n    @property\n    def newline(self) -> Newline:\n        \"\"\"Get the Newline used in this document (e.g. '\\r\\n', '\\n'. etc.)\"\"\"\n        return self._newline\n\n    def get_size(self, tab_width: int) -> Size:\n        \"\"\"The Size of the document, taking into account the tab rendering width.\n\n        Args:\n            tab_width: The width to use for tab indents.\n\n        Returns:\n            The size (width, height) of the document.\n        \"\"\"\n        lines = self._lines\n        cell_lengths = [cell_len(line.expandtabs(tab_width)) for line in lines]\n        max_cell_length = max(cell_lengths, default=0)\n        height = len(lines)\n        return Size(max_cell_length, height)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def get_text_range(self, start: Location, end: Location) -> str:\n        \"\"\"Get the text that falls between the start and end locations.\n\n        Returns the text between `start` and `end`, including the appropriate\n        line separator character as specified by `Document._newline`. Note that\n        `_newline` is set automatically to the first line separator character\n        found in the document.\n\n        Args:\n            start: The start location of the selection.\n            end: The end location of the selection.\n\n        Returns:\n            The text between start (inclusive) and end (exclusive).\n        \"\"\"\n        if start == end:\n            return \"\"\n\n        top, bottom = sorted((start, end))\n        top_row, top_column = top\n        bottom_row, bottom_column = bottom\n        lines = self._lines\n        if top_row == bottom_row:\n            line = lines[top_row]\n            selected_text = line[top_column:bottom_column]\n        else:\n            start_line = lines[top_row]\n            end_line = lines[bottom_row] if bottom_row <= self.line_count - 1 else \"\"\n            selected_text = start_line[top_column:]\n            for row in range(top_row + 1, bottom_row):\n                selected_text += self._newline + lines[row]\n\n            if bottom_row < self.line_count:\n                selected_text += self._newline\n                selected_text += end_line[:bottom_column]\n\n        return selected_text\n\n    @property\n    def line_count(self) -> int:\n        \"\"\"Returns the number of lines in the document.\"\"\"\n        return len(self._lines)\n\n    @property\n    def start(self) -> Location:\n        \"\"\"Returns the location of the start of the document (0, 0).\"\"\"\n        return super().start\n\n    @property\n    def end(self) -> Location:\n        \"\"\"Returns the location of the end of the document.\"\"\"\n        last_line = self._lines[-1]\n        return (self.line_count - 1, len(last_line))\n\n    def get_index_from_location(self, location: Location) -> int:\n        \"\"\"Given a location, returns the index from the document's text.\n\n        Args:\n            location: The location in the document.\n\n        Returns:\n            The index in the document's text.\n        \"\"\"\n        row, column = location\n        index = row * len(self.newline) + column\n        for line_index in range(row):\n            index += len(self.get_line(line_index))\n        return index\n\n    def get_location_from_index(self, index: int) -> Location:\n        \"\"\"Given an index in the document's text, returns the corresponding location.\n\n        Args:\n            index: The index in the document's text.\n\n        Returns:\n            The corresponding location.\n        \"\"\"\n        column_index = 0\n        newline_length = len(self.newline)\n        for line_index in range(self.line_count):\n            next_column_index = (\n                column_index + len(self.get_line(line_index)) + newline_length\n            )\n            if index < next_column_index:\n                return (line_index, index - column_index)\n            elif index == next_column_index:\n                return (line_index + 1, 0)\n            column_index = next_column_index\n\n    def get_line(self, index: int) -> str:\n        \"\"\"Returns the line with the given index from the document.\n\n        Args:\n            index: The index of the line in the document.\n\n        Returns:\n            The string representing the line.\n        \"\"\"\n        line_string = self[index]\n        return line_string\n\n    @overload\n    def __getitem__(self, line_index: int) -> str: ...\n\n    @overload\n    def __getitem__(self, line_index: slice) -> list[str]: ...\n\n    def __getitem__(self, line_index: int | slice) -> str | list[str]:\n        \"\"\"Return the content of a line as a string, excluding newline characters.\n\n        Args:\n            line_index: The index or slice of the line(s) to retrieve.\n\n        Returns:\n            The line or list of lines requested.\n        \"\"\"\n        return self._lines[line_index]\n\n\nLocation = Tuple[int, int]\n\"\"\"A location (row, column) within the document. Indexing starts at 0.\"\"\"\n\n\nclass Selection(NamedTuple):\n    \"\"\"A range of characters within a document from a start point to the end point.\n    The location of the cursor is always considered to be the `end` point of the selection.\n    The selection is inclusive of the minimum point and exclusive of the maximum point.\n    \"\"\"\n\n    start: Location = (0, 0)\n    \"\"\"The start location of the selection.\n\n    If you were to click and drag a selection inside a text-editor, this is where you *started* dragging.\n    \"\"\"\n    end: Location = (0, 0)\n    \"\"\"The end location of the selection.\n\n    If you were to click and drag a selection inside a text-editor, this is where you *finished* dragging.\n    \"\"\"\n\n    @classmethod\n    def cursor(cls, location: Location) -> \"Selection\":\n        \"\"\"Create a Selection with the same start and end point - a \"cursor\".\n\n        Args:\n            location: The location to create the zero-width Selection.\n        \"\"\"\n        return cls(location, location)\n\n    @property\n    def is_empty(self) -> bool:\n        \"\"\"Return True if the selection has 0 width, i.e. it's just a cursor.\"\"\"\n        start, end = self\n        return start == end\n\n```\n# Function Signature: def replace_range(self, start: Location, end: Location, text: str) -> EditResult:\n# Natural Language Annotations: \"\"\"Replace text at the given range.\n\n        This is the only method by which a document may be updated.\n\n        Args:\n            start: A tuple (row, column) where the edit starts.\n            end: A tuple (row, column) where the edit ends.\n            text: The text to insert between start and end.\n\n        Returns:\n            The EditResult containing information about the completed\n                replace operation.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef replace_range(start: Location, end: Location,", "  ```\ndef replace_range(self, start: Location, end:", "  ```\ndef replace_range(self, start: Location, end:"]}, "textual-src/textual/document/_wrapped_document.py-wrap_range": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/document/_wrapped_document.py:\n```\nfrom __future__ import annotations\n\nfrom bisect import bisect_right\n\nfrom rich.text import Text\n\nfrom textual._cells import cell_len, cell_width_to_column_index\nfrom textual._wrap import compute_wrap_offsets\nfrom textual.document._document import DocumentBase, Location\nfrom textual.expand_tabs import expand_tabs_inline, get_tab_widths\nfrom textual.geometry import Offset, clamp\n\nVerticalOffset = int\nLineIndex = int\nSectionOffset = int\n\n\nclass WrappedDocument:\n    \"\"\"A view into a Document which wraps the document at a certain\n    width and can be queried to retrieve lines from the *wrapped* version\n    of the document.\n\n    Allows for incremental updates, ensuring that we only re-wrap ranges of the document\n    that were influenced by edits.\n    \"\"\"\n\n    def __init__(\n        self,\n        document: DocumentBase,\n        width: int = 0,\n        tab_width: int = 4,\n    ) -> None:\n        \"\"\"Construct a WrappedDocument.\n\n        By default, a WrappedDocument is wrapped with width=0 (no wrapping).\n        To wrap the document, use the wrap() method.\n\n        Args:\n            document: The document to wrap.\n            width: The width to wrap at.\n            tab_width: The maximum width to consider for tab characters.\n        \"\"\"\n        self.document = document\n        \"\"\"The document wrapping is performed on.\"\"\"\n\n        self._wrap_offsets: list[list[int]] = []\n        \"\"\"Maps line indices to the offsets within the line where wrapping\n        breaks should be added.\"\"\"\n\n        self._tab_width_cache: list[list[int]] = []\n        \"\"\"Maps line indices to a list of tab widths. `[[2, 4]]` means that on line 0, the first\n        tab has width 2, and the second tab has width 4.\"\"\"\n\n        self._offset_to_line_info: list[tuple[LineIndex, SectionOffset]] = []\n        \"\"\"Maps y_offsets (from the top of the document) to line_index and the offset\n        of the section within the line.\"\"\"\n\n        self._line_index_to_offsets: list[list[VerticalOffset]] = []\n        \"\"\"Maps line indices to all the vertical offsets which correspond to that line.\"\"\"\n\n        self._width: int = width\n        \"\"\"The width the document is currently wrapped at. This will correspond with\n        the value last passed into the `wrap` method.\"\"\"\n\n        self._tab_width: int = tab_width\n        \"\"\"The maximum width to expand tabs to when considering their widths.\"\"\"\n\n        self.wrap(width, tab_width)\n\n    @property\n    def wrapped(self) -> bool:\n        \"\"\"True if the content is wrapped. This is not the same as wrapping being \"enabled\".\n        For example, an empty document can have wrapping enabled, but no wrapping has actually\n        occurred.\n\n        In other words, this is True if the length of any line in the document is greater\n        than the available width.\"\"\"\n        return len(self._line_index_to_offsets) == len(self._offset_to_line_info)\n\n    def wrap(self, width: int, tab_width: int | None = None) -> None:\n        \"\"\"Wrap and cache all lines in the document.\n\n        Args:\n            width: The width to wrap at. 0 for no wrapping.\n            tab_width: The maximum width to consider for tab characters. If None,\n                reuse the  tab width.\n        \"\"\"\n        self._width = width\n        if tab_width:\n            self._tab_width = tab_width\n\n        # We're starting wrapping from scratch\n        new_wrap_offsets: list[list[int]] = []\n        offset_to_line_info: list[tuple[LineIndex, SectionOffset]] = []\n        line_index_to_offsets: list[list[VerticalOffset]] = []\n        line_tab_widths: list[list[int]] = []\n\n        append_wrap_offset = new_wrap_offsets.append\n        append_line_info = offset_to_line_info.append\n        append_line_offsets = line_index_to_offsets.append\n        append_line_tab_widths = line_tab_widths.append\n\n        current_offset = 0\n        tab_width = self._tab_width\n        for line_index, line in enumerate(self.document.lines):\n            tab_sections = get_tab_widths(line, tab_width)\n            wrap_offsets = (\n                compute_wrap_offsets(\n                    line,\n                    width,\n                    tab_size=tab_width,\n                    precomputed_tab_sections=tab_sections,\n                )\n                if width\n                else []\n            )\n            append_line_tab_widths([width for _, width in tab_sections])\n            append_wrap_offset(wrap_offsets)\n            append_line_offsets([])\n            for section_y_offset in range(len(wrap_offsets) + 1):\n                append_line_info((line_index, section_y_offset))\n                line_index_to_offsets[line_index].append(current_offset)\n                current_offset += 1\n\n        self._wrap_offsets = new_wrap_offsets\n        self._offset_to_line_info = offset_to_line_info\n        self._line_index_to_offsets = line_index_to_offsets\n        self._tab_width_cache = line_tab_widths\n\n    @property\n    def lines(self) -> list[list[str]]:\n        \"\"\"The lines of the wrapped version of the Document.\n\n        Each index in the returned list represents a line index in the raw\n        document. The list[str] at each index is the content of the raw document line\n        split into multiple lines via wrapping.\n\n        Note that this is expensive to compute and is not cached.\n\n        Returns:\n            A list of lines from the wrapped version of the document.\n        \"\"\"\n        wrapped_lines: list[list[str]] = []\n        append = wrapped_lines.append\n        for line_index, line in enumerate(self.document.lines):\n            divided = Text(line).divide(self._wrap_offsets[line_index])\n            append([section.plain for section in divided])\n\n        return wrapped_lines\n\n    @property\n    def height(self) -> int:\n        \"\"\"The height of the wrapped document.\"\"\"\n        return sum(len(offsets) + 1 for offsets in self._wrap_offsets)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def location_to_offset(self, location: Location) -> Offset:\n        \"\"\"\n        Convert a location in the document to an offset within the wrapped/visual display of the document.\n\n        Args:\n            location: The location in the document.\n\n        Returns:\n            The Offset in the document's visual display corresponding to the given location.\n        \"\"\"\n        line_index, column_index = location\n\n        # Clamp the line index to the bounds of the document\n        line_index = clamp(line_index, 0, len(self._line_index_to_offsets))\n\n        # Find the section index of this location, so that we know which y_offset to use\n        wrap_offsets = self.get_offsets(line_index)\n        section_start_columns = [0, *wrap_offsets]\n        section_index = bisect_right(wrap_offsets, column_index)\n\n        # Get the y-offsets corresponding to this line index\n        y_offsets = self._line_index_to_offsets[line_index]\n        section_column_index = column_index - section_start_columns[section_index]\n\n        section = self.get_sections(line_index)[section_index]\n        x_offset = cell_len(\n            expand_tabs_inline(section[:section_column_index], self._tab_width)\n        )\n\n        return Offset(x_offset, y_offsets[section_index])\n\n    def get_target_document_column(\n        self,\n        line_index: int,\n        x_offset: int,\n        y_offset: int,\n    ) -> int:\n        \"\"\"Given a line index and the offsets within the wrapped version of that\n        line, return the corresponding column index in the raw document.\n\n        Args:\n             line_index: The index of the line in the document.\n             x_offset: The x-offset within the wrapped line.\n             y_offset: The y-offset within the wrapped line (supports negative indexing).\n\n        Returns:\n            The column index corresponding to the line index and y offset.\n        \"\"\"\n\n        # We've found the relevant line, now find the character by\n        # looking at the character corresponding to the offset width.\n        sections = self.get_sections(line_index)\n\n        # wrapped_section is the text that appears on a single y_offset within\n        # the TextArea. It's a potentially wrapped portion of a larger line from\n        # the original document.\n        target_section = sections[y_offset]\n\n        # Add the offsets from the wrapped sections above this one (from the same raw\n        # document line)\n        target_section_start = sum(\n            len(wrapped_section) for wrapped_section in sections[:y_offset]\n        )\n\n        # Get the column index within this wrapped section of the line\n        target_column_index = target_section_start + cell_width_to_column_index(\n            target_section, x_offset, self._tab_width\n        )\n\n        # If we're on the final section of a line, the cursor can legally rest beyond\n        # the end by a single cell. Otherwise, we'll need to ensure that we're\n        # keeping the cursor within the bounds of the target section.\n        if y_offset != len(sections) - 1 and y_offset != -1:\n            target_column_index = min(\n                target_column_index, target_section_start + len(target_section) - 1\n            )\n\n        return target_column_index\n\n    def get_sections(self, line_index: int) -> list[str]:\n        \"\"\"Return the sections for the given line index.\n\n        When wrapping is enabled, a single line in the document can visually span\n        multiple lines. The list returned represents that visually (each string in\n        the list represents a single section (y-offset) after wrapping happens).\n\n        Args:\n            line_index: The index of the line to get sections for.\n\n        Returns:\n            The wrapped line as a list of strings.\n        \"\"\"\n        line_offsets = self._wrap_offsets[line_index]\n        wrapped_lines = Text(self.document[line_index], end=\"\").divide(line_offsets)\n        return [line.plain for line in wrapped_lines]\n\n    def get_offsets(self, line_index: int) -> list[int]:\n        \"\"\"Given a line index, get the offsets within that line where wrapping\n        should occur for the current document.\n\n        Args:\n            line_index: The index of the line within the document.\n\n        Raises:\n            ValueError: When `line_index` is out of bounds.\n\n        Returns:\n            The offsets within the line where wrapping should occur.\n        \"\"\"\n        wrap_offsets = self._wrap_offsets\n        out_of_bounds = line_index < 0 or line_index >= len(wrap_offsets)\n        if out_of_bounds:\n            raise ValueError(\n                f\"The document line index {line_index!r} is out of bounds. \"\n                f\"The document contains {len(wrap_offsets)!r} lines.\"\n            )\n        return wrap_offsets[line_index]\n\n    def get_tab_widths(self, line_index: int) -> list[int]:\n        \"\"\"Return a list of the tab widths for the given line index.\n\n        Args:\n            line_index: The index of the line in the document.\n\n        Returns:\n            An ordered list of the expanded width of the tabs in the line.\n        \"\"\"\n        return self._tab_width_cache[line_index]\n\n```\n# Function Signature: def wrap_range(\n# Natural Language Annotations: \"\"\"Incrementally recompute wrapping based on a performed edit.\n\n        This must be called *after* the source document has been edited.\n\n        Args:\n            start: The start location of the edit that was performed in document-space.\n            old_end: The old end location of the edit in document-space.\n            new_end: The new end location of the edit in document-space.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef wrap_range(start: Location, old_end:", "  ```\ndef wrap_range(\n    self,\n    start:", "  ```\ndef wrap_range(self, start: Location, old_"]}, "textual-src/textual/document/_wrapped_document.py-offset_to_location": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/document/_wrapped_document.py:\n```\nfrom __future__ import annotations\n\nfrom bisect import bisect_right\n\nfrom rich.text import Text\n\nfrom textual._cells import cell_len, cell_width_to_column_index\nfrom textual._wrap import compute_wrap_offsets\nfrom textual.document._document import DocumentBase, Location\nfrom textual.expand_tabs import expand_tabs_inline, get_tab_widths\nfrom textual.geometry import Offset, clamp\n\nVerticalOffset = int\nLineIndex = int\nSectionOffset = int\n\n\nclass WrappedDocument:\n    \"\"\"A view into a Document which wraps the document at a certain\n    width and can be queried to retrieve lines from the *wrapped* version\n    of the document.\n\n    Allows for incremental updates, ensuring that we only re-wrap ranges of the document\n    that were influenced by edits.\n    \"\"\"\n\n    def __init__(\n        self,\n        document: DocumentBase,\n        width: int = 0,\n        tab_width: int = 4,\n    ) -> None:\n        \"\"\"Construct a WrappedDocument.\n\n        By default, a WrappedDocument is wrapped with width=0 (no wrapping).\n        To wrap the document, use the wrap() method.\n\n        Args:\n            document: The document to wrap.\n            width: The width to wrap at.\n            tab_width: The maximum width to consider for tab characters.\n        \"\"\"\n        self.document = document\n        \"\"\"The document wrapping is performed on.\"\"\"\n\n        self._wrap_offsets: list[list[int]] = []\n        \"\"\"Maps line indices to the offsets within the line where wrapping\n        breaks should be added.\"\"\"\n\n        self._tab_width_cache: list[list[int]] = []\n        \"\"\"Maps line indices to a list of tab widths. `[[2, 4]]` means that on line 0, the first\n        tab has width 2, and the second tab has width 4.\"\"\"\n\n        self._offset_to_line_info: list[tuple[LineIndex, SectionOffset]] = []\n        \"\"\"Maps y_offsets (from the top of the document) to line_index and the offset\n        of the section within the line.\"\"\"\n\n        self._line_index_to_offsets: list[list[VerticalOffset]] = []\n        \"\"\"Maps line indices to all the vertical offsets which correspond to that line.\"\"\"\n\n        self._width: int = width\n        \"\"\"The width the document is currently wrapped at. This will correspond with\n        the value last passed into the `wrap` method.\"\"\"\n\n        self._tab_width: int = tab_width\n        \"\"\"The maximum width to expand tabs to when considering their widths.\"\"\"\n\n        self.wrap(width, tab_width)\n\n    @property\n    def wrapped(self) -> bool:\n        \"\"\"True if the content is wrapped. This is not the same as wrapping being \"enabled\".\n        For example, an empty document can have wrapping enabled, but no wrapping has actually\n        occurred.\n\n        In other words, this is True if the length of any line in the document is greater\n        than the available width.\"\"\"\n        return len(self._line_index_to_offsets) == len(self._offset_to_line_info)\n\n    def wrap(self, width: int, tab_width: int | None = None) -> None:\n        \"\"\"Wrap and cache all lines in the document.\n\n        Args:\n            width: The width to wrap at. 0 for no wrapping.\n            tab_width: The maximum width to consider for tab characters. If None,\n                reuse the  tab width.\n        \"\"\"\n        self._width = width\n        if tab_width:\n            self._tab_width = tab_width\n\n        # We're starting wrapping from scratch\n        new_wrap_offsets: list[list[int]] = []\n        offset_to_line_info: list[tuple[LineIndex, SectionOffset]] = []\n        line_index_to_offsets: list[list[VerticalOffset]] = []\n        line_tab_widths: list[list[int]] = []\n\n        append_wrap_offset = new_wrap_offsets.append\n        append_line_info = offset_to_line_info.append\n        append_line_offsets = line_index_to_offsets.append\n        append_line_tab_widths = line_tab_widths.append\n\n        current_offset = 0\n        tab_width = self._tab_width\n        for line_index, line in enumerate(self.document.lines):\n            tab_sections = get_tab_widths(line, tab_width)\n            wrap_offsets = (\n                compute_wrap_offsets(\n                    line,\n                    width,\n                    tab_size=tab_width,\n                    precomputed_tab_sections=tab_sections,\n                )\n                if width\n                else []\n            )\n            append_line_tab_widths([width for _, width in tab_sections])\n            append_wrap_offset(wrap_offsets)\n            append_line_offsets([])\n            for section_y_offset in range(len(wrap_offsets) + 1):\n                append_line_info((line_index, section_y_offset))\n                line_index_to_offsets[line_index].append(current_offset)\n                current_offset += 1\n\n        self._wrap_offsets = new_wrap_offsets\n        self._offset_to_line_info = offset_to_line_info\n        self._line_index_to_offsets = line_index_to_offsets\n        self._tab_width_cache = line_tab_widths\n\n    @property\n    def lines(self) -> list[list[str]]:\n        \"\"\"The lines of the wrapped version of the Document.\n\n        Each index in the returned list represents a line index in the raw\n        document. The list[str] at each index is the content of the raw document line\n        split into multiple lines via wrapping.\n\n        Note that this is expensive to compute and is not cached.\n\n        Returns:\n            A list of lines from the wrapped version of the document.\n        \"\"\"\n        wrapped_lines: list[list[str]] = []\n        append = wrapped_lines.append\n        for line_index, line in enumerate(self.document.lines):\n            divided = Text(line).divide(self._wrap_offsets[line_index])\n            append([section.plain for section in divided])\n\n        return wrapped_lines\n\n    @property\n    def height(self) -> int:\n        \"\"\"The height of the wrapped document.\"\"\"\n        return sum(len(offsets) + 1 for offsets in self._wrap_offsets)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def location_to_offset(self, location: Location) -> Offset:\n        \"\"\"\n        Convert a location in the document to an offset within the wrapped/visual display of the document.\n\n        Args:\n            location: The location in the document.\n\n        Returns:\n            The Offset in the document's visual display corresponding to the given location.\n        \"\"\"\n        line_index, column_index = location\n\n        # Clamp the line index to the bounds of the document\n        line_index = clamp(line_index, 0, len(self._line_index_to_offsets))\n\n        # Find the section index of this location, so that we know which y_offset to use\n        wrap_offsets = self.get_offsets(line_index)\n        section_start_columns = [0, *wrap_offsets]\n        section_index = bisect_right(wrap_offsets, column_index)\n\n        # Get the y-offsets corresponding to this line index\n        y_offsets = self._line_index_to_offsets[line_index]\n        section_column_index = column_index - section_start_columns[section_index]\n\n        section = self.get_sections(line_index)[section_index]\n        x_offset = cell_len(\n            expand_tabs_inline(section[:section_column_index], self._tab_width)\n        )\n\n        return Offset(x_offset, y_offsets[section_index])\n\n    def get_target_document_column(\n        self,\n        line_index: int,\n        x_offset: int,\n        y_offset: int,\n    ) -> int:\n        \"\"\"Given a line index and the offsets within the wrapped version of that\n        line, return the corresponding column index in the raw document.\n\n        Args:\n             line_index: The index of the line in the document.\n             x_offset: The x-offset within the wrapped line.\n             y_offset: The y-offset within the wrapped line (supports negative indexing).\n\n        Returns:\n            The column index corresponding to the line index and y offset.\n        \"\"\"\n\n        # We've found the relevant line, now find the character by\n        # looking at the character corresponding to the offset width.\n        sections = self.get_sections(line_index)\n\n        # wrapped_section is the text that appears on a single y_offset within\n        # the TextArea. It's a potentially wrapped portion of a larger line from\n        # the original document.\n        target_section = sections[y_offset]\n\n        # Add the offsets from the wrapped sections above this one (from the same raw\n        # document line)\n        target_section_start = sum(\n            len(wrapped_section) for wrapped_section in sections[:y_offset]\n        )\n\n        # Get the column index within this wrapped section of the line\n        target_column_index = target_section_start + cell_width_to_column_index(\n            target_section, x_offset, self._tab_width\n        )\n\n        # If we're on the final section of a line, the cursor can legally rest beyond\n        # the end by a single cell. Otherwise, we'll need to ensure that we're\n        # keeping the cursor within the bounds of the target section.\n        if y_offset != len(sections) - 1 and y_offset != -1:\n            target_column_index = min(\n                target_column_index, target_section_start + len(target_section) - 1\n            )\n\n        return target_column_index\n\n    def get_sections(self, line_index: int) -> list[str]:\n        \"\"\"Return the sections for the given line index.\n\n        When wrapping is enabled, a single line in the document can visually span\n        multiple lines. The list returned represents that visually (each string in\n        the list represents a single section (y-offset) after wrapping happens).\n\n        Args:\n            line_index: The index of the line to get sections for.\n\n        Returns:\n            The wrapped line as a list of strings.\n        \"\"\"\n        line_offsets = self._wrap_offsets[line_index]\n        wrapped_lines = Text(self.document[line_index], end=\"\").divide(line_offsets)\n        return [line.plain for line in wrapped_lines]\n\n    def get_offsets(self, line_index: int) -> list[int]:\n        \"\"\"Given a line index, get the offsets within that line where wrapping\n        should occur for the current document.\n\n        Args:\n            line_index: The index of the line within the document.\n\n        Raises:\n            ValueError: When `line_index` is out of bounds.\n\n        Returns:\n            The offsets within the line where wrapping should occur.\n        \"\"\"\n        wrap_offsets = self._wrap_offsets\n        out_of_bounds = line_index < 0 or line_index >= len(wrap_offsets)\n        if out_of_bounds:\n            raise ValueError(\n                f\"The document line index {line_index!r} is out of bounds. \"\n                f\"The document contains {len(wrap_offsets)!r} lines.\"\n            )\n        return wrap_offsets[line_index]\n\n    def get_tab_widths(self, line_index: int) -> list[int]:\n        \"\"\"Return a list of the tab widths for the given line index.\n\n        Args:\n            line_index: The index of the line in the document.\n\n        Returns:\n            An ordered list of the expanded width of the tabs in the line.\n        \"\"\"\n        return self._tab_width_cache[line_index]\n\n```\n# Function Signature: def offset_to_location(self, offset: Offset) -> Location:\n# Natural Language Annotations: \"\"\"Given an offset within the wrapped/visual display of the document,\n        return the corresponding location in the document.\n\n        Args:\n            offset: The y-offset within the document.\n\n        Raises:\n            ValueError: When the given offset does not correspond to a line\n                in the document.\n\n        Returns:\n            The Location in the document corresponding to the given offset.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef offset_to_location(self, offset: Offset", "  ```\ndef offset_to_location(self, offset: Offset", "  ```\ndef offset_to_location(self, offset: Offset"]}, "textual-src/textual/css/parse.py-substitute_references": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/css/parse.py:\n```\nfrom __future__ import annotations\n\nimport dataclasses\nfrom functools import lru_cache\nfrom typing import Iterable, Iterator, NoReturn\n\nfrom ..suggestions import get_suggestion\nfrom ._help_renderables import HelpText\nfrom ._styles_builder import DeclarationError, StylesBuilder\nfrom .errors import UnresolvedVariableError\nfrom .model import (\n    CombinatorType,\n    Declaration,\n    RuleSet,\n    Selector,\n    SelectorSet,\n    SelectorType,\n)\nfrom .styles import Styles\nfrom .tokenize import Token, tokenize, tokenize_declarations, tokenize_values\nfrom .tokenizer import EOFError, ReferencedBy\nfrom .types import CSSLocation, Specificity3\n\nSELECTOR_MAP: dict[str, tuple[SelectorType, Specificity3]] = {\n    \"selector\": (SelectorType.TYPE, (0, 0, 1)),\n    \"selector_start\": (SelectorType.TYPE, (0, 0, 1)),\n    \"selector_class\": (SelectorType.CLASS, (0, 1, 0)),\n    \"selector_start_class\": (SelectorType.CLASS, (0, 1, 0)),\n    \"selector_id\": (SelectorType.ID, (1, 0, 0)),\n    \"selector_start_id\": (SelectorType.ID, (1, 0, 0)),\n    \"selector_universal\": (SelectorType.UNIVERSAL, (0, 0, 0)),\n    \"selector_start_universal\": (SelectorType.UNIVERSAL, (0, 0, 0)),\n    \"nested\": (SelectorType.NESTED, (0, 0, 0)),\n}\n\n\ndef _add_specificity(\n    specificity1: Specificity3, specificity2: Specificity3\n) -> Specificity3:\n    \"\"\"Add specificity tuples together.\n\n    Args:\n        specificity1: Specificity triple.\n        specificity2: Specificity triple.\n\n    Returns:\n        Combined specificity.\n    \"\"\"\n\n    a1, b1, c1 = specificity1\n    a2, b2, c2 = specificity2\n    return (a1 + a2, b1 + b2, c1 + c2)\n\n\n@lru_cache(maxsize=1024)\ndef parse_selectors(css_selectors: str) -> tuple[SelectorSet, ...]:\n    if not css_selectors.strip():\n        return ()\n    tokens = iter(tokenize(css_selectors, (\"\", \"\")))\n\n    get_selector = SELECTOR_MAP.get\n    combinator: CombinatorType | None = CombinatorType.DESCENDENT\n    selectors: list[Selector] = []\n    rule_selectors: list[list[Selector]] = []\n\n    while True:\n        try:\n            token = next(tokens, None)\n        except EOFError:\n            break\n        if token is None:\n            break\n        token_name = token.name\n\n        if token_name == \"pseudo_class\":\n            selectors[-1]._add_pseudo_class(token.value.lstrip(\":\"))\n        elif token_name == \"whitespace\":\n            if combinator is None or combinator == CombinatorType.SAME:\n                combinator = CombinatorType.DESCENDENT\n        elif token_name == \"new_selector\":\n            rule_selectors.append(selectors[:])\n            selectors.clear()\n            combinator = None\n        elif token_name == \"declaration_set_start\":\n            break\n        elif token_name == \"combinator_child\":\n            combinator = CombinatorType.CHILD\n        else:\n            _selector, specificity = get_selector(\n                token_name, (SelectorType.TYPE, (0, 0, 0))\n            )\n            selectors.append(\n                Selector(\n                    name=token.value.lstrip(\".#\"),\n                    combinator=combinator or CombinatorType.DESCENDENT,\n                    type=_selector,\n                    specificity=specificity,\n                )\n            )\n            combinator = CombinatorType.SAME\n    if selectors:\n        rule_selectors.append(selectors[:])\n\n    selector_set = tuple(SelectorSet.from_selectors(rule_selectors))\n    return selector_set\n\n\ndef parse_rule_set(\n    scope: str,\n    tokens: Iterator[Token],\n    token: Token,\n    is_default_rules: bool = False,\n    tie_breaker: int = 0,\n) -> Iterable[RuleSet]:\n    get_selector = SELECTOR_MAP.get\n    combinator: CombinatorType | None = CombinatorType.DESCENDENT\n    selectors: list[Selector] = []\n    rule_selectors: list[list[Selector]] = []\n    styles_builder = StylesBuilder()\n\n    while True:\n        if token.name == \"pseudo_class\":\n            selectors[-1]._add_pseudo_class(token.value.lstrip(\":\"))\n        elif token.name == \"whitespace\":\n            if combinator is None or combinator == CombinatorType.SAME:\n                combinator = CombinatorType.DESCENDENT\n        elif token.name == \"new_selector\":\n            rule_selectors.append(selectors[:])\n            selectors.clear()\n            combinator = None\n        elif token.name == \"declaration_set_start\":\n            break\n        elif token.name == \"combinator_child\":\n            combinator = CombinatorType.CHILD\n        else:\n            _selector, specificity = get_selector(\n                token.name, (SelectorType.TYPE, (0, 0, 0))\n            )\n            selectors.append(\n                Selector(\n                    name=token.value.lstrip(\".#\"),\n                    combinator=combinator or CombinatorType.DESCENDENT,\n                    type=_selector,\n                    specificity=specificity,\n                )\n            )\n            combinator = CombinatorType.SAME\n\n        token = next(tokens)\n\n    if selectors:\n        if scope and selectors[0].name != scope:\n            scope_selector, scope_specificity = get_selector(\n                scope, (SelectorType.TYPE, (0, 0, 0))\n            )\n            selectors.insert(\n                0,\n                Selector(\n                    name=scope,\n                    combinator=CombinatorType.DESCENDENT,\n                    type=scope_selector,\n                    specificity=scope_specificity,\n                ),\n            )\n        rule_selectors.append(selectors[:])\n\n    declaration = Declaration(token, \"\")\n    errors: list[tuple[Token, str | HelpText]] = []\n    nested_rules: list[RuleSet] = []\n\n    while True:\n        token = next(tokens)\n\n        token_name = token.name\n        if token_name in (\"whitespace\", \"declaration_end\"):\n            continue\n        if token_name in {\n            \"selector_start_id\",\n            \"selector_start_class\",\n            \"selector_start_universal\",\n            \"selector_start\",\n            \"nested\",\n        }:\n            recursive_parse: list[RuleSet] = list(\n                parse_rule_set(\n                    \"\",\n                    tokens,\n                    token,\n                    is_default_rules=is_default_rules,\n                    tie_breaker=tie_breaker,\n                )\n            )\n\n            def combine_selectors(\n                selectors1: list[Selector], selectors2: list[Selector]\n            ) -> list[Selector]:\n                \"\"\"Combine lists of selectors together, processing any nesting.\n\n                Args:\n                    selectors1: List of selectors.\n                    selectors2: Second list of selectors.\n\n                Returns:\n                    Combined selectors.\n                \"\"\"\n                if selectors2 and selectors2[0].type == SelectorType.NESTED:\n                    final_selector = selectors1[-1]\n                    nested_selector = selectors2[0]\n                    merged_selector = dataclasses.replace(\n                        final_selector,\n                        pseudo_classes=(\n                            final_selector.pseudo_classes\n                            | nested_selector.pseudo_classes\n                        ),\n                        specificity=_add_specificity(\n                            final_selector.specificity, nested_selector.specificity\n                        ),\n                    )\n                    return [*selectors1[:-1], merged_selector, *selectors2[1:]]\n                else:\n                    return selectors1 + selectors2\n\n            for rule_selector in rule_selectors:\n                for rule_set in recursive_parse:\n                    nested_rule_set = RuleSet(\n                        [\n                            SelectorSet(\n                                combine_selectors(\n                                    rule_selector, recursive_selectors.selectors\n                                )\n                            )._total_specificity()\n                            for recursive_selectors in rule_set.selector_set\n                        ],\n                        rule_set.styles,\n                        rule_set.errors,\n                        rule_set.is_default_rules,\n                        rule_set.tie_breaker + tie_breaker,\n                    )\n                    nested_rules.append(nested_rule_set)\n            continue\n        if token_name == \"declaration_name\":\n            try:\n                styles_builder.add_declaration(declaration)\n            except DeclarationError as error:\n                errors.append((error.token, error.message))\n            declaration = Declaration(token, \"\")\n            declaration.name = token.value.rstrip(\":\")\n        elif token_name == \"declaration_set_end\":\n            break\n        else:\n            declaration.tokens.append(token)\n\n    try:\n        styles_builder.add_declaration(declaration)\n    except DeclarationError as error:\n        errors.append((error.token, error.message))\n\n    rule_set = RuleSet(\n        list(SelectorSet.from_selectors(rule_selectors)),\n        styles_builder.styles,\n        errors,\n        is_default_rules=is_default_rules,\n        tie_breaker=tie_breaker,\n    )\n\n    rule_set._post_parse()\n    yield rule_set\n\n    for nested_rule_set in nested_rules:\n        nested_rule_set._post_parse()\n        yield nested_rule_set\n\n\ndef parse_declarations(css: str, read_from: CSSLocation) -> Styles:\n    \"\"\"Parse declarations and return a Styles object.\n\n    Args:\n        css: String containing CSS.\n        read_from: The location where the CSS was read from.\n\n    Returns:\n        A styles object.\n    \"\"\"\n\n    tokens = iter(tokenize_declarations(css, read_from))\n    styles_builder = StylesBuilder()\n\n    declaration: Declaration | None = None\n    errors: list[tuple[Token, str | HelpText]] = []\n    while True:\n        token = next(tokens, None)\n        if token is None:\n            break\n        token_name = token.name\n        if token_name in (\"whitespace\", \"declaration_end\", \"eof\"):\n            continue\n        if token_name == \"declaration_name\":\n            if declaration:\n                try:\n                    styles_builder.add_declaration(declaration)\n                except DeclarationError as error:\n                    errors.append((error.token, error.message))\n                    raise\n            declaration = Declaration(token, \"\")\n            declaration.name = token.value.rstrip(\":\")\n        elif token_name == \"declaration_set_end\":\n            break\n        else:\n            if declaration:\n                declaration.tokens.append(token)\n\n    if declaration:\n        try:\n            styles_builder.add_declaration(declaration)\n        except DeclarationError as error:\n            errors.append((error.token, error.message))\n            raise\n\n    return styles_builder.styles\n\n\ndef _unresolved(variable_name: str, variables: Iterable[str], token: Token) -> NoReturn:\n    \"\"\"Raise a TokenError regarding an unresolved variable.\n\n    Args:\n        variable_name: A variable name.\n        variables: Possible choices used to generate suggestion.\n        token: The Token.\n\n    Raises:\n        UnresolvedVariableError: Always raises a TokenError.\n    \"\"\"\n    message = f\"reference to undefined variable '${variable_name}'\"\n    suggested_variable = get_suggestion(variable_name, list(variables))\n    if suggested_variable:\n        message += f\"; did you mean '${suggested_variable}'?\"\n\n    raise UnresolvedVariableError(\n        token.read_from,\n        token.code,\n        token.start,\n        message,\n        end=token.end,\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def substitute_references(\n# Natural Language Annotations: \"\"\"Replace variable references with values by substituting variable reference\n    tokens with the tokens representing their values.\n\n    Args:\n        tokens: Iterator of Tokens which may contain tokens\n            with the name \"variable_ref\".\n\n    Returns:\n        Yields Tokens such that any variable references (tokens where\n            token.name == \"variable_ref\") have been replaced with the tokens representing\n            the value. In other words, an Iterable of Tokens similar to the original input,\n            but with variables resolved. Substituted tokens will have their referenced_by\n            attribute populated with information about where the tokens are being substituted to.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef substitute_references(tokens: Iterator[Token", "  ```\ndef substitute_references(tokens: Iterable[Token", "  ```\ndef substitute_references(tokens: Iterator[Token"]}, "textual-src/textual/css/parse.py-parse": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/css/parse.py:\n```\nfrom __future__ import annotations\n\nimport dataclasses\nfrom functools import lru_cache\nfrom typing import Iterable, Iterator, NoReturn\n\nfrom ..suggestions import get_suggestion\nfrom ._help_renderables import HelpText\nfrom ._styles_builder import DeclarationError, StylesBuilder\nfrom .errors import UnresolvedVariableError\nfrom .model import (\n    CombinatorType,\n    Declaration,\n    RuleSet,\n    Selector,\n    SelectorSet,\n    SelectorType,\n)\nfrom .styles import Styles\nfrom .tokenize import Token, tokenize, tokenize_declarations, tokenize_values\nfrom .tokenizer import EOFError, ReferencedBy\nfrom .types import CSSLocation, Specificity3\n\nSELECTOR_MAP: dict[str, tuple[SelectorType, Specificity3]] = {\n    \"selector\": (SelectorType.TYPE, (0, 0, 1)),\n    \"selector_start\": (SelectorType.TYPE, (0, 0, 1)),\n    \"selector_class\": (SelectorType.CLASS, (0, 1, 0)),\n    \"selector_start_class\": (SelectorType.CLASS, (0, 1, 0)),\n    \"selector_id\": (SelectorType.ID, (1, 0, 0)),\n    \"selector_start_id\": (SelectorType.ID, (1, 0, 0)),\n    \"selector_universal\": (SelectorType.UNIVERSAL, (0, 0, 0)),\n    \"selector_start_universal\": (SelectorType.UNIVERSAL, (0, 0, 0)),\n    \"nested\": (SelectorType.NESTED, (0, 0, 0)),\n}\n\n\ndef _add_specificity(\n    specificity1: Specificity3, specificity2: Specificity3\n) -> Specificity3:\n    \"\"\"Add specificity tuples together.\n\n    Args:\n        specificity1: Specificity triple.\n        specificity2: Specificity triple.\n\n    Returns:\n        Combined specificity.\n    \"\"\"\n\n    a1, b1, c1 = specificity1\n    a2, b2, c2 = specificity2\n    return (a1 + a2, b1 + b2, c1 + c2)\n\n\n@lru_cache(maxsize=1024)\ndef parse_selectors(css_selectors: str) -> tuple[SelectorSet, ...]:\n    if not css_selectors.strip():\n        return ()\n    tokens = iter(tokenize(css_selectors, (\"\", \"\")))\n\n    get_selector = SELECTOR_MAP.get\n    combinator: CombinatorType | None = CombinatorType.DESCENDENT\n    selectors: list[Selector] = []\n    rule_selectors: list[list[Selector]] = []\n\n    while True:\n        try:\n            token = next(tokens, None)\n        except EOFError:\n            break\n        if token is None:\n            break\n        token_name = token.name\n\n        if token_name == \"pseudo_class\":\n            selectors[-1]._add_pseudo_class(token.value.lstrip(\":\"))\n        elif token_name == \"whitespace\":\n            if combinator is None or combinator == CombinatorType.SAME:\n                combinator = CombinatorType.DESCENDENT\n        elif token_name == \"new_selector\":\n            rule_selectors.append(selectors[:])\n            selectors.clear()\n            combinator = None\n        elif token_name == \"declaration_set_start\":\n            break\n        elif token_name == \"combinator_child\":\n            combinator = CombinatorType.CHILD\n        else:\n            _selector, specificity = get_selector(\n                token_name, (SelectorType.TYPE, (0, 0, 0))\n            )\n            selectors.append(\n                Selector(\n                    name=token.value.lstrip(\".#\"),\n                    combinator=combinator or CombinatorType.DESCENDENT,\n                    type=_selector,\n                    specificity=specificity,\n                )\n            )\n            combinator = CombinatorType.SAME\n    if selectors:\n        rule_selectors.append(selectors[:])\n\n    selector_set = tuple(SelectorSet.from_selectors(rule_selectors))\n    return selector_set\n\n\ndef parse_rule_set(\n    scope: str,\n    tokens: Iterator[Token],\n    token: Token,\n    is_default_rules: bool = False,\n    tie_breaker: int = 0,\n) -> Iterable[RuleSet]:\n    get_selector = SELECTOR_MAP.get\n    combinator: CombinatorType | None = CombinatorType.DESCENDENT\n    selectors: list[Selector] = []\n    rule_selectors: list[list[Selector]] = []\n    styles_builder = StylesBuilder()\n\n    while True:\n        if token.name == \"pseudo_class\":\n            selectors[-1]._add_pseudo_class(token.value.lstrip(\":\"))\n        elif token.name == \"whitespace\":\n            if combinator is None or combinator == CombinatorType.SAME:\n                combinator = CombinatorType.DESCENDENT\n        elif token.name == \"new_selector\":\n            rule_selectors.append(selectors[:])\n            selectors.clear()\n            combinator = None\n        elif token.name == \"declaration_set_start\":\n            break\n        elif token.name == \"combinator_child\":\n            combinator = CombinatorType.CHILD\n        else:\n            _selector, specificity = get_selector(\n                token.name, (SelectorType.TYPE, (0, 0, 0))\n            )\n            selectors.append(\n                Selector(\n                    name=token.value.lstrip(\".#\"),\n                    combinator=combinator or CombinatorType.DESCENDENT,\n                    type=_selector,\n                    specificity=specificity,\n                )\n            )\n            combinator = CombinatorType.SAME\n\n        token = next(tokens)\n\n    if selectors:\n        if scope and selectors[0].name != scope:\n            scope_selector, scope_specificity = get_selector(\n                scope, (SelectorType.TYPE, (0, 0, 0))\n            )\n            selectors.insert(\n                0,\n                Selector(\n                    name=scope,\n                    combinator=CombinatorType.DESCENDENT,\n                    type=scope_selector,\n                    specificity=scope_specificity,\n                ),\n            )\n        rule_selectors.append(selectors[:])\n\n    declaration = Declaration(token, \"\")\n    errors: list[tuple[Token, str | HelpText]] = []\n    nested_rules: list[RuleSet] = []\n\n    while True:\n        token = next(tokens)\n\n        token_name = token.name\n        if token_name in (\"whitespace\", \"declaration_end\"):\n            continue\n        if token_name in {\n            \"selector_start_id\",\n            \"selector_start_class\",\n            \"selector_start_universal\",\n            \"selector_start\",\n            \"nested\",\n        }:\n            recursive_parse: list[RuleSet] = list(\n                parse_rule_set(\n                    \"\",\n                    tokens,\n                    token,\n                    is_default_rules=is_default_rules,\n                    tie_breaker=tie_breaker,\n                )\n            )\n\n            def combine_selectors(\n                selectors1: list[Selector], selectors2: list[Selector]\n            ) -> list[Selector]:\n                \"\"\"Combine lists of selectors together, processing any nesting.\n\n                Args:\n                    selectors1: List of selectors.\n                    selectors2: Second list of selectors.\n\n                Returns:\n                    Combined selectors.\n                \"\"\"\n                if selectors2 and selectors2[0].type == SelectorType.NESTED:\n                    final_selector = selectors1[-1]\n                    nested_selector = selectors2[0]\n                    merged_selector = dataclasses.replace(\n                        final_selector,\n                        pseudo_classes=(\n                            final_selector.pseudo_classes\n                            | nested_selector.pseudo_classes\n                        ),\n                        specificity=_add_specificity(\n                            final_selector.specificity, nested_selector.specificity\n                        ),\n                    )\n                    return [*selectors1[:-1], merged_selector, *selectors2[1:]]\n                else:\n                    return selectors1 + selectors2\n\n            for rule_selector in rule_selectors:\n                for rule_set in recursive_parse:\n                    nested_rule_set = RuleSet(\n                        [\n                            SelectorSet(\n                                combine_selectors(\n                                    rule_selector, recursive_selectors.selectors\n                                )\n                            )._total_specificity()\n                            for recursive_selectors in rule_set.selector_set\n                        ],\n                        rule_set.styles,\n                        rule_set.errors,\n                        rule_set.is_default_rules,\n                        rule_set.tie_breaker + tie_breaker,\n                    )\n                    nested_rules.append(nested_rule_set)\n            continue\n        if token_name == \"declaration_name\":\n            try:\n                styles_builder.add_declaration(declaration)\n            except DeclarationError as error:\n                errors.append((error.token, error.message))\n            declaration = Declaration(token, \"\")\n            declaration.name = token.value.rstrip(\":\")\n        elif token_name == \"declaration_set_end\":\n            break\n        else:\n            declaration.tokens.append(token)\n\n    try:\n        styles_builder.add_declaration(declaration)\n    except DeclarationError as error:\n        errors.append((error.token, error.message))\n\n    rule_set = RuleSet(\n        list(SelectorSet.from_selectors(rule_selectors)),\n        styles_builder.styles,\n        errors,\n        is_default_rules=is_default_rules,\n        tie_breaker=tie_breaker,\n    )\n\n    rule_set._post_parse()\n    yield rule_set\n\n    for nested_rule_set in nested_rules:\n        nested_rule_set._post_parse()\n        yield nested_rule_set\n\n\ndef parse_declarations(css: str, read_from: CSSLocation) -> Styles:\n    \"\"\"Parse declarations and return a Styles object.\n\n    Args:\n        css: String containing CSS.\n        read_from: The location where the CSS was read from.\n\n    Returns:\n        A styles object.\n    \"\"\"\n\n    tokens = iter(tokenize_declarations(css, read_from))\n    styles_builder = StylesBuilder()\n\n    declaration: Declaration | None = None\n    errors: list[tuple[Token, str | HelpText]] = []\n    while True:\n        token = next(tokens, None)\n        if token is None:\n            break\n        token_name = token.name\n        if token_name in (\"whitespace\", \"declaration_end\", \"eof\"):\n            continue\n        if token_name == \"declaration_name\":\n            if declaration:\n                try:\n                    styles_builder.add_declaration(declaration)\n                except DeclarationError as error:\n                    errors.append((error.token, error.message))\n                    raise\n            declaration = Declaration(token, \"\")\n            declaration.name = token.value.rstrip(\":\")\n        elif token_name == \"declaration_set_end\":\n            break\n        else:\n            if declaration:\n                declaration.tokens.append(token)\n\n    if declaration:\n        try:\n            styles_builder.add_declaration(declaration)\n        except DeclarationError as error:\n            errors.append((error.token, error.message))\n            raise\n\n    return styles_builder.styles\n\n\ndef _unresolved(variable_name: str, variables: Iterable[str], token: Token) -> NoReturn:\n    \"\"\"Raise a TokenError regarding an unresolved variable.\n\n    Args:\n        variable_name: A variable name.\n        variables: Possible choices used to generate suggestion.\n        token: The Token.\n\n    Raises:\n        UnresolvedVariableError: Always raises a TokenError.\n    \"\"\"\n    message = f\"reference to undefined variable '${variable_name}'\"\n    suggested_variable = get_suggestion(variable_name, list(variables))\n    if suggested_variable:\n        message += f\"; did you mean '${suggested_variable}'?\"\n\n    raise UnresolvedVariableError(\n        token.read_from,\n        token.code,\n        token.start,\n        message,\n        end=token.end,\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: def parse(\n# Natural Language Annotations: \"\"\"Parse CSS by tokenizing it, performing variable substitution,\n    and generating rule sets from it.\n\n    Args:\n        scope: CSS type name.\n        css: The input CSS.\n        read_from: The source location of the CSS.\n        variables: Substitution variables to substitute tokens for.\n        is_default_rules: True if the rules we're extracting are\n            default (i.e. in Widget.DEFAULT_CSS) rules. False if they're from user defined CSS.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef parse(\n    scope: str,\n    css:", "  ```\ndef parse(\n    scope: str,\n    css:", "  ```\ndef parse(\n    scope: str,\n    css:"]}, "textual-src/textual/css/stylesheet.py-_parse_rules": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/css/stylesheet.py:\n```\nfrom __future__ import annotations\n\nimport os\nfrom collections import defaultdict\nfrom itertools import chain\nfrom operator import itemgetter\nfrom pathlib import Path, PurePath\nfrom typing import Iterable, NamedTuple, Sequence, cast\n\nimport rich.repr\nfrom rich.console import Console, ConsoleOptions, RenderableType, RenderResult\nfrom rich.markup import render\nfrom rich.padding import Padding\nfrom rich.panel import Panel\nfrom rich.text import Text\n\nfrom ..cache import LRUCache\nfrom ..dom import DOMNode\nfrom ..widget import Widget\nfrom .errors import StylesheetError\nfrom .match import _check_selectors\nfrom .model import RuleSet\nfrom .parse import parse\nfrom .styles import RulesMap, Styles\nfrom .tokenize import Token, tokenize_values\nfrom .tokenizer import TokenError\nfrom .types import CSSLocation, Specificity3, Specificity6\n\n_DEFAULT_STYLES = Styles()\n\n\nclass StylesheetParseError(StylesheetError):\n    \"\"\"Raised when the stylesheet could not be parsed.\"\"\"\n\n    def __init__(self, errors: StylesheetErrors) -> None:\n        self.errors = errors\n\n    def __rich__(self) -> RenderableType:\n        return self.errors\n\n\nclass StylesheetErrors:\n    \"\"\"A renderable for stylesheet errors.\"\"\"\n\n    def __init__(self, rules: list[RuleSet]) -> None:\n        self.rules = rules\n        self.variables: dict[str, str] = {}\n\n    @classmethod\n    def _get_snippet(cls, code: str, line_no: int) -> RenderableType:\n        from rich.syntax import Syntax\n\n        syntax = Syntax(\n            code,\n            lexer=\"scss\",\n            theme=\"ansi_light\",\n            line_numbers=True,\n            indent_guides=True,\n            line_range=(max(0, line_no - 2), line_no + 2),\n            highlight_lines={line_no},\n        )\n        return syntax\n\n    def __rich_console__(\n        self, console: Console, options: ConsoleOptions\n    ) -> RenderResult:\n        error_count = 0\n        errors = list(\n            dict.fromkeys(chain.from_iterable(_rule.errors for _rule in self.rules))\n        )\n\n        for token, message in errors:\n            error_count += 1\n\n            if token.referenced_by:\n                line_idx, col_idx = token.referenced_by.location\n            else:\n                line_idx, col_idx = token.location\n            line_no, col_no = line_idx + 1, col_idx + 1\n\n            display_path, widget_var = token.read_from\n            if display_path:\n                link_path = str(Path(display_path).absolute())\n                filename = Path(link_path).name\n            else:\n                link_path = \"\"\n                filename = \"<unknown>\"\n            # If we have a widget/variable from where the CSS was read, then line/column\n            # numbers are relative to the inline CSS and we'll display them next to the\n            # widget/variable.\n            # Otherwise, they're absolute positions in a TCSS file and we can show them\n            # next to the file path.\n            if widget_var:\n                path_string = link_path or filename\n                widget_string = f\" in {widget_var}:{line_no}:{col_no}\"\n            else:\n                path_string = f\"{link_path or filename}:{line_no}:{col_no}\"\n                widget_string = \"\"\n\n            title = Text.assemble(\n                \"Error at \", path_string, widget_string, style=\"bold red\"\n            )\n            yield \"\"\n            yield Panel(\n                self._get_snippet(\n                    token.referenced_by.code if token.referenced_by else token.code,\n                    line_no,\n                ),\n                title=title,\n                title_align=\"left\",\n                border_style=\"red\",\n            )\n            yield Padding(message, pad=(0, 0, 1, 3))\n\n        yield \"\"\n        yield render(\n            f\" [b][red]CSS parsing failed:[/] {error_count} error{'s' if error_count != 1 else ''}[/] found in stylesheet\"\n        )\n\n\nclass CssSource(NamedTuple):\n    \"\"\"Contains the CSS content and whether or not the CSS comes from user defined stylesheets\n    vs widget-level stylesheets.\n\n    Args:\n        content: The CSS as a string.\n        is_defaults: True if the CSS is default (i.e. that defined at the widget level).\n            False if it's user CSS (which will override the defaults).\n        tie_breaker: Specificity tie breaker.\n        scope: Scope of CSS.\n    \"\"\"\n\n    content: str\n    is_defaults: bool\n    tie_breaker: int = 0\n    scope: str = \"\"\n\n\n@rich.repr.auto(angular=True)\nclass Stylesheet:\n    \"\"\"A Stylesheet generated from Textual CSS.\"\"\"\n\n    def __init__(self, *, variables: dict[str, str] | None = None) -> None:\n        self._rules: list[RuleSet] = []\n        self._rules_map: dict[str, list[RuleSet]] | None = None\n        self._variables = variables or {}\n        self.__variable_tokens: dict[str, list[Token]] | None = None\n        self.source: dict[CSSLocation, CssSource] = {}\n        self._require_parse = False\n        self._invalid_css: set[str] = set()\n        self._parse_cache: LRUCache[tuple, list[RuleSet]] = LRUCache(64)\n\n    def __rich_repr__(self) -> rich.repr.Result:\n        yield list(self.source.keys())\n\n    @property\n    def _variable_tokens(self) -> dict[str, list[Token]]:\n        if self.__variable_tokens is None:\n            self.__variable_tokens = tokenize_values(self._variables)\n        return self.__variable_tokens\n\n    @property\n    def rules(self) -> list[RuleSet]:\n        \"\"\"List of rule sets.\n\n        Returns:\n            List of rules sets for this stylesheet.\n        \"\"\"\n        if self._require_parse:\n            self.parse()\n            self._require_parse = False\n        assert self._rules is not None\n        return self._rules\n\n    @property\n    def rules_map(self) -> dict[str, list[RuleSet]]:\n        \"\"\"Structure that maps a selector on to a list of rules.\n\n        Returns:\n            Mapping of selector to rule sets.\n        \"\"\"\n        if self._rules_map is None:\n            rules_map: dict[str, list[RuleSet]] = defaultdict(list)\n            for rule in self.rules:\n                for name in rule.selector_names:\n                    rules_map[name].append(rule)\n            self._rules_map = dict(rules_map)\n        return self._rules_map\n\n    @property\n    def css(self) -> str:\n        \"\"\"The equivalent TCSS for this stylesheet.\n\n        Note that this may not produce the same content as the file(s) used to generate the stylesheet.\n        \"\"\"\n        return \"\\n\\n\".join(rule_set.css for rule_set in self.rules)\n\n    def copy(self) -> Stylesheet:\n        \"\"\"Create a copy of this stylesheet.\n\n        Returns:\n            New stylesheet.\n        \"\"\"\n        stylesheet = Stylesheet(variables=self._variables.copy())\n        stylesheet.source = self.source.copy()\n        return stylesheet\n\n    def set_variables(self, variables: dict[str, str]) -> None:\n        \"\"\"Set CSS variables.\n\n        Args:\n            variables: A mapping of name to variable.\n        \"\"\"\n        self._variables = variables\n        self.__variable_tokens = None\n        self._invalid_css = set()\n        self._parse_cache.clear()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def read(self, filename: str | PurePath) -> None:\n        \"\"\"Read Textual CSS file.\n\n        Args:\n            filename: Filename of CSS.\n\n        Raises:\n            StylesheetError: If the CSS could not be read.\n            StylesheetParseError: If the CSS is invalid.\n        \"\"\"\n        filename = os.path.expanduser(filename)\n        try:\n            with open(filename, \"rt\") as css_file:\n                css = css_file.read()\n            path = os.path.abspath(filename)\n        except Exception:\n            raise StylesheetError(f\"unable to read CSS file {filename!r}\") from None\n        self.source[(str(path), \"\")] = CssSource(css, False, 0)\n        self._require_parse = True\n\n    def read_all(self, paths: Sequence[PurePath]) -> None:\n        \"\"\"Read multiple CSS files, in order.\n\n        Args:\n            paths: The paths of the CSS files to read, in order.\n\n        Raises:\n            StylesheetError: If the CSS could not be read.\n            StylesheetParseError: If the CSS is invalid.\n        \"\"\"\n        for path in paths:\n            self.read(path)\n\n    def has_source(self, path: str, class_var: str = \"\") -> bool:\n        \"\"\"Check if the stylesheet has this CSS source already.\n\n        Args:\n            path: The file path of the source in question.\n            class_var: The widget class variable we might be reading the CSS from.\n\n        Returns:\n            Whether the stylesheet is aware of this CSS source or not.\n        \"\"\"\n        return (path, class_var) in self.source\n\n    def add_source(\n        self,\n        css: str,\n        read_from: CSSLocation | None = None,\n        is_default_css: bool = False,\n        tie_breaker: int = 0,\n        scope: str = \"\",\n    ) -> None:\n        \"\"\"Parse CSS from a string.\n\n        Args:\n            css: String with CSS source.\n            read_from: The original source location of the CSS.\n            path: The path of the source if a file, or some other identifier.\n            is_default_css: True if the CSS is defined in the Widget, False if the CSS is defined\n                in a user stylesheet.\n            tie_breaker: Integer representing the priority of this source.\n            scope: CSS type name to limit scope or empty string for no scope.\n\n        Raises:\n            StylesheetError: If the CSS could not be read.\n            StylesheetParseError: If the CSS is invalid.\n        \"\"\"\n\n        if read_from is None:\n            read_from = (\"\", str(hash(css)))\n\n        if read_from in self.source and self.source[read_from].content == css:\n            # Location already in source and CSS is identical.\n            content, is_defaults, source_tie_breaker, scope = self.source[read_from]\n            if source_tie_breaker > tie_breaker:\n                self.source[read_from] = CssSource(\n                    content, is_defaults, tie_breaker, scope\n                )\n            return\n        self.source[read_from] = CssSource(css, is_default_css, tie_breaker, scope)\n        self._require_parse = True\n        self._rules_map = None\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def reparse(self) -> None:\n        \"\"\"Re-parse source, applying new variables.\n\n        Raises:\n            StylesheetError: If the CSS could not be read.\n            StylesheetParseError: If the CSS is invalid.\n        \"\"\"\n        # Do this in a fresh Stylesheet so if there are errors we don't break self.\n        stylesheet = Stylesheet(variables=self._variables)\n        for read_from, (css, is_defaults, tie_breaker, scope) in self.source.items():\n            stylesheet.add_source(\n                css,\n                read_from=read_from,\n                is_default_css=is_defaults,\n                tie_breaker=tie_breaker,\n                scope=scope,\n            )\n        try:\n            stylesheet.parse()\n        except Exception:\n            # If we don't update self's invalid CSS, we might end up reparsing this CSS\n            # before Textual quits application mode.\n            # See https://github.com/Textualize/textual/issues/3581.\n            self._invalid_css.update(stylesheet._invalid_css)\n            raise\n        else:\n            self._rules = stylesheet.rules\n            self._rules_map = None\n            self.source = stylesheet.source\n            self._require_parse = False\n\n    @classmethod\n    def _check_rule(\n        cls, rule_set: RuleSet, css_path_nodes: list[DOMNode]\n    ) -> Iterable[Specificity3]:\n        \"\"\"Check a rule set, return specificity of applicable rules.\n\n        Args:\n            rule_set: A rule set.\n            css_path_nodes: A list of the nodes from the App to the node being checked.\n\n        Yields:\n            Specificity of any matching selectors.\n        \"\"\"\n        for selector_set in rule_set.selector_set:\n            if _check_selectors(selector_set.selectors, css_path_nodes):\n                yield selector_set.specificity\n\n    def apply(\n        self,\n        node: DOMNode,\n        *,\n        animate: bool = False,\n        cache: dict[tuple, RulesMap] | None = None,\n    ) -> None:\n        \"\"\"Apply the stylesheet to a DOM node.\n\n        Args:\n            node: The `DOMNode` to apply the stylesheet to.\n                Applies the styles defined in this `Stylesheet` to the node.\n                If the same rule is defined multiple times for the node (e.g. multiple\n                classes modifying the same CSS property), then only the most specific\n                rule will be applied.\n            animate: Animate changed rules.\n            cache: An optional cache when applying a group of nodes.\n        \"\"\"\n        # Dictionary of rule attribute names e.g. \"text_background\" to list of tuples.\n        # The tuples contain the rule specificity, and the value for that rule.\n        # We can use this to determine, for a given rule, whether we should apply it\n        # or not by examining the specificity. If we have two rules for the\n        # same attribute, then we can choose the most specific rule and use that.\n        rule_attributes: defaultdict[str, list[tuple[Specificity6, object]]]\n        rule_attributes = defaultdict(list)\n\n        rules_map = self.rules_map\n\n        # Discard rules which are not applicable early\n        limit_rules = {\n            rule\n            for name in rules_map.keys() & node._selector_names\n            for rule in rules_map[name]\n        }\n        rules = list(filter(limit_rules.__contains__, reversed(self.rules)))\n\n        node._has_hover_style = any(\"hover\" in rule.pseudo_classes for rule in rules)\n        node._has_focus_within = any(\n            \"focus-within\" in rule.pseudo_classes for rule in rules\n        )\n\n        cache_key: tuple | None\n        if cache is not None:\n            cache_key = (\n                node._parent,\n                (\n                    None\n                    if node._id is None\n                    else (node._id if f\"#{node._id}\" in rules_map else None)\n                ),\n                node.classes,\n                node.pseudo_classes,\n                node._css_type_name,\n            )\n            cached_result: RulesMap | None = cache.get(cache_key)\n            if cached_result is not None:\n                self.replace_rules(node, cached_result, animate=animate)\n                self._process_component_classes(node)\n                return\n        else:\n            cache_key = None\n\n        _check_rule = self._check_rule\n        css_path_nodes = node.css_path_nodes\n\n        # Rules that may be set to the special value `initial`\n        initial: set[str] = set()\n        # Rules in DEFAULT_CSS set to the special value `initial`\n        initial_defaults: set[str] = set()\n\n        for rule in rules:\n            is_default_rules = rule.is_default_rules\n            tie_breaker = rule.tie_breaker\n            for base_specificity in _check_rule(rule, css_path_nodes):\n                for key, rule_specificity, value in rule.styles.extract_rules(\n                    base_specificity, is_default_rules, tie_breaker\n                ):\n                    if value is None:\n                        if is_default_rules:\n                            initial_defaults.add(key)\n                        else:\n                            initial.add(key)\n                    rule_attributes[key].append((rule_specificity, value))\n\n        if rule_attributes:\n            # For each rule declared for this node, keep only the most specific one\n            get_first_item = itemgetter(0)\n            node_rules: RulesMap = cast(\n                RulesMap,\n                {\n                    name: max(specificity_rules, key=get_first_item)[1]\n                    for name, specificity_rules in rule_attributes.items()\n                },\n            )\n\n            # Set initial values\n            for initial_rule_name in initial:\n                # Rules with a value of None should be set to the default value\n                if node_rules[initial_rule_name] is None:  # type: ignore[literal-required]\n                    # Exclude non default values\n                    # rule[0] is the specificity, rule[0][0] is 0 for default rules\n                    default_rules = [\n                        rule\n                        for rule in rule_attributes[initial_rule_name]\n                        if not rule[0][0]\n                    ]\n                    if default_rules:\n                        # There is a default value\n                        new_value = max(default_rules, key=get_first_item)[1]\n                        node_rules[initial_rule_name] = new_value  # type: ignore[literal-required]\n                    else:\n                        # No default value\n                        initial_defaults.add(initial_rule_name)\n\n            # Rules in DEFAULT_CSS set to initial\n            for initial_rule_name in initial_defaults:\n                if node_rules[initial_rule_name] is None:  # type: ignore[literal-required]\n                    default_rules = [\n                        rule\n                        for rule in rule_attributes[initial_rule_name]\n                        if rule[0][0]\n                    ]\n                    if default_rules:\n                        # There is a default value\n                        rule_value = max(default_rules, key=get_first_item)[1]\n                    else:\n                        rule_value = getattr(_DEFAULT_STYLES, initial_rule_name)\n                    node_rules[initial_rule_name] = rule_value  # type: ignore[literal-required]\n\n            if cache is not None:\n                assert cache_key is not None\n                cache[cache_key] = node_rules\n            self.replace_rules(node, node_rules, animate=animate)\n        self._process_component_classes(node)\n\n    def _process_component_classes(self, node: DOMNode) -> None:\n        \"\"\"Process component classes for the given node.\n\n        Args:\n            node: A DOM Node.\n        \"\"\"\n        component_classes = node._get_component_classes()\n        if component_classes:\n            # Create virtual nodes that exist to extract styles\n            refresh_node = False\n            old_component_styles = node._component_styles.copy()\n            node._component_styles.clear()\n            for component in sorted(component_classes):\n                virtual_node = DOMNode(classes=component)\n                virtual_node._attach(node)\n                self.apply(virtual_node, animate=False)\n                if (\n                    not refresh_node\n                    and old_component_styles.get(component) != virtual_node.styles\n                ):\n                    # If the styles have changed we want to refresh the node\n                    refresh_node = True\n                node._component_styles[component] = virtual_node.styles\n            if refresh_node:\n                node.refresh()\n\n    @classmethod\n    def replace_rules(\n        cls, node: DOMNode, rules: RulesMap, animate: bool = False\n    ) -> None:\n        \"\"\"Replace style rules on a node, animating as required.\n\n        Args:\n            node: A DOM node.\n            rules: Mapping of rules.\n            animate: Enable animation.\n        \"\"\"\n\n        # Alias styles and base styles\n        styles = node.styles\n        base_styles = styles.base\n\n        # Styles currently used on new rules\n        modified_rule_keys = base_styles._rules.keys() | rules.keys()\n\n        if animate:\n            new_styles = Styles(node, rules)\n            if new_styles == base_styles:\n                # Nothing to animate, return early\n                return\n            current_render_rules = styles.get_render_rules()\n            is_animatable = styles.is_animatable\n            get_current_render_rule = current_render_rules.get\n            new_render_rules = new_styles.get_render_rules()\n            get_new_render_rule = new_render_rules.get\n            animator = node.app.animator\n            base = node.styles.base\n            for key in modified_rule_keys:\n                # Get old and new render rules\n                old_render_value = get_current_render_rule(key)\n                new_render_value = get_new_render_rule(key)\n                # Get new rule value (may be None)\n                new_value = rules.get(key)\n\n                # Check if this can / should be animated. It doesn't suffice to check\n                # if the current and target values are different because a previous\n                # animation may have been scheduled but may have not started yet.\n                if is_animatable(key) and (\n                    new_render_value != old_render_value\n                    or animator.is_being_animated(base, key)\n                ):\n                    transition = new_styles._get_transition(key)\n                    if transition is not None:\n                        duration, easing, delay = transition\n                        animator.animate(\n                            base,\n                            key,\n                            new_render_value,\n                            final_value=new_value,\n                            duration=duration,\n                            delay=delay,\n                            easing=easing,\n                        )\n                        continue\n                # Default is to set value (if new_value is None, rule will be removed)\n                setattr(base_styles, key, new_value)\n        else:\n            # Not animated, so we apply the rules directly\n            get_rule = rules.get\n\n            for key in modified_rule_keys:\n                setattr(base_styles, key, get_rule(key))\n\n        node.notify_style_update()\n\n    def update(self, root: DOMNode, animate: bool = False) -> None:\n        \"\"\"Update styles on node and its children.\n\n        Args:\n            root: Root note to update.\n            animate: Enable CSS animation.\n        \"\"\"\n\n        self.update_nodes(root.walk_children(with_self=True), animate=animate)\n\n    def update_nodes(self, nodes: Iterable[DOMNode], animate: bool = False) -> None:\n        \"\"\"Update styles for nodes.\n\n        Args:\n            nodes: Nodes to update.\n            animate: Enable CSS animation.\n        \"\"\"\n        cache: dict[tuple, RulesMap] = {}\n        apply = self.apply\n\n        for node in nodes:\n            apply(node, animate=animate, cache=cache)\n            if isinstance(node, Widget) and node.is_scrollable:\n                if node.show_vertical_scrollbar:\n                    apply(node.vertical_scrollbar, cache=cache)\n                if node.show_horizontal_scrollbar:\n                    apply(node.horizontal_scrollbar, cache=cache)\n                if node.show_horizontal_scrollbar and node.show_vertical_scrollbar:\n                    apply(node.scrollbar_corner, cache=cache)\n\n```\n# Function Signature: def _parse_rules(\n# Natural Language Annotations: \"\"\"Parse CSS and return rules.\n\n        Args:\n            css: String containing Textual CSS.\n            read_from: Original CSS location.\n            is_default_rules: True if the rules we're extracting are\n                default (i.e. in Widget.DEFAULT_CSS) rules. False if they're from user defined CSS.\n            scope: Scope of rules, or empty string for global scope.\n\n        Raises:\n            StylesheetError: If the CSS is invalid.\n\n        Returns:\n            List of RuleSets.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef _parse_rules(\n        css: str,\n", "  ```\nfrom typing import Optional, List\n\nfrom .parse import Node", "  ```\nimport os\nimport re\nfrom collections import defaultdict\nfrom"]}, "textual-src/textual/css/stylesheet.py-parse": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/textual/src/textual/css/stylesheet.py:\n```\nfrom __future__ import annotations\n\nimport os\nfrom collections import defaultdict\nfrom itertools import chain\nfrom operator import itemgetter\nfrom pathlib import Path, PurePath\nfrom typing import Iterable, NamedTuple, Sequence, cast\n\nimport rich.repr\nfrom rich.console import Console, ConsoleOptions, RenderableType, RenderResult\nfrom rich.markup import render\nfrom rich.padding import Padding\nfrom rich.panel import Panel\nfrom rich.text import Text\n\nfrom ..cache import LRUCache\nfrom ..dom import DOMNode\nfrom ..widget import Widget\nfrom .errors import StylesheetError\nfrom .match import _check_selectors\nfrom .model import RuleSet\nfrom .parse import parse\nfrom .styles import RulesMap, Styles\nfrom .tokenize import Token, tokenize_values\nfrom .tokenizer import TokenError\nfrom .types import CSSLocation, Specificity3, Specificity6\n\n_DEFAULT_STYLES = Styles()\n\n\nclass StylesheetParseError(StylesheetError):\n    \"\"\"Raised when the stylesheet could not be parsed.\"\"\"\n\n    def __init__(self, errors: StylesheetErrors) -> None:\n        self.errors = errors\n\n    def __rich__(self) -> RenderableType:\n        return self.errors\n\n\nclass StylesheetErrors:\n    \"\"\"A renderable for stylesheet errors.\"\"\"\n\n    def __init__(self, rules: list[RuleSet]) -> None:\n        self.rules = rules\n        self.variables: dict[str, str] = {}\n\n    @classmethod\n    def _get_snippet(cls, code: str, line_no: int) -> RenderableType:\n        from rich.syntax import Syntax\n\n        syntax = Syntax(\n            code,\n            lexer=\"scss\",\n            theme=\"ansi_light\",\n            line_numbers=True,\n            indent_guides=True,\n            line_range=(max(0, line_no - 2), line_no + 2),\n            highlight_lines={line_no},\n        )\n        return syntax\n\n    def __rich_console__(\n        self, console: Console, options: ConsoleOptions\n    ) -> RenderResult:\n        error_count = 0\n        errors = list(\n            dict.fromkeys(chain.from_iterable(_rule.errors for _rule in self.rules))\n        )\n\n        for token, message in errors:\n            error_count += 1\n\n            if token.referenced_by:\n                line_idx, col_idx = token.referenced_by.location\n            else:\n                line_idx, col_idx = token.location\n            line_no, col_no = line_idx + 1, col_idx + 1\n\n            display_path, widget_var = token.read_from\n            if display_path:\n                link_path = str(Path(display_path).absolute())\n                filename = Path(link_path).name\n            else:\n                link_path = \"\"\n                filename = \"<unknown>\"\n            # If we have a widget/variable from where the CSS was read, then line/column\n            # numbers are relative to the inline CSS and we'll display them next to the\n            # widget/variable.\n            # Otherwise, they're absolute positions in a TCSS file and we can show them\n            # next to the file path.\n            if widget_var:\n                path_string = link_path or filename\n                widget_string = f\" in {widget_var}:{line_no}:{col_no}\"\n            else:\n                path_string = f\"{link_path or filename}:{line_no}:{col_no}\"\n                widget_string = \"\"\n\n            title = Text.assemble(\n                \"Error at \", path_string, widget_string, style=\"bold red\"\n            )\n            yield \"\"\n            yield Panel(\n                self._get_snippet(\n                    token.referenced_by.code if token.referenced_by else token.code,\n                    line_no,\n                ),\n                title=title,\n                title_align=\"left\",\n                border_style=\"red\",\n            )\n            yield Padding(message, pad=(0, 0, 1, 3))\n\n        yield \"\"\n        yield render(\n            f\" [b][red]CSS parsing failed:[/] {error_count} error{'s' if error_count != 1 else ''}[/] found in stylesheet\"\n        )\n\n\nclass CssSource(NamedTuple):\n    \"\"\"Contains the CSS content and whether or not the CSS comes from user defined stylesheets\n    vs widget-level stylesheets.\n\n    Args:\n        content: The CSS as a string.\n        is_defaults: True if the CSS is default (i.e. that defined at the widget level).\n            False if it's user CSS (which will override the defaults).\n        tie_breaker: Specificity tie breaker.\n        scope: Scope of CSS.\n    \"\"\"\n\n    content: str\n    is_defaults: bool\n    tie_breaker: int = 0\n    scope: str = \"\"\n\n\n@rich.repr.auto(angular=True)\nclass Stylesheet:\n    \"\"\"A Stylesheet generated from Textual CSS.\"\"\"\n\n    def __init__(self, *, variables: dict[str, str] | None = None) -> None:\n        self._rules: list[RuleSet] = []\n        self._rules_map: dict[str, list[RuleSet]] | None = None\n        self._variables = variables or {}\n        self.__variable_tokens: dict[str, list[Token]] | None = None\n        self.source: dict[CSSLocation, CssSource] = {}\n        self._require_parse = False\n        self._invalid_css: set[str] = set()\n        self._parse_cache: LRUCache[tuple, list[RuleSet]] = LRUCache(64)\n\n    def __rich_repr__(self) -> rich.repr.Result:\n        yield list(self.source.keys())\n\n    @property\n    def _variable_tokens(self) -> dict[str, list[Token]]:\n        if self.__variable_tokens is None:\n            self.__variable_tokens = tokenize_values(self._variables)\n        return self.__variable_tokens\n\n    @property\n    def rules(self) -> list[RuleSet]:\n        \"\"\"List of rule sets.\n\n        Returns:\n            List of rules sets for this stylesheet.\n        \"\"\"\n        if self._require_parse:\n            self.parse()\n            self._require_parse = False\n        assert self._rules is not None\n        return self._rules\n\n    @property\n    def rules_map(self) -> dict[str, list[RuleSet]]:\n        \"\"\"Structure that maps a selector on to a list of rules.\n\n        Returns:\n            Mapping of selector to rule sets.\n        \"\"\"\n        if self._rules_map is None:\n            rules_map: dict[str, list[RuleSet]] = defaultdict(list)\n            for rule in self.rules:\n                for name in rule.selector_names:\n                    rules_map[name].append(rule)\n            self._rules_map = dict(rules_map)\n        return self._rules_map\n\n    @property\n    def css(self) -> str:\n        \"\"\"The equivalent TCSS for this stylesheet.\n\n        Note that this may not produce the same content as the file(s) used to generate the stylesheet.\n        \"\"\"\n        return \"\\n\\n\".join(rule_set.css for rule_set in self.rules)\n\n    def copy(self) -> Stylesheet:\n        \"\"\"Create a copy of this stylesheet.\n\n        Returns:\n            New stylesheet.\n        \"\"\"\n        stylesheet = Stylesheet(variables=self._variables.copy())\n        stylesheet.source = self.source.copy()\n        return stylesheet\n\n    def set_variables(self, variables: dict[str, str]) -> None:\n        \"\"\"Set CSS variables.\n\n        Args:\n            variables: A mapping of name to variable.\n        \"\"\"\n        self._variables = variables\n        self.__variable_tokens = None\n        self._invalid_css = set()\n        self._parse_cache.clear()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def read(self, filename: str | PurePath) -> None:\n        \"\"\"Read Textual CSS file.\n\n        Args:\n            filename: Filename of CSS.\n\n        Raises:\n            StylesheetError: If the CSS could not be read.\n            StylesheetParseError: If the CSS is invalid.\n        \"\"\"\n        filename = os.path.expanduser(filename)\n        try:\n            with open(filename, \"rt\") as css_file:\n                css = css_file.read()\n            path = os.path.abspath(filename)\n        except Exception:\n            raise StylesheetError(f\"unable to read CSS file {filename!r}\") from None\n        self.source[(str(path), \"\")] = CssSource(css, False, 0)\n        self._require_parse = True\n\n    def read_all(self, paths: Sequence[PurePath]) -> None:\n        \"\"\"Read multiple CSS files, in order.\n\n        Args:\n            paths: The paths of the CSS files to read, in order.\n\n        Raises:\n            StylesheetError: If the CSS could not be read.\n            StylesheetParseError: If the CSS is invalid.\n        \"\"\"\n        for path in paths:\n            self.read(path)\n\n    def has_source(self, path: str, class_var: str = \"\") -> bool:\n        \"\"\"Check if the stylesheet has this CSS source already.\n\n        Args:\n            path: The file path of the source in question.\n            class_var: The widget class variable we might be reading the CSS from.\n\n        Returns:\n            Whether the stylesheet is aware of this CSS source or not.\n        \"\"\"\n        return (path, class_var) in self.source\n\n    def add_source(\n        self,\n        css: str,\n        read_from: CSSLocation | None = None,\n        is_default_css: bool = False,\n        tie_breaker: int = 0,\n        scope: str = \"\",\n    ) -> None:\n        \"\"\"Parse CSS from a string.\n\n        Args:\n            css: String with CSS source.\n            read_from: The original source location of the CSS.\n            path: The path of the source if a file, or some other identifier.\n            is_default_css: True if the CSS is defined in the Widget, False if the CSS is defined\n                in a user stylesheet.\n            tie_breaker: Integer representing the priority of this source.\n            scope: CSS type name to limit scope or empty string for no scope.\n\n        Raises:\n            StylesheetError: If the CSS could not be read.\n            StylesheetParseError: If the CSS is invalid.\n        \"\"\"\n\n        if read_from is None:\n            read_from = (\"\", str(hash(css)))\n\n        if read_from in self.source and self.source[read_from].content == css:\n            # Location already in source and CSS is identical.\n            content, is_defaults, source_tie_breaker, scope = self.source[read_from]\n            if source_tie_breaker > tie_breaker:\n                self.source[read_from] = CssSource(\n                    content, is_defaults, tie_breaker, scope\n                )\n            return\n        self.source[read_from] = CssSource(css, is_default_css, tie_breaker, scope)\n        self._require_parse = True\n        self._rules_map = None\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def reparse(self) -> None:\n        \"\"\"Re-parse source, applying new variables.\n\n        Raises:\n            StylesheetError: If the CSS could not be read.\n            StylesheetParseError: If the CSS is invalid.\n        \"\"\"\n        # Do this in a fresh Stylesheet so if there are errors we don't break self.\n        stylesheet = Stylesheet(variables=self._variables)\n        for read_from, (css, is_defaults, tie_breaker, scope) in self.source.items():\n            stylesheet.add_source(\n                css,\n                read_from=read_from,\n                is_default_css=is_defaults,\n                tie_breaker=tie_breaker,\n                scope=scope,\n            )\n        try:\n            stylesheet.parse()\n        except Exception:\n            # If we don't update self's invalid CSS, we might end up reparsing this CSS\n            # before Textual quits application mode.\n            # See https://github.com/Textualize/textual/issues/3581.\n            self._invalid_css.update(stylesheet._invalid_css)\n            raise\n        else:\n            self._rules = stylesheet.rules\n            self._rules_map = None\n            self.source = stylesheet.source\n            self._require_parse = False\n\n    @classmethod\n    def _check_rule(\n        cls, rule_set: RuleSet, css_path_nodes: list[DOMNode]\n    ) -> Iterable[Specificity3]:\n        \"\"\"Check a rule set, return specificity of applicable rules.\n\n        Args:\n            rule_set: A rule set.\n            css_path_nodes: A list of the nodes from the App to the node being checked.\n\n        Yields:\n            Specificity of any matching selectors.\n        \"\"\"\n        for selector_set in rule_set.selector_set:\n            if _check_selectors(selector_set.selectors, css_path_nodes):\n                yield selector_set.specificity\n\n    def apply(\n        self,\n        node: DOMNode,\n        *,\n        animate: bool = False,\n        cache: dict[tuple, RulesMap] | None = None,\n    ) -> None:\n        \"\"\"Apply the stylesheet to a DOM node.\n\n        Args:\n            node: The `DOMNode` to apply the stylesheet to.\n                Applies the styles defined in this `Stylesheet` to the node.\n                If the same rule is defined multiple times for the node (e.g. multiple\n                classes modifying the same CSS property), then only the most specific\n                rule will be applied.\n            animate: Animate changed rules.\n            cache: An optional cache when applying a group of nodes.\n        \"\"\"\n        # Dictionary of rule attribute names e.g. \"text_background\" to list of tuples.\n        # The tuples contain the rule specificity, and the value for that rule.\n        # We can use this to determine, for a given rule, whether we should apply it\n        # or not by examining the specificity. If we have two rules for the\n        # same attribute, then we can choose the most specific rule and use that.\n        rule_attributes: defaultdict[str, list[tuple[Specificity6, object]]]\n        rule_attributes = defaultdict(list)\n\n        rules_map = self.rules_map\n\n        # Discard rules which are not applicable early\n        limit_rules = {\n            rule\n            for name in rules_map.keys() & node._selector_names\n            for rule in rules_map[name]\n        }\n        rules = list(filter(limit_rules.__contains__, reversed(self.rules)))\n\n        node._has_hover_style = any(\"hover\" in rule.pseudo_classes for rule in rules)\n        node._has_focus_within = any(\n            \"focus-within\" in rule.pseudo_classes for rule in rules\n        )\n\n        cache_key: tuple | None\n        if cache is not None:\n            cache_key = (\n                node._parent,\n                (\n                    None\n                    if node._id is None\n                    else (node._id if f\"#{node._id}\" in rules_map else None)\n                ),\n                node.classes,\n                node.pseudo_classes,\n                node._css_type_name,\n            )\n            cached_result: RulesMap | None = cache.get(cache_key)\n            if cached_result is not None:\n                self.replace_rules(node, cached_result, animate=animate)\n                self._process_component_classes(node)\n                return\n        else:\n            cache_key = None\n\n        _check_rule = self._check_rule\n        css_path_nodes = node.css_path_nodes\n\n        # Rules that may be set to the special value `initial`\n        initial: set[str] = set()\n        # Rules in DEFAULT_CSS set to the special value `initial`\n        initial_defaults: set[str] = set()\n\n        for rule in rules:\n            is_default_rules = rule.is_default_rules\n            tie_breaker = rule.tie_breaker\n            for base_specificity in _check_rule(rule, css_path_nodes):\n                for key, rule_specificity, value in rule.styles.extract_rules(\n                    base_specificity, is_default_rules, tie_breaker\n                ):\n                    if value is None:\n                        if is_default_rules:\n                            initial_defaults.add(key)\n                        else:\n                            initial.add(key)\n                    rule_attributes[key].append((rule_specificity, value))\n\n        if rule_attributes:\n            # For each rule declared for this node, keep only the most specific one\n            get_first_item = itemgetter(0)\n            node_rules: RulesMap = cast(\n                RulesMap,\n                {\n                    name: max(specificity_rules, key=get_first_item)[1]\n                    for name, specificity_rules in rule_attributes.items()\n                },\n            )\n\n            # Set initial values\n            for initial_rule_name in initial:\n                # Rules with a value of None should be set to the default value\n                if node_rules[initial_rule_name] is None:  # type: ignore[literal-required]\n                    # Exclude non default values\n                    # rule[0] is the specificity, rule[0][0] is 0 for default rules\n                    default_rules = [\n                        rule\n                        for rule in rule_attributes[initial_rule_name]\n                        if not rule[0][0]\n                    ]\n                    if default_rules:\n                        # There is a default value\n                        new_value = max(default_rules, key=get_first_item)[1]\n                        node_rules[initial_rule_name] = new_value  # type: ignore[literal-required]\n                    else:\n                        # No default value\n                        initial_defaults.add(initial_rule_name)\n\n            # Rules in DEFAULT_CSS set to initial\n            for initial_rule_name in initial_defaults:\n                if node_rules[initial_rule_name] is None:  # type: ignore[literal-required]\n                    default_rules = [\n                        rule\n                        for rule in rule_attributes[initial_rule_name]\n                        if rule[0][0]\n                    ]\n                    if default_rules:\n                        # There is a default value\n                        rule_value = max(default_rules, key=get_first_item)[1]\n                    else:\n                        rule_value = getattr(_DEFAULT_STYLES, initial_rule_name)\n                    node_rules[initial_rule_name] = rule_value  # type: ignore[literal-required]\n\n            if cache is not None:\n                assert cache_key is not None\n                cache[cache_key] = node_rules\n            self.replace_rules(node, node_rules, animate=animate)\n        self._process_component_classes(node)\n\n    def _process_component_classes(self, node: DOMNode) -> None:\n        \"\"\"Process component classes for the given node.\n\n        Args:\n            node: A DOM Node.\n        \"\"\"\n        component_classes = node._get_component_classes()\n        if component_classes:\n            # Create virtual nodes that exist to extract styles\n            refresh_node = False\n            old_component_styles = node._component_styles.copy()\n            node._component_styles.clear()\n            for component in sorted(component_classes):\n                virtual_node = DOMNode(classes=component)\n                virtual_node._attach(node)\n                self.apply(virtual_node, animate=False)\n                if (\n                    not refresh_node\n                    and old_component_styles.get(component) != virtual_node.styles\n                ):\n                    # If the styles have changed we want to refresh the node\n                    refresh_node = True\n                node._component_styles[component] = virtual_node.styles\n            if refresh_node:\n                node.refresh()\n\n    @classmethod\n    def replace_rules(\n        cls, node: DOMNode, rules: RulesMap, animate: bool = False\n    ) -> None:\n        \"\"\"Replace style rules on a node, animating as required.\n\n        Args:\n            node: A DOM node.\n            rules: Mapping of rules.\n            animate: Enable animation.\n        \"\"\"\n\n        # Alias styles and base styles\n        styles = node.styles\n        base_styles = styles.base\n\n        # Styles currently used on new rules\n        modified_rule_keys = base_styles._rules.keys() | rules.keys()\n\n        if animate:\n            new_styles = Styles(node, rules)\n            if new_styles == base_styles:\n                # Nothing to animate, return early\n                return\n            current_render_rules = styles.get_render_rules()\n            is_animatable = styles.is_animatable\n            get_current_render_rule = current_render_rules.get\n            new_render_rules = new_styles.get_render_rules()\n            get_new_render_rule = new_render_rules.get\n            animator = node.app.animator\n            base = node.styles.base\n            for key in modified_rule_keys:\n                # Get old and new render rules\n                old_render_value = get_current_render_rule(key)\n                new_render_value = get_new_render_rule(key)\n                # Get new rule value (may be None)\n                new_value = rules.get(key)\n\n                # Check if this can / should be animated. It doesn't suffice to check\n                # if the current and target values are different because a previous\n                # animation may have been scheduled but may have not started yet.\n                if is_animatable(key) and (\n                    new_render_value != old_render_value\n                    or animator.is_being_animated(base, key)\n                ):\n                    transition = new_styles._get_transition(key)\n                    if transition is not None:\n                        duration, easing, delay = transition\n                        animator.animate(\n                            base,\n                            key,\n                            new_render_value,\n                            final_value=new_value,\n                            duration=duration,\n                            delay=delay,\n                            easing=easing,\n                        )\n                        continue\n                # Default is to set value (if new_value is None, rule will be removed)\n                setattr(base_styles, key, new_value)\n        else:\n            # Not animated, so we apply the rules directly\n            get_rule = rules.get\n\n            for key in modified_rule_keys:\n                setattr(base_styles, key, get_rule(key))\n\n        node.notify_style_update()\n\n    def update(self, root: DOMNode, animate: bool = False) -> None:\n        \"\"\"Update styles on node and its children.\n\n        Args:\n            root: Root note to update.\n            animate: Enable CSS animation.\n        \"\"\"\n\n        self.update_nodes(root.walk_children(with_self=True), animate=animate)\n\n    def update_nodes(self, nodes: Iterable[DOMNode], animate: bool = False) -> None:\n        \"\"\"Update styles for nodes.\n\n        Args:\n            nodes: Nodes to update.\n            animate: Enable CSS animation.\n        \"\"\"\n        cache: dict[tuple, RulesMap] = {}\n        apply = self.apply\n\n        for node in nodes:\n            apply(node, animate=animate, cache=cache)\n            if isinstance(node, Widget) and node.is_scrollable:\n                if node.show_vertical_scrollbar:\n                    apply(node.vertical_scrollbar, cache=cache)\n                if node.show_horizontal_scrollbar:\n                    apply(node.horizontal_scrollbar, cache=cache)\n                if node.show_horizontal_scrollbar and node.show_vertical_scrollbar:\n                    apply(node.scrollbar_corner, cache=cache)\n\n```\n# Function Signature: def parse(self) -> None:\n# Natural Language Annotations: \"\"\"Parse the source in the stylesheet.\n\n        Raises:\n            StylesheetParseError: If there are any CSS related errors.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef parse(self) -> None:\n    self.__init", "  ```python\ndef parse(self) -> None:\n    \"\"\"Parse", "  ```\ndef parse(self) -> None:\n    \"\"\"Parse the"]}, "gpt-engineer-gpt_engineer/core/chat_to_files.py-parse_diffs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/gpt-engineer/gpt_engineer/core/chat_to_files.py:\n```\n\"\"\"\nThis Python script provides functionalities for parsing chat transcripts that contain file paths and code blocks,\napplying diffs to these files, and parsing unified git diff format strings. The script is designed to work within\na larger system that involves processing and manipulating code files based on chat inputs and diff information.\n\nKey Components:\n- chat_to_files_dict: Parses a chat transcript, extracting file paths and associated code blocks, and organizes\n  them into a FilesDict object, which is a custom dictionary format designed to hold file contents keyed by their paths.\n\n- apply_diffs: Takes a dictionary of Diff objects (which represent changes to be made to files) and a FilesDict\n  object containing the current state of files. It applies the changes described by the Diff objects to the\n  corresponding files in the FilesDict, updating the file contents as specified by the diffs.\n\n- parse_diffs: Parses a string containing diffs in the unified git diff format, extracting the changes described\n  in the diffs and organizing them into a dictionary of Diff objects, keyed by the filename to which each diff applies.\n\n- parse_diff_block: Parses a single block of text from a diff string, translating it into a Diff object that\n  represents the changes described in that block of text.\n\nThis script is intended for use in environments where code collaboration or review is conducted through chat interfaces,\nallowing for the dynamic application of changes to code bases and the efficient handling of file and diff information in chat transcripts.\n\"\"\"\n\nimport logging\nimport re\n\nfrom typing import Dict, Tuple\n\nfrom regex import regex\n\nfrom gpt_engineer.core.diff import ADD, REMOVE, RETAIN, Diff, Hunk\nfrom gpt_engineer.core.files_dict import FilesDict, file_to_lines_dict\n\n# Initialize a logger for this module\nlogger = logging.getLogger(__name__)\n\n\ndef chat_to_files_dict(chat: str) -> FilesDict:\n    \"\"\"\n    Converts a chat string containing file paths and code blocks into a FilesDict object.\n\n    Args:\n    - chat (str): The chat string containing file paths and code blocks.\n\n    Returns:\n    - FilesDict: A dictionary with file paths as keys and code blocks as values.\n    \"\"\"\n    # Regex to match file paths and associated code blocks\n    regex = r\"(\\S+)\\n\\s*```[^\\n]*\\n(.+?)```\"\n    matches = re.finditer(regex, chat, re.DOTALL)\n\n    files_dict = FilesDict()\n    for match in matches:\n        # Clean and standardize the file path\n        path = re.sub(r'[\\:<>\"|?*]', \"\", match.group(1))\n        path = re.sub(r\"^\\[(.*)\\]$\", r\"\\1\", path)\n        path = re.sub(r\"^`(.*)`$\", r\"\\1\", path)\n        path = re.sub(r\"[\\]\\:]$\", \"\", path)\n\n        # Extract and clean the code content\n        content = match.group(2)\n\n        # Add the cleaned path and content to the FilesDict\n        files_dict[path.strip()] = content.strip()\n\n    return files_dict\n\n\ndef apply_diffs(diffs: Dict[str, Diff], files: FilesDict) -> FilesDict:\n    \"\"\"\n    Applies diffs to the provided files.\n\n    Args:\n    - diffs (Dict[str, Diff]): A dictionary of diffs to apply, keyed by filename.\n    - files (FilesDict): The original files to which diffs will be applied.\n\n    Returns:\n    - FilesDict: The updated files after applying diffs.\n    \"\"\"\n    files = FilesDict(files.copy())\n    REMOVE_FLAG = \"<REMOVE_LINE>\"  # Placeholder to mark lines for removal\n    for diff in diffs.values():\n        if diff.is_new_file():\n            # If it's a new file, create it with the content from the diff\n            files[diff.filename_post] = \"\\n\".join(\n                line[1] for hunk in diff.hunks for line in hunk.lines\n            )\n        else:\n            # Convert the file content to a dictionary of lines\n            line_dict = file_to_lines_dict(files[diff.filename_pre])\n            for hunk in diff.hunks:\n                current_line = hunk.start_line_pre_edit\n                for line in hunk.lines:\n                    if line[0] == RETAIN:\n                        current_line += 1\n                    elif line[0] == ADD:\n                        # Handle added lines\n                        current_line -= 1\n                        if (\n                            current_line in line_dict.keys()\n                            and line_dict[current_line] != REMOVE_FLAG\n                        ):\n                            line_dict[current_line] += \"\\n\" + line[1]\n                        else:\n                            line_dict[current_line] = line[1]\n                        current_line += 1\n                    elif line[0] == REMOVE:\n                        # Mark removed lines with REMOVE_FLAG\n                        line_dict[current_line] = REMOVE_FLAG\n                        current_line += 1\n\n            # Remove lines marked for removal\n            line_dict = {\n                key: line_content\n                for key, line_content in line_dict.items()\n                if REMOVE_FLAG not in line_content\n            }\n            # Reassemble the file content\n            files[diff.filename_post] = \"\\n\".join(line_dict.values())\n    return files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef parse_diff_block(diff_block: str) -> dict:\n    \"\"\"\n    Parses a block of diff text into a Diff object.\n\n    Args:\n    - diff_block (str): A single block of diff text.\n\n    Returns:\n    - dict: A dictionary containing a single Diff object keyed by the post-edit filename.\n    \"\"\"\n    lines = diff_block.strip().split(\"\\n\")[1:-1]  # Exclude the opening and closing ```\n    diffs = {}\n    current_diff = None\n    hunk_lines = []\n    filename_pre = None\n    filename_post = None\n    hunk_header = None\n\n    for line in lines:\n        if line.startswith(\"--- \"):\n            # Pre-edit filename\n            filename_pre = line[4:]\n        elif line.startswith(\"+++ \"):\n            # Post-edit filename and initiation of a new Diff object\n            if (\n                filename_post is not None\n                and current_diff is not None\n                and hunk_header is not None\n            ):\n                current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n                hunk_lines = []\n            filename_post = line[4:]\n            current_diff = Diff(filename_pre, filename_post)\n            diffs[filename_post] = current_diff\n        elif line.startswith(\"@@ \"):\n            # Start of a new hunk in the diff\n            if hunk_lines and current_diff is not None and hunk_header is not None:\n                current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n                hunk_lines = []\n            hunk_header = parse_hunk_header(line)\n        elif line.startswith(\"+\"):\n            # Added line\n            hunk_lines.append((ADD, line[1:]))\n        elif line.startswith(\"-\"):\n            # Removed line\n            hunk_lines.append((REMOVE, line[1:]))\n        else:\n            # Retained line\n            hunk_lines.append((RETAIN, line[1:]))\n\n    # Append the last hunk if any\n    if current_diff is not None and hunk_lines and hunk_header is not None:\n        current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n\n    return diffs\n\n\ndef parse_hunk_header(header_line) -> Tuple[int, int, int, int]:\n    \"\"\"\n    Parses the header of a hunk from a diff.\n\n    Args:\n    - header_line (str): The header line of a hunk.\n\n    Returns:\n    - tuple: A tuple containing start and length information for pre- and post-edit.\n    \"\"\"\n    pattern = re.compile(r\"^@@ -\\d{1,},\\d{1,} \\+\\d{1,},\\d{1,} @@$\")\n\n    if not pattern.match(header_line):\n        # Return a default value if the header does not match the expected format\n        return 0, 0, 0, 0\n\n    pre, post = header_line.split(\" \")[1:3]\n    start_line_pre_edit, hunk_len_pre_edit = map(int, pre[1:].split(\",\"))\n    start_line_post_edit, hunk_len_post_edit = map(int, post[1:].split(\",\"))\n    return (\n        start_line_pre_edit,\n        hunk_len_pre_edit,\n        start_line_post_edit,\n        hunk_len_post_edit,\n    )\n\n```\n# Function Signature: def parse_diffs(diff_string: str) -> dict:\n# Natural Language Annotations: \"\"\"\n    Parses a diff string in the unified git diff format.\n\n    Args:\n    - diff_string (str): The diff string to parse.\n\n    Returns:\n    - dict: A dictionary of Diff objects keyed by filename.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef parse_diffs(diff_string: str) ->", "  ```\ndef parse_diffs(diff_string: str) ->", "  ```\ndef parse_diffs(diff_string: str) ->"]}, "gpt-engineer-gpt_engineer/core/ai.py-start": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/gpt-engineer/gpt_engineer/core/ai.py:\n```\n\"\"\"\nAI Module\n\nThis module provides an AI class that interfaces with language models to perform various tasks such as\nstarting a conversation, advancing the conversation, and handling message serialization. It also includes\nbackoff strategies for handling rate limit errors from the OpenAI API.\n\nClasses:\n    AI: A class that interfaces with language models for conversation management and message serialization.\n\nFunctions:\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\n\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Union\n\nimport backoff\nimport openai\nimport pyperclip\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    messages_from_dict,\n    messages_to_dict,\n)\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_openai import AzureChatOpenAI, ChatOpenAI\n\nfrom gpt_engineer.core.token_usage import TokenUsageLog\n\n# Type hint for a chat message\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\n# Set up logging\nlogger = logging.getLogger(__name__)\n\n\nclass AI:\n    \"\"\"\n    A class that interfaces with language models for conversation management and message serialization.\n\n    This class provides methods to start and advance conversations, handle message serialization,\n    and implement backoff strategies for rate limit errors when interacting with the OpenAI API.\n\n    Attributes\n    ----------\n    temperature : float\n        The temperature setting for the language model.\n    azure_endpoint : str\n        The endpoint URL for the Azure-hosted language model.\n    model_name : str\n        The name of the language model to use.\n    streaming : bool\n        A flag indicating whether to use streaming for the language model.\n    llm : BaseChatModel\n        The language model instance for conversation management.\n    token_usage_log : TokenUsageLog\n        A log for tracking token usage during conversations.\n\n    Methods\n    -------\n    start(system: str, user: str, step_name: str) -> List[Message]\n        Start the conversation with a system message and a user message.\n    next(messages: List[Message], prompt: Optional[str], step_name: str) -> List[Message]\n        Advances the conversation by sending message history to LLM and updating with the response.\n    backoff_inference(messages: List[Message]) -> Any\n        Perform inference using the language model with an exponential backoff strategy.\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n    deserialize_messages(jsondictstr: str) -> List[Message]\n        Deserialize a JSON string to a list of messages.\n    _create_chat_model() -> BaseChatModel\n        Create a chat model with the specified model name and temperature.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name=\"gpt-4-turbo\",\n        temperature=0.1,\n        azure_endpoint=None,\n        streaming=True,\n        vision=False,\n    ):\n        \"\"\"\n        Initialize the AI class.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            The name of the model to use, by default \"gpt-4\".\n        temperature : float, optional\n            The temperature to use for the model, by default 0.1.\n        \"\"\"\n        self.temperature = temperature\n        self.azure_endpoint = azure_endpoint\n        self.model_name = model_name\n        self.streaming = streaming\n        self.vision = (\n            (\"vision-preview\" in model_name)\n            or (\"gpt-4-turbo\" in model_name and \"preview\" not in model_name)\n            or (\"claude\" in model_name)\n        )\n        self.llm = self._create_chat_model()\n        self.token_usage_log = TokenUsageLog(model_name)\n\n        logger.debug(f\"Using model {self.model_name}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _extract_content(self, content):\n        \"\"\"\n        Extracts text content from a message, supporting both string and list types.\n        Parameters\n        ----------\n        content : Union[str, List[dict]]\n            The content of a message, which could be a string or a list.\n        Returns\n        -------\n        str\n            The extracted text content.\n        \"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, list) and content and \"text\" in content[0]:\n            # Assuming the structure of list content is [{'type': 'text', 'text': 'Some text'}, ...]\n            return content[0][\"text\"]\n        else:\n            return \"\"\n\n    def _collapse_text_messages(self, messages: List[Message]):\n        \"\"\"\n        Combine consecutive messages of the same type into a single message, where if the message content\n        is a list type, the first text element's content is taken. This method keeps `combined_content` as a string.\n\n        This method iterates through the list of messages, combining consecutive messages of the same type\n        by joining their content with a newline character. If the content is a list, it extracts text from the first\n        text element's content. This reduces the number of messages and simplifies the conversation for processing.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to collapse.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages after collapsing consecutive messages of the same type.\n        \"\"\"\n        collapsed_messages = []\n        if not messages:\n            return collapsed_messages\n\n        previous_message = messages[0]\n        combined_content = self._extract_content(previous_message.content)\n\n        for current_message in messages[1:]:\n            if current_message.type == previous_message.type:\n                combined_content += \"\\n\\n\" + self._extract_content(\n                    current_message.content\n                )\n            else:\n                collapsed_messages.append(\n                    previous_message.__class__(content=combined_content)\n                )\n                previous_message = current_message\n                combined_content = self._extract_content(current_message.content)\n\n        collapsed_messages.append(previous_message.__class__(content=combined_content))\n        return collapsed_messages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=7, max_time=45)\n    def backoff_inference(self, messages):\n        \"\"\"\n        Perform inference using the language model while implementing an exponential backoff strategy.\n\n        This function will retry the inference in case of a rate limit error from the OpenAI API.\n        It uses an exponential backoff strategy, meaning the wait time between retries increases\n        exponentially. The function will attempt to retry up to 7 times within a span of 45 seconds.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of chat messages which will be passed to the language model for processing.\n\n        callbacks : List[Callable]\n            A list of callback functions that are triggered after each inference. These functions\n            can be used for logging, monitoring, or other auxiliary tasks.\n\n        Returns\n        -------\n        Any\n            The output from the language model after processing the provided messages.\n\n        Raises\n        ------\n        openai.error.RateLimitError\n            If the number of retries exceeds the maximum or if the rate limit persists beyond the\n            allotted time, the function will ultimately raise a RateLimitError.\n\n        Example\n        -------\n        >>> messages = [SystemMessage(content=\"Hello\"), HumanMessage(content=\"How's the weather?\")]\n        >>> response = backoff_inference(messages)\n        \"\"\"\n        return self.llm.invoke(messages)  # type: ignore\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        \"\"\"\n        Serialize a list of messages to a JSON string.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to serialize.\n\n        Returns\n        -------\n        str\n            The serialized messages as a JSON string.\n        \"\"\"\n        return json.dumps(messages_to_dict(messages))\n\n    @staticmethod\n    def deserialize_messages(jsondictstr: str) -> List[Message]:\n        \"\"\"\n        Deserialize a JSON string to a list of messages.\n\n        Parameters\n        ----------\n        jsondictstr : str\n            The JSON string to deserialize.\n\n        Returns\n        -------\n        List[Message]\n            The deserialized list of messages.\n        \"\"\"\n        data = json.loads(jsondictstr)\n        # Modify implicit is_chunk property to ALWAYS false\n        # since Langchain's Message schema is stricter\n        prevalidated_data = [\n            {**item, \"tools\": {**item.get(\"tools\", {}), \"is_chunk\": False}}\n            for item in data\n        ]\n        return list(messages_from_dict(prevalidated_data))  # type: ignore\n\n    def _create_chat_model(self) -> BaseChatModel:\n        \"\"\"\n        Create a chat model with the specified model name and temperature.\n\n        Parameters\n        ----------\n        model : str\n            The name of the model to create.\n        temperature : float\n            The temperature to use for the model.\n\n        Returns\n        -------\n        BaseChatModel\n            The created chat model.\n        \"\"\"\n        if self.azure_endpoint:\n            return AzureChatOpenAI(\n                azure_endpoint=self.azure_endpoint,\n                openai_api_version=os.getenv(\n                    \"OPENAI_API_VERSION\", \"2024-05-01-preview\"\n                ),\n                deployment_name=self.model_name,\n                openai_api_type=\"azure\",\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n        elif \"claude\" in self.model_name:\n            return ChatAnthropic(\n                model=self.model_name,\n                temperature=self.temperature,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                streaming=self.streaming,\n                max_tokens_to_sample=4096,\n            )\n        elif self.vision:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                max_tokens=4096,  # vision models default to low max token limits\n            )\n        else:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n\n\ndef serialize_messages(messages: List[Message]) -> str:\n    return AI.serialize_messages(messages)\n\n\nclass ClipboardAI(AI):\n    # Ignore not init superclass\n    def __init__(self, **_):  # type: ignore\n        self.vision = False\n        self.token_usage_log = TokenUsageLog(\"clipboard_llm\")\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        return \"\\n\\n\".join([f\"{m.type}:\\n{m.content}\" for m in messages])\n\n    @staticmethod\n    def multiline_input():\n        print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n        content = []\n        while True:\n            try:\n                line = input()\n            except EOFError:\n                break\n            content.append(line)\n        return \"\\n\".join(content)\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Not yet fully supported\n        \"\"\"\n        if prompt:\n            messages.append(HumanMessage(content=prompt))\n\n        logger.debug(f\"Creating a new chat completion: {messages}\")\n\n        msgs = self.serialize_messages(messages)\n        pyperclip.copy(msgs)\n        Path(\"clipboard.txt\").write_text(msgs)\n        print(\n            \"Messages copied to clipboard and written to clipboard.txt,\",\n            len(msgs),\n            \"characters in total\",\n        )\n\n        response = self.multiline_input()\n\n        messages.append(AIMessage(content=response))\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        return messages\n\n```\n# Function Signature: def start(self, system: str, user: Any, *, step_name: str) -> List[Message]:\n# Natural Language Annotations: \"\"\"\n        Start the conversation with a system message and a user message.\n\n        Parameters\n        ----------\n        system : str\n            The content of the system message.\n        user : str\n            The content of the user message.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages in the conversation.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef start(self, system: str, user: Any,", "  ```\ndef start(self, system: str, user: Any,", "  ```\ndef start(self, system: str, user: Any,"]}, "gpt-engineer-gpt_engineer/core/ai.py-next": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/gpt-engineer/gpt_engineer/core/ai.py:\n```\n\"\"\"\nAI Module\n\nThis module provides an AI class that interfaces with language models to perform various tasks such as\nstarting a conversation, advancing the conversation, and handling message serialization. It also includes\nbackoff strategies for handling rate limit errors from the OpenAI API.\n\nClasses:\n    AI: A class that interfaces with language models for conversation management and message serialization.\n\nFunctions:\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\n\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Union\n\nimport backoff\nimport openai\nimport pyperclip\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    messages_from_dict,\n    messages_to_dict,\n)\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_openai import AzureChatOpenAI, ChatOpenAI\n\nfrom gpt_engineer.core.token_usage import TokenUsageLog\n\n# Type hint for a chat message\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\n# Set up logging\nlogger = logging.getLogger(__name__)\n\n\nclass AI:\n    \"\"\"\n    A class that interfaces with language models for conversation management and message serialization.\n\n    This class provides methods to start and advance conversations, handle message serialization,\n    and implement backoff strategies for rate limit errors when interacting with the OpenAI API.\n\n    Attributes\n    ----------\n    temperature : float\n        The temperature setting for the language model.\n    azure_endpoint : str\n        The endpoint URL for the Azure-hosted language model.\n    model_name : str\n        The name of the language model to use.\n    streaming : bool\n        A flag indicating whether to use streaming for the language model.\n    llm : BaseChatModel\n        The language model instance for conversation management.\n    token_usage_log : TokenUsageLog\n        A log for tracking token usage during conversations.\n\n    Methods\n    -------\n    start(system: str, user: str, step_name: str) -> List[Message]\n        Start the conversation with a system message and a user message.\n    next(messages: List[Message], prompt: Optional[str], step_name: str) -> List[Message]\n        Advances the conversation by sending message history to LLM and updating with the response.\n    backoff_inference(messages: List[Message]) -> Any\n        Perform inference using the language model with an exponential backoff strategy.\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n    deserialize_messages(jsondictstr: str) -> List[Message]\n        Deserialize a JSON string to a list of messages.\n    _create_chat_model() -> BaseChatModel\n        Create a chat model with the specified model name and temperature.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name=\"gpt-4-turbo\",\n        temperature=0.1,\n        azure_endpoint=None,\n        streaming=True,\n        vision=False,\n    ):\n        \"\"\"\n        Initialize the AI class.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            The name of the model to use, by default \"gpt-4\".\n        temperature : float, optional\n            The temperature to use for the model, by default 0.1.\n        \"\"\"\n        self.temperature = temperature\n        self.azure_endpoint = azure_endpoint\n        self.model_name = model_name\n        self.streaming = streaming\n        self.vision = (\n            (\"vision-preview\" in model_name)\n            or (\"gpt-4-turbo\" in model_name and \"preview\" not in model_name)\n            or (\"claude\" in model_name)\n        )\n        self.llm = self._create_chat_model()\n        self.token_usage_log = TokenUsageLog(model_name)\n\n        logger.debug(f\"Using model {self.model_name}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def _extract_content(self, content):\n        \"\"\"\n        Extracts text content from a message, supporting both string and list types.\n        Parameters\n        ----------\n        content : Union[str, List[dict]]\n            The content of a message, which could be a string or a list.\n        Returns\n        -------\n        str\n            The extracted text content.\n        \"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, list) and content and \"text\" in content[0]:\n            # Assuming the structure of list content is [{'type': 'text', 'text': 'Some text'}, ...]\n            return content[0][\"text\"]\n        else:\n            return \"\"\n\n    def _collapse_text_messages(self, messages: List[Message]):\n        \"\"\"\n        Combine consecutive messages of the same type into a single message, where if the message content\n        is a list type, the first text element's content is taken. This method keeps `combined_content` as a string.\n\n        This method iterates through the list of messages, combining consecutive messages of the same type\n        by joining their content with a newline character. If the content is a list, it extracts text from the first\n        text element's content. This reduces the number of messages and simplifies the conversation for processing.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to collapse.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages after collapsing consecutive messages of the same type.\n        \"\"\"\n        collapsed_messages = []\n        if not messages:\n            return collapsed_messages\n\n        previous_message = messages[0]\n        combined_content = self._extract_content(previous_message.content)\n\n        for current_message in messages[1:]:\n            if current_message.type == previous_message.type:\n                combined_content += \"\\n\\n\" + self._extract_content(\n                    current_message.content\n                )\n            else:\n                collapsed_messages.append(\n                    previous_message.__class__(content=combined_content)\n                )\n                previous_message = current_message\n                combined_content = self._extract_content(current_message.content)\n\n        collapsed_messages.append(previous_message.__class__(content=combined_content))\n        return collapsed_messages\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=7, max_time=45)\n    def backoff_inference(self, messages):\n        \"\"\"\n        Perform inference using the language model while implementing an exponential backoff strategy.\n\n        This function will retry the inference in case of a rate limit error from the OpenAI API.\n        It uses an exponential backoff strategy, meaning the wait time between retries increases\n        exponentially. The function will attempt to retry up to 7 times within a span of 45 seconds.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of chat messages which will be passed to the language model for processing.\n\n        callbacks : List[Callable]\n            A list of callback functions that are triggered after each inference. These functions\n            can be used for logging, monitoring, or other auxiliary tasks.\n\n        Returns\n        -------\n        Any\n            The output from the language model after processing the provided messages.\n\n        Raises\n        ------\n        openai.error.RateLimitError\n            If the number of retries exceeds the maximum or if the rate limit persists beyond the\n            allotted time, the function will ultimately raise a RateLimitError.\n\n        Example\n        -------\n        >>> messages = [SystemMessage(content=\"Hello\"), HumanMessage(content=\"How's the weather?\")]\n        >>> response = backoff_inference(messages)\n        \"\"\"\n        return self.llm.invoke(messages)  # type: ignore\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        \"\"\"\n        Serialize a list of messages to a JSON string.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to serialize.\n\n        Returns\n        -------\n        str\n            The serialized messages as a JSON string.\n        \"\"\"\n        return json.dumps(messages_to_dict(messages))\n\n    @staticmethod\n    def deserialize_messages(jsondictstr: str) -> List[Message]:\n        \"\"\"\n        Deserialize a JSON string to a list of messages.\n\n        Parameters\n        ----------\n        jsondictstr : str\n            The JSON string to deserialize.\n\n        Returns\n        -------\n        List[Message]\n            The deserialized list of messages.\n        \"\"\"\n        data = json.loads(jsondictstr)\n        # Modify implicit is_chunk property to ALWAYS false\n        # since Langchain's Message schema is stricter\n        prevalidated_data = [\n            {**item, \"tools\": {**item.get(\"tools\", {}), \"is_chunk\": False}}\n            for item in data\n        ]\n        return list(messages_from_dict(prevalidated_data))  # type: ignore\n\n    def _create_chat_model(self) -> BaseChatModel:\n        \"\"\"\n        Create a chat model with the specified model name and temperature.\n\n        Parameters\n        ----------\n        model : str\n            The name of the model to create.\n        temperature : float\n            The temperature to use for the model.\n\n        Returns\n        -------\n        BaseChatModel\n            The created chat model.\n        \"\"\"\n        if self.azure_endpoint:\n            return AzureChatOpenAI(\n                azure_endpoint=self.azure_endpoint,\n                openai_api_version=os.getenv(\n                    \"OPENAI_API_VERSION\", \"2024-05-01-preview\"\n                ),\n                deployment_name=self.model_name,\n                openai_api_type=\"azure\",\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n        elif \"claude\" in self.model_name:\n            return ChatAnthropic(\n                model=self.model_name,\n                temperature=self.temperature,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                streaming=self.streaming,\n                max_tokens_to_sample=4096,\n            )\n        elif self.vision:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                max_tokens=4096,  # vision models default to low max token limits\n            )\n        else:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n\n\ndef serialize_messages(messages: List[Message]) -> str:\n    return AI.serialize_messages(messages)\n\n\nclass ClipboardAI(AI):\n    # Ignore not init superclass\n    def __init__(self, **_):  # type: ignore\n        self.vision = False\n        self.token_usage_log = TokenUsageLog(\"clipboard_llm\")\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        return \"\\n\\n\".join([f\"{m.type}:\\n{m.content}\" for m in messages])\n\n    @staticmethod\n    def multiline_input():\n        print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n        content = []\n        while True:\n            try:\n                line = input()\n            except EOFError:\n                break\n            content.append(line)\n        return \"\\n\".join(content)\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Not yet fully supported\n        \"\"\"\n        if prompt:\n            messages.append(HumanMessage(content=prompt))\n\n        logger.debug(f\"Creating a new chat completion: {messages}\")\n\n        msgs = self.serialize_messages(messages)\n        pyperclip.copy(msgs)\n        Path(\"clipboard.txt\").write_text(msgs)\n        print(\n            \"Messages copied to clipboard and written to clipboard.txt,\",\n            len(msgs),\n            \"characters in total\",\n        )\n\n        response = self.multiline_input()\n\n        messages.append(AIMessage(content=response))\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        return messages\n\n```\n# Function Signature: def next(\n# Natural Language Annotations: \"\"\"\n        Advances the conversation by sending message history\n        to LLM and updating with the response.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages in the conversation.\n        prompt : Optional[str], optional\n            The prompt to use, by default None.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The updated list of messages in the conversation.\n        \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef next(\n    self,\n    messages: List[", "  ```\ndef next(\n    messages: List[Message],\n   ", "  ```\ndef next(messages: List[Message], prompt: Optional["]}, "gpt-engineer-gpt_engineer/core/project_config.py-to_toml": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/gpt-engineer/gpt_engineer/core/project_config.py:\n```\n\"\"\"\nFunctions for reading and writing the `gpt-engineer.toml` configuration file.\n\nThe `gpt-engineer.toml` file is a TOML file that contains project-specific configuration used by the GPT Engineer CLI and gptengineer.app.\n\"\"\"\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\n\nimport tomlkit\n\ndefault_config_filename = \"gpt-engineer.toml\"\n\nexample_config = \"\"\"\n[run]\nbuild = \"npm run build\"\ntest = \"npm run test\"\nlint = \"quick-lint-js\"\n\n[paths]\nbase = \"./frontend\"  # base directory to operate in (for monorepos)\nsrc = \"./src\"        # source directory (under the base directory) from which context will be retrieved\n\n[gptengineer-app]  # this namespace is used for gptengineer.app, may be used for internal experiments\nproject_id = \"...\"\n\n# we support multiple OpenAPI schemas, used as context for the LLM\nopenapi = [\n    { url = \"https://api.gptengineer.app/openapi.json\" },\n    { url = \"https://some-color-translating-api/openapi.json\" },\n]\n\"\"\"\n\n\n@dataclass\nclass _PathsConfig:\n    base: str | None = None\n    src: str | None = None\n\n\n@dataclass\nclass _RunConfig:\n    build: str | None = None\n    test: str | None = None\n    lint: str | None = None\n    format: str | None = None\n\n\n@dataclass\nclass _OpenApiConfig:\n    url: str\n\n\n@dataclass\nclass _GptEngineerAppConfig:\n    project_id: str\n    openapi: list[_OpenApiConfig] | None = None\n\n\ndef filter_none(d: dict) -> dict:\n    # Drop None values and empty dictionaries from a dictionary\n    return {\n        k: v\n        for k, v in (\n            (k, filter_none(v) if isinstance(v, dict) else v)\n            for k, v in d.items()\n            if v is not None\n        )\n        if not (isinstance(v, dict) and not v)  # Check for non-empty after filtering\n    }\n\n\n@dataclass\nclass Config:\n    \"\"\"Configuration for the GPT Engineer CLI and gptengineer.app via `gpt-engineer.toml`.\"\"\"\n\n    paths: _PathsConfig = field(default_factory=_PathsConfig)\n    run: _RunConfig = field(default_factory=_RunConfig)\n    gptengineer_app: _GptEngineerAppConfig | None = None\n\n    @classmethod\n    def from_toml(cls, config_file: Path | str):\n        if isinstance(config_file, str):\n            config_file = Path(config_file)\n        config_dict = read_config(config_file)\n        return cls.from_dict(config_dict)\n\n    @classmethod\n    def from_dict(cls, config_dict: dict):\n        run = _RunConfig(**config_dict.get(\"run\", {}))\n        paths = _PathsConfig(**config_dict.get(\"paths\", {}))\n\n        # load optional gptengineer-app section\n        gptengineer_app_dict = config_dict.get(\"gptengineer-app\", {})\n        gptengineer_app = None\n        if gptengineer_app_dict:\n            assert (\n                \"project_id\" in gptengineer_app_dict\n            ), \"project_id is required in gptengineer-app section\"\n            gptengineer_app = _GptEngineerAppConfig(\n                # required if gptengineer-app section is present\n                project_id=gptengineer_app_dict[\"project_id\"],\n                openapi=[\n                    _OpenApiConfig(**openapi)\n                    for openapi in gptengineer_app_dict.get(\"openapi\", [])\n                ]\n                or None,\n            )\n\n        return cls(paths=paths, run=run, gptengineer_app=gptengineer_app)\n\n    def to_dict(self) -> dict:\n        d = asdict(self)\n        d[\"gptengineer-app\"] = d.pop(\"gptengineer_app\", None)\n\n        # Drop None values and empty dictionaries\n        # Needed because tomlkit.dumps() doesn't handle None values,\n        # and we don't want to write empty sections.\n        d = filter_none(d)\n\n        return d\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef read_config(config_file: Path) -> tomlkit.TOMLDocument:\n    \"\"\"Read the configuration file\"\"\"\n    assert config_file.exists(), f\"Config file {config_file} does not exist\"\n    with open(config_file, \"r\") as f:\n        return tomlkit.load(f)\n\n```\n# Function Signature: def to_toml(self, config_file: Path | str, save=True) -> str:\n# Natural Language Annotations: \"\"\"Write the configuration to a TOML file.\"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef to_toml(self, config_file: Path", "  ```\ndef to_toml(self, config_file: Path", "  ```python\ndef to_toml(self, config_file:"]}, "gpt-engineer-gpt_engineer/core/diff.py-is_similar": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/py_data/gpt-engineer/gpt_engineer/core/diff.py:\n```\n\"\"\"\nFile Overview:\n\nThis Python module is designed for processing and analyzing diffs in source code files. Diffs represent the changes between two versions of a file, which are crucial in version control systems for tracking file modifications. The module focuses on the detailed examination of these diffs, enabling users to understand, validate, and correct changes between file versions.\n\nKey Features:\n\n1. The `Hunk` class encapsulates a contiguous block of changes within a file. It includes detailed information such as start lines before and after edits, lengths of change blocks, and specific line changes categorized as additions, deletions, or unchanged.\n\n2. The `Diff` class represents a complete set of changes across a file and may contain multiple `Hunk` objects. It facilitates operations like generating string representations of diffs, and validating and correcting hunks based on the original file content.\n\n3. Functions within the module allow for the validation of hunks against original files, identifying mismatches, and making necessary corrections. This feature ensures that diffs are accurate and reflect true changes.\n\n4. Utility functions `is_similar` and `count_ratio` offer the capability to compare strings for similarity, accounting for variations in spacing and case. This aids in the validation process by allowing a flexible comparison of code lines.\n\nDependencies:\n\n- `logging`: Utilized for logging warnings and errors encountered during the validation and correction process.\n- `collections.Counter`: Used for counting occurrences of characters in strings, supporting the string similarity assessment functions.\n\nFunctions and Classes:\n\n1. `Hunk`: Class representing a block of changes within a file, with methods for managing and validating these changes.\n\n2. `Diff`: Class representing the entire set of changes in a file, containing multiple `Hunk` instances and methods for overall diff management.\n\n3. `is_similar(str1, str2, similarity_threshold)`: Function to compare two strings for similarity, useful in validating line changes in hunks.\n\n4. `count_ratio(str1, str2)`: Function that computes the ratio of common characters to the length of the longer string, aiding in the assessment of line similarity.\n\nThis module is essential for developers and teams utilizing version control systems, providing tools for a deeper analysis and correction of diffs, ensuring the integrity and accuracy of code changes.\n\n\"\"\"\nimport logging\n\nfrom collections import Counter\nfrom typing import List\n\nRETAIN = \"retain\"\nADD = \"add\"\nREMOVE = \"remove\"\n\n\nclass Hunk:\n    \"\"\"\n    Represents a section of a file diff, containing changes made to that section.\n\n    Attributes:\n        start_line_pre_edit (int): The starting line number in the original file.\n        hunk_len_pre_edit (int): The length of the hunk in the original file.\n        start_line_post_edit (int): The starting line number in the edited file.\n        hunk_len_post_edit (int): The length of the hunk in the edited file.\n        lines (list): A list of tuples representing the lines in the hunk and their types (RETAIN, ADD, REMOVE).\n        category_counts (dict): A count of lines by their type.\n        is_new_file (bool): Flag indicating if the hunk represents a new file.\n    \"\"\"\n\n    def __init__(\n        self,\n        start_line_pre_edit,\n        hunk_len_pre_edit,\n        start_line_post_edit,\n        hunk_len_post_edit,\n        lines,\n    ) -> None:\n        self.start_line_pre_edit = start_line_pre_edit\n        self.hunk_len_pre_edit = hunk_len_pre_edit\n        self.start_line_post_edit = start_line_post_edit\n        self.hunk_len_post_edit = hunk_len_post_edit\n        self.category_counts = {RETAIN: 0, ADD: 0, REMOVE: 0}\n        self.lines = list()\n        self.add_lines(lines)\n        self.forward_block_len = 10\n        # Note that this assumption should not be done on hunk level, however, if the below is true, no validation is possible anyway.\n        if self.category_counts[RETAIN] == 0 and self.category_counts[REMOVE] == 0:\n            self.is_new_file = True\n        else:\n            self.is_new_file = False\n\n    def add_retained_line(self, line, index) -> None:\n        \"\"\"Adds a retained line to the hunk at the specified index.\"\"\"\n        self.lines.insert(index, (RETAIN, line))\n        self.category_counts[RETAIN] += 1\n\n    def relabel_line(self, index, new_label) -> None:\n        \"\"\"Changes the label of a line at the specified index.\"\"\"\n        old_label = self.lines[index][0]\n        self.lines[index] = (new_label, self.lines[index][1])\n        self.category_counts[old_label] -= 1\n        self.category_counts[new_label] += 1\n\n    def pop_line(self, line, index) -> None:\n        \"\"\"Removes a line from the hunk at the specified index.\"\"\"\n        self.lines.pop(index)\n        assert self.category_counts[line[0]] > 0\n        self.category_counts[line[0]] -= 1\n\n    def add_lines(self, new_lines) -> None:\n        \"\"\"Adds multiple lines to the hunk.\"\"\"\n        for line in new_lines:\n            self.lines.append(line)\n            self.category_counts[line[0]] += 1\n\n    def hunk_to_string(self) -> str:\n        \"\"\"Converts the hunk to a string representation.\"\"\"\n        string = f\"@@ -{self.start_line_pre_edit},{self.hunk_len_pre_edit} +{self.start_line_post_edit},{self.hunk_len_post_edit} @@\\n\"\n        for line_type, line_content in self.lines:\n            line_prefix = (\n                \" \" if line_type == RETAIN else \"+\" if line_type == ADD else \"-\"\n            )\n            string += f\"{line_prefix}{line_content}\\n\"\n        return string\n\n    def make_forward_block(self, hunk_ind: int, forward_block_len) -> str:\n        \"\"\"Creates a block of lines for forward comparison.\"\"\"\n        forward_lines = [\n            line[1] for line in self.lines[hunk_ind:] if not line[0] == ADD\n        ]\n        forward_block = \"\\n\".join(forward_lines[0:forward_block_len])\n        return forward_block\n\n    def check_start_line(self, lines_dict: dict) -> bool:\n        \"\"\"Check if the starting line of a hunk is present in the original code and returns a boolean value accordingly.\"\"\"\n        if self.is_new_file:\n            # this hunk cannot be falsified and is by definition true\n            return True\n        if self.start_line_pre_edit in lines_dict:\n            # check the location of the actual starting line:\n            is_similar(self.lines[0][1], lines_dict[self.start_line_pre_edit])\n        else:\n            pass\n\n    def find_start_line(self, lines_dict: dict, problems: list) -> bool:\n        \"\"\"Finds the starting line of the hunk in the original code and returns a boolean value accordingly. If the starting line is not found, it appends a problem message to the problems list.\"\"\"\n\n        # ToDo handle the case where the start line is 0 or 1 characters separately\n        if self.lines[0][0] == ADD:\n            # handle the case where the start line is an add\n            start_line = None\n            # find the first line that is not an add\n            for index, line in enumerate(self.lines):\n                if line[0] != ADD:\n                    for line_number, line_content in lines_dict.items():\n                        # if the line is similar to a non-blank line in line_dict, we can pick the line prior to it\n                        if is_similar(line[1], line_content) and line[1] != \"\":\n                            start_line = line_number - 1\n                            break\n                    # if the start line is not found, append a problem message\n                    if start_line is None:\n                        problems.append(\n                            f\"In {self.hunk_to_string()}:can not find the starting line of the diff\"\n                        )\n                        return False\n\n                    else:\n                        # the line prior to the start line is found now we insert it to the first place as the start line\n                        self.start_line_pre_edit = start_line\n                        retain_line = lines_dict.get(start_line, \"\")\n                        if retain_line:\n                            self.add_retained_line(lines_dict[start_line], 0)\n                            return self.validate_and_correct(lines_dict, problems)\n                        else:\n                            problems.append(\n                                f\"In {self.hunk_to_string()}:The starting line of the diff {self.hunk_to_string()} does not exist in the code\"\n                            )\n                            return False\n        pot_start_lines = {\n            key: is_similar(self.lines[0][1], line) for key, line in lines_dict.items()\n        }\n        sum_of_matches = sum(pot_start_lines.values())\n        if sum_of_matches == 0:\n            # before we go any further, we should check if it's a comment from LLM\n            if self.lines[0][1].count(\"#\") > 0:\n                # if it is, we can mark it as an ADD lines\n                self.relabel_line(0, ADD)\n                # and restart the validation at the next line\n                return self.validate_and_correct(lines_dict, problems)\n\n            else:\n                problems.append(\n                    f\"In {self.hunk_to_string()}:The starting line of the diff {self.hunk_to_string()} does not exist in the code\"\n                )\n                return False\n        elif sum_of_matches == 1:\n            start_ind = list(pot_start_lines.keys())[\n                list(pot_start_lines.values()).index(True)\n            ]  # lines are one indexed\n        else:\n            logging.warning(\"multiple candidates for starting index\")\n            # ToDo handle all the cases better again here. Smartest choice is that, for each candidate check match to the next line etc (recursively)\n            start_ind = list(pot_start_lines.keys())[\n                list(pot_start_lines.values()).index(True)\n            ]\n        self.start_line_pre_edit = start_ind\n\n        # This should now be fulfilled by default\n        assert is_similar(self.lines[0][1], lines_dict[self.start_line_pre_edit])\n        return True\n\n    def validate_lines(self, lines_dict: dict, problems: list) -> bool:\n        \"\"\"Validates the lines of the hunk against the original file and returns a boolean value accordingly. If the lines do not match, it appends a problem message to the problems list.\"\"\"\n        hunk_ind = 0\n        file_ind = self.start_line_pre_edit\n        # make an orig hunk lines for logging\n        # orig_hunk_lines = deepcopy(self.lines)\n        while hunk_ind < len(self.lines) and file_ind <= max(lines_dict):\n            if self.lines[hunk_ind][0] == ADD:\n                # this cannot be validated, jump one index\n                hunk_ind += 1\n            elif not is_similar(self.lines[hunk_ind][1], lines_dict[file_ind]):\n                # before we go any further, we should relabel the comment from LLM\n                if self.lines[hunk_ind][1].count(\"#\") > 0:\n                    self.relabel_line(hunk_ind, ADD)\n                    continue\n\n                # make a forward block from the code for comparisons\n                forward_code = \"\\n\".join(\n                    [\n                        lines_dict[ind]\n                        for ind in range(\n                            file_ind,\n                            min(\n                                file_ind + self.forward_block_len,\n                                max(lines_dict.keys()),\n                            ),\n                        )\n                    ]\n                )\n                # make the original forward block for quantitative comparison\n                forward_block = self.make_forward_block(\n                    hunk_ind, self.forward_block_len\n                )\n                orig_count_ratio = count_ratio(forward_block, forward_code)\n                # Here we have 2 cases\n                # 1) some lines were simply skipped in the diff and we should add them to the diff\n                # If this is the case, adding the line to the diff, should give an improved forward diff\n                forward_block_missing_line = self.make_forward_block(\n                    hunk_ind, self.forward_block_len - 1\n                )\n                # insert the missing line in front of the block\n                forward_block_missing_line = \"\\n\".join(\n                    [lines_dict[file_ind], forward_block_missing_line]\n                )\n                missing_line_count_ratio = count_ratio(\n                    forward_block_missing_line, forward_code\n                )\n                # 2) Additional lines, not belonging to the code were added to the diff\n                forward_block_false_line = self.make_forward_block(\n                    hunk_ind + 1, self.forward_block_len\n                )\n                false_line_count_ratio = count_ratio(\n                    forward_block_false_line, forward_code\n                )\n                if (\n                    orig_count_ratio >= missing_line_count_ratio\n                    and orig_count_ratio >= false_line_count_ratio\n                ):\n                    problems.append(\n                        f\"In Hunk:{self.hunk_to_string()}, there was at least one mismatch.\"\n                    )\n                    return False\n\n                elif missing_line_count_ratio > false_line_count_ratio:\n                    self.add_retained_line(lines_dict[file_ind], hunk_ind)\n                    hunk_ind += 1\n                    file_ind += 1\n                    # NOTE: IF THE LLM SKIPS SOME LINES AND HAS ADDs ADJACENT TO THE SKIPPED BLOCK,\n                    # WE CANNOT KNOW WHETHER THE ADDs SHOULD BE BEFORE OR AFTER THE BLOCK. WE OPT FOR PUTTING IT BEFORE.\n                    # IF IT MATTERED, WE ASSUME THE LLM WOULD NOT SKIP THE BLOCK\n                else:\n                    self.pop_line(self.lines[hunk_ind], hunk_ind)\n\n            else:\n                hunk_ind += 1\n                file_ind += 1\n        # if we have not validated all lines, we have a problem\n        if hunk_ind < len(self.lines) - 1:\n            remaining_lines = \"\\n\".join(\n                f\"{line_type}: {line_content}\"\n                for line_type, line_content in self.lines[file_ind + 1 :]\n            )\n            problems.append(\n                f\"In {self.hunk_to_string()}:Hunk validation stopped before the lines {remaining_lines} were validated. The diff is incorrect\"\n            )\n            return False\n        return True\n\n    def validate_and_correct(\n        self,\n        lines_dict: dict,\n        problems: list,\n    ) -> bool:\n        \"\"\"\n        Validates and corrects the hunk based on the original lines.\n\n        This function attempts to validate the hunk by comparing its lines to the original file and making corrections\n        where necessary. It also identifies problems such as non-matching lines or incorrect line types.\n        \"\"\"\n        start_true = self.check_start_line(lines_dict)\n\n        if not start_true:\n            if not self.find_start_line(lines_dict, problems):\n                return False\n\n        # Now we should be able to validate the hunk line by line and add missing line\n        if not self.validate_lines(lines_dict, problems):\n            return False\n        # Pass the validation\n        return True\n\n\nclass Diff:\n    \"\"\"\n    Represents a file diff, containing multiple hunks of changes.\n\n    Attributes:\n        filename_pre (str): The name of the original file.\n        filename_post (str): The name of the edited file.\n        hunks (list): A list of Hunk objects representing the changes in the diff.\n    \"\"\"\n\n    def __init__(self, filename_pre, filename_post) -> None:\n        self.filename_pre = filename_pre\n        self.filename_post = filename_post\n        self.hunks = []\n\n    def is_new_file(self) -> bool:\n        \"\"\"Determines if the diff represents a new file.\"\"\"\n        if self.filename_pre == \"/dev/null\":\n            return True\n        return any(hunk.is_new_file for hunk in self.hunks)\n\n    def diff_to_string(self) -> str:\n        \"\"\"Converts the diff to a string representation.\"\"\"\n        string = f\"--- {self.filename_pre}\\n+++ {self.filename_post}\\n\"\n        for hunk in self.hunks:\n            string += hunk.hunk_to_string()\n        return string.strip()\n\n    def validate_and_correct(self, lines_dict: dict) -> List[str]:\n        \"\"\"Validates and corrects each hunk in the diff.\"\"\"\n        problems = []\n        past_hunk = None\n        cut_lines_dict = lines_dict.copy()\n        for hunk in self.hunks:\n            if past_hunk is not None:\n                # make sure to not cut so much that the start_line gets out of range\n                cut_ind = min(\n                    past_hunk.start_line_pre_edit + past_hunk.hunk_len_pre_edit,\n                    hunk.start_line_pre_edit,\n                )\n                cut_lines_dict = {\n                    key: val for key, val in cut_lines_dict.items() if key >= (cut_ind)\n                }\n            is_valid = hunk.validate_and_correct(cut_lines_dict, problems)\n            if not is_valid and len(problems) > 0:\n                for idx, val in enumerate(problems):\n                    print(f\"\\nInvalid Hunk NO.{idx}---\\n{val}\\n---\")\n                self.hunks.remove(hunk)\n            # now correct the numbers, assuming the start line pre-edit has been fixed\n            hunk.hunk_len_pre_edit = (\n                hunk.category_counts[RETAIN] + hunk.category_counts[REMOVE]\n            )\n            hunk.hunk_len_post_edit = (\n                hunk.category_counts[RETAIN] + hunk.category_counts[ADD]\n            )\n            if past_hunk is not None:\n                hunk.start_line_post_edit = (\n                    hunk.start_line_pre_edit\n                    + past_hunk.hunk_len_post_edit\n                    - past_hunk.hunk_len_pre_edit\n                    + past_hunk.start_line_post_edit\n                    - past_hunk.start_line_pre_edit\n                )\n            else:\n                hunk.start_line_post_edit = hunk.start_line_pre_edit\n            past_hunk = hunk\n        return problems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef count_ratio(str1, str2) -> float:\n    \"\"\"\n    Computes the ratio of common characters to the length of the longer string, ignoring spaces and case.\n\n    Parameters:\n    - str1, str2 (str): The strings to compare.\n\n    Returns:\n    - float: The ratio of common characters to the length of the longer string.\n    \"\"\"\n    str1, str2 = str1.replace(\" \", \"\").lower(), str2.replace(\" \", \"\").lower()\n\n    counter1, counter2 = Counter(str1), Counter(str2)\n    intersection = sum((counter1 & counter2).values())\n    longer_length = max(len(str1), len(str2))\n    if longer_length == 0:\n        return 1\n    else:\n        return intersection / longer_length\n\n```\n# Function Signature: def is_similar(str1, str2, similarity_threshold=0.9) -> bool:\n# Natural Language Annotations: \"\"\"\n    Compares two strings for similarity, ignoring spaces and case.\n\n    Parameters\n    ----------\n    str1, str2 : str\n        The strings to compare.\n    similarity_threshold: float\n        How similar must the strings be\n\n    Returns\n    -------\n    bool\n        True if the strings are similar, False otherwise.\n    \"\"\"\nPlease return the generated code surrounded by ```\n", "response": ["  ```\ndef is_similar(str1, str2, similarity_", "  ```\ndef is_similar(str1, str2, similarity_", "  ```\ndef is_similar(str1, str2, similarity_"]}}