{"dtm-dtmsvr/storage/boltdb/boltdb.go-cleanupExpiredData": null, "dtm-dtmsvr/config/config.go-MustLoadConfig": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/dtm/dtmsvr/config/config.go:\n```\npackage config\n\nimport (\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\n\t\"github.com/dtm-labs/dtm/client/dtmcli\"\n\t\"github.com/dtm-labs/logger\"\n\t\"gopkg.in/yaml.v3\"\n)\n\nconst (\n\t// DtmMetricsPort is metric port\n\tDtmMetricsPort = 8889\n\t// Mysql is mysql driver\n\tMysql = \"mysql\"\n\t// Redis is redis driver\n\tRedis = \"redis\"\n\t// BoltDb is boltdb driver\n\tBoltDb = \"boltdb\"\n\t// Postgres is postgres driver\n\tPostgres = \"postgres\"\n\t// SQLServer is SQL Server driver\n\tSQLServer = \"sqlserver\"\n)\n\n// MicroService config type for microservice based grpc\ntype MicroService struct {\n\tDriver   string `yaml:\"Driver\" default:\"default\"`\n\tTarget   string `yaml:\"Target\"`\n\tEndPoint string `yaml:\"EndPoint\"`\n}\n\n// HTTPMicroService is the config type for microservice based on http, like springcloud\ntype HTTPMicroService struct {\n\tDriver          string `yaml:\"Driver\" default:\"default\"`\n\tRegistryType    string `yaml:\"RegistryType\" default:\"\"`\n\tRegistryAddress string `yaml:\"RegistryAddress\" default:\"\"`\n\tRegistryOptions string `yaml:\"RegistryOptions\" default:\"{}\"`\n\tTarget          string `yaml:\"Target\"`\n\tEndPoint        string `yaml:\"EndPoint\"`\n}\n\n// Log config customize log\ntype Log struct {\n\tOutputs            string `yaml:\"Outputs\" default:\"stderr\"`\n\tRotationEnable     int64  `yaml:\"RotationEnable\" default:\"0\"`\n\tRotationConfigJSON string `yaml:\"RotationConfigJSON\" default:\"{}\"`\n}\n\n// Store defines storage relevant info\ntype Store struct {\n\tDriver             string `yaml:\"Driver\" default:\"boltdb\"`\n\tHost               string `yaml:\"Host\"`\n\tPort               int64  `yaml:\"Port\"`\n\tUser               string `yaml:\"User\"`\n\tPassword           string `yaml:\"Password\"`\n\tDb                 string `yaml:\"Db\" default:\"dtm\"`\n\tSchema             string `yaml:\"Schema\" default:\"public\"`\n\tMaxOpenConns       int64  `yaml:\"MaxOpenConns\" default:\"500\"`\n\tMaxIdleConns       int64  `yaml:\"MaxIdleConns\" default:\"500\"`\n\tConnMaxLifeTime    int64  `yaml:\"ConnMaxLifeTime\" default:\"5\"`\n\tDataExpire         int64  `yaml:\"DataExpire\" default:\"604800\"`        // Trans data will expire in 7 days. only for redis/boltdb.\n\tFinishedDataExpire int64  `yaml:\"FinishedDataExpire\" default:\"86400\"` // finished Trans data will expire in 1 days. only for redis.\n\tRedisPrefix        string `yaml:\"RedisPrefix\" default:\"{a}\"`          // Redis storage prefix. store data to only one slot in cluster\n}\n\n// IsDB checks config driver is mysql or postgres\nfunc (s *Store) IsDB() bool {\n\treturn s.Driver == dtmcli.DBTypeMysql || s.Driver == dtmcli.DBTypePostgres || s.Driver == dtmcli.DBTypeSQLServer\n}\n\n// GetDBConf returns db conf info\nfunc (s *Store) GetDBConf() dtmcli.DBConf {\n\treturn dtmcli.DBConf{\n\t\tDriver:   s.Driver,\n\t\tHost:     s.Host,\n\t\tPort:     s.Port,\n\t\tUser:     s.User,\n\t\tPassword: s.Password,\n\t\tDb:       s.Db,\n\t\tSchema:   s.Schema,\n\t}\n}\n\n// Type is the type for the config of dtm server\ntype Type struct {\n\tStore                         Store            `yaml:\"Store\"`\n\tTransCronInterval             int64            `yaml:\"TransCronInterval\" default:\"3\"`\n\tTimeoutToFail                 int64            `yaml:\"TimeoutToFail\" default:\"35\"`\n\tRetryInterval                 int64            `yaml:\"RetryInterval\" default:\"10\"`\n\tRequestTimeout                int64            `yaml:\"RequestTimeout\" default:\"3\"`\n\tHTTPPort                      int64            `yaml:\"HttpPort\" default:\"36789\"`\n\tGrpcPort                      int64            `yaml:\"GrpcPort\" default:\"36790\"`\n\tJSONRPCPort                   int64            `yaml:\"JsonRpcPort\" default:\"36791\"`\n\tMicroService                  MicroService     `yaml:\"MicroService\"`\n\tHTTPMicroService              HTTPMicroService `yaml:\"HttpMicroService\"`\n\tUpdateBranchSync              int64            `yaml:\"UpdateBranchSync\"`\n\tUpdateBranchAsyncGoroutineNum int64            `yaml:\"UpdateBranchAsyncGoroutineNum\" default:\"1\"`\n\tLogLevel                      string           `yaml:\"LogLevel\" default:\"info\"`\n\tLog                           Log              `yaml:\"Log\"`\n\tTimeZoneOffset                string           `yaml:\"TimeZoneOffset\"`\n\tConfigUpdateInterval          int64            `yaml:\"ConfigUpdateInterval\" default:\"3\"`\n\tAlertRetryLimit               int64            `yaml:\"AlertRetryLimit\" default:\"3\"`\n\tAlertWebHook                  string           `yaml:\"AlertWebHook\"`\n\tAdminBasePath                 string           `yaml:\"AdminBasePath\"`\n}\n\n// Config config\nvar Config = Type{}\n\n// MustLoadConfig load config from env and file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func MustLoadConfig(confFile string) {\n# Natural Language Annotations: // MustLoadConfig load config from env and file\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MustLoadConfig(confFile string) {\n\t// Load the configuration from a file\n\tcontent, err := ioutil.ReadFile(confFile)\n\tif err != nil {\n\t\tlogger.Fatalf(\"Failed to read config file: %v\", err)\n\t}\n\t\n\t// Determine the file type and unmarshal accordingly\n\tif err := yaml.Unmarshal(content, &Config); err != nil {\n\t\tlogger.Fatalf(\"Failed to unmarshal config file: %v\", err)\n\t}\n\t\n\t// Alternatively or in conjunction, environment variables could be loaded here\n\n\t// You can add additional logic here to override or fill in defaults from environment variables\n\n\t// An example might be overriding certain configuration settings if environment variables are set, e.g.\n\t// if envVar := os.Getenv(\"SOME_ENV_VAR\"); envVar != \"\" {\n\t//     Config.SomeConfigField = envVar\n\t// }\n\t\n\tlogger.Infof(\"Configuration loaded successfully: %+v\", Config)\n}\n```"]}, "dtm-client/dtmcli/dtmimp/utils.go-MustRemarshal": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/dtm/client/dtmcli/dtmimp/utils.go:\n```\n/*\n * Copyright (c) 2021 yedf. All rights reserved.\n * Use of this source code is governed by a BSD-style\n * license that can be found in the LICENSE file.\n */\n\npackage dtmimp\n\nimport (\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/dtm-labs/logger\"\n\t\"github.com/go-resty/resty/v2\"\n)\n\n// Logf an alias of Infof\n// Deprecated: use logger.Errorf\nvar Logf = logger.Infof\n\n// LogRedf an alias of Errorf\n// Deprecated: use logger.Errorf\nvar LogRedf = logger.Errorf\n\n// FatalIfError fatal if error is not nil\n// Deprecated: use logger.FatalIfError\nvar FatalIfError = logger.FatalIfError\n\n// LogIfFatalf fatal if cond is true\n// Deprecated: use logger.FatalfIf\nvar LogIfFatalf = logger.FatalfIf\n\n// AsError wrap a panic value as an error\nfunc AsError(x interface{}) error {\n\tlogger.Errorf(\"panic wrapped to error: '%v'\", x)\n\tif e, ok := x.(error); ok {\n\t\treturn e\n\t}\n\treturn fmt.Errorf(\"%v\", x)\n}\n\n// P2E panic to error\nfunc P2E(perr *error) {\n\tif x := recover(); x != nil {\n\t\t*perr = AsError(x)\n\t}\n}\n\n// E2P error to panic\nfunc E2P(err error) {\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// CatchP catch panic to error\nfunc CatchP(f func()) (rerr error) {\n\tdefer P2E(&rerr)\n\tf()\n\treturn nil\n}\n\n// PanicIf name is clear\nfunc PanicIf(cond bool, err error) {\n\tif cond {\n\t\tpanic(err)\n\t}\n}\n\n// MustAtoi is string to int\nfunc MustAtoi(s string) int {\n\tr, err := strconv.Atoi(s)\n\tif err != nil {\n\t\tE2P(errors.New(\"convert to int error: \" + s))\n\t}\n\treturn r\n}\n\n// OrString return the first not empty string\nfunc OrString(ss ...string) string {\n\tfor _, s := range ss {\n\t\tif s != \"\" {\n\t\t\treturn s\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// If ternary operator\nfunc If(condition bool, trueObj interface{}, falseObj interface{}) interface{} {\n\tif condition {\n\t\treturn trueObj\n\t}\n\treturn falseObj\n}\n\n// MustMarshal checked version for marshal\nfunc MustMarshal(v interface{}) []byte {\n\tb, err := json.Marshal(v)\n\tE2P(err)\n\treturn b\n}\n\n// MustMarshalString string version of MustMarshal\nfunc MustMarshalString(v interface{}) string {\n\treturn string(MustMarshal(v))\n}\n\n// MustUnmarshal checked version for unmarshal\nfunc MustUnmarshal(b []byte, obj interface{}) {\n\terr := json.Unmarshal(b, obj)\n\tE2P(err)\n}\n\n// MustUnmarshalString string version of MustUnmarshal\nfunc MustUnmarshalString(s string, obj interface{}) {\n\tMustUnmarshal([]byte(s), obj)\n}\n\n// MustRemarshal marshal and unmarshal, and check error\n\n\n\n\n\n\n\n// GetFuncName get current call func name\nfunc GetFuncName() string {\n\tpc, _, _, _ := runtime.Caller(1)\n\tnm := runtime.FuncForPC(pc).Name()\n\treturn nm[strings.LastIndex(nm, \".\")+1:]\n}\n\n// MayReplaceLocalhost when run in docker compose, change localhost to host.docker.internal for accessing host network\nfunc MayReplaceLocalhost(host string) string {\n\tif os.Getenv(\"IS_DOCKER\") != \"\" {\n\t\treturn strings.Replace(strings.Replace(host,\n\t\t\t\"localhost\", \"host.docker.internal\", 1),\n\t\t\t\"127.0.0.1\", \"host.docker.internal\", 1)\n\t}\n\treturn host\n}\n\nvar sqlDbs = &mapCache{cache: map[string]*sql.DB{}}\n\ntype mapCache struct {\n\tmutex sync.Mutex\n\tcache map[string]*sql.DB\n}\n\nfunc (m *mapCache) LoadOrStore(conf DBConf, factory func(conf DBConf) (*sql.DB, error)) (*sql.DB, error) {\n\tm.mutex.Lock()\n\tdefer m.mutex.Unlock()\n\tdsn := GetDsn(conf)\n\tif db, ok := m.cache[dsn]; ok {\n\t\treturn db, nil\n\t}\n\tdb, err := factory(conf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tm.cache[dsn] = db\n\treturn db, nil\n}\n\n// PooledDB get pooled sql.DB\nfunc PooledDB(conf DBConf) (*sql.DB, error) {\n\treturn sqlDbs.LoadOrStore(conf, StandaloneDB)\n}\n\n// StandaloneDB get a standalone db instance\nfunc StandaloneDB(conf DBConf) (*sql.DB, error) {\n\tdsn := GetDsn(conf)\n\tlogger.Infof(\"opening standalone %s: %s\", conf.Driver, strings.Replace(dsn, conf.Password, \"****\", 1))\n\treturn sql.Open(conf.Driver, dsn)\n}\n\n// XaDB return a standalone db instance for xa\nfunc XaDB(conf DBConf) (*sql.DB, error) {\n\tdsn := GetDsn(conf)\n\tif conf.Driver == DBTypeMysql {\n\t\tdsn += \"&autocommit=0\"\n\t}\n\tlogger.Infof(\"opening xa standalone %s: %s\", conf.Driver, strings.Replace(dsn, conf.Password, \"****\", 1))\n\treturn sql.Open(conf.Driver, dsn)\n}\n\n// XaClose will log and close the db\nfunc XaClose(db *sql.DB) {\n\tlogger.Infof(\"closing xa db\")\n\t_ = db.Close()\n}\n\n// DBExec use raw db to exec\nfunc DBExec(dbType string, db DB, sql string, values ...interface{}) (affected int64, rerr error) {\n\tif sql == \"\" {\n\t\treturn 0, nil\n\t}\n\tbegan := time.Now()\n\tif len(values) > 0 {\n\t\tsql = GetDBSpecial(dbType).GetPlaceHoldSQL(sql)\n\t}\n\tr, rerr := db.Exec(sql, values...)\n\tused := time.Since(began) / time.Millisecond\n\tif rerr == nil {\n\t\taffected, rerr = r.RowsAffected()\n\t\tlogger.Debugf(\"used: %d ms affected: %d for %s %v\", used, affected, sql, values)\n\t} else {\n\t\tlogger.Errorf(\"used: %d ms exec error: %v for %s %v\", used, rerr, sql, values)\n\t}\n\treturn\n}\n\n// GetDsn get dsn from map config\nfunc GetDsn(conf DBConf) string {\n\thost := MayReplaceLocalhost(conf.Host)\n\tdriver := conf.Driver\n\tdsn := map[string]string{\n\t\t\"mysql\": fmt.Sprintf(\"%s:%s@tcp(%s:%d)/%s?charset=utf8mb4&parseTime=true&loc=Local&interpolateParams=true\",\n\t\t\tconf.User, conf.Password, host, conf.Port, conf.Db),\n\t\t\"postgres\": fmt.Sprintf(\"host=%s user=%s password=%s dbname='%s' search_path=%s port=%d sslmode=disable\",\n\t\t\thost, conf.User, conf.Password, conf.Db, conf.Schema, conf.Port),\n\t\t// sqlserver://sa:mypass@localhost:1234?database=master&connection+timeout=30\n\t\t\"sqlserver\": getSQLServerConnectionString(&conf, &host),\n\t}[driver]\n\tPanicIf(dsn == \"\", fmt.Errorf(\"unknow driver: %s\", driver))\n\treturn dsn\n}\n\nfunc getSQLServerConnectionString(conf *DBConf, host *string) string {\n\tquery := url.Values{}\n\tquery.Add(\"database\", conf.Db)\n\tu := &url.URL{\n\t\tScheme: \"sqlserver\",\n\t\tUser:   url.UserPassword(conf.User, conf.Password),\n\t\tHost:   fmt.Sprintf(\"%s:%d\", *host, conf.Port),\n\t\t// Path:  instance, // if connecting to an instance instead of a port\n\t\tRawQuery: query.Encode(),\n\t}\n\treturn u.String()\n}\n\n// RespAsErrorByJSONRPC  translate json rpc resty response to error\nfunc RespAsErrorByJSONRPC(resp *resty.Response) error {\n\tstr := resp.String()\n\tvar result map[string]interface{}\n\tMustUnmarshalString(str, &result)\n\tif result[\"error\"] != nil {\n\t\trerr := result[\"error\"].(map[string]interface{})\n\t\tif rerr[\"code\"] == JrpcCodeFailure {\n\t\t\treturn fmt.Errorf(\"%s. %w\", str, ErrFailure)\n\t\t} else if rerr[\"code\"] == JrpcCodeOngoing {\n\t\t\treturn ErrOngoing\n\t\t}\n\t\treturn errors.New(resp.String())\n\t}\n\treturn nil\n}\n\n// DeferDo a common defer do used in dtmcli/dtmgrpc\nfunc DeferDo(rerr *error, success func() error, fail func() error) {\n\tif x := recover(); x != nil {\n\t\t*rerr = AsError(x)\n\t\t_ = fail()\n\t\tpanic(x)\n\t} else if *rerr != nil {\n\t\t_ = fail()\n\t} else {\n\t\t*rerr = success()\n\t}\n}\n\n// Escape solve CodeQL reported problem\nfunc Escape(input string) string {\n\tv := strings.Replace(input, \"\\n\", \"\", -1)\n\tv = strings.Replace(v, \"\\r\", \"\", -1)\n\tv = strings.Replace(v, \";\", \"\", -1)\n\t// v = strings.Replace(v, \"'\", \"\", -1)\n\treturn v\n}\n\n// EscapeGet escape get\nfunc EscapeGet(qs url.Values, key string) string {\n\treturn Escape(qs.Get(key))\n}\n\n// InsertBarrier insert a record to barrier\nfunc InsertBarrier(tx DB, transType string, gid string, branchID string, op string, barrierID string, reason string, dbType string, barrierTableName string) (int64, error) {\n\tif op == \"\" {\n\t\treturn 0, nil\n\t}\n\tif dbType == \"\" {\n\t\tdbType = currentDBType\n\t}\n\tif barrierTableName == \"\" {\n\t\tbarrierTableName = BarrierTableName\n\t}\n\tsql := GetDBSpecial(dbType).GetInsertIgnoreTemplate(barrierTableName+\"(trans_type, gid, branch_id, op, barrier_id, reason) values(?,?,?,?,?,?)\", \"uniq_barrier\")\n\treturn DBExec(dbType, tx, sql, transType, gid, branchID, op, barrierID, reason)\n}\n\n```\n# Function Signature: func MustRemarshal(from interface{}, to interface{}) {\n# Natural Language Annotations: // MustRemarshal marshal and unmarshal, and check error\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MustRemarshal(from interface{}, to interface{}) {\n\t// Marshal the 'from' object\n\tdata := MustMarshal(from)\n\t// Unmarshal the data into the 'to' object\n\tMustUnmarshal(data, to)\n}\n```"]}, "kitex-tool/internal_pkg/generator/type.go-FixHasStreamingForExtendedService": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/generator/type.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generator\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"strings\"\n\t\"text/template\"\n\n\t\"github.com/cloudwego/thriftgo/generator/golang/streaming\"\n\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/util\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// File .\ntype File struct {\n\tName    string\n\tContent string\n}\n\n// PackageInfo contains information to generate a package for a service.\ntype PackageInfo struct {\n\tNamespace    string            // a dot-separated string for generating service package under kitex_gen\n\tDependencies map[string]string // package name => import path, used for searching imports\n\t*ServiceInfo                   // the target service\n\n\t// the following fields will be filled and used by the generator\n\tCodec            string\n\tNoFastAPI        bool\n\tVersion          string\n\tRealServiceName  string\n\tImports          map[string]map[string]bool // import path => alias\n\tExternalKitexGen string\n\tFeatures         []feature\n\tFrugalPretouch   bool\n\tModule           string\n\tProtocol         transport.Protocol\n\tIDLName          string\n\tServerPkg        string\n}\n\n// AddImport .\nfunc (p *PackageInfo) AddImport(pkg, path string) {\n\tif p.Imports == nil {\n\t\tp.Imports = make(map[string]map[string]bool)\n\t}\n\tif pkg != \"\" {\n\t\tif p.ExternalKitexGen != \"\" && strings.Contains(path, KitexGenPath) {\n\t\t\tparts := strings.Split(path, KitexGenPath)\n\t\t\tpath = util.JoinPath(p.ExternalKitexGen, parts[len(parts)-1])\n\t\t}\n\t\tif path == pkg {\n\t\t\tp.Imports[path] = nil\n\t\t} else {\n\t\t\tif p.Imports[path] == nil {\n\t\t\t\tp.Imports[path] = make(map[string]bool)\n\t\t\t}\n\t\t\tp.Imports[path][pkg] = true\n\t\t}\n\t}\n}\n\n// AddImports .\nfunc (p *PackageInfo) AddImports(pkgs ...string) {\n\tfor _, pkg := range pkgs {\n\t\tif path, ok := p.Dependencies[pkg]; ok {\n\t\t\tp.AddImport(pkg, path)\n\t\t} else {\n\t\t\tp.AddImport(pkg, pkg)\n\t\t}\n\t}\n}\n\n// PkgInfo .\ntype PkgInfo struct {\n\tPkgName    string\n\tPkgRefName string\n\tImportPath string\n}\n\n// ServiceInfo .\ntype ServiceInfo struct {\n\tPkgInfo\n\tServiceName           string\n\tRawServiceName        string\n\tServiceTypeName       func() string\n\tBase                  *ServiceInfo\n\tMethods               []*MethodInfo\n\tCombineServices       []*ServiceInfo\n\tHasStreaming          bool\n\tServiceFilePath       string\n\tProtocol              string\n\tHandlerReturnKeepResp bool\n\tUseThriftReflection   bool\n}\n\n// AllMethods returns all methods that the service have.\nfunc (s *ServiceInfo) AllMethods() (ms []*MethodInfo) {\n\tms = append(ms, s.Methods...)\n\tfor base := s.Base; base != nil; base = base.Base {\n\t\tms = append(base.Methods, ms...)\n\t}\n\treturn ms\n}\n\n// FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\n\n\n\n\n\n\n\n\n\n\n\n// HasStreamingRecursive recursively check if the service has streaming method\n\n\n\n\n\n\n\n\n\n\n// MethodInfo .\ntype MethodInfo struct {\n\tPkgInfo\n\tServiceName            string\n\tName                   string\n\tRawName                string\n\tOneway                 bool\n\tVoid                   bool\n\tArgs                   []*Parameter\n\tArgsLength             int\n\tResp                   *Parameter\n\tExceptions             []*Parameter\n\tArgStructName          string\n\tResStructName          string\n\tIsResponseNeedRedirect bool // int -> int*\n\tGenArgResultStruct     bool\n\tClientStreaming        bool\n\tServerStreaming        bool\n\tStreaming              *streaming.Streaming\n}\n\n// Parameter .\ntype Parameter struct {\n\tDeps    []PkgInfo\n\tName    string\n\tRawName string\n\tType    string // *PkgA.StructB\n}\n\nvar funcs = map[string]interface{}{\n\t\"ToLower\":       strings.ToLower,\n\t\"LowerFirst\":    util.LowerFirst,\n\t\"UpperFirst\":    util.UpperFirst,\n\t\"NotPtr\":        util.NotPtr,\n\t\"ReplaceString\": util.ReplaceString,\n\t\"SnakeString\":   util.SnakeString,\n\t\"HasFeature\":    HasFeature,\n\t\"FilterImports\": FilterImports,\n\t\"backquoted\":    BackQuoted,\n}\n\nfunc AddTemplateFunc(key string, f interface{}) {\n\tfuncs[key] = f\n}\n\nvar templateNames = []string{\n\t\"@client.go-NewClient-option\",\n\t\"@client.go-NewStreamClient-option\",\n\t\"@client.go-EOF\",\n\t\"@server.go-NewServer-option\",\n\t\"@server.go-EOF\",\n\t\"@invoker.go-NewInvoker-option\",\n\t\"@invoker.go-EOF\",\n}\n\nfunc wrapTemplate(point, content string) string {\n\treturn fmt.Sprintf(`{{define \"%s\"}}%s{{end}}`, point, content)\n}\n\nvar templateExtensions = (func() map[string]string {\n\tm := make(map[string]string)\n\tfor _, name := range templateNames {\n\t\t// create dummy templates\n\t\tm[name] = wrapTemplate(name, \"\")\n\t}\n\treturn m\n})()\n\n// SetTemplateExtension .\nfunc SetTemplateExtension(name, text string) {\n\tif _, ok := templateExtensions[name]; ok {\n\t\ttemplateExtensions[name] = text\n\t}\n}\n\nfunc applyExtension(name string, x *template.Template) (*template.Template, error) {\n\tvar err error\n\tfor _, n := range templateNames {\n\t\tx, err = x.Parse(templateExtensions[n])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse extension %q for %s: %w (%#q)\",\n\t\t\t\tn, name, err, templateExtensions[n])\n\t\t}\n\t}\n\treturn x, nil\n}\n\n// Task .\ntype Task struct {\n\tName string\n\tPath string\n\tText string\n\t*template.Template\n\tExt *APIExtension\n}\n\n// Build .\nfunc (t *Task) Build() error {\n\tx, err := template.New(t.Name).Funcs(funcs).Parse(t.Text)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// old fashion\n\tx, err = applyExtension(t.Name, x)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// new fashion\n\tfor _, str := range t.makeExtension() {\n\t\tif x, err = x.Parse(str); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tt.Template = x\n\treturn nil\n}\n\nfunc fileTemplateExtension(name string) (option, eof string) {\n\tfor _, tn := range templateNames {\n\t\tif strings.HasPrefix(tn, \"@\"+name+\"-\") {\n\t\t\tif strings.HasSuffix(tn, \"-option\") {\n\t\t\t\toption = tn\n\t\t\t} else if strings.HasSuffix(tn, \"-EOF\") {\n\t\t\t\teof = tn\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (t *Task) makeExtension() (res []string) {\n\tif t.Ext == nil {\n\t\treturn\n\t}\n\tp1, p2 := fileTemplateExtension(t.Name)\n\tif t.Ext.ExtendOption != \"\" {\n\t\tres = append(res, wrapTemplate(p1, t.Ext.ExtendOption))\n\t}\n\tif t.Ext.ExtendFile != \"\" {\n\t\tres = append(res, wrapTemplate(p2, t.Ext.ExtendFile))\n\t}\n\treturn\n}\n\n// Render .\nfunc (t *Task) Render(data interface{}) (*File, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &File{t.Path, buf.String()}, nil\n}\n\nfunc (t *Task) RenderString(data interface{}) (string, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn buf.String(), nil\n}\n\nfunc FilterImports(Imports map[string]map[string]bool, ms []*MethodInfo) map[string]map[string]bool {\n\tres := map[string]map[string]bool{}\n\tfor _, m := range ms {\n\t\tif m.Resp != nil {\n\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor _, arg := range m.Args {\n\t\t\tfor _, dep := range arg.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn res\n}\n\nfunc BackQuoted(s string) string {\n\treturn \"`\" + s + \"`\"\n}\n\n```\n# Function Signature: func (s *ServiceInfo) FixHasStreamingForExtendedService() {\n# Natural Language Annotations: // FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\nfunc (s *ServiceInfo) FixHasStreamingForExtendedService() {\n\tif s.HasStreaming {\n\t\treturn\n\t}\n\tfor _, method := range s.Methods {\n\t\tif method.ClientStreaming || method.ServerStreaming {\n\t\t\ts.HasStreaming = true\n\t\t\treturn\n\t\t}\n\t}\n\tfor _, combinedService := range s.CombineServices {\n\t\tcombinedService.FixHasStreamingForExtendedService()\n\t\tif combinedService.HasStreaming {\n\t\t\ts.HasStreaming = true\n\t\t\treturn\n\t\t}\n\t}\n\tif s.Base != nil {\n\t\ts.Base.FixHasStreamingForExtendedService()\n\t\tif s.Base.HasStreaming {\n\t\t\ts.HasStreaming = true\n\t\t}\n\t}\n}\n```"]}, "kitex-tool/internal_pkg/generator/type.go-HasStreamingRecursive": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/generator/type.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generator\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"strings\"\n\t\"text/template\"\n\n\t\"github.com/cloudwego/thriftgo/generator/golang/streaming\"\n\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/util\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// File .\ntype File struct {\n\tName    string\n\tContent string\n}\n\n// PackageInfo contains information to generate a package for a service.\ntype PackageInfo struct {\n\tNamespace    string            // a dot-separated string for generating service package under kitex_gen\n\tDependencies map[string]string // package name => import path, used for searching imports\n\t*ServiceInfo                   // the target service\n\n\t// the following fields will be filled and used by the generator\n\tCodec            string\n\tNoFastAPI        bool\n\tVersion          string\n\tRealServiceName  string\n\tImports          map[string]map[string]bool // import path => alias\n\tExternalKitexGen string\n\tFeatures         []feature\n\tFrugalPretouch   bool\n\tModule           string\n\tProtocol         transport.Protocol\n\tIDLName          string\n\tServerPkg        string\n}\n\n// AddImport .\nfunc (p *PackageInfo) AddImport(pkg, path string) {\n\tif p.Imports == nil {\n\t\tp.Imports = make(map[string]map[string]bool)\n\t}\n\tif pkg != \"\" {\n\t\tif p.ExternalKitexGen != \"\" && strings.Contains(path, KitexGenPath) {\n\t\t\tparts := strings.Split(path, KitexGenPath)\n\t\t\tpath = util.JoinPath(p.ExternalKitexGen, parts[len(parts)-1])\n\t\t}\n\t\tif path == pkg {\n\t\t\tp.Imports[path] = nil\n\t\t} else {\n\t\t\tif p.Imports[path] == nil {\n\t\t\t\tp.Imports[path] = make(map[string]bool)\n\t\t\t}\n\t\t\tp.Imports[path][pkg] = true\n\t\t}\n\t}\n}\n\n// AddImports .\nfunc (p *PackageInfo) AddImports(pkgs ...string) {\n\tfor _, pkg := range pkgs {\n\t\tif path, ok := p.Dependencies[pkg]; ok {\n\t\t\tp.AddImport(pkg, path)\n\t\t} else {\n\t\t\tp.AddImport(pkg, pkg)\n\t\t}\n\t}\n}\n\n// PkgInfo .\ntype PkgInfo struct {\n\tPkgName    string\n\tPkgRefName string\n\tImportPath string\n}\n\n// ServiceInfo .\ntype ServiceInfo struct {\n\tPkgInfo\n\tServiceName           string\n\tRawServiceName        string\n\tServiceTypeName       func() string\n\tBase                  *ServiceInfo\n\tMethods               []*MethodInfo\n\tCombineServices       []*ServiceInfo\n\tHasStreaming          bool\n\tServiceFilePath       string\n\tProtocol              string\n\tHandlerReturnKeepResp bool\n\tUseThriftReflection   bool\n}\n\n// AllMethods returns all methods that the service have.\nfunc (s *ServiceInfo) AllMethods() (ms []*MethodInfo) {\n\tms = append(ms, s.Methods...)\n\tfor base := s.Base; base != nil; base = base.Base {\n\t\tms = append(base.Methods, ms...)\n\t}\n\treturn ms\n}\n\n// FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\n\n\n\n\n\n\n\n\n\n\n\n// HasStreamingRecursive recursively check if the service has streaming method\n\n\n\n\n\n\n\n\n\n\n// MethodInfo .\ntype MethodInfo struct {\n\tPkgInfo\n\tServiceName            string\n\tName                   string\n\tRawName                string\n\tOneway                 bool\n\tVoid                   bool\n\tArgs                   []*Parameter\n\tArgsLength             int\n\tResp                   *Parameter\n\tExceptions             []*Parameter\n\tArgStructName          string\n\tResStructName          string\n\tIsResponseNeedRedirect bool // int -> int*\n\tGenArgResultStruct     bool\n\tClientStreaming        bool\n\tServerStreaming        bool\n\tStreaming              *streaming.Streaming\n}\n\n// Parameter .\ntype Parameter struct {\n\tDeps    []PkgInfo\n\tName    string\n\tRawName string\n\tType    string // *PkgA.StructB\n}\n\nvar funcs = map[string]interface{}{\n\t\"ToLower\":       strings.ToLower,\n\t\"LowerFirst\":    util.LowerFirst,\n\t\"UpperFirst\":    util.UpperFirst,\n\t\"NotPtr\":        util.NotPtr,\n\t\"ReplaceString\": util.ReplaceString,\n\t\"SnakeString\":   util.SnakeString,\n\t\"HasFeature\":    HasFeature,\n\t\"FilterImports\": FilterImports,\n\t\"backquoted\":    BackQuoted,\n}\n\nfunc AddTemplateFunc(key string, f interface{}) {\n\tfuncs[key] = f\n}\n\nvar templateNames = []string{\n\t\"@client.go-NewClient-option\",\n\t\"@client.go-NewStreamClient-option\",\n\t\"@client.go-EOF\",\n\t\"@server.go-NewServer-option\",\n\t\"@server.go-EOF\",\n\t\"@invoker.go-NewInvoker-option\",\n\t\"@invoker.go-EOF\",\n}\n\nfunc wrapTemplate(point, content string) string {\n\treturn fmt.Sprintf(`{{define \"%s\"}}%s{{end}}`, point, content)\n}\n\nvar templateExtensions = (func() map[string]string {\n\tm := make(map[string]string)\n\tfor _, name := range templateNames {\n\t\t// create dummy templates\n\t\tm[name] = wrapTemplate(name, \"\")\n\t}\n\treturn m\n})()\n\n// SetTemplateExtension .\nfunc SetTemplateExtension(name, text string) {\n\tif _, ok := templateExtensions[name]; ok {\n\t\ttemplateExtensions[name] = text\n\t}\n}\n\nfunc applyExtension(name string, x *template.Template) (*template.Template, error) {\n\tvar err error\n\tfor _, n := range templateNames {\n\t\tx, err = x.Parse(templateExtensions[n])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse extension %q for %s: %w (%#q)\",\n\t\t\t\tn, name, err, templateExtensions[n])\n\t\t}\n\t}\n\treturn x, nil\n}\n\n// Task .\ntype Task struct {\n\tName string\n\tPath string\n\tText string\n\t*template.Template\n\tExt *APIExtension\n}\n\n// Build .\nfunc (t *Task) Build() error {\n\tx, err := template.New(t.Name).Funcs(funcs).Parse(t.Text)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// old fashion\n\tx, err = applyExtension(t.Name, x)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// new fashion\n\tfor _, str := range t.makeExtension() {\n\t\tif x, err = x.Parse(str); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tt.Template = x\n\treturn nil\n}\n\nfunc fileTemplateExtension(name string) (option, eof string) {\n\tfor _, tn := range templateNames {\n\t\tif strings.HasPrefix(tn, \"@\"+name+\"-\") {\n\t\t\tif strings.HasSuffix(tn, \"-option\") {\n\t\t\t\toption = tn\n\t\t\t} else if strings.HasSuffix(tn, \"-EOF\") {\n\t\t\t\teof = tn\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (t *Task) makeExtension() (res []string) {\n\tif t.Ext == nil {\n\t\treturn\n\t}\n\tp1, p2 := fileTemplateExtension(t.Name)\n\tif t.Ext.ExtendOption != \"\" {\n\t\tres = append(res, wrapTemplate(p1, t.Ext.ExtendOption))\n\t}\n\tif t.Ext.ExtendFile != \"\" {\n\t\tres = append(res, wrapTemplate(p2, t.Ext.ExtendFile))\n\t}\n\treturn\n}\n\n// Render .\nfunc (t *Task) Render(data interface{}) (*File, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &File{t.Path, buf.String()}, nil\n}\n\nfunc (t *Task) RenderString(data interface{}) (string, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn buf.String(), nil\n}\n\nfunc FilterImports(Imports map[string]map[string]bool, ms []*MethodInfo) map[string]map[string]bool {\n\tres := map[string]map[string]bool{}\n\tfor _, m := range ms {\n\t\tif m.Resp != nil {\n\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor _, arg := range m.Args {\n\t\t\tfor _, dep := range arg.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn res\n}\n\nfunc BackQuoted(s string) string {\n\treturn \"`\" + s + \"`\"\n}\n\n```\n# Function Signature: func (s *ServiceInfo) HasStreamingRecursive() bool {\n# Natural Language Annotations: // HasStreamingRecursive recursively check if the service has streaming method\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc (s *ServiceInfo) HasStreamingRecursive() bool {\n\t// Check if the current service has streaming methods\n\tif s.HasStreaming {\n\t\treturn true\n\t}\n\n\t// Recursively check if any base service has streaming methods\n\tif s.Base != nil {\n\t\treturn s.Base.HasStreamingRecursive()\n\t}\n\t\n\t// If no streaming methods were found, return false\n\treturn false\n}\n```"]}, "kitex-tool/internal_pkg/generator/generator.go-Unpack": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/generator/generator.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package generator .\npackage generator\n\nimport (\n\t\"fmt\"\n\t\"go/token\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/log\"\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/tpl\"\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/util\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Constants .\nconst (\n\tKitexGenPath = \"kitex_gen\"\n\tDefaultCodec = \"thrift\"\n\n\tBuildFileName       = \"build.sh\"\n\tBootstrapFileName   = \"bootstrap.sh\"\n\tToolVersionFileName = \"kitex_info.yaml\"\n\tHandlerFileName     = \"handler.go\"\n\tMainFileName        = \"main.go\"\n\tClientFileName      = \"client.go\"\n\tServerFileName      = \"server.go\"\n\tInvokerFileName     = \"invoker.go\"\n\tServiceFileName     = \"*service.go\"\n\tExtensionFilename   = \"extensions.yaml\"\n\n\tDefaultThriftPluginTimeLimit = time.Minute\n)\n\nvar (\n\tkitexImportPath = \"github.com/cloudwego/kitex\"\n\n\tglobalMiddlewares  []Middleware\n\tglobalDependencies = map[string]string{\n\t\t\"kitex\":     kitexImportPath,\n\t\t\"client\":    ImportPathTo(\"client\"),\n\t\t\"server\":    ImportPathTo(\"server\"),\n\t\t\"callopt\":   ImportPathTo(\"client/callopt\"),\n\t\t\"frugal\":    \"github.com/cloudwego/frugal\",\n\t\t\"fieldmask\": \"github.com/cloudwego/thriftgo/fieldmask\",\n\t}\n)\n\n// SetKitexImportPath sets the import path of kitex.\n// Must be called before generating code.\nfunc SetKitexImportPath(path string) {\n\tfor k, v := range globalDependencies {\n\t\tglobalDependencies[k] = strings.ReplaceAll(v, kitexImportPath, path)\n\t}\n\tkitexImportPath = path\n}\n\n// ImportPathTo returns an import path to the specified package under kitex.\nfunc ImportPathTo(pkg string) string {\n\treturn util.JoinPath(kitexImportPath, pkg)\n}\n\n// AddGlobalMiddleware adds middleware for all generators\nfunc AddGlobalMiddleware(mw Middleware) {\n\tglobalMiddlewares = append(globalMiddlewares, mw)\n}\n\n// AddGlobalDependency adds dependency for all generators\nfunc AddGlobalDependency(ref, path string) bool {\n\tif _, ok := globalDependencies[ref]; !ok {\n\t\tglobalDependencies[ref] = path\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Generator generates the codes of main package and scripts for building a server based on kitex.\ntype Generator interface {\n\tGenerateService(pkg *PackageInfo) ([]*File, error)\n\tGenerateMainPackage(pkg *PackageInfo) ([]*File, error)\n\tGenerateCustomPackage(pkg *PackageInfo) ([]*File, error)\n}\n\n// Config .\ntype Config struct {\n\tVerbose               bool\n\tGenerateMain          bool // whether stuff in the main package should be generated\n\tGenerateInvoker       bool // generate main.go with invoker when main package generate\n\tVersion               string\n\tNoFastAPI             bool\n\tModuleName            string\n\tServiceName           string\n\tUse                   string\n\tIDLType               string\n\tIncludes              util.StringSlice\n\tThriftOptions         util.StringSlice\n\tProtobufOptions       util.StringSlice\n\tHessian2Options       util.StringSlice\n\tIDL                   string // the IDL file passed on the command line\n\tOutputPath            string // the output path for main pkg and kitex_gen\n\tPackagePrefix         string\n\tCombineService        bool // combine services to one service\n\tCopyIDL               bool\n\tThriftPlugins         util.StringSlice\n\tProtobufPlugins       util.StringSlice\n\tFeatures              []feature\n\tFrugalPretouch        bool\n\tThriftPluginTimeLimit time.Duration\n\tCompilerPath          string // specify the path of thriftgo or protoc\n\n\tExtensionFile string\n\ttmplExt       *TemplateExtension\n\n\tRecord    bool\n\tRecordCmd []string\n\n\tTemplateDir string\n\n\tGenPath string\n\n\tDeepCopyAPI           bool\n\tProtocol              string\n\tHandlerReturnKeepResp bool\n\n\tNoDependencyCheck bool\n}\n\n// Pack packs the Config into a slice of \"key=val\" strings.\nfunc (c *Config) Pack() (res []string) {\n\tt := reflect.TypeOf(c).Elem()\n\tv := reflect.ValueOf(c).Elem()\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tf := t.Field(i)\n\t\tx := v.Field(i)\n\t\tn := f.Name\n\n\t\t// skip the plugin arguments to avoid the 'strings in strings' trouble\n\t\tif f.Name == \"ThriftPlugins\" || !token.IsExported(f.Name) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif str, ok := x.Interface().(interface{ String() string }); ok {\n\t\t\tres = append(res, n+\"=\"+str.String())\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch x.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tres = append(res, n+\"=\"+fmt.Sprint(x.Bool()))\n\t\tcase reflect.String:\n\t\t\tres = append(res, n+\"=\"+x.String())\n\t\tcase reflect.Slice:\n\t\t\tvar ss []string\n\t\t\tif x.Type().Elem().Kind() == reflect.Int {\n\t\t\t\tfor i := 0; i < x.Len(); i++ {\n\t\t\t\t\tss = append(ss, strconv.Itoa(int(x.Index(i).Int())))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor i := 0; i < x.Len(); i++ {\n\t\t\t\t\tss = append(ss, x.Index(i).String())\n\t\t\t\t}\n\t\t\t}\n\t\t\tres = append(res, n+\"=\"+strings.Join(ss, \";\"))\n\t\tdefault:\n\t\t\tpanic(fmt.Errorf(\"unsupported field type: %+v\", f))\n\t\t}\n\t}\n\treturn res\n}\n\n// Unpack restores the Config from a slice of \"key=val\" strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// AddFeature add registered feature to config\nfunc (c *Config) AddFeature(key string) bool {\n\tif f, ok := getFeature(key); ok {\n\t\tc.Features = append(c.Features, f)\n\t\treturn true\n\t}\n\treturn false\n}\n\n// ApplyExtension applies template extension.\nfunc (c *Config) ApplyExtension() error {\n\ttemplateExtExist := false\n\tpath := util.JoinPath(c.TemplateDir, ExtensionFilename)\n\tif c.TemplateDir != \"\" && util.Exists(path) {\n\t\ttemplateExtExist = true\n\t}\n\n\tif c.ExtensionFile == \"\" && !templateExtExist {\n\t\treturn nil\n\t}\n\n\text := new(TemplateExtension)\n\tif c.ExtensionFile != \"\" {\n\t\tif err := ext.FromYAMLFile(c.ExtensionFile); err != nil {\n\t\t\treturn fmt.Errorf(\"read template extension %q failed: %s\", c.ExtensionFile, err.Error())\n\t\t}\n\t}\n\n\tif templateExtExist {\n\t\tyamlExt := new(TemplateExtension)\n\t\tif err := yamlExt.FromYAMLFile(path); err != nil {\n\t\t\treturn fmt.Errorf(\"read template extension %q failed: %s\", path, err.Error())\n\t\t}\n\t\text.Merge(yamlExt)\n\t}\n\n\tfor _, fn := range ext.FeatureNames {\n\t\tRegisterFeature(fn)\n\t}\n\tfor _, fn := range ext.EnableFeatures {\n\t\tc.AddFeature(fn)\n\t}\n\tfor path, alias := range ext.Dependencies {\n\t\tAddGlobalDependency(alias, path)\n\t}\n\n\tc.tmplExt = ext\n\treturn nil\n}\n\n// NewGenerator .\nfunc NewGenerator(config *Config, middlewares []Middleware) Generator {\n\tmws := append(globalMiddlewares, middlewares...)\n\tg := &generator{Config: config, middlewares: mws}\n\tif g.IDLType == \"\" {\n\t\tg.IDLType = DefaultCodec\n\t}\n\treturn g\n}\n\n// Middleware used generator\ntype Middleware func(HandleFunc) HandleFunc\n\n// HandleFunc used generator\ntype HandleFunc func(*Task, *PackageInfo) (*File, error)\n\ntype generator struct {\n\t*Config\n\tmiddlewares []Middleware\n}\n\nfunc (g *generator) chainMWs(handle HandleFunc) HandleFunc {\n\tfor i := len(g.middlewares) - 1; i > -1; i-- {\n\t\thandle = g.middlewares[i](handle)\n\t}\n\treturn handle\n}\n\nfunc (g *generator) GenerateMainPackage(pkg *PackageInfo) (fs []*File, err error) {\n\tg.updatePackageInfo(pkg)\n\n\ttasks := []*Task{\n\t\t{\n\t\t\tName: BuildFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, BuildFileName),\n\t\t\tText: tpl.BuildTpl,\n\t\t},\n\t\t{\n\t\t\tName: BootstrapFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, \"script\", BootstrapFileName),\n\t\t\tText: tpl.BootstrapTpl,\n\t\t},\n\t\t{\n\t\t\tName: ToolVersionFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, ToolVersionFileName),\n\t\t\tText: tpl.ToolVersionTpl,\n\t\t},\n\t}\n\tif !g.Config.GenerateInvoker {\n\t\ttasks = append(tasks, &Task{\n\t\t\tName: MainFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, MainFileName),\n\t\t\tText: tpl.MainTpl,\n\t\t})\n\t}\n\tfor _, t := range tasks {\n\t\tif util.Exists(t.Path) {\n\t\t\tlog.Info(t.Path, \"exists. Skipped.\")\n\t\t\tcontinue\n\t\t}\n\t\tg.setImports(t.Name, pkg)\n\t\thandle := func(task *Task, pkg *PackageInfo) (*File, error) {\n\t\t\treturn task.Render(pkg)\n\t\t}\n\t\tf, err := g.chainMWs(handle)(t, pkg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t}\n\n\thandlerFilePath := filepath.Join(g.OutputPath, HandlerFileName)\n\tif util.Exists(handlerFilePath) {\n\t\tcomp := newCompleter(\n\t\t\tpkg.ServiceInfo.AllMethods(),\n\t\t\thandlerFilePath,\n\t\t\tpkg.ServiceInfo.ServiceName)\n\t\tf, err := comp.CompleteMethods()\n\t\tif err != nil {\n\t\t\tif err == errNoNewMethod {\n\t\t\t\treturn fs, nil\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t} else {\n\t\ttask := Task{\n\t\t\tName: HandlerFileName,\n\t\t\tPath: handlerFilePath,\n\t\t\tText: tpl.HandlerTpl + \"\\n\" + tpl.HandlerMethodsTpl,\n\t\t}\n\t\tg.setImports(task.Name, pkg)\n\t\thandle := func(task *Task, pkg *PackageInfo) (*File, error) {\n\t\t\treturn task.Render(pkg)\n\t\t}\n\t\tf, err := g.chainMWs(handle)(&task, pkg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t}\n\treturn\n}\n\nfunc (g *generator) GenerateService(pkg *PackageInfo) ([]*File, error) {\n\tg.updatePackageInfo(pkg)\n\toutput := util.JoinPath(g.OutputPath, util.CombineOutputPath(g.GenPath, pkg.Namespace))\n\tsvcPkg := strings.ToLower(pkg.ServiceName)\n\toutput = util.JoinPath(output, svcPkg)\n\text := g.tmplExt\n\tif ext == nil {\n\t\text = new(TemplateExtension)\n\t}\n\n\ttasks := []*Task{\n\t\t{\n\t\t\tName: ClientFileName,\n\t\t\tPath: util.JoinPath(output, ClientFileName),\n\t\t\tText: tpl.ClientTpl,\n\t\t\tExt:  ext.ExtendClient,\n\t\t},\n\t\t{\n\t\t\tName: ServerFileName,\n\t\t\tPath: util.JoinPath(output, ServerFileName),\n\t\t\tText: tpl.ServerTpl,\n\t\t\tExt:  ext.ExtendServer,\n\t\t},\n\t\t{\n\t\t\tName: InvokerFileName,\n\t\t\tPath: util.JoinPath(output, InvokerFileName),\n\t\t\tText: tpl.InvokerTpl,\n\t\t\tExt:  ext.ExtendInvoker,\n\t\t},\n\t\t{\n\t\t\tName: ServiceFileName,\n\t\t\tPath: util.JoinPath(output, svcPkg+\".go\"),\n\t\t\tText: tpl.ServiceTpl,\n\t\t},\n\t}\n\n\tvar fs []*File\n\tfor _, t := range tasks {\n\t\tif err := t.Build(); err != nil {\n\t\t\terr = fmt.Errorf(\"build %s failed: %w\", t.Name, err)\n\t\t\treturn nil, err\n\t\t}\n\t\tg.setImports(t.Name, pkg)\n\t\tif t.Ext != nil {\n\t\t\tfor _, path := range t.Ext.ImportPaths {\n\t\t\t\tif alias, exist := ext.Dependencies[path]; exist {\n\t\t\t\t\tpkg.AddImports(alias)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\thandle := func(task *Task, pkg *PackageInfo) (*File, error) {\n\t\t\treturn task.Render(pkg)\n\t\t}\n\t\tf, err := g.chainMWs(handle)(t, pkg)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"render %s failed: %w\", t.Name, err)\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t}\n\treturn fs, nil\n}\n\nfunc (g *generator) updatePackageInfo(pkg *PackageInfo) {\n\tpkg.NoFastAPI = g.NoFastAPI\n\tpkg.Codec = g.IDLType\n\tpkg.Version = g.Version\n\tpkg.RealServiceName = g.ServiceName\n\tpkg.Features = g.Features\n\tpkg.ExternalKitexGen = g.Use\n\tpkg.FrugalPretouch = g.FrugalPretouch\n\tpkg.Module = g.ModuleName\n\tif strings.EqualFold(g.Protocol, transport.HESSIAN2.String()) {\n\t\tpkg.Protocol = transport.HESSIAN2\n\t}\n\tif pkg.Dependencies == nil {\n\t\tpkg.Dependencies = make(map[string]string)\n\t}\n\n\tfor ref, path := range globalDependencies {\n\t\tif _, ok := pkg.Dependencies[ref]; !ok {\n\t\t\tpkg.Dependencies[ref] = path\n\t\t}\n\t}\n}\n\nfunc (g *generator) setImports(name string, pkg *PackageInfo) {\n\tpkg.Imports = make(map[string]map[string]bool)\n\tswitch name {\n\tcase ClientFileName:\n\t\tpkg.AddImports(\"client\")\n\t\tif pkg.HasStreaming {\n\t\t\tpkg.AddImport(\"streaming\", \"github.com/cloudwego/kitex/pkg/streaming\")\n\t\t\tpkg.AddImport(\"transport\", \"github.com/cloudwego/kitex/transport\")\n\t\t}\n\t\tif len(pkg.AllMethods()) > 0 {\n\t\t\tif needCallOpt(pkg) {\n\t\t\t\tpkg.AddImports(\"callopt\")\n\t\t\t}\n\t\t\tpkg.AddImports(\"context\")\n\t\t}\n\t\tfallthrough\n\tcase HandlerFileName:\n\t\tfor _, m := range pkg.ServiceInfo.AllMethods() {\n\t\t\tif !m.ServerStreaming && !m.ClientStreaming {\n\t\t\t\tpkg.AddImports(\"context\")\n\t\t\t}\n\t\t\tfor _, a := range m.Args {\n\t\t\t\tfor _, dep := range a.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !m.Void && m.Resp != nil {\n\t\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\tcase ServerFileName, InvokerFileName:\n\t\tif len(pkg.CombineServices) == 0 {\n\t\t\tpkg.AddImport(pkg.ServiceInfo.PkgRefName, pkg.ServiceInfo.ImportPath)\n\t\t}\n\t\tpkg.AddImports(\"server\")\n\tcase ServiceFileName:\n\t\tpkg.AddImports(\"errors\")\n\t\tpkg.AddImports(\"client\")\n\t\tpkg.AddImport(\"kitex\", \"github.com/cloudwego/kitex/pkg/serviceinfo\")\n\t\tpkg.AddImport(pkg.ServiceInfo.PkgRefName, pkg.ServiceInfo.ImportPath)\n\t\tif len(pkg.AllMethods()) > 0 {\n\t\t\tpkg.AddImports(\"context\")\n\t\t}\n\t\tfor _, m := range pkg.ServiceInfo.AllMethods() {\n\t\t\tif m.ClientStreaming || m.ServerStreaming {\n\t\t\t\tpkg.AddImports(\"fmt\")\n\t\t\t}\n\t\t\tif m.GenArgResultStruct {\n\t\t\t\tpkg.AddImports(\"proto\")\n\t\t\t} else {\n\t\t\t\t// for method Arg and Result\n\t\t\t\tpkg.AddImport(m.PkgRefName, m.ImportPath)\n\t\t\t}\n\t\t\tfor _, a := range m.Args {\n\t\t\t\tfor _, dep := range a.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif m.Streaming.IsStreaming || pkg.Codec == \"protobuf\" {\n\t\t\t\t// protobuf handler support both PingPong and Unary (streaming) requests\n\t\t\t\tpkg.AddImport(\"streaming\", \"github.com/cloudwego/kitex/pkg/streaming\")\n\t\t\t}\n\t\t\tif !m.Void && m.Resp != nil {\n\t\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor _, e := range m.Exceptions {\n\t\t\t\tfor _, dep := range e.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif pkg.FrugalPretouch {\n\t\t\tpkg.AddImports(\"sync\")\n\t\t\tif len(pkg.AllMethods()) > 0 {\n\t\t\t\tpkg.AddImports(\"frugal\")\n\t\t\t\tpkg.AddImports(\"reflect\")\n\t\t\t}\n\t\t}\n\tcase MainFileName:\n\t\tpkg.AddImport(\"log\", \"log\")\n\t\tpkg.AddImport(pkg.PkgRefName, util.JoinPath(pkg.ImportPath, strings.ToLower(pkg.ServiceName)))\n\t}\n}\n\nfunc needCallOpt(pkg *PackageInfo) bool {\n\t// callopt is referenced only by non-streaming methods\n\tneedCallOpt := false\n\tswitch pkg.Codec {\n\tcase \"thrift\":\n\t\tfor _, m := range pkg.ServiceInfo.AllMethods() {\n\t\t\tif !m.Streaming.IsStreaming {\n\t\t\t\tneedCallOpt = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\tcase \"protobuf\":\n\t\tneedCallOpt = true\n\t}\n\treturn needCallOpt\n}\n\n```\n# Function Signature: func (c *Config) Unpack(args []string) error {\n# Natural Language Annotations: // Unpack restores the Config from a slice of \"key=val\" strings.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc (c *Config) Unpack(args []string) error {\n\tt := reflect.TypeOf(c).Elem()\n\tv := reflect.ValueOf(c).Elem()\n\n\tfor _, arg := range args {\n\t\tparts := strings.SplitN(arg, \"=\", 2)\n\t\tif len(parts) != 2 {\n\t\t\treturn fmt.Errorf(\"invalid argument: %s\", arg)\n\t\t}\n\t\tkey, value := parts[0], parts[1]\n\n\t\tfield, ok := t.FieldByName(key)\n\t\tif !ok || !token.IsExported(field.Name) {\n\t\t\tcontinue\n\t\t}\n\n\t\tfieldValue := v.FieldByName(key)\n\n\t\tswitch fieldValue.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tboolValue, err := strconv.ParseBool(value)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid boolean value for %s: %s\", key, value)\n\t\t\t}\n\t\t\tfieldValue.SetBool(boolValue)\n\t\tcase reflect.String:\n\t\t\tfieldValue.SetString(value)\n\t\tcase reflect.Slice:\n\t\t\telemKind := fieldValue.Type().Elem().Kind()\n\t\t\tvals := strings.Split(value, \";\")\n\t\t\tsliceValue := reflect.MakeSlice(fieldValue.Type(), len(vals), len(vals))\n\n\t\t\tfor i, val := range vals {\n\t\t\t\tswitch elemKind {\n\t\t\t\tcase reflect.Int:\n\t\t\t\t\tintValue, err := strconv.Atoi(val)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"invalid integer value for %s: %s\", key, val)\n\t\t\t\t\t}\n\t\t\t\t\tsliceValue.Index(i).SetInt(int64(intValue))\n\t\t\t\tcase reflect.String:\n\t\t\t\t\tsliceValue.Index(i).SetString(val)\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Errorf(\"unsupported slice element type for field %s\", key)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfieldValue.Set(sliceValue)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unsupported field type for field %s\", key)\n\t\t}\n\t}\n\treturn nil\n}\n```"]}, "kitex-tool/internal_pkg/util/util.go-CombineOutputPath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/util/util.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage util\n\nimport (\n\t\"fmt\"\n\t\"go/build\"\n\t\"go/format\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"os\"\n\t\"os/exec\"\n\t\"os/user\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"unicode\"\n)\n\n// StringSlice implements the flag.Value interface on string slices\n// to allow a flag to be set multiple times.\ntype StringSlice []string\n\nfunc (ss *StringSlice) String() string {\n\treturn fmt.Sprintf(\"%v\", *ss)\n}\n\n// Set implements the flag.Value interface.\nfunc (ss *StringSlice) Set(value string) error {\n\t*ss = append(*ss, value)\n\treturn nil\n}\n\n// FormatCode formats go source codes.\nfunc FormatCode(code []byte) ([]byte, error) {\n\tformatCode, err := format.Source(code)\n\tif err != nil {\n\t\treturn code, fmt.Errorf(\"format code error: %s\", err)\n\t}\n\treturn formatCode, nil\n}\n\n// GetGOPATH retrieves the GOPATH from environment variables or the `go env` command.\nfunc GetGOPATH() (string, error) {\n\tgoPath := os.Getenv(\"GOPATH\")\n\t// If there are many path in GOPATH, pick up the first one.\n\tif GoPaths := strings.Split(goPath, \":\"); len(GoPaths) >= 1 && strings.TrimSpace(GoPaths[0]) != \"\" {\n\t\treturn strings.TrimSpace(GoPaths[0]), nil\n\t}\n\t// GOPATH not set through environment variables, try to get one by executing \"go env GOPATH\"\n\toutput, err := exec.Command(\"go\", \"env\", \"GOPATH\").Output()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tgoPath = strings.TrimSpace(string(output))\n\tif len(goPath) == 0 {\n\t\tbuildContext := build.Default\n\t\tgoPath = buildContext.GOPATH\n\t}\n\n\tif len(goPath) == 0 {\n\t\treturn \"\", fmt.Errorf(\"GOPATH not found\")\n\t}\n\treturn goPath, nil\n}\n\n// Exists reports whether a file exists.\nfunc Exists(path string) bool {\n\tfi, err := os.Stat(path)\n\tif err != nil {\n\t\treturn os.IsExist(err)\n\t}\n\treturn !fi.IsDir()\n}\n\n// LowerFirst converts the first letter to upper case for the given string.\nfunc LowerFirst(s string) string {\n\trs := []rune(s)\n\trs[0] = unicode.ToLower(rs[0])\n\treturn string(rs)\n}\n\n// ReplaceString be used in string substitution.\nfunc ReplaceString(s, old, new string, n int) string {\n\treturn strings.Replace(s, old, new, n)\n}\n\n// SnakeString converts the string 's' to a snake string\nfunc SnakeString(s string) string {\n\tdata := make([]byte, 0, len(s)*2)\n\tj := false\n\tfor _, d := range []byte(s) {\n\t\tif d >= 'A' && d <= 'Z' {\n\t\t\tif j {\n\t\t\t\tdata = append(data, '_')\n\t\t\t\tj = false\n\t\t\t}\n\t\t} else if d != '_' {\n\t\t\tj = true\n\t\t}\n\t\tdata = append(data, d)\n\t}\n\treturn strings.ToLower(string(data))\n}\n\n// UpperFirst converts the first letter to upper case for the given string.\nfunc UpperFirst(s string) string {\n\trs := []rune(s)\n\trs[0] = unicode.ToUpper(rs[0])\n\treturn string(rs)\n}\n\n// NotPtr converts an pointer type into non-pointer type.\nfunc NotPtr(s string) string {\n\treturn strings.ReplaceAll(s, \"*\", \"\")\n}\n\n// SearchGoMod searches go.mod from the given directory (which must be an absolute path) to\n// the root directory. When the go.mod is found, its module name and path will be returned.\nfunc SearchGoMod(cwd string) (moduleName, path string, found bool) {\n\tfor {\n\t\tpath = filepath.Join(cwd, \"go.mod\")\n\t\tdata, err := ioutil.ReadFile(path)\n\t\tif err == nil {\n\t\t\tre := regexp.MustCompile(`^\\s*module\\s+(\\S+)\\s*`)\n\t\t\tfor _, line := range strings.Split(string(data), \"\\n\") {\n\t\t\t\tm := re.FindStringSubmatch(line)\n\t\t\t\tif m != nil {\n\t\t\t\t\treturn m[1], cwd, true\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"<module name not found in '%s'>\", path), path, true\n\t\t}\n\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn\n\t\t}\n\t\tparentCwd := filepath.Dir(cwd)\n\t\tif parentCwd == cwd {\n\t\t\tbreak\n\t\t}\n\t\tcwd = parentCwd\n\t}\n\treturn\n}\n\nfunc RunGitCommand(gitLink string) (string, string, error) {\n\tu, err := user.Current()\n\tif err != nil {\n\t\treturn \"\", \"Failed to get home dir\", err\n\t}\n\tcachePath := JoinPath(u.HomeDir, \".kitex\", \"cache\")\n\n\tbranch := \"\"\n\tif strings.Contains(gitLink, \".git@\") {\n\t\tstrs := strings.Split(gitLink, \".git@\")\n\t\tbranch = strs[1]\n\t\tgitLink = strs[0] + \".git\"\n\t}\n\tpullLink := gitLink\n\n\tgitLink = strings.TrimPrefix(gitLink, \"git@\")\n\n\tgitLink = strings.TrimSuffix(gitLink, \".git\")\n\n\trepoLink := \"\"\n\tif strings.Contains(gitLink, \"://\") {\n\t\trepoLink = strings.Split(gitLink, \"://\")[1]\n\t} else {\n\t\trepoLink = strings.ReplaceAll(gitLink, \":\", \"/\")\n\t}\n\n\tbranchSuffix := \"\"\n\tif branch != \"\" {\n\t\tbranchSuffix = \"@\" + branch\n\t}\n\tgitPath := JoinPath(cachePath, repoLink+branchSuffix)\n\n\t_, err = os.Stat(JoinPath(gitPath, \".git\"))\n\tif err != nil && !os.IsExist(err) {\n\t\terr = os.MkdirAll(gitPath, os.ModePerm)\n\t\tif err != nil {\n\t\t\treturn \"\", \"Failed to create cache directory,please check your permission for ~/.kitex/cache\", err\n\t\t}\n\t\tcmdClone := exec.Command(\"git\", \"clone\", pullLink, \".\")\n\t\tcmdClone.Dir = gitPath\n\t\tout, gitErr := cmdClone.CombinedOutput()\n\t\tif gitErr != nil {\n\t\t\treturn \"\", string(out), gitErr\n\t\t}\n\t\tif branch != \"\" {\n\t\t\tcmdCheckout := exec.Command(\"git\", \"checkout\", branch)\n\t\t\tcmdCheckout.Dir = gitPath\n\t\t\tout, gitErr = cmdCheckout.CombinedOutput()\n\t\t\treturn gitPath, string(out), gitErr\n\t\t} else {\n\t\t\treturn gitPath, \"\", nil\n\t\t}\n\t}\n\n\tcmdPull := exec.Command(\"git\", \"pull\")\n\tcmdPull.Dir = gitPath\n\tout, gitErr := cmdPull.CombinedOutput()\n\tif gitErr != nil {\n\t\treturn \"\", string(out), gitErr\n\t}\n\n\treturn gitPath, \"\", nil\n}\n\n// CombineOutputPath read the output and path variables and render them into the final path\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// JoinPath joins dirs as golang import format, such as xx/xx/xx\nfunc JoinPath(elem ...string) string {\n\tif runtime.GOOS == \"windows\" {\n\t\treturn strings.ReplaceAll(filepath.Join(elem...), \"\\\\\", \"/\")\n\t}\n\treturn filepath.Join(elem...)\n}\n\n// DownloadFile Download file to local\nfunc DownloadFile(remotePath, localPath string) error {\n\tresp, err := http.Get(remotePath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn fmt.Errorf(\"failed to download file, http status: %s\", resp.Status)\n\t}\n\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = ioutil.WriteFile(localPath, body, 0o644)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// IDLName returns the name of the IDL file.\nfunc IDLName(filename string) string {\n\treturn filepath.Base(filename)\n}\n\ntype Import struct {\n\tAlias string\n\tPath  string\n}\n\nfunc SortImports(imps map[string]string, localPrefix string) (ret []Import) {\n\tstds := make([]Import, 0, len(imps))\n\tlocals := make([]Import, 0, len(imps))\n\tthirds := make([]Import, 0, len(imps))\n\tfor path, alias := range imps {\n\t\tif strings.HasPrefix(path, localPrefix+\"/\") {\n\t\t\tlocals = append(locals, Import{alias, path})\n\t\t} else if !strings.Contains(path, \".\") {\n\t\t\tstds = append(stds, Import{alias, path})\n\t\t} else {\n\t\t\tthirds = append(thirds, Import{alias, path})\n\t\t}\n\t}\n\n\tsort.SliceStable(stds, func(i, j int) bool {\n\t\treturn stds[i].Path < stds[j].Path\n\t})\n\tret = append(ret, stds...)\n\tif len(thirds) > 0 {\n\t\tret = append(ret, Import{\"\", \"\"})\n\t}\n\tsort.SliceStable(thirds, func(i, j int) bool {\n\t\treturn thirds[i].Path < thirds[j].Path\n\t})\n\tret = append(ret, thirds...)\n\tif len(locals) > 0 {\n\t\tret = append(ret, Import{\"\", \"\"})\n\t}\n\tsort.SliceStable(locals, func(i, j int) bool {\n\t\treturn locals[i].Path < locals[j].Path\n\t})\n\tret = append(ret, locals...)\n\treturn ret\n}\n\nfunc (i Import) PackageName() string {\n\tif i.Alias != \"\" {\n\t\treturn i.Alias\n\t} else {\n\t\treturn strings.ToLower(filepath.Base(i.Path))\n\t}\n}\n\nfunc PrintlImports(imports []Import) string {\n\tbuilder := strings.Builder{}\n\tfor _, v := range imports {\n\t\tif v.Path != \"\" {\n\t\t\tbuilder.WriteString(fmt.Sprintf(\"%s %q\\n\", v.Alias, v.Path))\n\t\t} else {\n\t\t\tbuilder.WriteString(\"\\n\")\n\t\t}\n\t}\n\treturn builder.String()\n}\n\n```\n# Function Signature: func CombineOutputPath(outputPath, ns string) string {\n# Natural Language Annotations: // CombineOutputPath read the output and path variables and render them into the final path\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// CombineOutputPath reads the output and path variables and renders them into the final path.\nfunc CombineOutputPath(outputPath, ns string) string {\n\tif ns == \"\" {\n\t\treturn outputPath\n\t}\n\treturn filepath.Join(outputPath, ns)\n}\n```"]}, "kitex-pkg/rpcinfo/stats_util.go-CalcEventCostUs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/rpcinfo/stats_util.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *  http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage rpcinfo\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"runtime/debug\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// Record records the event to RPCStats.\nfunc Record(ctx context.Context, ri RPCInfo, event stats.Event, err error) {\n\tif ctx == nil || ri.Stats() == nil {\n\t\treturn\n\t}\n\tif err != nil {\n\t\tri.Stats().Record(ctx, event, stats.StatusError, err.Error())\n\t} else {\n\t\tri.Stats().Record(ctx, event, stats.StatusInfo, \"\")\n\t}\n}\n\n// CalcEventCostUs calculates the duration between start and end and returns in microsecond.\n\n\n\n\n\n\n\n// ClientPanicToErr to transform the panic info to error, and output the error if needed.\nfunc ClientPanicToErr(ctx context.Context, panicInfo interface{}, ri RPCInfo, logErr bool) error {\n\te := fmt.Errorf(\"KITEX: client panic, to_service=%s to_method=%s error=%v\\nstack=%s\",\n\t\tri.To().ServiceName(), ri.To().Method(), panicInfo, debug.Stack())\n\trpcStats := AsMutableRPCStats(ri.Stats())\n\trpcStats.SetPanicked(e)\n\tif logErr {\n\t\tklog.CtxErrorf(ctx, \"%s\", e.Error())\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func CalcEventCostUs(start, end Event) uint64 {\n# Natural Language Annotations: // CalcEventCostUs calculates the duration between start and end and returns in microsecond.\nPlease return the generated code surrounded by ```\n", "response": ["To create the `CalcEventCostUs` function as described, you will need to compute the difference between two `Event` instances in terms of time and then convert this difference into microseconds. Here's an implementation based on the given function signature and description:\n\n```go\npackage rpcinfo\n\nimport (\n\t\"time\"\n)\n\n// Assuming Event type has a method Time() that returns a time.Time instance\ntype Event struct {\n\ttimestamp time.Time\n}\n\nfunc (e Event) Time() time.Time {\n\treturn e.timestamp\n}\n\n// CalcEventCostUs calculates the duration between start and end and returns it in microseconds.\nfunc CalcEventCostUs(start, end Event) uint64 {\n\tduration := end.Time().Sub(start.Time())\n\treturn uint64(duration.Microseconds())\n}\n```\n\nNote: This implementation assumes that the `Event` type has a method `Time()` that returns a `time.Time` object. Adjust the implementation accordingly if the actual `Event` structure is different."]}, "kitex-pkg/rpcinfo/invocation.go-NewInvocation": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/rpcinfo/invocation.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage rpcinfo\n\nimport (\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\nvar (\n\t_              Invocation       = (*invocation)(nil)\n\t_              InvocationSetter = (*invocation)(nil)\n\tinvocationPool sync.Pool\n\tglobalSeqID    int32 = 0\n)\n\nfunc init() {\n\tinvocationPool.New = newInvocation\n}\n\n// InvocationSetter is used to set information about an RPC.\ntype InvocationSetter interface {\n\tSetPackageName(name string)\n\tSetServiceName(name string)\n\tSetMethodName(name string)\n\tSetSeqID(seqID int32)\n\tSetBizStatusErr(err kerrors.BizStatusErrorIface)\n\tSetExtra(key string, value interface{})\n\tReset()\n}\ntype invocation struct {\n\tpackageName string\n\tserviceName string\n\tmethodName  string\n\tseqID       int32\n\tbizErr      kerrors.BizStatusErrorIface\n\textra       map[string]interface{}\n}\n\n// NewInvocation creates a new Invocation with the given service, method and optional package.\n\n\n\n\n\n\n\n\n\n\n\n// NewServerInvocation to get Invocation for new request in server side\nfunc NewServerInvocation() Invocation {\n\tivk := invocationPool.Get().(*invocation)\n\treturn ivk\n}\n\nfunc genSeqID() int32 {\n\tid := atomic.AddInt32(&globalSeqID, 1)\n\tif id == 0 {\n\t\t// seqID is non-0 to avoid potential default value judgments leading to error handling\n\t\tid = atomic.AddInt32(&globalSeqID, 1)\n\t}\n\treturn id\n}\n\nfunc newInvocation() interface{} {\n\treturn &invocation{}\n}\n\n// SeqID implements the Invocation interface.\nfunc (i *invocation) SeqID() int32 {\n\treturn i.seqID\n}\n\n// SetSeqID implements the InvocationSetter interface.\nfunc (i *invocation) SetSeqID(seqID int32) {\n\ti.seqID = seqID\n}\n\nfunc (i *invocation) PackageName() string {\n\treturn i.packageName\n}\n\nfunc (i *invocation) SetPackageName(name string) {\n\ti.packageName = name\n}\n\nfunc (i *invocation) ServiceName() string {\n\treturn i.serviceName\n}\n\n// SetServiceName implements the InvocationSetter interface.\nfunc (i *invocation) SetServiceName(name string) {\n\ti.serviceName = name\n}\n\n// MethodName implements the Invocation interface.\nfunc (i *invocation) MethodName() string {\n\treturn i.methodName\n}\n\n// SetMethodName implements the InvocationSetter interface.\nfunc (i *invocation) SetMethodName(name string) {\n\ti.methodName = name\n}\n\n// BizStatusErr implements the Invocation interface.\nfunc (i *invocation) BizStatusErr() kerrors.BizStatusErrorIface {\n\treturn i.bizErr\n}\n\n// SetBizStatusErr implements the InvocationSetter interface.\nfunc (i *invocation) SetBizStatusErr(err kerrors.BizStatusErrorIface) {\n\ti.bizErr = err\n}\n\nfunc (i *invocation) SetExtra(key string, value interface{}) {\n\tif i.extra == nil {\n\t\ti.extra = map[string]interface{}{}\n\t}\n\ti.extra[key] = value\n}\n\nfunc (i *invocation) Extra(key string) interface{} {\n\tif i.extra == nil {\n\t\treturn nil\n\t}\n\treturn i.extra[key]\n}\n\n// Reset implements the InvocationSetter interface.\nfunc (i *invocation) Reset() {\n\ti.zero()\n}\n\n// Recycle reuses the invocation.\nfunc (i *invocation) Recycle() {\n\ti.zero()\n\tinvocationPool.Put(i)\n}\n\nfunc (i *invocation) zero() {\n\ti.seqID = 0\n\ti.packageName = \"\"\n\ti.serviceName = \"\"\n\ti.methodName = \"\"\n\ti.bizErr = nil\n\tfor key := range i.extra {\n\t\tdelete(i.extra, key)\n\t}\n}\n\n```\n# Function Signature: func NewInvocation(service, method string, pkgOpt ...string) *invocation {\n# Natural Language Annotations: // NewInvocation creates a new Invocation with the given service, method and optional package.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// NewInvocation creates a new Invocation with the given service, method and optional package.\nfunc NewInvocation(service, method string, pkgOpt ...string) *invocation {\n\tinv := newInvocation().(*invocation)\n\tinv.SetServiceName(service)\n\tinv.SetMethodName(method)\n\tinv.SetSeqID(genSeqID())\n\n\t// Check if a package name is provided in the optional parameters\n\tif len(pkgOpt) > 0 {\n\t\tinv.SetPackageName(pkgOpt[0])\n\t}\n\t\n\treturn inv\n}\n```"]}, "kitex-pkg/retry/retryer.go-InitWithPolicies": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/retry/retryer.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package retry implements rpc retry\npackage retry\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// RPCCallFunc is the definition with wrap rpc call\ntype RPCCallFunc func(context.Context, Retryer) (rpcinfo rpcinfo.RPCInfo, resp interface{}, err error)\n\n// GenRetryKeyFunc to generate retry key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenRetryKeyFunc func(ctx context.Context, ri rpcinfo.RPCInfo) string\n\n// Retryer is the interface for Retry implements\ntype Retryer interface {\n\t// AllowRetry to check if current request satisfy retry condition[eg: circuit, retry times == 0, chain stop, ddl].\n\t// If not satisfy won't execute Retryer.Do and return the reason message\n\t// Execute anyway for the first time regardless of able to retry.\n\tAllowRetry(ctx context.Context) (msg string, ok bool)\n\n\t// ShouldRetry to check if retry request can be called, it is checked in retryer.Do.\n\t// If not satisfy will return the reason message\n\tShouldRetry(ctx context.Context, err error, callTimes int, req interface{}, cbKey string) (msg string, ok bool)\n\tUpdatePolicy(policy Policy) error\n\n\t// Retry policy execute func. recycleRI is to decide if the firstRI can be recycled.\n\tDo(ctx context.Context, rpcCall RPCCallFunc, firstRI rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error)\n\tAppendErrMsgIfNeeded(ctx context.Context, err error, ri rpcinfo.RPCInfo, msg string)\n\n\t// Prepare to do something needed before retry call.\n\tPrepare(ctx context.Context, prevRI, retryRI rpcinfo.RPCInfo)\n\tDump() map[string]interface{}\n\tType() Type\n}\n\n// NewRetryContainerWithCB build Container that doesn't do circuit breaker statistic but get statistic result.\n// Which is used in case that circuit breaker is enabled.\n// eg:\n//\n//\t   cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key)\n//\t   retryC := retry.NewRetryContainerWithCB(cbs.ServiceControl(), cbs.ServicePanel())\n//\t\t  var opts []client.Option\n//\t\t  opts = append(opts, client.WithRetryContainer(retryC))\n//\t   // enable service circuit breaker\n//\t\t  opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW()))\nfunc NewRetryContainerWithCB(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp))\n}\n\nfunc newCBSuite(opts []circuitbreak.CBSuiteOption) *circuitbreak.CBSuite {\n\treturn circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key, opts...)\n}\n\n// NewRetryContainerWithCBStat build Container that need to do circuit breaker statistic.\n// Which is used in case that the service CB key is customized.\n// eg:\n//\n//\tcbs := circuitbreak.NewCBSuite(YourGenServiceCBKeyFunc)\n//\tretry.NewRetryContainerWithCBStat(cbs.ServiceControl(), cbs.ServicePanel())\nfunc NewRetryContainerWithCBStat(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp), WithContainerCBStat())\n}\n\n// NewRetryContainerWithPercentageLimit build a Container to limiting the percentage of retry requests;\n// This is the RECOMMENDED initializer if you want to control PRECISELY the percentage of retry requests.\nfunc NewRetryContainerWithPercentageLimit() *Container {\n\treturn NewRetryContainer(WithContainerEnablePercentageLimit())\n}\n\n// ContainerOption is used when initializing a Container\ntype ContainerOption func(rc *Container)\n\n// WithContainerCBSuite specifies the CBSuite used in the retry circuitbreak\n// retryer will use its ServiceControl and ServicePanel\n// Its priority is lower than WithContainerCBControl and WithContainerCBPanel\nfunc WithContainerCBSuite(cbs *circuitbreak.CBSuite) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuite = cbs\n\t}\n}\n\n// WithCustomizeKeyFunc specifies the GenRetryKeyFunc to customize retry key\nfunc WithCustomizeKeyFunc(fn GenRetryKeyFunc) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.genRetryKey = fn\n\t}\n}\n\n// WithContainerCBSuiteOptions specifies the circuitbreak.CBSuiteOption for initializing circuitbreak.CBSuite\nfunc WithContainerCBSuiteOptions(opts ...circuitbreak.CBSuiteOption) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuiteOptions = opts\n\t}\n}\n\n// WithContainerCBControl specifies the circuitbreak.Control used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with panel\nfunc WithContainerCBControl(ctrl *circuitbreak.Control) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbCtl = ctrl\n\t}\n}\n\n// WithContainerCBPanel specifies the circuitbreaker.Panel used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with control\nfunc WithContainerCBPanel(panel circuitbreaker.Panel) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbPanel = panel\n\t}\n}\n\n// WithContainerCBStat instructs the circuitbreak.RecordStat is called within the retryer\nfunc WithContainerCBStat() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbStat = true\n\t}\n}\n\n// WithContainerEnablePercentageLimit should be called for limiting the percentage of retry requests\nfunc WithContainerEnablePercentageLimit() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.enablePercentageLimit = true\n\t}\n}\n\n// NewRetryContainer build Container that need to build circuit breaker and do circuit breaker statistic.\n// The caller is responsible for calling Container.Close() to release resources referenced.\nfunc NewRetryContainer(opts ...ContainerOption) *Container {\n\trc := &Container{\n\t\tcbContainer: &cbContainer{\n\t\t\tcbSuite: nil,\n\t\t},\n\t\tretryerMap: sync.Map{},\n\t}\n\tfor _, opt := range opts {\n\t\topt(rc)\n\t}\n\n\tif rc.cbContainer.enablePercentageLimit {\n\t\t// ignore cbSuite/cbCtl/cbPanel options\n\t\trc.cbContainer = &cbContainer{\n\t\t\tenablePercentageLimit: true,\n\t\t\tcbSuite:               newCBSuite(rc.cbContainer.cbSuiteOptions),\n\t\t\tcbSuiteOptions:        rc.cbContainer.cbSuiteOptions,\n\t\t}\n\t}\n\n\tcontainer := rc.cbContainer\n\tif container.cbCtl == nil && container.cbPanel == nil {\n\t\tif container.cbSuite == nil {\n\t\t\tcontainer.cbSuite = newCBSuite(rc.cbContainer.cbSuiteOptions)\n\t\t\tcontainer.cbStat = true\n\t\t}\n\t\tcontainer.cbCtl = container.cbSuite.ServiceControl()\n\t\tcontainer.cbPanel = container.cbSuite.ServicePanel()\n\t}\n\tif !container.IsValid() {\n\t\tpanic(\"KITEX: invalid container\")\n\t}\n\treturn rc\n}\n\nfunc defaultGenRetryKey(_ context.Context, rpcInfo rpcinfo.RPCInfo) string {\n\treturn rpcInfo.To().Method()\n}\n\n// Container is a wrapper for Retryer.\ntype Container struct {\n\thasCodeCfg  bool\n\tretryerMap  sync.Map // <method: retryer>\n\tcbContainer *cbContainer\n\tmsg         string\n\tsync.RWMutex\n\n\tgenRetryKey GenRetryKeyFunc\n\n\t// shouldResultRetry is only used with FailureRetry\n\tshouldResultRetry *ShouldResultRetry\n}\n\n// Recommended usage: NewRetryContainerWithPercentageLimit()\n// For more details, refer to the following comments for each field.\ntype cbContainer struct {\n\t// In NewRetryContainer, if cbCtrl & cbPanel are not set, Kitex will use cbSuite.ServiceControl() and\n\t// cbSuite.ServicePanel(); If cbSuite is nil, Kitex will create one.\n\tcbSuite *circuitbreak.CBSuite\n\n\t// It's more recommended to rely on the cbSuite than specifying cbCtl & cbPanel with corresponding options,\n\t// since cbCtl & cbPanel should be correctly paired, and with the cbSuite, Kitex will ensure it by using the\n\t// cbSuite.ServiceControl() and cbSuite.ServicePanel().\n\tcbCtl   *circuitbreak.Control\n\tcbPanel circuitbreaker.Panel\n\n\t// If cbStat && !enablePercentageLimit, retryer will call `circuitbreak.RecordStat` after rpcCall to record\n\t// rpc failures/timeouts, for cutting down on the retry requests when the error rate is beyond the threshold.\n\tcbStat bool\n\n\t// If enabled, Kitex will always create a cbSuite and use its cbCtl & cbPanel, and retryer will call\n\t// recordRetryStat before rpcCall, to precisely control the percentage of retry requests over all requests.\n\tenablePercentageLimit bool\n\n\t// for creating CBSuite inside NewRetryContainer\n\tcbSuiteOptions []circuitbreak.CBSuiteOption\n}\n\n// IsValid returns true when both cbCtl & cbPanel are not nil\n// It's the user's responsibility to guarantee that cbCtl & cbPanel are correctly paired.\nfunc (c *cbContainer) IsValid() bool {\n\treturn c.cbCtl != nil && c.cbPanel != nil\n}\n\n// InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// DeletePolicy to delete the method by method.\nfunc (rc *Container) DeletePolicy(key string) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\t_, ok := rc.retryerMap.Load(key)\n\tif ok {\n\t\trc.retryerMap.Delete(key)\n\t\trc.msg = fmt.Sprintf(\"delete retryer[%s] at %s\", key, time.Now())\n\t}\n}\n\n// NotifyPolicyChange to receive policy when it changes\nfunc (rc *Container) NotifyPolicyChange(key string, p Policy) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\tr, ok := rc.retryerMap.Load(key)\n\tif ok && r != nil {\n\t\tretryer, ok := r.(Retryer)\n\t\tif ok {\n\t\t\tif retryer.Type() == p.Type {\n\t\t\t\tretryer.UpdatePolicy(p)\n\t\t\t\trc.msg = fmt.Sprintf(\"update retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t\t\treturn\n\t\t\t}\n\t\t\trc.retryerMap.Delete(key)\n\t\t\trc.msg = fmt.Sprintf(\"delete retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t}\n\t}\n\trc.initRetryer(key, p)\n}\n\n// Init to build Retryer with code config.\nfunc (rc *Container) Init(mp map[string]Policy, rr *ShouldResultRetry) (err error) {\n\t// NotifyPolicyChange func may execute before Init func.\n\t// Because retry Container is built before Client init, NotifyPolicyChange can be triggered first\n\trc.updateRetryer(rr)\n\tif err = rc.InitWithPolicies(mp); err != nil {\n\t\treturn fmt.Errorf(\"NewRetryer in Init failed, err=%w\", err)\n\t}\n\treturn nil\n}\n\n// PrepareRetryContext adds necessary keys to context for retry\n// These keys should be added to `ctx` no matter whether there's a need to retry, to avoid sharing the same\n// object objects with another method call, since `ctx` might be reused in user-defined middlewares.\nfunc PrepareRetryContext(ctx context.Context) context.Context {\n\t// reqOp can be used to avoid multiple writes to the request object.\n\t// If a blocking write is needed, implement a lock based on it (spin-lock for example).\n\treqOp := OpNo\n\tctx = context.WithValue(ctx, CtxReqOp, &reqOp)\n\n\t// `respOp` is used to avoid concurrent write/read on the response object, especially for backup requests.\n\t// If `respOp` is modified by one request of this method call, all other requests will skip decoding.\n\trespOp := OpNo\n\tctx = context.WithValue(ctx, CtxRespOp, &respOp)\n\treturn ctx\n}\n\n// WithRetryIfNeeded to check if there is a retryer can be used and if current call can retry.\n// When the retry condition is satisfied, use retryer to call\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewRetryer build a retryer with policy\nfunc NewRetryer(p Policy, r *ShouldResultRetry, cbC *cbContainer) (retryer Retryer, err error) {\n\t// just one retry policy can be enabled at same time\n\tif p.Type == BackupType {\n\t\tretryer, err = newBackupRetryer(p, cbC)\n\t} else {\n\t\tretryer, err = newFailureRetryer(p, r, cbC)\n\t}\n\treturn\n}\n\nfunc (rc *Container) getRetryer(ctx context.Context, ri rpcinfo.RPCInfo) Retryer {\n\tkeyFunc := defaultGenRetryKey\n\tif rc.genRetryKey != nil {\n\t\tkeyFunc = rc.genRetryKey\n\t}\n\t// the priority of specific method is high\n\tr, ok := rc.retryerMap.Load(keyFunc(ctx, ri))\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\tr, ok = rc.retryerMap.Load(Wildcard)\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\treturn nil\n}\n\n// Dump is used to show current retry policy\nfunc (rc *Container) Dump() interface{} {\n\trc.RLock()\n\tdm := make(map[string]interface{})\n\tdm[\"has_code_cfg\"] = rc.hasCodeCfg\n\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\tif r, ok := value.(Retryer); ok {\n\t\t\tdm[key.(string)] = r.Dump()\n\t\t}\n\t\treturn true\n\t})\n\tif rc.msg != \"\" {\n\t\tdm[\"msg\"] = rc.msg\n\t}\n\trc.RUnlock()\n\treturn dm\n}\n\nfunc (rc *Container) initRetryer(method string, p Policy) error {\n\tretryer, err := NewRetryer(p, rc.shouldResultRetry, rc.cbContainer)\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"new retryer[%s-%s] failed, err=%s, at %s\", method, p.Type, err.Error(), time.Now())\n\t\trc.msg = errMsg\n\t\tklog.Warnf(errMsg)\n\t\treturn err\n\t}\n\n\trc.retryerMap.Store(method, retryer)\n\tif p.Enable {\n\t\trc.msg = fmt.Sprintf(\"new retryer[%s-%s] at %s\", method, retryer.Type(), time.Now())\n\t} else {\n\t\trc.msg = fmt.Sprintf(\"disable retryer[%s-%s](enable=%t) %s\", method, p.Type, p.Enable, time.Now())\n\t}\n\treturn nil\n}\n\nfunc (rc *Container) updateRetryer(rr *ShouldResultRetry) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\trc.shouldResultRetry = rr\n\tif rc.shouldResultRetry != nil {\n\t\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\t\tif fr, ok := value.(*failureRetryer); ok {\n\t\t\t\tfr.setSpecifiedResultRetryIfNeeded(rc.shouldResultRetry)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Close releases all possible resources referenced.\nfunc (rc *Container) Close() (err error) {\n\tif rc.cbContainer != nil && rc.cbContainer.cbSuite != nil {\n\t\terr = rc.cbContainer.cbSuite.Close()\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (rc *Container) InitWithPolicies(methodPolicies map[string]Policy) error {\n# Natural Language Annotations: // InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\nfunc (rc *Container) InitWithPolicies(methodPolicies map[string]Policy) error {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\trc.msg = \"\"\n\n\tfor method, policy := range methodPolicies {\n\t\tif err := rc.initRetryer(method, policy); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n```"]}, "kitex-pkg/retry/retryer.go-WithRetryIfNeeded": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/retry/retryer.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package retry implements rpc retry\npackage retry\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// RPCCallFunc is the definition with wrap rpc call\ntype RPCCallFunc func(context.Context, Retryer) (rpcinfo rpcinfo.RPCInfo, resp interface{}, err error)\n\n// GenRetryKeyFunc to generate retry key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenRetryKeyFunc func(ctx context.Context, ri rpcinfo.RPCInfo) string\n\n// Retryer is the interface for Retry implements\ntype Retryer interface {\n\t// AllowRetry to check if current request satisfy retry condition[eg: circuit, retry times == 0, chain stop, ddl].\n\t// If not satisfy won't execute Retryer.Do and return the reason message\n\t// Execute anyway for the first time regardless of able to retry.\n\tAllowRetry(ctx context.Context) (msg string, ok bool)\n\n\t// ShouldRetry to check if retry request can be called, it is checked in retryer.Do.\n\t// If not satisfy will return the reason message\n\tShouldRetry(ctx context.Context, err error, callTimes int, req interface{}, cbKey string) (msg string, ok bool)\n\tUpdatePolicy(policy Policy) error\n\n\t// Retry policy execute func. recycleRI is to decide if the firstRI can be recycled.\n\tDo(ctx context.Context, rpcCall RPCCallFunc, firstRI rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error)\n\tAppendErrMsgIfNeeded(ctx context.Context, err error, ri rpcinfo.RPCInfo, msg string)\n\n\t// Prepare to do something needed before retry call.\n\tPrepare(ctx context.Context, prevRI, retryRI rpcinfo.RPCInfo)\n\tDump() map[string]interface{}\n\tType() Type\n}\n\n// NewRetryContainerWithCB build Container that doesn't do circuit breaker statistic but get statistic result.\n// Which is used in case that circuit breaker is enabled.\n// eg:\n//\n//\t   cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key)\n//\t   retryC := retry.NewRetryContainerWithCB(cbs.ServiceControl(), cbs.ServicePanel())\n//\t\t  var opts []client.Option\n//\t\t  opts = append(opts, client.WithRetryContainer(retryC))\n//\t   // enable service circuit breaker\n//\t\t  opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW()))\nfunc NewRetryContainerWithCB(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp))\n}\n\nfunc newCBSuite(opts []circuitbreak.CBSuiteOption) *circuitbreak.CBSuite {\n\treturn circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key, opts...)\n}\n\n// NewRetryContainerWithCBStat build Container that need to do circuit breaker statistic.\n// Which is used in case that the service CB key is customized.\n// eg:\n//\n//\tcbs := circuitbreak.NewCBSuite(YourGenServiceCBKeyFunc)\n//\tretry.NewRetryContainerWithCBStat(cbs.ServiceControl(), cbs.ServicePanel())\nfunc NewRetryContainerWithCBStat(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp), WithContainerCBStat())\n}\n\n// NewRetryContainerWithPercentageLimit build a Container to limiting the percentage of retry requests;\n// This is the RECOMMENDED initializer if you want to control PRECISELY the percentage of retry requests.\nfunc NewRetryContainerWithPercentageLimit() *Container {\n\treturn NewRetryContainer(WithContainerEnablePercentageLimit())\n}\n\n// ContainerOption is used when initializing a Container\ntype ContainerOption func(rc *Container)\n\n// WithContainerCBSuite specifies the CBSuite used in the retry circuitbreak\n// retryer will use its ServiceControl and ServicePanel\n// Its priority is lower than WithContainerCBControl and WithContainerCBPanel\nfunc WithContainerCBSuite(cbs *circuitbreak.CBSuite) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuite = cbs\n\t}\n}\n\n// WithCustomizeKeyFunc specifies the GenRetryKeyFunc to customize retry key\nfunc WithCustomizeKeyFunc(fn GenRetryKeyFunc) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.genRetryKey = fn\n\t}\n}\n\n// WithContainerCBSuiteOptions specifies the circuitbreak.CBSuiteOption for initializing circuitbreak.CBSuite\nfunc WithContainerCBSuiteOptions(opts ...circuitbreak.CBSuiteOption) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuiteOptions = opts\n\t}\n}\n\n// WithContainerCBControl specifies the circuitbreak.Control used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with panel\nfunc WithContainerCBControl(ctrl *circuitbreak.Control) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbCtl = ctrl\n\t}\n}\n\n// WithContainerCBPanel specifies the circuitbreaker.Panel used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with control\nfunc WithContainerCBPanel(panel circuitbreaker.Panel) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbPanel = panel\n\t}\n}\n\n// WithContainerCBStat instructs the circuitbreak.RecordStat is called within the retryer\nfunc WithContainerCBStat() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbStat = true\n\t}\n}\n\n// WithContainerEnablePercentageLimit should be called for limiting the percentage of retry requests\nfunc WithContainerEnablePercentageLimit() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.enablePercentageLimit = true\n\t}\n}\n\n// NewRetryContainer build Container that need to build circuit breaker and do circuit breaker statistic.\n// The caller is responsible for calling Container.Close() to release resources referenced.\nfunc NewRetryContainer(opts ...ContainerOption) *Container {\n\trc := &Container{\n\t\tcbContainer: &cbContainer{\n\t\t\tcbSuite: nil,\n\t\t},\n\t\tretryerMap: sync.Map{},\n\t}\n\tfor _, opt := range opts {\n\t\topt(rc)\n\t}\n\n\tif rc.cbContainer.enablePercentageLimit {\n\t\t// ignore cbSuite/cbCtl/cbPanel options\n\t\trc.cbContainer = &cbContainer{\n\t\t\tenablePercentageLimit: true,\n\t\t\tcbSuite:               newCBSuite(rc.cbContainer.cbSuiteOptions),\n\t\t\tcbSuiteOptions:        rc.cbContainer.cbSuiteOptions,\n\t\t}\n\t}\n\n\tcontainer := rc.cbContainer\n\tif container.cbCtl == nil && container.cbPanel == nil {\n\t\tif container.cbSuite == nil {\n\t\t\tcontainer.cbSuite = newCBSuite(rc.cbContainer.cbSuiteOptions)\n\t\t\tcontainer.cbStat = true\n\t\t}\n\t\tcontainer.cbCtl = container.cbSuite.ServiceControl()\n\t\tcontainer.cbPanel = container.cbSuite.ServicePanel()\n\t}\n\tif !container.IsValid() {\n\t\tpanic(\"KITEX: invalid container\")\n\t}\n\treturn rc\n}\n\nfunc defaultGenRetryKey(_ context.Context, rpcInfo rpcinfo.RPCInfo) string {\n\treturn rpcInfo.To().Method()\n}\n\n// Container is a wrapper for Retryer.\ntype Container struct {\n\thasCodeCfg  bool\n\tretryerMap  sync.Map // <method: retryer>\n\tcbContainer *cbContainer\n\tmsg         string\n\tsync.RWMutex\n\n\tgenRetryKey GenRetryKeyFunc\n\n\t// shouldResultRetry is only used with FailureRetry\n\tshouldResultRetry *ShouldResultRetry\n}\n\n// Recommended usage: NewRetryContainerWithPercentageLimit()\n// For more details, refer to the following comments for each field.\ntype cbContainer struct {\n\t// In NewRetryContainer, if cbCtrl & cbPanel are not set, Kitex will use cbSuite.ServiceControl() and\n\t// cbSuite.ServicePanel(); If cbSuite is nil, Kitex will create one.\n\tcbSuite *circuitbreak.CBSuite\n\n\t// It's more recommended to rely on the cbSuite than specifying cbCtl & cbPanel with corresponding options,\n\t// since cbCtl & cbPanel should be correctly paired, and with the cbSuite, Kitex will ensure it by using the\n\t// cbSuite.ServiceControl() and cbSuite.ServicePanel().\n\tcbCtl   *circuitbreak.Control\n\tcbPanel circuitbreaker.Panel\n\n\t// If cbStat && !enablePercentageLimit, retryer will call `circuitbreak.RecordStat` after rpcCall to record\n\t// rpc failures/timeouts, for cutting down on the retry requests when the error rate is beyond the threshold.\n\tcbStat bool\n\n\t// If enabled, Kitex will always create a cbSuite and use its cbCtl & cbPanel, and retryer will call\n\t// recordRetryStat before rpcCall, to precisely control the percentage of retry requests over all requests.\n\tenablePercentageLimit bool\n\n\t// for creating CBSuite inside NewRetryContainer\n\tcbSuiteOptions []circuitbreak.CBSuiteOption\n}\n\n// IsValid returns true when both cbCtl & cbPanel are not nil\n// It's the user's responsibility to guarantee that cbCtl & cbPanel are correctly paired.\nfunc (c *cbContainer) IsValid() bool {\n\treturn c.cbCtl != nil && c.cbPanel != nil\n}\n\n// InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// DeletePolicy to delete the method by method.\nfunc (rc *Container) DeletePolicy(key string) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\t_, ok := rc.retryerMap.Load(key)\n\tif ok {\n\t\trc.retryerMap.Delete(key)\n\t\trc.msg = fmt.Sprintf(\"delete retryer[%s] at %s\", key, time.Now())\n\t}\n}\n\n// NotifyPolicyChange to receive policy when it changes\nfunc (rc *Container) NotifyPolicyChange(key string, p Policy) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\tr, ok := rc.retryerMap.Load(key)\n\tif ok && r != nil {\n\t\tretryer, ok := r.(Retryer)\n\t\tif ok {\n\t\t\tif retryer.Type() == p.Type {\n\t\t\t\tretryer.UpdatePolicy(p)\n\t\t\t\trc.msg = fmt.Sprintf(\"update retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t\t\treturn\n\t\t\t}\n\t\t\trc.retryerMap.Delete(key)\n\t\t\trc.msg = fmt.Sprintf(\"delete retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t}\n\t}\n\trc.initRetryer(key, p)\n}\n\n// Init to build Retryer with code config.\nfunc (rc *Container) Init(mp map[string]Policy, rr *ShouldResultRetry) (err error) {\n\t// NotifyPolicyChange func may execute before Init func.\n\t// Because retry Container is built before Client init, NotifyPolicyChange can be triggered first\n\trc.updateRetryer(rr)\n\tif err = rc.InitWithPolicies(mp); err != nil {\n\t\treturn fmt.Errorf(\"NewRetryer in Init failed, err=%w\", err)\n\t}\n\treturn nil\n}\n\n// PrepareRetryContext adds necessary keys to context for retry\n// These keys should be added to `ctx` no matter whether there's a need to retry, to avoid sharing the same\n// object objects with another method call, since `ctx` might be reused in user-defined middlewares.\nfunc PrepareRetryContext(ctx context.Context) context.Context {\n\t// reqOp can be used to avoid multiple writes to the request object.\n\t// If a blocking write is needed, implement a lock based on it (spin-lock for example).\n\treqOp := OpNo\n\tctx = context.WithValue(ctx, CtxReqOp, &reqOp)\n\n\t// `respOp` is used to avoid concurrent write/read on the response object, especially for backup requests.\n\t// If `respOp` is modified by one request of this method call, all other requests will skip decoding.\n\trespOp := OpNo\n\tctx = context.WithValue(ctx, CtxRespOp, &respOp)\n\treturn ctx\n}\n\n// WithRetryIfNeeded to check if there is a retryer can be used and if current call can retry.\n// When the retry condition is satisfied, use retryer to call\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewRetryer build a retryer with policy\nfunc NewRetryer(p Policy, r *ShouldResultRetry, cbC *cbContainer) (retryer Retryer, err error) {\n\t// just one retry policy can be enabled at same time\n\tif p.Type == BackupType {\n\t\tretryer, err = newBackupRetryer(p, cbC)\n\t} else {\n\t\tretryer, err = newFailureRetryer(p, r, cbC)\n\t}\n\treturn\n}\n\nfunc (rc *Container) getRetryer(ctx context.Context, ri rpcinfo.RPCInfo) Retryer {\n\tkeyFunc := defaultGenRetryKey\n\tif rc.genRetryKey != nil {\n\t\tkeyFunc = rc.genRetryKey\n\t}\n\t// the priority of specific method is high\n\tr, ok := rc.retryerMap.Load(keyFunc(ctx, ri))\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\tr, ok = rc.retryerMap.Load(Wildcard)\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\treturn nil\n}\n\n// Dump is used to show current retry policy\nfunc (rc *Container) Dump() interface{} {\n\trc.RLock()\n\tdm := make(map[string]interface{})\n\tdm[\"has_code_cfg\"] = rc.hasCodeCfg\n\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\tif r, ok := value.(Retryer); ok {\n\t\t\tdm[key.(string)] = r.Dump()\n\t\t}\n\t\treturn true\n\t})\n\tif rc.msg != \"\" {\n\t\tdm[\"msg\"] = rc.msg\n\t}\n\trc.RUnlock()\n\treturn dm\n}\n\nfunc (rc *Container) initRetryer(method string, p Policy) error {\n\tretryer, err := NewRetryer(p, rc.shouldResultRetry, rc.cbContainer)\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"new retryer[%s-%s] failed, err=%s, at %s\", method, p.Type, err.Error(), time.Now())\n\t\trc.msg = errMsg\n\t\tklog.Warnf(errMsg)\n\t\treturn err\n\t}\n\n\trc.retryerMap.Store(method, retryer)\n\tif p.Enable {\n\t\trc.msg = fmt.Sprintf(\"new retryer[%s-%s] at %s\", method, retryer.Type(), time.Now())\n\t} else {\n\t\trc.msg = fmt.Sprintf(\"disable retryer[%s-%s](enable=%t) %s\", method, p.Type, p.Enable, time.Now())\n\t}\n\treturn nil\n}\n\nfunc (rc *Container) updateRetryer(rr *ShouldResultRetry) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\trc.shouldResultRetry = rr\n\tif rc.shouldResultRetry != nil {\n\t\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\t\tif fr, ok := value.(*failureRetryer); ok {\n\t\t\t\tfr.setSpecifiedResultRetryIfNeeded(rc.shouldResultRetry)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Close releases all possible resources referenced.\nfunc (rc *Container) Close() (err error) {\n\tif rc.cbContainer != nil && rc.cbContainer.cbSuite != nil {\n\t\terr = rc.cbContainer.cbSuite.Close()\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n# Natural Language Annotations: // WithRetryIfNeeded to check if there is a retryer can be used and if current call can retry.\n// When the retry condition is satisfied, use retryer to call\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n    // Attempt to retrieve a retryer if available for the given RPCInfo\n    retryer := rc.getRetryer(ctx, ri)\n\n    // If no specific retryer is configured, proceed without attempting to retry\n    if retryer == nil {\n        lastRI, _, err = rpcCall(ctx, retryer)\n        return lastRI, false, err\n    }\n\n    // Check if the retry conditions are satisfied\n    msg, ok := retryer.AllowRetry(ctx)\n    if !ok {\n        klog.Infof(\"Retry not allowed for %s: %s\", ri.To().Method(), msg)\n        lastRI, _, err = rpcCall(ctx, retryer)\n        return lastRI, false, err\n    }\n\n    // Perform the RPC call with retry logic\n    lastRI, recycleRI, err = retryer.Do(ctx, rpcCall, ri, request)\n    return lastRI, recycleRI, err\n}\n```"]}, "kitex-pkg/discovery/discovery.go-DefaultDiff": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/discovery/discovery.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package discovery defines interfaces for service discovery.\n// Developers that are willing to customize service discovery\n// should implement their own Resolver and supply it with the\n// option WithResolver at client's creation.\npackage discovery\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// DefaultWeight is the default weight for an instance.\nconst DefaultWeight = 10\n\n// Result contains the result of service discovery process.\n// Cacheable tells whether the instance list can/should be cached.\n// When Cacheable is true, CacheKey can be used to map the instance list in cache.\ntype Result struct {\n\tCacheable bool\n\tCacheKey  string\n\tInstances []Instance\n}\n\n// Change contains the difference between the current discovery result and the previous one.\n// It is designed for providing detail information when dispatching an event for service\n// discovery result change.\n// Since the loadbalancer may rely on caching the result of resolver to improve performance,\n// the resolver implementation should dispatch an event when result changes.\ntype Change struct {\n\tResult  Result\n\tAdded   []Instance\n\tUpdated []Instance\n\tRemoved []Instance\n}\n\n// Resolver resolves the target endpoint into a list of Instance.\ntype Resolver interface {\n\t// Target should return a description for the given target that is suitable for being a key for cache.\n\tTarget(ctx context.Context, target rpcinfo.EndpointInfo) (description string)\n\n\t// Resolve returns a list of instances for the given description of a target.\n\tResolve(ctx context.Context, desc string) (Result, error)\n\n\t// Diff computes the difference between two results.\n\t// When `next` is cacheable, the Change should be cacheable, too. And the `Result` field's CacheKey in\n\t// the return value should be set with the given cacheKey.\n\tDiff(cacheKey string, prev, next Result) (Change, bool)\n\n\t// Name returns the name of the resolver.\n\tName() string\n}\n\n// DefaultDiff provides a natural implementation for the Diff method of the Resolver interface.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instance struct {\n\taddr   net.Addr\n\tweight int\n\ttags   map[string]string\n}\n\nfunc (i *instance) Address() net.Addr {\n\treturn i.addr\n}\n\nfunc (i *instance) Weight() int {\n\treturn i.weight\n}\n\nfunc (i *instance) Tag(key string) (value string, exist bool) {\n\tvalue, exist = i.tags[key]\n\treturn\n}\n\n// NewInstance creates a Instance using the given network, address and tags\nfunc NewInstance(network, address string, weight int, tags map[string]string) Instance {\n\treturn &instance{\n\t\taddr:   utils.NewNetAddr(network, address),\n\t\tweight: weight,\n\t\ttags:   tags,\n\t}\n}\n\n// SynthesizedResolver synthesizes a Resolver using a resolve function.\ntype SynthesizedResolver struct {\n\tTargetFunc  func(ctx context.Context, target rpcinfo.EndpointInfo) string\n\tResolveFunc func(ctx context.Context, key string) (Result, error)\n\tDiffFunc    func(key string, prev, next Result) (Change, bool)\n\tNameFunc    func() string\n}\n\n// Target implements the Resolver interface.\nfunc (sr SynthesizedResolver) Target(ctx context.Context, target rpcinfo.EndpointInfo) string {\n\tif sr.TargetFunc == nil {\n\t\treturn \"\"\n\t}\n\treturn sr.TargetFunc(ctx, target)\n}\n\n// Resolve implements the Resolver interface.\nfunc (sr SynthesizedResolver) Resolve(ctx context.Context, key string) (Result, error) {\n\treturn sr.ResolveFunc(ctx, key)\n}\n\n// Diff implements the Resolver interface.\nfunc (sr SynthesizedResolver) Diff(key string, prev, next Result) (Change, bool) {\n\tif sr.DiffFunc == nil {\n\t\treturn DefaultDiff(key, prev, next)\n\t}\n\treturn sr.DiffFunc(key, prev, next)\n}\n\n// Name implements the Resolver interface\nfunc (sr SynthesizedResolver) Name() string {\n\tif sr.NameFunc == nil {\n\t\treturn \"\"\n\t}\n\treturn sr.NameFunc()\n}\n\n// Instance contains information of an instance from the target service.\ntype Instance interface {\n\tAddress() net.Addr\n\tWeight() int\n\tTag(key string) (value string, exist bool)\n}\n\n```\n# Function Signature: func DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n# Natural Language Annotations: // DefaultDiff provides a natural implementation for the Diff method of the Resolver interface.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// DefaultDiff provides a natural implementation for the Diff method of the Resolver interface.\nfunc DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n\tadded := []Instance{}\n\tupdated := []Instance{}\n\tremoved := []Instance{}\n\n\tnextInstancesMap := make(map[net.Addr]Instance)\n\tfor _, inst := range next.Instances {\n\t\tnextInstancesMap[inst.Address()] = inst\n\t}\n\n\tprevInstancesMap := make(map[net.Addr]Instance)\n\tfor _, inst := range prev.Instances {\n\t\tprevInstancesMap[inst.Address()] = inst\n\t\t// If an instance in prev is not found in next, it is removed\n\t\tif _, found := nextInstancesMap[inst.Address()]; !found {\n\t\t\tremoved = append(removed, inst)\n\t\t}\n\t}\n\n\tfor _, inst := range next.Instances {\n\t\tif prevInst, found := prevInstancesMap[inst.Address()]; found {\n\t\t\t// If it is found in both prev and next, check if it was updated\n\t\t\tif prevInst.Weight() != inst.Weight() || !mapsEqual(prevInst.(*instance).tags, inst.(*instance).tags) {\n\t\t\t\tupdated = append(updated, inst)\n\t\t\t}\n\t\t} else {\n\t\t\t// If it is not found in prev, it has been added\n\t\t\tadded = append(added, inst)\n\t\t}\n\t}\n\n\tchange := Change{\n\t\tResult:  next,\n\t\tAdded:   added,\n\t\tUpdated: updated,\n\t\tRemoved: removed,\n\t}\n\n\tcacheable := next.Cacheable\n\tif cacheable {\n\t\tchange.Result.CacheKey = cacheKey\n\t}\n\n\treturn change, cacheable\n}\n\n// Helper function to compare two maps\nfunc mapsEqual(a, b map[string]string) bool {\n\tif len(a) != len(b) {\n\t\treturn false\n\t}\n\tfor k, v := range a {\n\t\tif bv, ok := b[k]; !ok || bv != v {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n```"]}, "kitex-pkg/protocol/bthrift/binary.go-SetSpanCache": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/binary.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package bthrift .\npackage bthrift\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/pkg/mem\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nvar (\n\t// Binary protocol for bthrift.\n\tBinary binaryProtocol\n\t_      BTProtocol = binaryProtocol{}\n)\n\nvar allocator Allocator\n\nconst binaryInplaceThreshold = 4096 // 4k\n\ntype binaryProtocol struct{}\n\n// SetSpanCache enable/disable binary protocol bytes/string allocator\n\n\n\n\n\n\n\n\n// SetAllocator set binary protocol bytes/string allocator.\nfunc SetAllocator(alloc Allocator) {\n\tallocator = alloc\n}\n\nfunc (binaryProtocol) WriteMessageBegin(buf []byte, name string, typeID thrift.TMessageType, seqid int32) int {\n\toffset := 0\n\tversion := uint32(thrift.VERSION_1) | uint32(typeID)\n\toffset += Binary.WriteI32(buf, int32(version))\n\toffset += Binary.WriteString(buf[offset:], name)\n\toffset += Binary.WriteI32(buf[offset:], seqid)\n\treturn offset\n}\n\nfunc (binaryProtocol) WriteMessageEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteStructBegin(buf []byte, name string) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteStructEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteFieldBegin(buf []byte, name string, typeID thrift.TType, id int16) int {\n\treturn Binary.WriteByte(buf, int8(typeID)) + Binary.WriteI16(buf[1:], id)\n}\n\nfunc (binaryProtocol) WriteFieldEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteFieldStop(buf []byte) int {\n\treturn Binary.WriteByte(buf, thrift.STOP)\n}\n\nfunc (binaryProtocol) WriteMapBegin(buf []byte, keyType, valueType thrift.TType, size int) int {\n\treturn Binary.WriteByte(buf, int8(keyType)) +\n\t\tBinary.WriteByte(buf[1:], int8(valueType)) +\n\t\tBinary.WriteI32(buf[2:], int32(size))\n}\n\nfunc (binaryProtocol) WriteMapEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteListBegin(buf []byte, elemType thrift.TType, size int) int {\n\treturn Binary.WriteByte(buf, int8(elemType)) +\n\t\tBinary.WriteI32(buf[1:], int32(size))\n}\n\nfunc (binaryProtocol) WriteListEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteSetBegin(buf []byte, elemType thrift.TType, size int) int {\n\treturn Binary.WriteByte(buf, int8(elemType)) +\n\t\tBinary.WriteI32(buf[1:], int32(size))\n}\n\nfunc (binaryProtocol) WriteSetEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteBool(buf []byte, value bool) int {\n\tif value {\n\t\treturn Binary.WriteByte(buf, 1)\n\t}\n\treturn Binary.WriteByte(buf, 0)\n}\n\nfunc (binaryProtocol) WriteByte(buf []byte, value int8) int {\n\tbuf[0] = byte(value)\n\treturn 1\n}\n\nfunc (binaryProtocol) WriteI16(buf []byte, value int16) int {\n\tbinary.BigEndian.PutUint16(buf, uint16(value))\n\treturn 2\n}\n\nfunc (binaryProtocol) WriteI32(buf []byte, value int32) int {\n\tbinary.BigEndian.PutUint32(buf, uint32(value))\n\treturn 4\n}\n\nfunc (binaryProtocol) WriteI64(buf []byte, value int64) int {\n\tbinary.BigEndian.PutUint64(buf, uint64(value))\n\treturn 8\n}\n\nfunc (binaryProtocol) WriteDouble(buf []byte, value float64) int {\n\treturn Binary.WriteI64(buf, int64(math.Float64bits(value)))\n}\n\nfunc (binaryProtocol) WriteString(buf []byte, value string) int {\n\tl := Binary.WriteI32(buf, int32(len(value)))\n\tcopy(buf[l:], value)\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) WriteBinary(buf, value []byte) int {\n\tl := Binary.WriteI32(buf, int32(len(value)))\n\tcopy(buf[l:], value)\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) WriteStringNocopy(buf []byte, binaryWriter BinaryWriter, value string) int {\n\treturn Binary.WriteBinaryNocopy(buf, binaryWriter, utils.StringToSliceByte(value))\n}\n\nfunc (binaryProtocol) WriteBinaryNocopy(buf []byte, binaryWriter BinaryWriter, value []byte) int {\n\tl := Binary.WriteI32(buf, int32(len(value)))\n\tif binaryWriter != nil && len(value) > binaryInplaceThreshold {\n\t\tbinaryWriter.WriteDirect(value, len(buf[l:]))\n\t\treturn l\n\t}\n\tcopy(buf[l:], value)\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) MessageBeginLength(name string, typeID thrift.TMessageType, seqid int32) int {\n\tversion := uint32(thrift.VERSION_1) | uint32(typeID)\n\treturn Binary.I32Length(int32(version)) + Binary.StringLength(name) + Binary.I32Length(seqid)\n}\n\nfunc (binaryProtocol) MessageEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) StructBeginLength(name string) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) StructEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) FieldBeginLength(name string, typeID thrift.TType, id int16) int {\n\treturn Binary.ByteLength(int8(typeID)) + Binary.I16Length(id)\n}\n\nfunc (binaryProtocol) FieldEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) FieldStopLength() int {\n\treturn Binary.ByteLength(thrift.STOP)\n}\n\nfunc (binaryProtocol) MapBeginLength(keyType, valueType thrift.TType, size int) int {\n\treturn Binary.ByteLength(int8(keyType)) +\n\t\tBinary.ByteLength(int8(valueType)) +\n\t\tBinary.I32Length(int32(size))\n}\n\nfunc (binaryProtocol) MapEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) ListBeginLength(elemType thrift.TType, size int) int {\n\treturn Binary.ByteLength(int8(elemType)) +\n\t\tBinary.I32Length(int32(size))\n}\n\nfunc (binaryProtocol) ListEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) SetBeginLength(elemType thrift.TType, size int) int {\n\treturn Binary.ByteLength(int8(elemType)) +\n\t\tBinary.I32Length(int32(size))\n}\n\nfunc (binaryProtocol) SetEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) BoolLength(value bool) int {\n\tif value {\n\t\treturn Binary.ByteLength(1)\n\t}\n\treturn Binary.ByteLength(0)\n}\n\nfunc (binaryProtocol) ByteLength(value int8) int {\n\treturn 1\n}\n\nfunc (binaryProtocol) I16Length(value int16) int {\n\treturn 2\n}\n\nfunc (binaryProtocol) I32Length(value int32) int {\n\treturn 4\n}\n\nfunc (binaryProtocol) I64Length(value int64) int {\n\treturn 8\n}\n\nfunc (binaryProtocol) DoubleLength(value float64) int {\n\treturn Binary.I64Length(int64(math.Float64bits(value)))\n}\n\nfunc (binaryProtocol) StringLength(value string) int {\n\treturn Binary.I32Length(int32(len(value))) + len(value)\n}\n\nfunc (binaryProtocol) BinaryLength(value []byte) int {\n\treturn Binary.I32Length(int32(len(value))) + len(value)\n}\n\nfunc (binaryProtocol) StringLengthNocopy(value string) int {\n\treturn Binary.BinaryLengthNocopy(utils.StringToSliceByte(value))\n}\n\nfunc (binaryProtocol) BinaryLengthNocopy(value []byte) int {\n\tl := Binary.I32Length(int32(len(value)))\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) ReadMessageBegin(buf []byte) (name string, typeID thrift.TMessageType, seqid int32, length int, err error) {\n\tsize, l, e := Binary.ReadI32(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size > 0 {\n\t\terr = perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Missing version in ReadMessageBegin\")\n\t\treturn\n\t}\n\ttypeID = thrift.TMessageType(size & 0x0ff)\n\tversion := int64(size) & thrift.VERSION_MASK\n\tif version != thrift.VERSION_1 {\n\t\terr = perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Bad version in ReadMessageBegin\")\n\t\treturn\n\t}\n\tname, l, e = Binary.ReadString(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tseqid, l, e = Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\treturn\n}\n\nfunc (binaryProtocol) ReadMessageEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadStructBegin(buf []byte) (name string, length int, err error) {\n\treturn\n}\n\nfunc (binaryProtocol) ReadStructEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadFieldBegin(buf []byte) (name string, typeID thrift.TType, id int16, length int, err error) {\n\tt, l, e := Binary.ReadByte(buf)\n\tlength += l\n\ttypeID = thrift.TType(t)\n\tif e != nil {\n\t\terr = e\n\t\treturn\n\t}\n\tif t != thrift.STOP {\n\t\tid, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\t}\n\treturn\n}\n\nfunc (binaryProtocol) ReadFieldEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadMapBegin(buf []byte) (keyType, valueType thrift.TType, size, length int, err error) {\n\tk, l, e := Binary.ReadByte(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tkeyType = thrift.TType(k)\n\tv, l, e := Binary.ReadByte(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tvalueType = thrift.TType(v)\n\tsize32, l, e := Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn\n}\n\nfunc (binaryProtocol) ReadMapEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadListBegin(buf []byte) (elemType thrift.TType, size, length int, err error) {\n\tb, l, e := Binary.ReadByte(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, l, e := Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\n\treturn\n}\n\nfunc (binaryProtocol) ReadListEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadSetBegin(buf []byte) (elemType thrift.TType, size, length int, err error) {\n\tb, l, e := Binary.ReadByte(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, l, e := Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn\n}\n\nfunc (binaryProtocol) ReadSetEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadBool(buf []byte) (value bool, length int, err error) {\n\tb, l, e := Binary.ReadByte(buf)\n\tv := true\n\tif b != 1 {\n\t\tv = false\n\t}\n\treturn v, l, e\n}\n\nfunc (binaryProtocol) ReadByte(buf []byte) (value int8, length int, err error) {\n\tif len(buf) < 1 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadByte] buf length less than 1\")\n\t}\n\treturn int8(buf[0]), 1, err\n}\n\nfunc (binaryProtocol) ReadI16(buf []byte) (value int16, length int, err error) {\n\tif len(buf) < 2 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadI16] buf length less than 2\")\n\t}\n\tvalue = int16(binary.BigEndian.Uint16(buf))\n\treturn value, 2, err\n}\n\nfunc (binaryProtocol) ReadI32(buf []byte) (value int32, length int, err error) {\n\tif len(buf) < 4 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadI32] buf length less than 4\")\n\t}\n\tvalue = int32(binary.BigEndian.Uint32(buf))\n\treturn value, 4, err\n}\n\nfunc (binaryProtocol) ReadI64(buf []byte) (value int64, length int, err error) {\n\tif len(buf) < 8 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadI64] buf length less than 8\")\n\t}\n\tvalue = int64(binary.BigEndian.Uint64(buf))\n\treturn value, 8, err\n}\n\nfunc (binaryProtocol) ReadDouble(buf []byte) (value float64, length int, err error) {\n\tif len(buf) < 8 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadDouble] buf length less than 8\")\n\t}\n\tvalue = math.Float64frombits(binary.BigEndian.Uint64(buf))\n\treturn value, 8, err\n}\n\nfunc (binaryProtocol) ReadString(buf []byte) (value string, length int, err error) {\n\tsize, l, e := Binary.ReadI32(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = e\n\t\treturn\n\t}\n\tif size < 0 || int(size) > len(buf) {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadString] the string size greater than buf length\")\n\t}\n\talloc := allocator\n\tif alloc != nil {\n\t\tdata := alloc.Copy(buf[length : length+int(size)])\n\t\tvalue = utils.SliceByteToString(data)\n\t} else {\n\t\tvalue = string(buf[length : length+int(size)])\n\t}\n\tlength += int(size)\n\treturn\n}\n\nfunc (binaryProtocol) ReadBinary(buf []byte) (value []byte, length int, err error) {\n\t_size, l, e := Binary.ReadI32(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = e\n\t\treturn\n\t}\n\tsize := int(_size)\n\tif size < 0 || size > len(buf) {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadBinary] the binary size greater than buf length\")\n\t}\n\talloc := allocator\n\tif alloc != nil {\n\t\tvalue = alloc.Copy(buf[length : length+size])\n\t} else {\n\t\tvalue = make([]byte, size)\n\t\tcopy(value, buf[length:length+size])\n\t}\n\tlength += size\n\treturn\n}\n\n// Skip .\nfunc (binaryProtocol) Skip(buf []byte, fieldType thrift.TType) (length int, err error) {\n\treturn SkipDefaultDepth(buf, Binary, fieldType)\n}\n\n// SkipDefaultDepth skips over the next data element from the provided input TProtocol object.\nfunc SkipDefaultDepth(buf []byte, prot BTProtocol, typeID thrift.TType) (int, error) {\n\treturn Skip(buf, prot, typeID, thrift.DEFAULT_RECURSION_DEPTH)\n}\n\n// Skip skips over the next data element from the provided input TProtocol object.\nfunc Skip(buf []byte, self BTProtocol, fieldType thrift.TType, maxDepth int) (length int, err error) {\n\tif maxDepth <= 0 {\n\t\treturn 0, thrift.NewTProtocolExceptionWithType(thrift.DEPTH_LIMIT, errors.New(\"depth limit exceeded\"))\n\t}\n\n\tvar l int\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tlength += 1\n\t\treturn\n\tcase thrift.BYTE:\n\t\tlength += 1\n\t\treturn\n\tcase thrift.I16:\n\t\tlength += 2\n\t\treturn\n\tcase thrift.I32:\n\t\tlength += 4\n\t\treturn\n\tcase thrift.I64:\n\t\tlength += 8\n\t\treturn\n\tcase thrift.DOUBLE:\n\t\tlength += 8\n\t\treturn\n\tcase thrift.STRING:\n\t\tvar sl int32\n\t\tsl, l, err = self.ReadI32(buf)\n\t\tlength += l + int(sl)\n\t\treturn\n\tcase thrift.STRUCT:\n\t\t_, l, err = self.ReadStructBegin(buf)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tfor {\n\t\t\t_, typeID, _, l, e := self.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif typeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, e = Skip(buf[length:], self, typeID, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t\tl, e = self.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e := self.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tcase thrift.MAP:\n\t\tkeyType, valueType, size, l, e := self.ReadMapBegin(buf)\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, e := Skip(buf[length:], self, keyType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t\tl, e = Skip(buf[length:], self, valueType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e = self.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tcase thrift.SET:\n\t\telemType, size, l, e := self.ReadSetBegin(buf)\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, e = Skip(buf[length:], self, elemType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e = self.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tcase thrift.LIST:\n\t\telemType, size, l, e := self.ReadListBegin(buf)\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, e = Skip(buf[length:], self, elemType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e = self.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tdefault:\n\t\treturn 0, thrift.NewTProtocolExceptionWithType(thrift.INVALID_DATA, fmt.Errorf(\"unknown data type %d\", fieldType))\n\t}\n}\n\n```\n# Function Signature: func SetSpanCache(enable bool) {\n# Natural Language Annotations: // SetSpanCache enable/disable binary protocol bytes/string allocator\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// SetSpanCache enable/disable binary protocol bytes/string allocator\nfunc SetSpanCache(enable bool) {\n\tif enable {\n\t\t// Set the allocator to default or enabled allocator.\n\t\tallocator = DefaultEnabledAllocator // Assume DefaultEnabledAllocator is defined elsewhere\n\t} else {\n\t\t// Disable the allocator by setting it to nil or a no-op allocator.\n\t\tallocator = nil\n\t}\n}\n```"]}, "kitex-pkg/protocol/bthrift/unknown.go-GetUnknownFields": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/unknown.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage bthrift\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/cloudwego/thriftgo/generator/golang/extension/unknown\"\n)\n\n// UnknownField is used to describe an unknown field.\ntype UnknownField struct {\n\tName    string\n\tID      int16\n\tType    int\n\tKeyType int\n\tValType int\n\tValue   interface{}\n}\n\n// GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ConvertUnknownFields converts buf to deserialized unknown fields.\nfunc ConvertUnknownFields(buf unknown.Fields) (fields []UnknownField, err error) {\n\tif len(buf) == 0 {\n\t\treturn nil, errors.New(\"_unknownFields is empty\")\n\t}\n\tvar offset int\n\tvar l int\n\tvar name string\n\tvar fieldTypeId thrift.TType\n\tvar fieldId int16\n\tvar f UnknownField\n\tfor {\n\t\tif offset == len(buf) {\n\t\t\treturn\n\t\t}\n\t\tname, fieldTypeId, fieldId, l, err = Binary.ReadFieldBegin(buf[offset:])\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read field %d begin error: %v\", fieldId, err)\n\t\t}\n\t\tl, err = readUnknownField(&f, buf[offset:], name, fieldTypeId, fieldId)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read unknown field %d error: %v\", fieldId, err)\n\t\t}\n\t\tfields = append(fields, f)\n\t}\n}\n\nfunc readUnknownField(f *UnknownField, buf []byte, name string, fieldType thrift.TType, id int16) (length int, err error) {\n\tvar size int\n\tvar l int\n\tf.Name = name\n\tf.ID = id\n\tf.Type = int(fieldType)\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tf.Value, l, err = Binary.ReadBool(buf[length:])\n\t\tlength += l\n\tcase thrift.BYTE:\n\t\tf.Value, l, err = Binary.ReadByte(buf[length:])\n\t\tlength += l\n\tcase thrift.I16:\n\t\tf.Value, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\tcase thrift.I32:\n\t\tf.Value, l, err = Binary.ReadI32(buf[length:])\n\t\tlength += l\n\tcase thrift.I64:\n\t\tf.Value, l, err = Binary.ReadI64(buf[length:])\n\t\tlength += l\n\tcase thrift.DOUBLE:\n\t\tf.Value, l, err = Binary.ReadDouble(buf[length:])\n\t\tlength += l\n\tcase thrift.STRING:\n\t\tf.Value, l, err = Binary.ReadString(buf[length:])\n\t\tlength += l\n\tcase thrift.SET:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadSetBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tset := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&set[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read set elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set end error: %w\", err)\n\t\t}\n\t\tf.Value = set\n\tcase thrift.LIST:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadListBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tlist := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&list[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read list elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list end error: %w\", err)\n\t\t}\n\t\tf.Value = list\n\tcase thrift.MAP:\n\t\tvar kttype, vttype thrift.TType\n\t\tkttype, vttype, size, l, err = Binary.ReadMapBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map begin error: %w\", err)\n\t\t}\n\t\tf.KeyType = int(kttype)\n\t\tf.ValType = int(vttype)\n\t\tflatMap := make([]UnknownField, size*2)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&flatMap[2*i], buf[length:], \"\", thrift.TType(f.KeyType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map key error: %w\", err2)\n\t\t\t}\n\t\t\tl, err2 = readUnknownField(&flatMap[2*i+1], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map value error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map end error: %w\", err)\n\t\t}\n\t\tf.Value = flatMap\n\tcase thrift.STRUCT:\n\t\t_, l, err = Binary.ReadStructBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct begin error: %w\", err)\n\t\t}\n\t\tvar field UnknownField\n\t\tvar fields []UnknownField\n\t\tfor {\n\t\t\tname, fieldTypeID, fieldID, l, err := Binary.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field begin error: %w\", err)\n\t\t\t}\n\t\t\tif fieldTypeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, err = readUnknownField(&field, buf[length:], name, fieldTypeID, fieldID)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read struct field error: %w\", err)\n\t\t\t}\n\t\t\tl, err = Binary.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field end error: %w\", err)\n\t\t\t}\n\t\t\tfields = append(fields, field)\n\t\t}\n\t\tl, err = Binary.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct end error: %w\", err)\n\t\t}\n\t\tf.Value = fields\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\tif err != nil {\n\t\treturn length, err\n\t}\n\treturn\n}\n\n// UnknownFieldsLength returns the length of fs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc unknownFieldLength(f *UnknownField) (length int, err error) {\n\t// use constants to avoid some type assert\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\tlength += Binary.BoolLength(false)\n\tcase unknown.TByte:\n\t\tlength += Binary.ByteLength(0)\n\tcase unknown.TDouble:\n\t\tlength += Binary.DoubleLength(0)\n\tcase unknown.TI16:\n\t\tlength += Binary.I16Length(0)\n\tcase unknown.TI32:\n\t\tlength += Binary.I32Length(0)\n\tcase unknown.TI64:\n\t\tlength += Binary.I64Length(0)\n\tcase unknown.TString:\n\t\tlength += Binary.StringLength(f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.SetBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.SetEndLength()\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.ListBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.ListEndLength()\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\tlength += Binary.MapBeginLength(thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := unknownFieldLength(&kvs[i])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t\tl, err = unknownFieldLength(&kvs[i+1])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.MapEndLength()\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\tlength += Binary.StructBeginLength(f.Name)\n\t\tl, err := UnknownFieldsLength(fs)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t\tlength += Binary.FieldStopLength()\n\t\tlength += Binary.StructEndLength()\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n// WriteUnknownFields writes fs into buf, and return written offset of the buf.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc writeUnknownField(buf []byte, f *UnknownField) (offset int, err error) {\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\toffset += Binary.WriteBool(buf, f.Value.(bool))\n\tcase unknown.TByte:\n\t\toffset += Binary.WriteByte(buf, f.Value.(int8))\n\tcase unknown.TDouble:\n\t\toffset += Binary.WriteDouble(buf, f.Value.(float64))\n\tcase unknown.TI16:\n\t\toffset += Binary.WriteI16(buf, f.Value.(int16))\n\tcase unknown.TI32:\n\t\toffset += Binary.WriteI32(buf, f.Value.(int32))\n\tcase unknown.TI64:\n\t\toffset += Binary.WriteI64(buf, f.Value.(int64))\n\tcase unknown.TString:\n\t\toffset += Binary.WriteString(buf, f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteSetBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteSetEnd(buf[offset:])\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteListBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteListEnd(buf[offset:])\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteMapBegin(buf, thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := writeUnknownField(buf[offset:], &kvs[i])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t\tl, err = writeUnknownField(buf[offset:], &kvs[i+1])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteMapEnd(buf[offset:])\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteStructBegin(buf, f.Name)\n\t\tl, err := WriteUnknownFields(buf[offset:], fs)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t\toffset += Binary.WriteFieldStop(buf[offset:])\n\t\toffset += Binary.WriteStructEnd(buf[offset:])\n\tdefault:\n\t\treturn offset, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n```\n# Function Signature: func GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n# Natural Language Annotations: // GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// GetUnknownFields deserializes unknownFields stored in v to a list of *UnknownFields.\nfunc GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n\t// Ensure the input type is correct and contains the expected field\n\tval := reflect.ValueOf(v)\n\tif val.Kind() != reflect.Struct {\n\t\treturn nil, errors.New(\"input must be a struct\")\n\t}\n\n\t// Try to find the unknown fields in the struct\n\tunknownFieldsValue := reflect.Indirect(val).FieldByName(\"_unknownFields\")\n\tif !unknownFieldsValue.IsValid() {\n\t\treturn nil, errors.New(\"struct does not have a _unknownFields field\")\n\t}\n\n\t// Actual deserialization of the unknown fields\n\tbuf, ok := unknownFieldsValue.Interface().(unknown.Fields)\n\tif !ok {\n\t\treturn nil, errors.New(\"_unknownFields is not of type unknown.Fields\")\n\t}\n\n\tfields, err = ConvertUnknownFields(buf)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to convert unknown fields: %v\", err)\n\t}\n\n\treturn fields, nil\n}\n```"]}, "kitex-pkg/protocol/bthrift/unknown.go-WriteUnknownFields": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/unknown.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage bthrift\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/cloudwego/thriftgo/generator/golang/extension/unknown\"\n)\n\n// UnknownField is used to describe an unknown field.\ntype UnknownField struct {\n\tName    string\n\tID      int16\n\tType    int\n\tKeyType int\n\tValType int\n\tValue   interface{}\n}\n\n// GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ConvertUnknownFields converts buf to deserialized unknown fields.\nfunc ConvertUnknownFields(buf unknown.Fields) (fields []UnknownField, err error) {\n\tif len(buf) == 0 {\n\t\treturn nil, errors.New(\"_unknownFields is empty\")\n\t}\n\tvar offset int\n\tvar l int\n\tvar name string\n\tvar fieldTypeId thrift.TType\n\tvar fieldId int16\n\tvar f UnknownField\n\tfor {\n\t\tif offset == len(buf) {\n\t\t\treturn\n\t\t}\n\t\tname, fieldTypeId, fieldId, l, err = Binary.ReadFieldBegin(buf[offset:])\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read field %d begin error: %v\", fieldId, err)\n\t\t}\n\t\tl, err = readUnknownField(&f, buf[offset:], name, fieldTypeId, fieldId)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read unknown field %d error: %v\", fieldId, err)\n\t\t}\n\t\tfields = append(fields, f)\n\t}\n}\n\nfunc readUnknownField(f *UnknownField, buf []byte, name string, fieldType thrift.TType, id int16) (length int, err error) {\n\tvar size int\n\tvar l int\n\tf.Name = name\n\tf.ID = id\n\tf.Type = int(fieldType)\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tf.Value, l, err = Binary.ReadBool(buf[length:])\n\t\tlength += l\n\tcase thrift.BYTE:\n\t\tf.Value, l, err = Binary.ReadByte(buf[length:])\n\t\tlength += l\n\tcase thrift.I16:\n\t\tf.Value, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\tcase thrift.I32:\n\t\tf.Value, l, err = Binary.ReadI32(buf[length:])\n\t\tlength += l\n\tcase thrift.I64:\n\t\tf.Value, l, err = Binary.ReadI64(buf[length:])\n\t\tlength += l\n\tcase thrift.DOUBLE:\n\t\tf.Value, l, err = Binary.ReadDouble(buf[length:])\n\t\tlength += l\n\tcase thrift.STRING:\n\t\tf.Value, l, err = Binary.ReadString(buf[length:])\n\t\tlength += l\n\tcase thrift.SET:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadSetBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tset := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&set[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read set elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set end error: %w\", err)\n\t\t}\n\t\tf.Value = set\n\tcase thrift.LIST:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadListBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tlist := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&list[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read list elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list end error: %w\", err)\n\t\t}\n\t\tf.Value = list\n\tcase thrift.MAP:\n\t\tvar kttype, vttype thrift.TType\n\t\tkttype, vttype, size, l, err = Binary.ReadMapBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map begin error: %w\", err)\n\t\t}\n\t\tf.KeyType = int(kttype)\n\t\tf.ValType = int(vttype)\n\t\tflatMap := make([]UnknownField, size*2)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&flatMap[2*i], buf[length:], \"\", thrift.TType(f.KeyType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map key error: %w\", err2)\n\t\t\t}\n\t\t\tl, err2 = readUnknownField(&flatMap[2*i+1], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map value error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map end error: %w\", err)\n\t\t}\n\t\tf.Value = flatMap\n\tcase thrift.STRUCT:\n\t\t_, l, err = Binary.ReadStructBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct begin error: %w\", err)\n\t\t}\n\t\tvar field UnknownField\n\t\tvar fields []UnknownField\n\t\tfor {\n\t\t\tname, fieldTypeID, fieldID, l, err := Binary.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field begin error: %w\", err)\n\t\t\t}\n\t\t\tif fieldTypeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, err = readUnknownField(&field, buf[length:], name, fieldTypeID, fieldID)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read struct field error: %w\", err)\n\t\t\t}\n\t\t\tl, err = Binary.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field end error: %w\", err)\n\t\t\t}\n\t\t\tfields = append(fields, field)\n\t\t}\n\t\tl, err = Binary.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct end error: %w\", err)\n\t\t}\n\t\tf.Value = fields\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\tif err != nil {\n\t\treturn length, err\n\t}\n\treturn\n}\n\n// UnknownFieldsLength returns the length of fs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc unknownFieldLength(f *UnknownField) (length int, err error) {\n\t// use constants to avoid some type assert\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\tlength += Binary.BoolLength(false)\n\tcase unknown.TByte:\n\t\tlength += Binary.ByteLength(0)\n\tcase unknown.TDouble:\n\t\tlength += Binary.DoubleLength(0)\n\tcase unknown.TI16:\n\t\tlength += Binary.I16Length(0)\n\tcase unknown.TI32:\n\t\tlength += Binary.I32Length(0)\n\tcase unknown.TI64:\n\t\tlength += Binary.I64Length(0)\n\tcase unknown.TString:\n\t\tlength += Binary.StringLength(f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.SetBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.SetEndLength()\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.ListBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.ListEndLength()\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\tlength += Binary.MapBeginLength(thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := unknownFieldLength(&kvs[i])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t\tl, err = unknownFieldLength(&kvs[i+1])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.MapEndLength()\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\tlength += Binary.StructBeginLength(f.Name)\n\t\tl, err := UnknownFieldsLength(fs)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t\tlength += Binary.FieldStopLength()\n\t\tlength += Binary.StructEndLength()\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n// WriteUnknownFields writes fs into buf, and return written offset of the buf.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc writeUnknownField(buf []byte, f *UnknownField) (offset int, err error) {\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\toffset += Binary.WriteBool(buf, f.Value.(bool))\n\tcase unknown.TByte:\n\t\toffset += Binary.WriteByte(buf, f.Value.(int8))\n\tcase unknown.TDouble:\n\t\toffset += Binary.WriteDouble(buf, f.Value.(float64))\n\tcase unknown.TI16:\n\t\toffset += Binary.WriteI16(buf, f.Value.(int16))\n\tcase unknown.TI32:\n\t\toffset += Binary.WriteI32(buf, f.Value.(int32))\n\tcase unknown.TI64:\n\t\toffset += Binary.WriteI64(buf, f.Value.(int64))\n\tcase unknown.TString:\n\t\toffset += Binary.WriteString(buf, f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteSetBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteSetEnd(buf[offset:])\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteListBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteListEnd(buf[offset:])\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteMapBegin(buf, thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := writeUnknownField(buf[offset:], &kvs[i])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t\tl, err = writeUnknownField(buf[offset:], &kvs[i+1])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteMapEnd(buf[offset:])\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteStructBegin(buf, f.Name)\n\t\tl, err := WriteUnknownFields(buf[offset:], fs)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t\toffset += Binary.WriteFieldStop(buf[offset:])\n\t\toffset += Binary.WriteStructEnd(buf[offset:])\n\tdefault:\n\t\treturn offset, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n```\n# Function Signature: func WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n# Natural Language Annotations: // WriteUnknownFields writes fs into buf, and return written offset of the buf.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WriteUnknownFields writes fs into buf, and return written offset of the buf.\nfunc WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n\tfor _, f := range fs {\n\t\t// Write the beginning of the field with its type, ID, and name\n\t\tl, err := Binary.WriteFieldBegin(buf[offset:], f.Name, thrift.TType(f.Type), f.ID)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, fmt.Errorf(\"write field begin error: %w\", err)\n\t\t}\n\n\t\t// Write the field value based on its type\n\t\tl, err = writeUnknownField(buf[offset:], &f)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, fmt.Errorf(\"write field value error: %w\", err)\n\t\t}\n\n\t\t// Write the end of the field\n\t\tl, err = Binary.WriteFieldEnd(buf[offset:])\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, fmt.Errorf(\"write field end error: %w\", err)\n\t\t}\n\t}\n\treturn\n}\n```"]}, "kitex-pkg/circuitbreak/circuitbreak.go-NewCircuitBreakerMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/circuitbreak.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// Parameter contains parameters for circuit breaker.\ntype Parameter struct {\n\t// Enabled means if to enable the circuit breaker.\n\tEnabled bool\n\t// ErrorRate means the rate at which breaks.\n\tErrorRate float64\n\t// MinimalSample means the minimal sample need before break.\n\tMinimalSample int64\n}\n\n// ErrorType means the error type.\ntype ErrorType int\n\n// Constants for ErrorType.\nconst (\n\t// TypeIgnorable means ignorable error, which is ignored by the circuit breaker.\n\tTypeIgnorable ErrorType = iota\n\t// TypeTimeout means timeout error.\n\tTypeTimeout\n\t// TypeFailure means the request failed, but it isn't timeout.\n\tTypeFailure\n\t// TypeSuccess means the request successes.\n\tTypeSuccess\n)\n\n// WrapErrorWithType is used to define the ErrorType for CircuitBreaker.\n// If you don't want the error trigger fuse, you can set the ErrorType to TypeIgnorable,\n// the error won't be regarded as failed.\n// eg: return circuitbreak.WrapErrorWithType.WithCause(err, circuitbreak.TypeIgnorable) in customized middleware.\nfunc WrapErrorWithType(err error, errorType ErrorType) CircuitBreakerAwareError {\n\treturn &errorWrapperWithType{err: err, errType: errorType}\n}\n\ntype GetErrorTypeFunc func(ctx context.Context, request, response interface{}, err error) ErrorType\n\n// Control is the control strategy of the circuit breaker.\ntype Control struct {\n\t// Implement this to generate a key for the circuit breaker panel.\n\tGetKey func(ctx context.Context, request interface{}) (key string, enabled bool)\n\n\t// Implement this to determine the type of error.\n\tGetErrorType GetErrorTypeFunc\n\n\t// Implement this to provide more detailed information about the circuit breaker.\n\t// The err argument is always a kerrors.ErrCircuitBreak.\n\tDecorateError func(ctx context.Context, request interface{}, err error) error\n}\n\n// NewCircuitBreakerMW creates a circuit breaker MW using the given Control strategy and Panel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// RecordStat to report request result to circuit breaker\nfunc RecordStat(ctx context.Context, request, response interface{}, err error, cbKey string, ctl *Control, panel circuitbreaker.Panel) {\n\tswitch ctl.GetErrorType(ctx, request, response, err) {\n\tcase TypeTimeout:\n\t\tpanel.Timeout(cbKey)\n\tcase TypeFailure:\n\t\tpanel.Fail(cbKey)\n\tcase TypeSuccess:\n\t\tpanel.Succeed(cbKey)\n\t}\n}\n\n// CircuitBreakerAwareError is used to wrap ErrorType\ntype CircuitBreakerAwareError interface {\n\terror\n\tTypeForCircuitBreaker() ErrorType\n}\n\ntype errorWrapperWithType struct {\n\terrType ErrorType\n\terr     error\n}\n\nfunc (e errorWrapperWithType) TypeForCircuitBreaker() ErrorType {\n\treturn e.errType\n}\n\nfunc (e errorWrapperWithType) Error() string {\n\treturn e.err.Error()\n}\n\nfunc (e errorWrapperWithType) Unwrap() error {\n\treturn e.err\n}\n\nfunc (e errorWrapperWithType) Is(target error) bool {\n\treturn errors.Is(e.err, target)\n}\n\n```\n# Function Signature: func NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n# Natural Language Annotations: // NewCircuitBreakerMW creates a circuit breaker MW using the given Control strategy and Panel.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// NewCircuitBreakerMW creates a circuit breaker MW using the given Control strategy and Panel.\nfunc NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n    return func(next endpoint.Endpoint) endpoint.Endpoint {\n        return func(ctx context.Context, request interface{}) (response interface{}, err error) {\n            // Generate the key and check if the circuit breaker is enabled\n            cbKey, enabled := control.GetKey(ctx, request)\n            if !enabled {\n                // If not enabled, just proceed with the next endpoint\n                return next(ctx, request)\n            }\n\n            // Execute the next endpoint and capture the response and error\n            response, err = next(ctx, request)\n\n            // Pass the error to the control's DecorateError function for additional processing\n            if err != nil {\n                err = control.DecorateError(ctx, request, err)\n            }\n\n            // Record the statistics with the provided information\n            RecordStat(ctx, request, response, err, cbKey, &control, panel)\n\n            return response, err\n        }\n    }\n}\n```"]}, "kitex-pkg/circuitbreak/cbsuite.go-RPCInfo2Key": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/cbsuite.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\tserviceCBKey  = \"service\"\n\tinstanceCBKey = \"instance\"\n\tcbConfig      = \"cb_config\"\n)\n\nvar defaultCBConfig = CBConfig{Enable: true, ErrRate: 0.5, MinSample: 200}\n\n// GetDefaultCBConfig return defaultConfig of CircuitBreaker.\nfunc GetDefaultCBConfig() CBConfig {\n\treturn defaultCBConfig\n}\n\n// CBConfig is policy config of CircuitBreaker.\n// DON'T FORGET to update DeepCopy() and Equals() if you add new fields.\ntype CBConfig struct {\n\tEnable    bool    `json:\"enable\"`\n\tErrRate   float64 `json:\"err_rate\"`\n\tMinSample int64   `json:\"min_sample\"`\n}\n\n// DeepCopy returns a full copy of CBConfig.\nfunc (c *CBConfig) DeepCopy() *CBConfig {\n\tif c == nil {\n\t\treturn nil\n\t}\n\treturn &CBConfig{\n\t\tEnable:    c.Enable,\n\t\tErrRate:   c.ErrRate,\n\t\tMinSample: c.MinSample,\n\t}\n}\n\nfunc (c *CBConfig) Equals(other *CBConfig) bool {\n\tif c == nil && other == nil {\n\t\treturn true\n\t}\n\tif c == nil || other == nil {\n\t\treturn false\n\t}\n\treturn c.Enable == other.Enable && c.ErrRate == other.ErrRate && c.MinSample == other.MinSample\n}\n\n// GenServiceCBKeyFunc to generate circuit breaker key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenServiceCBKeyFunc func(ri rpcinfo.RPCInfo) string\n\ntype instanceCBConfig struct {\n\tCBConfig\n\tsync.RWMutex\n}\n\n// CBSuite is default wrapper of CircuitBreaker. If you don't have customized policy, you can specify CircuitBreaker\n// middlewares like this:\n//\n//\tcbs := NewCBSuite(GenServiceCBKeyFunc)\n//\topts = append(opts, client.WithCircuitBreaker(cbs))\ntype CBSuite struct {\n\tservicePanel    circuitbreaker.Panel\n\tserviceControl  *Control\n\tinstancePanel   circuitbreaker.Panel\n\tinstanceControl *Control\n\n\tgenServiceCBKey GenServiceCBKeyFunc\n\tserviceCBConfig sync.Map // map[serviceCBKey]CBConfig\n\n\tinstanceCBConfig instanceCBConfig\n\n\tevents event.Queue\n\n\tconfig CBSuiteConfig\n}\n\n// NewCBSuite to build a new CBSuite.\n// Notice: Should NewCBSuite for every client in this version,\n// because event.Queue and event.Bus are not shared with all clients now.\nfunc NewCBSuite(genKey GenServiceCBKeyFunc, options ...CBSuiteOption) *CBSuite {\n\ts := &CBSuite{\n\t\tgenServiceCBKey: genKey,\n\t\tinstanceCBConfig: instanceCBConfig{\n\t\t\tCBConfig: defaultCBConfig,\n\t\t},\n\t\tconfig: CBSuiteConfig{\n\t\t\tserviceGetErrorTypeFunc:  ErrorTypeOnServiceLevel,\n\t\t\tinstanceGetErrorTypeFunc: ErrorTypeOnInstanceLevel,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\toption(&s.config)\n\t}\n\treturn s\n}\n\n// ServiceCBMW return a new service level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// InstanceCBMW return a new instance level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// ServicePanel return cb Panel of service\nfunc (s *CBSuite) ServicePanel() circuitbreaker.Panel {\n\tif s.servicePanel == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.servicePanel\n}\n\n// ServiceControl return cb Control of service\nfunc (s *CBSuite) ServiceControl() *Control {\n\tif s.serviceControl == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.serviceControl\n}\n\n// UpdateServiceCBConfig is to update service CircuitBreaker config.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateServiceCBConfig(key string, cfg CBConfig) {\n\ts.serviceCBConfig.Store(key, cfg)\n}\n\n// UpdateInstanceCBConfig is to update instance CircuitBreaker param.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateInstanceCBConfig(cfg CBConfig) {\n\ts.instanceCBConfig.Lock()\n\ts.instanceCBConfig.CBConfig = cfg\n\ts.instanceCBConfig.Unlock()\n}\n\n// SetEventBusAndQueue is to make CircuitBreaker relate to event change.\nfunc (s *CBSuite) SetEventBusAndQueue(bus event.Bus, events event.Queue) {\n\ts.events = events\n\tif bus != nil {\n\t\tbus.Watch(discovery.ChangeEventName, s.discoveryChangeHandler)\n\t}\n}\n\n// Dump is to dump CircuitBreaker info for debug query.\n\n\n\n\n\n\n\n\n// Close circuitbreaker.Panel to release associated resources.\nfunc (s *CBSuite) Close() error {\n\tif s.servicePanel != nil {\n\t\ts.servicePanel.Close()\n\t\ts.servicePanel = nil\n\t\ts.serviceControl = nil\n\t}\n\tif s.instancePanel != nil {\n\t\ts.instancePanel.Close()\n\t\ts.instancePanel = nil\n\t\ts.instanceControl = nil\n\t}\n\treturn nil\n}\n\nfunc (s *CBSuite) initServiceCB() {\n\tif s.servicePanel != nil && s.serviceControl != nil {\n\t\treturn\n\t}\n\tif s.genServiceCBKey == nil {\n\t\ts.genServiceCBKey = RPCInfo2Key\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.svcTripFunc,\n\t}\n\ts.servicePanel, _ = circuitbreaker.NewPanel(s.onServiceStateChange, opts)\n\n\tsvcKey := func(ctx context.Context, request interface{}) (serviceCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tserviceCBKey = s.genServiceCBKey(ri)\n\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\tenabled = cbConfig.(CBConfig).Enable\n\t\treturn\n\t}\n\ts.serviceControl = &Control{\n\t\tGetKey:       svcKey,\n\t\tGetErrorType: s.config.serviceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) initInstanceCB() {\n\tif s.instancePanel != nil && s.instanceControl != nil {\n\t\treturn\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.insTripFunc,\n\t}\n\ts.instancePanel, _ = circuitbreaker.NewPanel(s.onInstanceStateChange, opts)\n\n\tinstanceKey := func(ctx context.Context, request interface{}) (instCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tinstCBKey = ri.To().Address().String()\n\t\ts.instanceCBConfig.RLock()\n\t\tenabled = s.instanceCBConfig.Enable\n\t\ts.instanceCBConfig.RUnlock()\n\t\treturn\n\t}\n\ts.instanceControl = &Control{\n\t\tGetKey:       instanceKey,\n\t\tGetErrorType: s.config.instanceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) onStateChange(level, key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\tif s.events == nil {\n\t\treturn\n\t}\n\tsuccesses, failures, timeouts := m.Counts()\n\tvar errRate float64\n\tif sum := successes + failures + timeouts; sum > 0 {\n\t\terrRate = float64(failures+timeouts) / float64(sum)\n\t}\n\ts.events.Push(&event.Event{\n\t\tName: level + \"_cb\",\n\t\tTime: time.Now(),\n\t\tDetail: fmt.Sprintf(\"%s: %s -> %s, (succ: %d, err: %d, timeout: %d, rate: %f)\",\n\t\t\tkey, oldState, newState, successes, failures, timeouts, errRate),\n\t})\n}\n\nfunc (s *CBSuite) onServiceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(serviceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) onInstanceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(instanceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) discoveryChangeHandler(e *event.Event) {\n\tif s.instancePanel == nil {\n\t\treturn\n\t}\n\textra := e.Extra.(*discovery.Change)\n\tfor i := range extra.Removed {\n\t\tinstCBKey := extra.Removed[i].Address().String()\n\t\ts.instancePanel.RemoveBreaker(instCBKey)\n\t}\n}\n\nfunc (s *CBSuite) svcTripFunc(key string) circuitbreaker.TripFunc {\n\tpi, _ := s.serviceCBConfig.LoadOrStore(key, defaultCBConfig)\n\tp := pi.(CBConfig)\n\treturn circuitbreaker.RateTripFunc(p.ErrRate, p.MinSample)\n}\n\nfunc (s *CBSuite) insTripFunc(key string) circuitbreaker.TripFunc {\n\ts.instanceCBConfig.RLock()\n\terrRate := s.instanceCBConfig.ErrRate\n\tminSample := s.instanceCBConfig.MinSample\n\ts.instanceCBConfig.RUnlock()\n\treturn circuitbreaker.RateTripFunc(errRate, minSample)\n}\n\nfunc cbDebugInfo(panel circuitbreaker.Panel) map[string]interface{} {\n\tdumper, ok := panel.(interface {\n\t\tDumpBreakers() map[string]circuitbreaker.Breaker\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\tcbMap := make(map[string]interface{})\n\tfor key, breaker := range dumper.DumpBreakers() {\n\t\tcbState := breaker.State()\n\t\tif cbState == circuitbreaker.Closed {\n\t\t\tcontinue\n\t\t}\n\t\tcbMap[key] = map[string]interface{}{\n\t\t\t\"state\":             cbState,\n\t\t\t\"successes in 10s\":  breaker.Metricer().Successes(),\n\t\t\t\"failures in 10s\":   breaker.Metricer().Failures(),\n\t\t\t\"timeouts in 10s\":   breaker.Metricer().Timeouts(),\n\t\t\t\"error rate in 10s\": breaker.Metricer().ErrorRate(),\n\t\t}\n\t}\n\tif len(cbMap) == 0 {\n\t\tcbMap[\"msg\"] = \"all circuit breakers are in closed state\"\n\t}\n\treturn cbMap\n}\n\nfunc (s *CBSuite) configInfo() map[string]interface{} {\n\tsvcCBMap := make(map[string]interface{})\n\ts.serviceCBConfig.Range(func(key, value interface{}) bool {\n\t\tsvcCBMap[key.(string)] = value\n\t\treturn true\n\t})\n\ts.instanceCBConfig.RLock()\n\tinstCBConfig := s.instanceCBConfig.CBConfig\n\ts.instanceCBConfig.RUnlock()\n\n\tcbMap := make(map[string]interface{}, 2)\n\tcbMap[serviceCBKey] = svcCBMap\n\tcbMap[instanceCBKey] = instCBConfig\n\treturn cbMap\n}\n\n// RPCInfo2Key is to generate circuit breaker key through rpcinfo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func RPCInfo2Key(ri rpcinfo.RPCInfo) string {\n# Natural Language Annotations: // RPCInfo2Key is to generate circuit breaker key through rpcinfo\nPlease return the generated code surrounded by ```\n", "response": ["The `RPCInfo2Key` function is supposed to generate a circuit breaker key using the given `rpcinfo.RPCInfo` object. Considering typical use cases where you derive a key from an RPC information structure, it likely involves extracting relevant identifying information such as service name, method name, or addresses. Since the annotations mention generating a key through rpcinfo, let's implement a function that extracts these kinds of details:\n\n```go\n// RPCInfo2Key generates a circuit breaker key using the rpcinfo.RPCInfo.\nfunc RPCInfo2Key(ri rpcinfo.RPCInfo) string {\n\t// Extract the essential information from rpcinfo to form a unique key\n\tserviceName := ri.To().ServiceName()\n\tmethodName := ri.Invocation().MethodName()\n\taddress := ri.To().Address().String()\n\n\t// Combine these elements to ensure a unique identifier for the RPC call circuit breaker\n\tkeyComponents := []string{serviceName, methodName, address}\n\treturn strings.Join(keyComponents, \"_\")\n}\n```\n\nThis function concatenates key components from the `RPCInfo` object. Adjustments can be made depending on what specific properties of `RPCInfo` define uniqueness in your context."]}, "kitex-pkg/circuitbreak/default.go-ErrorTypeOnServiceLevel": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/default.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// some types of error won't trigger circuit breaker\nvar ignoreErrTypes = map[error]ErrorType{\n\tkerrors.ErrInternalException: TypeIgnorable,\n\tkerrors.ErrServiceDiscovery:  TypeIgnorable,\n\tkerrors.ErrACL:               TypeIgnorable,\n\tkerrors.ErrLoadbalance:       TypeIgnorable,\n\tkerrors.ErrRPCFinish:         TypeIgnorable,\n}\n\n// ErrorTypeOnServiceLevel determines the error type with a service level criteria.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\n\n\n\n\n\n\n\n// FailIfError return TypeFailure if err is not nil, otherwise TypeSuccess.\nfunc FailIfError(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\treturn TypeFailure\n\t}\n\treturn TypeSuccess\n}\n\n// NoDecoration returns the original err.\nfunc NoDecoration(ctx context.Context, request interface{}, err error) error {\n\treturn err\n}\n\n```\n# Function Signature: func ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n# Natural Language Annotations: // ErrorTypeOnServiceLevel determines the error type with a service level criteria.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err == nil {\n\t\treturn TypeSuccess\n\t}\n\n\tif errorType, exists := ignoreErrTypes[err]; exists && errorType == TypeIgnorable {\n\t\treturn TypeIgnorable\n\t}\n\n\treturn TypeFailure\n}\n```"]}, "kitex-pkg/circuitbreak/default.go-ErrorTypeOnInstanceLevel": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/default.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// some types of error won't trigger circuit breaker\nvar ignoreErrTypes = map[error]ErrorType{\n\tkerrors.ErrInternalException: TypeIgnorable,\n\tkerrors.ErrServiceDiscovery:  TypeIgnorable,\n\tkerrors.ErrACL:               TypeIgnorable,\n\tkerrors.ErrLoadbalance:       TypeIgnorable,\n\tkerrors.ErrRPCFinish:         TypeIgnorable,\n}\n\n// ErrorTypeOnServiceLevel determines the error type with a service level criteria.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\n\n\n\n\n\n\n\n// FailIfError return TypeFailure if err is not nil, otherwise TypeSuccess.\nfunc FailIfError(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\treturn TypeFailure\n\t}\n\treturn TypeSuccess\n}\n\n// NoDecoration returns the original err.\nfunc NoDecoration(ctx context.Context, request interface{}, err error) error {\n\treturn err\n}\n\n```\n# Function Signature: func ErrorTypeOnInstanceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n# Natural Language Annotations: // ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\nfunc ErrorTypeOnInstanceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\tif errors.Is(err, kerrors.ErrConnection) {\n\t\t\treturn TypeFailure\n\t\t}\n\t}\n\treturn TypeSuccess\n}\n```"]}, "kitex-pkg/utils/ring.go-NewRing": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/ring.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"runtime\"\n)\n\n// ErrRingFull means the ring is full.\nvar ErrRingFull = errors.New(\"ring is full\")\n\n// Deprecated: it's not used by kitex anymore.\n// NewRing creates a ringbuffer with fixed size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Deprecated: it's not used by kitex anymore.\n// Ring implements a fixed size hash list to manage data\ntype Ring struct {\n\tlength int\n\trings  []*ring\n}\n\n// Push appends item to the ring.\nfunc (r *Ring) Push(obj interface{}) error {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Push(obj)\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\terr := r.rings[idx].Push(obj)\n\t\tif err == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn ErrRingFull\n}\n\n// Pop returns the last item and removes it from the ring.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Dump dumps the data in the ring.\nfunc (r *Ring) Dump() interface{} {\n\tm := &ringDump{}\n\tdumpList := make([]*ringDump, 0, r.length)\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\tcurDump := &ringDump{}\n\t\tr.rings[idx].Dump(curDump)\n\t\tdumpList = append(dumpList, curDump)\n\t\tm.Cap += curDump.Cap\n\t\tm.Len += curDump.Len\n\t}\n\tm.Array = make([]interface{}, 0, m.Len)\n\tfor _, shardData := range dumpList {\n\t\tfor i := 0; i < shardData.Len; i++ {\n\t\t\tm.Array = append(m.Array, shardData.Array[i])\n\t\t}\n\t}\n\treturn m\n}\n\n```\n# Function Signature: func NewRing(size int) *Ring {\n# Natural Language Annotations: // Deprecated: it's not used by kitex anymore.\n// NewRing creates a ringbuffer with fixed size.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewRing(size int) *Ring {\n\tif size <= 0 {\n\t\treturn nil\n\t}\n\n\trings := make([]*ring, size)\n\tfor i := 0; i < size; i++ {\n\t\trings[i] = &ring{}\n\t}\n\n\treturn &Ring{\n\t\tlength: size,\n\t\trings:  rings,\n\t}\n}\n```"]}, "kitex-pkg/utils/thrift.go-Serialize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n# Natural Language Annotations: // Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n\t// Create a new memory buffer transport\n\tt.tb = thrift.NewTMemoryBuffer()\n\n\t// Reset the protocol with the new memory buffer\n\tt.tProt = thrift.NewTBinaryProtocol(t.tb, true, true)\n\n\t// Write the message into the protocol using Write method of TStruct\n\tif err = msg.Write(t.tProt); err != nil {\n\t\treturn nil, fmt.Errorf(\"error writing message to protocol: %w\", err)\n\t}\n\n\t// Retrieve the written bytes\n\tb = t.tb.Bytes()\n\treturn b, nil\n}\n```"]}, "kitex-pkg/utils/thrift.go-Deserialize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) (err error) {\n# Natural Language Annotations: // Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) (err error) {\n\t// Create a new transport and protocol to read from the byte slice `b`\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\n\t// Start reading the message\n\tif _, _, seqID, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\n\t// Read the struct using the message protocol\n\tif err := msg.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read struct error: %w\", err)\n\t}\n\n\t// Finish reading the message\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\n\treturn nil\n}\n```"]}, "kitex-pkg/utils/thrift.go-MarshalError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func MarshalError(method string, err error) []byte {\n# Natural Language Annotations: // MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MarshalError(method string, err error) []byte {\n    // Create a new buffer and protocol\n    transport := thrift.NewTMemoryBuffer()\n    proto := thrift.NewTBinaryProtocolTransport(transport)\n\n    // Initialize a thrift application exception with the error message\n    appException := thrift.NewTApplicationException(thrift.INTERNAL_ERROR, err.Error())\n\n    // Write the message begin with method name, message type as exception, and sequence id as zero\n    if writeErr := proto.WriteMessageBegin(method, thrift.EXCEPTION, 0); writeErr != nil {\n        // If there's an error writing message begin, report with an empty buffer\n        return nil\n    }\n\n    // Write the exception itself\n    if writeErr := appException.Write(proto); writeErr != nil {\n        // If there's an error writing the exception, report with an empty buffer\n        return nil\n    }\n\n    // Write the message end\n    if writeErr := proto.WriteMessageEnd(); writeErr != nil {\n        // If there's an error writing message end, report with an empty buffer\n        return nil\n    }\n\n    // Flush the transport to ensure all data is written\n    if flushErr := transport.Flush(nil); flushErr != nil {\n        // If there's an error flushing data, return an empty buffer\n        return nil\n    }\n\n    // Return the buffer's bytes representing the serialized exception\n    return transport.Bytes()\n}\n```"]}, "kitex-pkg/utils/rpcstats.go-CalculateEventCost": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/rpcstats.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// CalculateEventCost get events from rpcstats, and calculates the time duration of (end - start).\n// It returns 0 when get nil rpcinfo event of either stats event.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n# Natural Language Annotations: // CalculateEventCost get events from rpcstats, and calculates the time duration of (end - start).\n// It returns 0 when get nil rpcinfo event of either stats event.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n    // Retrieve the start and end times from the rpcstats using the provided start and end events\n    startTime, foundStart := rpcstats.GetEventTime(start)\n    endTime, foundEnd := rpcstats.GetEventTime(end)\n    \n    // Check if either the start or end event time is not found (nil)\n    if !foundStart || !foundEnd {\n        return 0\n    }\n    \n    // Calculate the duration between the start and end times\n    duration := endTime.Sub(startTime)\n    \n    return duration\n}\n```"]}, "kitex-pkg/utils/json.go-_Map2JSONStr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/json.go:\n```\n/*\n * Copyright 2024 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * MIT License\n *\n * Copyright (c) 2016 json-iterator\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all\n * copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n * The source code of this file is written based on json-iterator,\n * all modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"unicode/utf16\"\n\t\"unicode/utf8\"\n\t\"unsafe\"\n)\n\n// const of json keyword char\nconst (\n\tEmptyJSON  = \"{}\"\n\tComma      = ','\n\tColon      = ':'\n\tDQuotation = '\"'\n\tLeftBrace  = '{'\n\tRightBrace = '}'\n)\n\nconst (\n\tt1 = 0x00 // 0000 0000\n\ttx = 0x80 // 1000 0000\n\tt2 = 0xC0 // 1100 0000\n\tt3 = 0xE0 // 1110 0000\n\tt4 = 0xF0 // 1111 0000\n\tt5 = 0xF8 // 1111 1000\n\n\tmaskx = 0x3F // 0011 1111\n\n\trune1Max = 1<<7 - 1\n\trune2Max = 1<<11 - 1\n\trune3Max = 1<<16 - 1\n\n\tsurrogateMin = 0xD800\n\tsurrogateMax = 0xDFFF\n\n\tmaxRune   = '\\U0010FFFF' // Maximum valid Unicode code point.\n\truneError = '\\uFFFD'     // the \"error\" Rune or \"Unicode replacement character\"\n\n\thex = \"0123456789abcdef\"\n)\n\n// Map2JSONStr transform map[string]string to json str, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// JSONStr2Map transform json str to map[string]string, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc readString(buf []byte, idx, lastIdx int) (string, int, error) {\n\tvar err error\n\tvar c byte\n\tvar isNull bool\n\tif c, idx, err = nextToken(buf, idx, lastIdx); err != nil {\n\t\treturn \"\", idx, err\n\t}\n\tvar str []byte\n\tif c == '\"' {\n\t\tstart := idx\n\t\tnoESC := true\n\t\tfor idx <= lastIdx {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn \"\", idx, err\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '\"':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn *(*string)(unsafe.Pointer(&str)), idx, nil\n\t\t\tcase '\\\\':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", idx, err\n\t\t\t\t}\n\t\t\t\tif str, idx, err = readEscapedChar(c, buf, idx, str, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", 0, err\n\t\t\t\t}\n\t\t\t\tstart = idx\n\t\t\t\tnoESC = false\n\t\t\t}\n\t\t}\n\t} else if idx, isNull = checkNull(c, buf, idx, lastIdx); isNull {\n\t\treturn \"\", idx, nil\n\t}\n\terr = fmt.Errorf(\"json str is invalid, expects '\\\"' or n, but found %s\", string(c))\n\treturn *(*string)(unsafe.Pointer(&str)), idx, err\n}\n\nfunc readByte(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, fmt.Errorf(\"readByte no more data\")\n\t}\n\tc := buf[idx]\n\tidx++\n\treturn c, idx, nil\n}\n\nfunc nextToken(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, errors.New(\"nextToken no more data\")\n\t}\n\tvar c byte\n\tfor idx <= lastIdx {\n\t\tc = buf[idx]\n\t\tidx++\n\t\tswitch c {\n\t\tcase ' ', '\\n', '\\t', '\\r':\n\t\t\tcontinue\n\t\t}\n\t\treturn c, idx, nil\n\t}\n\treturn c, idx, nil\n}\n\nfunc checkNull(c byte, data []byte, idx, lastIdx int) (int, bool) {\n\tif c == 'n' {\n\t\tch, idx, _ := readByte(data, idx, lastIdx)\n\t\tif ch != 'u' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\treturn idx, true\n\t}\n\treturn idx, false\n}\n\nfunc readU4(buf []byte, idx, lastIdx int) (rune, int, error) {\n\tvar err error\n\tvar ret rune\n\tfor i := 0; i < 4; i++ {\n\t\tvar c byte\n\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\treturn ret, idx, err\n\t\t}\n\t\tif c >= '0' && c <= '9' {\n\t\t\tret = ret*16 + rune(c-'0')\n\t\t} else if c >= 'a' && c <= 'f' {\n\t\t\tret = ret*16 + rune(c-'a'+10)\n\t\t} else if c >= 'A' && c <= 'F' {\n\t\t\tret = ret*16 + rune(c-'A'+10)\n\t\t} else {\n\t\t\treturn ret, idx, fmt.Errorf(\"unicode invalid: expects 0~9 or a~f, but found %v\", string([]byte{c}))\n\t\t}\n\t}\n\treturn ret, idx, nil\n}\n\n// refer to json-iterator/go/iter_str readEscapedChar\nfunc readEscapedChar(c byte, buf []byte, idx int, str []byte, lastIdx int) ([]byte, int, error) {\n\tvar err error\n\tswitch c {\n\tcase 'u':\n\t\tvar r rune\n\t\tif r, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\treturn str, idx, err\n\t\t}\n\t\t// \u662f\u5426\u662f\u6269\u5c55\u5b57\u7b26\n\t\tif utf16.IsSurrogate(r) {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != '\\\\' {\n\t\t\t\tidx--\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn str, idx, nil\n\t\t\t}\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != 'u' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn readEscapedChar(c, buf, idx, str, lastIdx)\n\t\t\t}\n\t\t\tvar r2 rune\n\t\t\tif r2, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tcombined := utf16.DecodeRune(r, r2)\n\t\t\tif combined == '\\uFFFD' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\tstr = appendRune(str, r2)\n\t\t\t} else {\n\t\t\t\tstr = appendRune(str, combined)\n\t\t\t}\n\t\t} else {\n\t\t\tstr = appendRune(str, r)\n\t\t}\n\tcase '\"':\n\t\tstr = append(str, '\"')\n\tcase '\\\\':\n\t\tstr = append(str, '\\\\')\n\tcase '/':\n\t\tstr = append(str, '/')\n\tcase 'b':\n\t\tstr = append(str, '\\b')\n\tcase 'f':\n\t\tstr = append(str, '\\f')\n\tcase 'n':\n\t\tstr = append(str, '\\n')\n\tcase 'r':\n\t\tstr = append(str, '\\r')\n\tcase 't':\n\t\tstr = append(str, '\\t')\n\tdefault:\n\t\treturn str, idx, errors.New(\"invalid escape char after \\\\\")\n\t}\n\treturn str, idx, nil\n}\n\n// refer to json-iterator/go/stream_str writeStringSlowPath\nfunc wrapStrWithQuotation(s string, strBuilder *strings.Builder) {\n\tstrBuilder.WriteByte(DQuotation)\n\tvalLen := len(s)\n\ti := 0\n\tstart := i\n\tfor i < valLen {\n\t\tc := s[i]\n\t\tif c < utf8.RuneSelf && htmlSafeSet[c] {\n\t\t\ti++\n\t\t\tcontinue\n\t\t} else {\n\t\t\tif b := s[i]; b < utf8.RuneSelf {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tswitch b {\n\t\t\t\tcase '\\\\', '\"':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte(b)\n\t\t\t\tcase '\\n':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('n')\n\t\t\t\tcase '\\r':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('r')\n\t\t\t\tcase '\\t':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('t')\n\t\t\t\tdefault:\n\t\t\t\t\t// This encodes bytes < 0x20 except for \\t, \\n and \\r.\n\t\t\t\t\t// If escapeHTML is set, it also escapes <, >, and &\n\t\t\t\t\t// because they can lead to security holes when\n\t\t\t\t\t// user-controlled strings are rendered into JSON\n\t\t\t\t\t// and served to some browsers.\n\t\t\t\t\tstrBuilder.WriteString(`\\u00`)\n\t\t\t\t\tstrBuilder.WriteByte(hex[b>>4])\n\t\t\t\t\tstrBuilder.WriteByte(hex[b&0xF])\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc, size := utf8.DecodeRuneInString(s[i:])\n\t\t\tif c == utf8.RuneError && size == 1 {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\ufffd`)\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// U+2028 is LINE SEPARATOR.\n\t\t\t// U+2029 is PARAGRAPH SEPARATOR.\n\t\t\t// They are both technically valid characters in JSON strings,\n\t\t\t// but don't work in JSONP, which has to be evaluated as JavaScript,\n\t\t\t// and can lead to security holes there. It is valid JSON to\n\t\t\t// escape them, so we do so unconditionally.\n\t\t\t// See http://timelessrepo.com/json-isnt-a-javascript-subset for discussion.\n\t\t\tif c == '\\u2028' || c == '\\u2029' {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\u202`)\n\t\t\t\tstrBuilder.WriteByte(hex[c&0xF])\n\t\t\t\ti += size\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ti += size\n\t\t}\n\t}\n\tif start < valLen {\n\t\tstrBuilder.WriteString(s[start:])\n\t}\n\tstrBuilder.WriteByte(DQuotation)\n}\n\n// refer to json-iterator/go/iter_str appendRune\nfunc appendRune(p []byte, r rune) []byte {\n\t// Negative values are erroneous. Making it unsigned addresses the problem.\n\tswitch i := uint32(r); {\n\tcase i <= rune1Max:\n\t\tp = append(p, byte(r))\n\t\treturn p\n\tcase i <= rune2Max:\n\t\tp = append(p, t2|byte(r>>6))\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tcase i > maxRune, surrogateMin <= i && i <= surrogateMax:\n\t\tr = runeError\n\t\tfallthrough\n\tcase i <= rune3Max:\n\t\tp = append(p, t3|byte(r>>12))\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tdefault:\n\t\tp = append(p, t4|byte(r>>18))\n\t\tp = append(p, tx|byte(r>>12)&maskx)\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\t}\n}\n\nvar htmlSafeSet = [utf8.RuneSelf]bool{\n\t' ':      true,\n\t'!':      true,\n\t'\"':      false,\n\t'#':      true,\n\t'$':      true,\n\t'%':      true,\n\t'&':      false,\n\t'\\'':     true,\n\t'(':      true,\n\t')':      true,\n\t'*':      true,\n\t'+':      true,\n\t',':      true,\n\t'-':      true,\n\t'.':      true,\n\t'/':      true,\n\t'0':      true,\n\t'1':      true,\n\t'2':      true,\n\t'3':      true,\n\t'4':      true,\n\t'5':      true,\n\t'6':      true,\n\t'7':      true,\n\t'8':      true,\n\t'9':      true,\n\t':':      true,\n\t';':      true,\n\t'<':      false,\n\t'=':      true,\n\t'>':      false,\n\t'?':      true,\n\t'@':      true,\n\t'A':      true,\n\t'B':      true,\n\t'C':      true,\n\t'D':      true,\n\t'E':      true,\n\t'F':      true,\n\t'G':      true,\n\t'H':      true,\n\t'I':      true,\n\t'J':      true,\n\t'K':      true,\n\t'L':      true,\n\t'M':      true,\n\t'N':      true,\n\t'O':      true,\n\t'P':      true,\n\t'Q':      true,\n\t'R':      true,\n\t'S':      true,\n\t'T':      true,\n\t'U':      true,\n\t'V':      true,\n\t'W':      true,\n\t'X':      true,\n\t'Y':      true,\n\t'Z':      true,\n\t'[':      true,\n\t'\\\\':     false,\n\t']':      true,\n\t'^':      true,\n\t'_':      true,\n\t'`':      true,\n\t'a':      true,\n\t'b':      true,\n\t'c':      true,\n\t'd':      true,\n\t'e':      true,\n\t'f':      true,\n\t'g':      true,\n\t'h':      true,\n\t'i':      true,\n\t'j':      true,\n\t'k':      true,\n\t'l':      true,\n\t'm':      true,\n\t'n':      true,\n\t'o':      true,\n\t'p':      true,\n\t'q':      true,\n\t'r':      true,\n\t's':      true,\n\t't':      true,\n\t'u':      true,\n\t'v':      true,\n\t'w':      true,\n\t'x':      true,\n\t'y':      true,\n\t'z':      true,\n\t'{':      true,\n\t'|':      true,\n\t'}':      true,\n\t'~':      true,\n\t'\\u007f': true,\n}\n\n```\n# Function Signature: func _Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n# Natural Language Annotations: // Map2JSONStr transform map[string]string to json str, perf is better than use json lib directly\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc _Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n\tif mapInfo == nil {\n\t\treturn \"{}\", nil\n\t}\n\n\tvar sb strings.Builder\n\tsb.WriteByte(LeftBrace)\n\n\tfirst := true\n\tfor key, value := range mapInfo {\n\t\tif !first {\n\t\t\tsb.WriteByte(Comma)\n\t\t}\n\t\tfirst = false\n\n\t\twrapStrWithQuotation(key, &sb)\n\t\tsb.WriteByte(Colon)\n\t\twrapStrWithQuotation(value, &sb)\n\t}\n\n\tsb.WriteByte(RightBrace)\n\treturn sb.String(), nil\n}\n```"]}, "kitex-pkg/utils/json.go-_JSONStr2Map": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/json.go:\n```\n/*\n * Copyright 2024 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * MIT License\n *\n * Copyright (c) 2016 json-iterator\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all\n * copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n * The source code of this file is written based on json-iterator,\n * all modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"unicode/utf16\"\n\t\"unicode/utf8\"\n\t\"unsafe\"\n)\n\n// const of json keyword char\nconst (\n\tEmptyJSON  = \"{}\"\n\tComma      = ','\n\tColon      = ':'\n\tDQuotation = '\"'\n\tLeftBrace  = '{'\n\tRightBrace = '}'\n)\n\nconst (\n\tt1 = 0x00 // 0000 0000\n\ttx = 0x80 // 1000 0000\n\tt2 = 0xC0 // 1100 0000\n\tt3 = 0xE0 // 1110 0000\n\tt4 = 0xF0 // 1111 0000\n\tt5 = 0xF8 // 1111 1000\n\n\tmaskx = 0x3F // 0011 1111\n\n\trune1Max = 1<<7 - 1\n\trune2Max = 1<<11 - 1\n\trune3Max = 1<<16 - 1\n\n\tsurrogateMin = 0xD800\n\tsurrogateMax = 0xDFFF\n\n\tmaxRune   = '\\U0010FFFF' // Maximum valid Unicode code point.\n\truneError = '\\uFFFD'     // the \"error\" Rune or \"Unicode replacement character\"\n\n\thex = \"0123456789abcdef\"\n)\n\n// Map2JSONStr transform map[string]string to json str, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// JSONStr2Map transform json str to map[string]string, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc readString(buf []byte, idx, lastIdx int) (string, int, error) {\n\tvar err error\n\tvar c byte\n\tvar isNull bool\n\tif c, idx, err = nextToken(buf, idx, lastIdx); err != nil {\n\t\treturn \"\", idx, err\n\t}\n\tvar str []byte\n\tif c == '\"' {\n\t\tstart := idx\n\t\tnoESC := true\n\t\tfor idx <= lastIdx {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn \"\", idx, err\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '\"':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn *(*string)(unsafe.Pointer(&str)), idx, nil\n\t\t\tcase '\\\\':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", idx, err\n\t\t\t\t}\n\t\t\t\tif str, idx, err = readEscapedChar(c, buf, idx, str, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", 0, err\n\t\t\t\t}\n\t\t\t\tstart = idx\n\t\t\t\tnoESC = false\n\t\t\t}\n\t\t}\n\t} else if idx, isNull = checkNull(c, buf, idx, lastIdx); isNull {\n\t\treturn \"\", idx, nil\n\t}\n\terr = fmt.Errorf(\"json str is invalid, expects '\\\"' or n, but found %s\", string(c))\n\treturn *(*string)(unsafe.Pointer(&str)), idx, err\n}\n\nfunc readByte(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, fmt.Errorf(\"readByte no more data\")\n\t}\n\tc := buf[idx]\n\tidx++\n\treturn c, idx, nil\n}\n\nfunc nextToken(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, errors.New(\"nextToken no more data\")\n\t}\n\tvar c byte\n\tfor idx <= lastIdx {\n\t\tc = buf[idx]\n\t\tidx++\n\t\tswitch c {\n\t\tcase ' ', '\\n', '\\t', '\\r':\n\t\t\tcontinue\n\t\t}\n\t\treturn c, idx, nil\n\t}\n\treturn c, idx, nil\n}\n\nfunc checkNull(c byte, data []byte, idx, lastIdx int) (int, bool) {\n\tif c == 'n' {\n\t\tch, idx, _ := readByte(data, idx, lastIdx)\n\t\tif ch != 'u' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\treturn idx, true\n\t}\n\treturn idx, false\n}\n\nfunc readU4(buf []byte, idx, lastIdx int) (rune, int, error) {\n\tvar err error\n\tvar ret rune\n\tfor i := 0; i < 4; i++ {\n\t\tvar c byte\n\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\treturn ret, idx, err\n\t\t}\n\t\tif c >= '0' && c <= '9' {\n\t\t\tret = ret*16 + rune(c-'0')\n\t\t} else if c >= 'a' && c <= 'f' {\n\t\t\tret = ret*16 + rune(c-'a'+10)\n\t\t} else if c >= 'A' && c <= 'F' {\n\t\t\tret = ret*16 + rune(c-'A'+10)\n\t\t} else {\n\t\t\treturn ret, idx, fmt.Errorf(\"unicode invalid: expects 0~9 or a~f, but found %v\", string([]byte{c}))\n\t\t}\n\t}\n\treturn ret, idx, nil\n}\n\n// refer to json-iterator/go/iter_str readEscapedChar\nfunc readEscapedChar(c byte, buf []byte, idx int, str []byte, lastIdx int) ([]byte, int, error) {\n\tvar err error\n\tswitch c {\n\tcase 'u':\n\t\tvar r rune\n\t\tif r, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\treturn str, idx, err\n\t\t}\n\t\t// \u662f\u5426\u662f\u6269\u5c55\u5b57\u7b26\n\t\tif utf16.IsSurrogate(r) {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != '\\\\' {\n\t\t\t\tidx--\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn str, idx, nil\n\t\t\t}\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != 'u' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn readEscapedChar(c, buf, idx, str, lastIdx)\n\t\t\t}\n\t\t\tvar r2 rune\n\t\t\tif r2, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tcombined := utf16.DecodeRune(r, r2)\n\t\t\tif combined == '\\uFFFD' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\tstr = appendRune(str, r2)\n\t\t\t} else {\n\t\t\t\tstr = appendRune(str, combined)\n\t\t\t}\n\t\t} else {\n\t\t\tstr = appendRune(str, r)\n\t\t}\n\tcase '\"':\n\t\tstr = append(str, '\"')\n\tcase '\\\\':\n\t\tstr = append(str, '\\\\')\n\tcase '/':\n\t\tstr = append(str, '/')\n\tcase 'b':\n\t\tstr = append(str, '\\b')\n\tcase 'f':\n\t\tstr = append(str, '\\f')\n\tcase 'n':\n\t\tstr = append(str, '\\n')\n\tcase 'r':\n\t\tstr = append(str, '\\r')\n\tcase 't':\n\t\tstr = append(str, '\\t')\n\tdefault:\n\t\treturn str, idx, errors.New(\"invalid escape char after \\\\\")\n\t}\n\treturn str, idx, nil\n}\n\n// refer to json-iterator/go/stream_str writeStringSlowPath\nfunc wrapStrWithQuotation(s string, strBuilder *strings.Builder) {\n\tstrBuilder.WriteByte(DQuotation)\n\tvalLen := len(s)\n\ti := 0\n\tstart := i\n\tfor i < valLen {\n\t\tc := s[i]\n\t\tif c < utf8.RuneSelf && htmlSafeSet[c] {\n\t\t\ti++\n\t\t\tcontinue\n\t\t} else {\n\t\t\tif b := s[i]; b < utf8.RuneSelf {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tswitch b {\n\t\t\t\tcase '\\\\', '\"':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte(b)\n\t\t\t\tcase '\\n':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('n')\n\t\t\t\tcase '\\r':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('r')\n\t\t\t\tcase '\\t':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('t')\n\t\t\t\tdefault:\n\t\t\t\t\t// This encodes bytes < 0x20 except for \\t, \\n and \\r.\n\t\t\t\t\t// If escapeHTML is set, it also escapes <, >, and &\n\t\t\t\t\t// because they can lead to security holes when\n\t\t\t\t\t// user-controlled strings are rendered into JSON\n\t\t\t\t\t// and served to some browsers.\n\t\t\t\t\tstrBuilder.WriteString(`\\u00`)\n\t\t\t\t\tstrBuilder.WriteByte(hex[b>>4])\n\t\t\t\t\tstrBuilder.WriteByte(hex[b&0xF])\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc, size := utf8.DecodeRuneInString(s[i:])\n\t\t\tif c == utf8.RuneError && size == 1 {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\ufffd`)\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// U+2028 is LINE SEPARATOR.\n\t\t\t// U+2029 is PARAGRAPH SEPARATOR.\n\t\t\t// They are both technically valid characters in JSON strings,\n\t\t\t// but don't work in JSONP, which has to be evaluated as JavaScript,\n\t\t\t// and can lead to security holes there. It is valid JSON to\n\t\t\t// escape them, so we do so unconditionally.\n\t\t\t// See http://timelessrepo.com/json-isnt-a-javascript-subset for discussion.\n\t\t\tif c == '\\u2028' || c == '\\u2029' {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\u202`)\n\t\t\t\tstrBuilder.WriteByte(hex[c&0xF])\n\t\t\t\ti += size\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ti += size\n\t\t}\n\t}\n\tif start < valLen {\n\t\tstrBuilder.WriteString(s[start:])\n\t}\n\tstrBuilder.WriteByte(DQuotation)\n}\n\n// refer to json-iterator/go/iter_str appendRune\nfunc appendRune(p []byte, r rune) []byte {\n\t// Negative values are erroneous. Making it unsigned addresses the problem.\n\tswitch i := uint32(r); {\n\tcase i <= rune1Max:\n\t\tp = append(p, byte(r))\n\t\treturn p\n\tcase i <= rune2Max:\n\t\tp = append(p, t2|byte(r>>6))\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tcase i > maxRune, surrogateMin <= i && i <= surrogateMax:\n\t\tr = runeError\n\t\tfallthrough\n\tcase i <= rune3Max:\n\t\tp = append(p, t3|byte(r>>12))\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tdefault:\n\t\tp = append(p, t4|byte(r>>18))\n\t\tp = append(p, tx|byte(r>>12)&maskx)\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\t}\n}\n\nvar htmlSafeSet = [utf8.RuneSelf]bool{\n\t' ':      true,\n\t'!':      true,\n\t'\"':      false,\n\t'#':      true,\n\t'$':      true,\n\t'%':      true,\n\t'&':      false,\n\t'\\'':     true,\n\t'(':      true,\n\t')':      true,\n\t'*':      true,\n\t'+':      true,\n\t',':      true,\n\t'-':      true,\n\t'.':      true,\n\t'/':      true,\n\t'0':      true,\n\t'1':      true,\n\t'2':      true,\n\t'3':      true,\n\t'4':      true,\n\t'5':      true,\n\t'6':      true,\n\t'7':      true,\n\t'8':      true,\n\t'9':      true,\n\t':':      true,\n\t';':      true,\n\t'<':      false,\n\t'=':      true,\n\t'>':      false,\n\t'?':      true,\n\t'@':      true,\n\t'A':      true,\n\t'B':      true,\n\t'C':      true,\n\t'D':      true,\n\t'E':      true,\n\t'F':      true,\n\t'G':      true,\n\t'H':      true,\n\t'I':      true,\n\t'J':      true,\n\t'K':      true,\n\t'L':      true,\n\t'M':      true,\n\t'N':      true,\n\t'O':      true,\n\t'P':      true,\n\t'Q':      true,\n\t'R':      true,\n\t'S':      true,\n\t'T':      true,\n\t'U':      true,\n\t'V':      true,\n\t'W':      true,\n\t'X':      true,\n\t'Y':      true,\n\t'Z':      true,\n\t'[':      true,\n\t'\\\\':     false,\n\t']':      true,\n\t'^':      true,\n\t'_':      true,\n\t'`':      true,\n\t'a':      true,\n\t'b':      true,\n\t'c':      true,\n\t'd':      true,\n\t'e':      true,\n\t'f':      true,\n\t'g':      true,\n\t'h':      true,\n\t'i':      true,\n\t'j':      true,\n\t'k':      true,\n\t'l':      true,\n\t'm':      true,\n\t'n':      true,\n\t'o':      true,\n\t'p':      true,\n\t'q':      true,\n\t'r':      true,\n\t's':      true,\n\t't':      true,\n\t'u':      true,\n\t'v':      true,\n\t'w':      true,\n\t'x':      true,\n\t'y':      true,\n\t'z':      true,\n\t'{':      true,\n\t'|':      true,\n\t'}':      true,\n\t'~':      true,\n\t'\\u007f': true,\n}\n\n```\n# Function Signature: func _JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n# Natural Language Annotations: // JSONStr2Map transform json str to map[string]string, perf is better than use json lib directly\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc _JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n    mapInfo = make(map[string]string)\n    buf := []byte(jsonStr)\n    length := len(buf)\n    idx := 0\n    var token byte\n\n    // Read initial JSON object start\n    if token, idx, err = nextToken(buf, idx, length); err != nil {\n        return nil, err\n    }\n    if token != LeftBrace {\n        return nil, fmt.Errorf(\"json str is invalid, expects '{', but found %s\", string(token))\n    }\n\n    // Loop through the JSON string\n    for idx < length {\n        var key string\n        var value string\n\n        // Get the key\n        if key, idx, err = readString(buf, idx, length-1); err != nil {\n            return nil, err\n        }\n\n        // Look for the colon separator\n        if token, idx, err = nextToken(buf, idx, length-1); err != nil {\n            return nil, err\n        }\n        if token != Colon {\n            return nil, fmt.Errorf(\"json str is invalid, expects ':', but found %s\", string(token))\n        }\n\n        // Get the value\n        if value, idx, err = readString(buf, idx, length-1); err != nil {\n            return nil, err\n        }\n\n        // Add to the map\n        mapInfo[key] = value\n\n        // Check for data separation (Comma) or JSON object termination (RightBrace)\n        if token, idx, err = nextToken(buf, idx, length-1); err != nil {\n            return nil, err\n        }\n        if token == RightBrace {\n            return mapInfo, nil\n        } else if token != Comma {\n            return nil, fmt.Errorf(\"json str is invalid, expects ',' or '}', but found %s\", string(token))\n        }\n    }\n\n    // Ended without closing brace\n    return nil, fmt.Errorf(\"json str is incomplete, missing '}'\")\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCaller": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCaller(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetCaller(ctx context.Context) (string, bool) {\n    if rpcInfo, ok := rpcinfo.FromContext(ctx); ok { // Attempt to extract RPC information from context\n        if caller := rpcInfo.From(); caller != nil { // Check for presence of caller information\n            return caller.ServiceName(), true  // Retrieve and return service name\n        }\n    }\n    return \"\", false  // Return empty string and false if information could not be retrieved\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetMethod": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetMethod(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetMethod(ctx context.Context) (string, bool) {\n\tif rpcInfo := rpcinfo.FromContext(ctx); rpcInfo != nil {\n\t\tif method := rpcInfo.Method(); method != \"\" {\n\t\t\treturn method, true\n\t\t}\n\t}\n\treturn \"\", false\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCallerHandlerMethod": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n\t// Check if the context contains the method information\n\tif methodName, ok := ctx.Value(\"K_METHOD\").(string); ok && methodName != \"\" {\n\t\treturn methodName, true\n\t}\n\t// If method information is not available, return false\n\treturn \"\", false\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetIDLServiceName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetIDLServiceName(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetIDLServiceName(ctx context.Context) (string, bool) {\n\t// Retrieve RPC info from context\n\trpcInfo := rpcinfo.GetRPCInfo(ctx)\n\tif rpcInfo == nil {\n\t\treturn \"\", false\n\t}\n\n\t// Try to get service name from RPC info\n\tserviceName := rpcInfo.Invocation().ServiceName()\n\tif serviceName == \"\" {\n\t\treturn \"\", false\n\t}\n\n\treturn serviceName, true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCallerAddr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n# Natural Language Annotations: // GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn nil, false\n\t}\n\n\tremoteAddr := ri.From().Address()\n\tif remoteAddr == nil {\n\t\treturn nil, false\n\t}\n\n\treturn remoteAddr, true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCallerIP": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCallerIP(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetCallerIP(ctx context.Context) (string, bool) {\n\trpcInfo := rpcinfo.FromCtx(ctx)\n\tif rpcInfo == nil {\n\t\treturn \"\", false\n\t}\n\n\tcallerAddr := rpcInfo.From()\n\tif callerAddr == nil {\n\t\treturn \"\", false\n\t}\n\n\tip, _, err := net.SplitHostPort(callerAddr.String())\n\tif err != nil {\n\t\treturn \"\", false\n\t}\n\n\treturn ip, true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetTransportProtocol": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetTransportProtocol(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetTransportProtocol(ctx context.Context) (string, bool) {\n\tif rpcInfo, ok := rpcinfo.FromContext(ctx); ok {\n\t\tif transportProtocol := rpcInfo.Config().TransportProtocol(); transportProtocol != \"\" {\n\t\t\treturn transportProtocol, true\n\t\t}\n\t}\n\treturn \"\", false\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetRPCInfo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n# Natural Language Annotations: // GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n\tif ctx == nil {\n\t\treturn nil, false\n\t}\n\n\t// Try to extract RPCInfo from the context\n\tif rpcInfo, ok := rpcinfo.FromContext(ctx); ok {\n\t\treturn rpcInfo, true\n\t}\n\n\treturn nil, false\n}\n```"]}, "kitex-pkg/remote/trans_errors.go-NewTransError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans_errors.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage remote\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// corresponding with thrift TApplicationException, cannot change it\nconst (\n\tUnknownApplicationException = 0\n\tUnknownMethod               = 1\n\tInvalidMessageTypeException = 2\n\tWrongMethodName             = 3\n\tBadSequenceID               = 4\n\tMissingResult               = 5\n\tInternalError               = 6\n\tProtocolError               = 7\n\tInvalidTransform            = 8\n\tInvalidProtocol             = 9\n\tUnsupportedClientType       = 10\n\t// kitex's own type id from number 20\n\tUnknownService = 20\n\tNoServiceName  = 21\n)\n\nvar defaultTransErrorMessage = map[int32]string{\n\tUnknownApplicationException: \"unknown application exception\",\n\tUnknownMethod:               \"unknown method\",\n\tInvalidMessageTypeException: \"invalid message type\",\n\tWrongMethodName:             \"wrong method name\",\n\tBadSequenceID:               \"bad sequence ID\",\n\tMissingResult:               \"missing result\",\n\tInternalError:               \"unknown internal error\",\n\tProtocolError:               \"unknown protocol error\",\n\tInvalidTransform:            \"Invalid transform\",\n\tInvalidProtocol:             \"Invalid protocol\",\n\tUnsupportedClientType:       \"Unsupported client type\",\n\tUnknownService:              \"unknown service\",\n}\n\n// TransError is the error that can be transmitted, it corresponds to TApplicationException in Thrift\ntype TransError struct {\n\tmessage string\n\ttypeID  int32\n\trawErr  error\n}\n\n// Error implements the error interface.\nfunc (e TransError) Error() string {\n\tif e.message != \"\" {\n\t\treturn e.message\n\t}\n\treturn defaultTransErrorMessage[e.typeID]\n}\n\n// TypeID return err type id\nfunc (e TransError) TypeID() int32 {\n\treturn e.typeID\n}\n\n// Unwrap the transError to expose raw error\nfunc (e TransError) Unwrap() error {\n\treturn e.rawErr\n}\n\n// Is to check if inner error that transError wrap is target error\nfunc (e TransError) Is(target error) bool {\n\treturn e == target || errors.Is(e.rawErr, target)\n}\n\n// AppendMessage append extra msg for TransError\nfunc (e TransError) AppendMessage(extraMsg string) *TransError {\n\tif extraMsg == \"\" {\n\t\treturn &e\n\t}\n\tmsg := fmt.Sprintf(\"%s %s\", e.message, extraMsg)\n\t// should not modify origin error\n\treturn &TransError{message: msg, typeID: e.typeID, rawErr: e.rawErr}\n}\n\n// NewTransErrorWithMsg to build TransError with typeID and errMsg\nfunc NewTransErrorWithMsg(typeID int32, message string) *TransError {\n\treturn &TransError{message: message, typeID: typeID}\n}\n\n// NewTransError to build TransError with typeID and rawErr.\n// rawErr can be used by errors.Is(target) to check err type, like read timeout.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TypeId is used to assert Error with has 'TypeID() int32'\ntype TypeID interface {\n\tTypeID() int32\n}\n\n// TypeId is used to assert Error with has 'TypeId() int32' like TApplicationException\ntype TypeId interface {\n\tTypeId() int32\n}\n\n```\n# Function Signature: func NewTransError(typeID int32, err error) *TransError {\n# Natural Language Annotations: // NewTransError to build TransError with typeID and rawErr.\n// rawErr can be used by errors.Is(target) to check err type, like read timeout.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc NewTransError(typeID int32, err error) *TransError {\n\treturn &TransError{\n\t\tmessage: defaultTransErrorMessage[typeID],\n\t\ttypeID:  typeID,\n\t\trawErr:  err,\n\t}\n}\n```"]}, "kitex-pkg/remote/trans/gonet/bytebuffer.go-NewBufferReader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/gonet/bytebuffer.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage gonet\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n)\n\nvar rwPool sync.Pool\n\nfunc init() {\n\trwPool.New = newBufferReadWriter\n}\n\nvar _ remote.ByteBuffer = &bufferReadWriter{}\n\ntype bufferReadWriter struct {\n\treader netpoll.Reader\n\twriter netpoll.Writer\n\n\tioReader io.Reader\n\tioWriter io.Writer\n\n\treadSize int\n\tstatus   int\n}\n\nfunc newBufferReadWriter() interface{} {\n\treturn &bufferReadWriter{}\n}\n\n// NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewBufferWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyWriter.\nfunc NewBufferWriter(iw io.Writer) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(iw)\n\trw.ioWriter = iw\n\trw.status = remote.BitWritable\n\treturn rw\n}\n\n// NewBufferReadWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReadWriter.\nfunc NewBufferReadWriter(irw io.ReadWriter) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(irw)\n\trw.reader = netpoll.NewReader(irw)\n\trw.ioWriter = irw\n\trw.ioReader = irw\n\trw.status = remote.BitWritable | remote.BitReadable\n\treturn rw\n}\n\nfunc (rw *bufferReadWriter) readable() bool {\n\treturn rw.status&remote.BitReadable != 0\n}\n\nfunc (rw *bufferReadWriter) writable() bool {\n\treturn rw.status&remote.BitWritable != 0\n}\n\nfunc (rw *bufferReadWriter) Next(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Next\")\n\t}\n\tif p, err = rw.reader.Next(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Peek(n int) (buf []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Peek\")\n\t}\n\treturn rw.reader.Peek(n)\n}\n\nfunc (rw *bufferReadWriter) Skip(n int) (err error) {\n\tif !rw.readable() {\n\t\treturn errors.New(\"unreadable buffer, cannot support Skip\")\n\t}\n\treturn rw.reader.Skip(n)\n}\n\nfunc (rw *bufferReadWriter) ReadableLen() (n int) {\n\tif !rw.readable() {\n\t\treturn -1\n\t}\n\treturn rw.reader.Len()\n}\n\nfunc (rw *bufferReadWriter) ReadString(n int) (s string, err error) {\n\tif !rw.readable() {\n\t\treturn \"\", errors.New(\"unreadable buffer, cannot support ReadString\")\n\t}\n\tif s, err = rw.reader.ReadString(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) ReadBinary(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn p, errors.New(\"unreadable buffer, cannot support ReadBinary\")\n\t}\n\tif p, err = rw.reader.ReadBinary(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Read(p []byte) (n int, err error) {\n\tif !rw.readable() {\n\t\treturn -1, errors.New(\"unreadable buffer, cannot support Read\")\n\t}\n\tif rw.ioReader != nil {\n\t\treturn rw.ioReader.Read(p)\n\t}\n\treturn -1, errors.New(\"ioReader is nil\")\n}\n\nfunc (rw *bufferReadWriter) ReadLen() (n int) {\n\treturn rw.readSize\n}\n\nfunc (rw *bufferReadWriter) Malloc(n int) (buf []byte, err error) {\n\tif !rw.writable() {\n\t\treturn nil, errors.New(\"unwritable buffer, cannot support Malloc\")\n\t}\n\treturn rw.writer.Malloc(n)\n}\n\nfunc (rw *bufferReadWriter) MallocLen() (length int) {\n\tif !rw.writable() {\n\t\treturn -1\n\t}\n\treturn rw.writer.MallocLen()\n}\n\nfunc (rw *bufferReadWriter) WriteString(s string) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteString\")\n\t}\n\treturn rw.writer.WriteString(s)\n}\n\nfunc (rw *bufferReadWriter) WriteBinary(b []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn rw.writer.WriteBinary(b)\n}\n\nfunc (rw *bufferReadWriter) Flush() (err error) {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support Flush\")\n\t}\n\treturn rw.writer.Flush()\n}\n\nfunc (rw *bufferReadWriter) Write(p []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support Write\")\n\t}\n\tif rw.ioWriter != nil {\n\t\treturn rw.ioWriter.Write(p)\n\t}\n\treturn -1, errors.New(\"ioWriter is nil\")\n}\n\nfunc (rw *bufferReadWriter) Release(e error) (err error) {\n\tif rw.reader != nil {\n\t\terr = rw.reader.Release()\n\t}\n\trw.zero()\n\trwPool.Put(rw)\n\treturn\n}\n\n// WriteDirect is a way to write []byte without copying, and splits the original buffer.\n\n\n\n\n\n\n\nfunc (rw *bufferReadWriter) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tsubBuf, ok := buf.(*bufferReadWriter)\n\tif !ok {\n\t\treturn errors.New(\"AppendBuffer failed, Buffer is not bufferReadWriter\")\n\t}\n\tif err = rw.writer.Append(subBuf.writer); err != nil {\n\t\treturn\n\t}\n\treturn buf.Release(nil)\n}\n\n// NewBuffer returns a new writable remote.ByteBuffer.\nfunc (rw *bufferReadWriter) NewBuffer() remote.ByteBuffer {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) Bytes() (buf []byte, err error) {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) zero() {\n\trw.reader = nil\n\trw.writer = nil\n\trw.ioReader = nil\n\trw.ioWriter = nil\n\trw.readSize = 0\n\trw.status = 0\n}\n\n```\n# Function Signature: func NewBufferReader(ir io.Reader) remote.ByteBuffer {\n# Natural Language Annotations: // NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\nfunc NewBufferReader(ir io.Reader) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.reader = netpoll.NewReader(ir)\n\trw.ioReader = ir\n\trw.status = remote.BitReadable\n\treturn rw\n}\n```"]}, "kitex-pkg/remote/trans/gonet/bytebuffer.go-WriteDirect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/gonet/bytebuffer.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage gonet\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n)\n\nvar rwPool sync.Pool\n\nfunc init() {\n\trwPool.New = newBufferReadWriter\n}\n\nvar _ remote.ByteBuffer = &bufferReadWriter{}\n\ntype bufferReadWriter struct {\n\treader netpoll.Reader\n\twriter netpoll.Writer\n\n\tioReader io.Reader\n\tioWriter io.Writer\n\n\treadSize int\n\tstatus   int\n}\n\nfunc newBufferReadWriter() interface{} {\n\treturn &bufferReadWriter{}\n}\n\n// NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewBufferWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyWriter.\nfunc NewBufferWriter(iw io.Writer) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(iw)\n\trw.ioWriter = iw\n\trw.status = remote.BitWritable\n\treturn rw\n}\n\n// NewBufferReadWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReadWriter.\nfunc NewBufferReadWriter(irw io.ReadWriter) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(irw)\n\trw.reader = netpoll.NewReader(irw)\n\trw.ioWriter = irw\n\trw.ioReader = irw\n\trw.status = remote.BitWritable | remote.BitReadable\n\treturn rw\n}\n\nfunc (rw *bufferReadWriter) readable() bool {\n\treturn rw.status&remote.BitReadable != 0\n}\n\nfunc (rw *bufferReadWriter) writable() bool {\n\treturn rw.status&remote.BitWritable != 0\n}\n\nfunc (rw *bufferReadWriter) Next(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Next\")\n\t}\n\tif p, err = rw.reader.Next(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Peek(n int) (buf []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Peek\")\n\t}\n\treturn rw.reader.Peek(n)\n}\n\nfunc (rw *bufferReadWriter) Skip(n int) (err error) {\n\tif !rw.readable() {\n\t\treturn errors.New(\"unreadable buffer, cannot support Skip\")\n\t}\n\treturn rw.reader.Skip(n)\n}\n\nfunc (rw *bufferReadWriter) ReadableLen() (n int) {\n\tif !rw.readable() {\n\t\treturn -1\n\t}\n\treturn rw.reader.Len()\n}\n\nfunc (rw *bufferReadWriter) ReadString(n int) (s string, err error) {\n\tif !rw.readable() {\n\t\treturn \"\", errors.New(\"unreadable buffer, cannot support ReadString\")\n\t}\n\tif s, err = rw.reader.ReadString(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) ReadBinary(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn p, errors.New(\"unreadable buffer, cannot support ReadBinary\")\n\t}\n\tif p, err = rw.reader.ReadBinary(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Read(p []byte) (n int, err error) {\n\tif !rw.readable() {\n\t\treturn -1, errors.New(\"unreadable buffer, cannot support Read\")\n\t}\n\tif rw.ioReader != nil {\n\t\treturn rw.ioReader.Read(p)\n\t}\n\treturn -1, errors.New(\"ioReader is nil\")\n}\n\nfunc (rw *bufferReadWriter) ReadLen() (n int) {\n\treturn rw.readSize\n}\n\nfunc (rw *bufferReadWriter) Malloc(n int) (buf []byte, err error) {\n\tif !rw.writable() {\n\t\treturn nil, errors.New(\"unwritable buffer, cannot support Malloc\")\n\t}\n\treturn rw.writer.Malloc(n)\n}\n\nfunc (rw *bufferReadWriter) MallocLen() (length int) {\n\tif !rw.writable() {\n\t\treturn -1\n\t}\n\treturn rw.writer.MallocLen()\n}\n\nfunc (rw *bufferReadWriter) WriteString(s string) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteString\")\n\t}\n\treturn rw.writer.WriteString(s)\n}\n\nfunc (rw *bufferReadWriter) WriteBinary(b []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn rw.writer.WriteBinary(b)\n}\n\nfunc (rw *bufferReadWriter) Flush() (err error) {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support Flush\")\n\t}\n\treturn rw.writer.Flush()\n}\n\nfunc (rw *bufferReadWriter) Write(p []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support Write\")\n\t}\n\tif rw.ioWriter != nil {\n\t\treturn rw.ioWriter.Write(p)\n\t}\n\treturn -1, errors.New(\"ioWriter is nil\")\n}\n\nfunc (rw *bufferReadWriter) Release(e error) (err error) {\n\tif rw.reader != nil {\n\t\terr = rw.reader.Release()\n\t}\n\trw.zero()\n\trwPool.Put(rw)\n\treturn\n}\n\n// WriteDirect is a way to write []byte without copying, and splits the original buffer.\n\n\n\n\n\n\n\nfunc (rw *bufferReadWriter) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tsubBuf, ok := buf.(*bufferReadWriter)\n\tif !ok {\n\t\treturn errors.New(\"AppendBuffer failed, Buffer is not bufferReadWriter\")\n\t}\n\tif err = rw.writer.Append(subBuf.writer); err != nil {\n\t\treturn\n\t}\n\treturn buf.Release(nil)\n}\n\n// NewBuffer returns a new writable remote.ByteBuffer.\nfunc (rw *bufferReadWriter) NewBuffer() remote.ByteBuffer {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) Bytes() (buf []byte, err error) {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) zero() {\n\trw.reader = nil\n\trw.writer = nil\n\trw.ioReader = nil\n\trw.ioWriter = nil\n\trw.readSize = 0\n\trw.status = 0\n}\n\n```\n# Function Signature: func (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n# Natural Language Annotations: // WriteDirect is a way to write []byte without copying, and splits the original buffer.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support WriteDirect\")\n\t}\n\n\t// Check if the remaining capacity is sufficient\n\tif rw.writer.MallocLen() < len(p) + remainCap {\n\t\treturn errors.New(\"insufficient remaining capacity for WriteDirect\")\n\t}\n\n\t// Perform the direct write operation\n\tif err := rw.writer.Write(p); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/server_handler.go-invokeStreamUnaryHandler": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/server_handler.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage nphttp2\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/gofunc\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\tgrpcTransport \"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\ntype svrTransHandlerFactory struct{}\n\n// NewSvrTransHandlerFactory ...\nfunc NewSvrTransHandlerFactory() remote.ServerTransHandlerFactory {\n\treturn &svrTransHandlerFactory{}\n}\n\nfunc (f *svrTransHandlerFactory) NewTransHandler(opt *remote.ServerOption) (remote.ServerTransHandler, error) {\n\treturn newSvrTransHandler(opt)\n}\n\nfunc newSvrTransHandler(opt *remote.ServerOption) (*svrTransHandler, error) {\n\treturn &svrTransHandler{\n\t\topt:          opt,\n\t\tsvcSearchMap: opt.SvcSearchMap,\n\t\tcodec:        grpc.NewGRPCCodec(grpc.WithThriftCodec(opt.PayloadCodec)),\n\t}, nil\n}\n\nvar _ remote.ServerTransHandler = &svrTransHandler{}\n\ntype svrTransHandler struct {\n\topt          *remote.ServerOption\n\tsvcSearchMap map[string]*serviceinfo.ServiceInfo\n\tinkHdlFunc   endpoint.Endpoint\n\tcodec        remote.Codec\n}\n\nvar prefaceReadAtMost = func() int {\n\t// min(len(ClientPreface), len(flagBuf))\n\t// len(flagBuf) = 2 * codec.Size32\n\tif 2*codec.Size32 < grpcTransport.ClientPrefaceLen {\n\t\treturn 2 * codec.Size32\n\t}\n\treturn grpcTransport.ClientPrefaceLen\n}()\n\nfunc (t *svrTransHandler) ProtocolMatch(ctx context.Context, conn net.Conn) (err error) {\n\t// Check the validity of client preface.\n\tnpReader := conn.(interface{ Reader() netpoll.Reader }).Reader()\n\t// read at most avoid block\n\tpreface, err := npReader.Peek(prefaceReadAtMost)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif bytes.Equal(preface[:prefaceReadAtMost], grpcTransport.ClientPreface[:prefaceReadAtMost]) {\n\t\treturn nil\n\t}\n\treturn errors.New(\"error protocol not match\")\n}\n\nfunc (t *svrTransHandler) Write(ctx context.Context, conn net.Conn, msg remote.Message) (nctx context.Context, err error) {\n\tbuf := newBuffer(conn.(*serverConn))\n\tdefer buf.Release(err)\n\n\tif err = t.codec.Encode(ctx, msg, buf); err != nil {\n\t\treturn ctx, err\n\t}\n\treturn ctx, buf.Flush()\n}\n\nfunc (t *svrTransHandler) Read(ctx context.Context, conn net.Conn, msg remote.Message) (nctx context.Context, err error) {\n\tbuf := newBuffer(conn.(*serverConn))\n\tdefer buf.Release(err)\n\n\terr = t.codec.Decode(ctx, msg, buf)\n\treturn ctx, err\n}\n\n// \u53ea return write err\nfunc (t *svrTransHandler) OnRead(ctx context.Context, conn net.Conn) error {\n\tsvrTrans := ctx.Value(ctxKeySvrTransport).(*SvrTrans)\n\ttr := svrTrans.tr\n\n\ttr.HandleStreams(func(s *grpcTransport.Stream) {\n\t\tgofunc.GoFunc(ctx, func() {\n\t\t\tri := svrTrans.pool.Get().(rpcinfo.RPCInfo)\n\t\t\trCtx := rpcinfo.NewCtxWithRPCInfo(s.Context(), ri)\n\t\t\tdefer func() {\n\t\t\t\t// reset rpcinfo for performance (PR #584)\n\t\t\t\tif rpcinfo.PoolEnabled() {\n\t\t\t\t\tri = t.opt.InitOrResetRPCInfoFunc(ri, conn.RemoteAddr())\n\t\t\t\t\tsvrTrans.pool.Put(ri)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tink := ri.Invocation().(rpcinfo.InvocationSetter)\n\t\t\tsm := s.Method()\n\t\t\tif sm != \"\" && sm[0] == '/' {\n\t\t\t\tsm = sm[1:]\n\t\t\t}\n\t\t\tpos := strings.LastIndex(sm, \"/\")\n\t\t\tif pos == -1 {\n\t\t\t\terrDesc := fmt.Sprintf(\"malformed method name, method=%q\", s.Method())\n\t\t\t\ttr.WriteStatus(s, status.New(codes.Internal, errDesc))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tmethodName := sm[pos+1:]\n\t\t\tink.SetMethodName(methodName)\n\n\t\t\tif mutableTo := rpcinfo.AsMutableEndpointInfo(ri.To()); mutableTo != nil {\n\t\t\t\tif err := mutableTo.SetMethod(methodName); err != nil {\n\t\t\t\t\terrDesc := fmt.Sprintf(\"setMethod failed in streaming, method=%s, error=%s\", methodName, err.Error())\n\t\t\t\t\t_ = tr.WriteStatus(s, status.New(codes.Internal, errDesc))\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar serviceName string\n\t\t\tidx := strings.LastIndex(sm[:pos], \".\")\n\t\t\tif idx == -1 {\n\t\t\t\tink.SetPackageName(\"\")\n\t\t\t\tserviceName = sm[0:pos]\n\t\t\t} else {\n\t\t\t\tink.SetPackageName(sm[:idx])\n\t\t\t\tserviceName = sm[idx+1 : pos]\n\t\t\t}\n\t\t\tink.SetServiceName(serviceName)\n\n\t\t\t// set grpc transport flag before execute metahandler\n\t\t\trpcinfo.AsMutableRPCConfig(ri.Config()).SetTransportProtocol(transport.GRPC)\n\t\t\tvar err error\n\t\t\tfor _, shdlr := range t.opt.StreamingMetaHandlers {\n\t\t\t\trCtx, err = shdlr.OnReadStream(rCtx)\n\t\t\t\tif err != nil {\n\t\t\t\t\ttr.WriteStatus(s, convertStatus(err))\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\trCtx = t.startTracer(rCtx, ri)\n\t\t\tdefer func() {\n\t\t\t\tpanicErr := recover()\n\t\t\t\tif panicErr != nil {\n\t\t\t\t\tif conn != nil {\n\t\t\t\t\t\tklog.CtxErrorf(rCtx, \"KITEX: gRPC panic happened, close conn, remoteAddress=%s, error=%s\\nstack=%s\", conn.RemoteAddr(), panicErr, string(debug.Stack()))\n\t\t\t\t\t} else {\n\t\t\t\t\t\tklog.CtxErrorf(rCtx, \"KITEX: gRPC panic happened, error=%v\\nstack=%s\", panicErr, string(debug.Stack()))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tt.finishTracer(rCtx, ri, err, panicErr)\n\t\t\t}()\n\n\t\t\t// set recv grpc compressor at server to decode the pack from client\n\t\t\tremote.SetRecvCompressor(ri, s.RecvCompress())\n\t\t\t// set send grpc compressor at server to encode reply pack\n\t\t\tremote.SetSendCompressor(ri, s.SendCompress())\n\n\t\t\tsvcInfo := t.svcSearchMap[remote.BuildMultiServiceKey(serviceName, methodName)]\n\t\t\tvar methodInfo serviceinfo.MethodInfo\n\t\t\tif svcInfo != nil {\n\t\t\t\tmethodInfo = svcInfo.MethodInfo(methodName)\n\t\t\t}\n\n\t\t\trawStream := NewStream(rCtx, svcInfo, newServerConn(tr, s), t)\n\t\t\tst := newStreamWithMiddleware(rawStream, t.opt.RecvEndpoint, t.opt.SendEndpoint)\n\n\t\t\t// bind stream into ctx, in order to let user set header and trailer by provided api in meta_api.go\n\t\t\trCtx = streaming.NewCtxWithStream(rCtx, st)\n\n\t\t\tif methodInfo == nil {\n\t\t\t\tunknownServiceHandlerFunc := t.opt.GRPCUnknownServiceHandler\n\t\t\t\tif unknownServiceHandlerFunc != nil {\n\t\t\t\t\trpcinfo.Record(rCtx, ri, stats.ServerHandleStart, nil)\n\t\t\t\t\terr = unknownServiceHandlerFunc(rCtx, methodName, st)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\terr = kerrors.ErrBiz.WithCause(err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tif svcInfo == nil {\n\t\t\t\t\t\terr = remote.NewTransErrorWithMsg(remote.UnknownService, fmt.Sprintf(\"unknown service %s\", serviceName))\n\t\t\t\t\t} else {\n\t\t\t\t\t\terr = remote.NewTransErrorWithMsg(remote.UnknownMethod, fmt.Sprintf(\"unknown method %s\", methodName))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif streaming.UnaryCompatibleMiddleware(methodInfo.StreamingMode(), t.opt.CompatibleMiddlewareForUnary) {\n\t\t\t\t\t// making streaming unary APIs capable of using the same server middleware as non-streaming APIs\n\t\t\t\t\t// note: rawStream skips recv/send middleware for unary API requests to avoid confusion\n\t\t\t\t\terr = invokeStreamUnaryHandler(rCtx, rawStream, methodInfo, t.inkHdlFunc, ri)\n\t\t\t\t} else {\n\t\t\t\t\terr = t.inkHdlFunc(rCtx, &streaming.Args{Stream: st}, nil)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\ttr.WriteStatus(s, convertStatus(err))\n\t\t\t\tt.OnError(rCtx, err, conn)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif bizStatusErr := ri.Invocation().BizStatusErr(); bizStatusErr != nil {\n\t\t\t\tvar st *status.Status\n\t\t\t\tif sterr, ok := bizStatusErr.(status.Iface); ok {\n\t\t\t\t\tst = sterr.GRPCStatus()\n\t\t\t\t} else {\n\t\t\t\t\tst = status.New(codes.Internal, bizStatusErr.BizMessage())\n\t\t\t\t}\n\t\t\t\ts.SetBizStatusErr(bizStatusErr)\n\t\t\t\ttr.WriteStatus(s, st)\n\t\t\t\treturn\n\t\t\t}\n\t\t\ttr.WriteStatus(s, status.New(codes.OK, \"\"))\n\t\t})\n\t}, func(ctx context.Context, method string) context.Context {\n\t\treturn ctx\n\t})\n\treturn nil\n}\n\n// invokeStreamUnaryHandler allows unary APIs over HTTP2 to use the same server middleware as non-streaming APIs.\n// For thrift unary APIs over HTTP2, it's enabled by default.\n// For grpc(protobuf) unary APIs, it's disabled by default to keep backward compatibility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// msg \u662f\u89e3\u7801\u540e\u7684\u5b9e\u4f8b\uff0c\u5982 Arg \u6216 Result, \u89e6\u53d1\u4e0a\u5c42\u5904\u7406\uff0c\u7528\u4e8e\u5f02\u6b65 \u548c \u670d\u52a1\u7aef\u5904\u7406\nfunc (t *svrTransHandler) OnMessage(ctx context.Context, args, result remote.Message) (context.Context, error) {\n\tpanic(\"unimplemented\")\n}\n\ntype svrTransKey int\n\nconst ctxKeySvrTransport svrTransKey = 1\n\ntype SvrTrans struct {\n\ttr   grpcTransport.ServerTransport\n\tpool *sync.Pool // value is rpcInfo\n}\n\n// \u65b0\u8fde\u63a5\u5efa\u7acb\u65f6\u89e6\u53d1\uff0c\u4e3b\u8981\u7528\u4e8e\u670d\u52a1\u7aef\uff0c\u5bf9\u5e94 netpoll onPrepare\nfunc (t *svrTransHandler) OnActive(ctx context.Context, conn net.Conn) (context.Context, error) {\n\t// set readTimeout to infinity to avoid streaming break\n\t// use keepalive to check the health of connection\n\tif npConn, ok := conn.(netpoll.Connection); ok {\n\t\tnpConn.SetReadTimeout(grpcTransport.Infinity)\n\t} else {\n\t\tconn.SetReadDeadline(time.Now().Add(grpcTransport.Infinity))\n\t}\n\n\ttr, err := grpcTransport.NewServerTransport(ctx, conn, t.opt.GRPCCfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpool := &sync.Pool{\n\t\tNew: func() interface{} {\n\t\t\t// init rpcinfo\n\t\t\tri := t.opt.InitOrResetRPCInfoFunc(nil, conn.RemoteAddr())\n\t\t\treturn ri\n\t\t},\n\t}\n\tctx = context.WithValue(ctx, ctxKeySvrTransport, &SvrTrans{tr: tr, pool: pool})\n\treturn ctx, nil\n}\n\n// \u8fde\u63a5\u5173\u95ed\u65f6\u56de\u8c03\nfunc (t *svrTransHandler) OnInactive(ctx context.Context, conn net.Conn) {\n\ttr := ctx.Value(ctxKeySvrTransport).(*SvrTrans).tr\n\ttr.Close()\n}\n\n// \u4f20\u8f93\u5c42 error \u56de\u8c03\nfunc (t *svrTransHandler) OnError(ctx context.Context, err error, conn net.Conn) {\n\tvar de *kerrors.DetailedError\n\tif ok := errors.As(err, &de); ok && de.Stack() != \"\" {\n\t\tklog.CtxErrorf(ctx, \"KITEX: processing gRPC request error, remoteAddr=%s, error=%s\\nstack=%s\", conn.RemoteAddr(), err.Error(), de.Stack())\n\t} else {\n\t\tklog.CtxErrorf(ctx, \"KITEX: processing gRPC request error, remoteAddr=%s, error=%s\", conn.RemoteAddr(), err.Error())\n\t}\n}\n\nfunc (t *svrTransHandler) SetInvokeHandleFunc(inkHdlFunc endpoint.Endpoint) {\n\tt.inkHdlFunc = inkHdlFunc\n}\n\nfunc (t *svrTransHandler) SetPipeline(p *remote.TransPipeline) {\n}\n\nfunc (t *svrTransHandler) startTracer(ctx context.Context, ri rpcinfo.RPCInfo) context.Context {\n\tc := t.opt.TracerCtl.DoStart(ctx, ri)\n\treturn c\n}\n\nfunc (t *svrTransHandler) finishTracer(ctx context.Context, ri rpcinfo.RPCInfo, err error, panicErr interface{}) {\n\trpcStats := rpcinfo.AsMutableRPCStats(ri.Stats())\n\tif rpcStats == nil {\n\t\treturn\n\t}\n\tif panicErr != nil {\n\t\trpcStats.SetPanicked(panicErr)\n\t}\n\tt.opt.TracerCtl.DoFinish(ctx, ri, err)\n\trpcStats.Reset()\n}\n\n```\n# Function Signature: func invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo,\n# Natural Language Annotations: // invokeStreamUnaryHandler allows unary APIs over HTTP2 to use the same server middleware as non-streaming APIs.\n// For thrift unary APIs over HTTP2, it's enabled by default.\n// For grpc(protobuf) unary APIs, it's disabled by default to keep backward compatibility.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo, inkHdlFunc endpoint.Endpoint, ri rpcinfo.RPCInfo) error {\n\t// Ensure that the method information reflects a unary method.\n\tif mi.StreamingMode() != serviceinfo.Unary {\n\t\treturn errors.New(\"invokeStreamUnaryHandler: provided MethodInfo is not unary\")\n\t}\n\n\t// Prepare the service-level context by setting the RPCInfo.\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\t// Record the start of the server handling, if applicable.\n\trpcinfo.Record(ctx, ri, stats.ServerHandleStart, nil)\n\tdefer rpcinfo.Record(ctx, ri, stats.ServerHandleFinish, nil)\n\n\t// Create a new Args instance for the unary method invocation.\n\targs := &streaming.Args{Stream: st}\n\n\t// Use the inkHdlFunc to execute the unary handler logic.\n\thandlerErr := inkHdlFunc(ctx, args, nil)\n\n\t// Handle if there's an error post-handler invocation.\n\tif handlerErr != nil {\n\t\treturn kerrors.ErrBiz.WithCause(handlerErr)\n\t}\n\n\t// Ensure the response status is OK for completed operations.\n\ttr := ctx.Value(ctxKeySvrTransport).(*SvrTrans).tr\n\tbizStatusErr := ri.Invocation().BizStatusErr()\n\tif bizStatusErr != nil {\n\t\tst, ok := bizStatusErr.(status.Iface)\n\t\tif ok {\n\t\t\ttr.WriteStatus(st.GRPCStatus())\n\t\t} else {\n\t\t\ttr.WriteStatus(status.New(codes.Internal, bizStatusErr.BizMessage()))\n\t\t}\n\t} else {\n\t\ttr.WriteStatus(status.New(codes.OK, \"\"))\n\t}\n\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/metadata/metadata.go-FromOutgoingContext": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/metadata/metadata.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package metadata define the structure of the metadata supported by gRPC library.\n// Please refer to https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md\n// for more information about custom-metadata.\npackage metadata\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n)\n\n// DecodeKeyValue returns k, v, nil.\n//\n// Deprecated: use k and v directly instead.\nfunc DecodeKeyValue(k, v string) (string, string, error) {\n\treturn k, v, nil\n}\n\n// MD is a mapping from metadata keys to values. Users should use the following\n// two convenience functions New and Pairs to generate MD.\ntype MD map[string][]string\n\n// New creates an MD from a given key-value map.\n//\n// Only the following ASCII characters are allowed in keys:\n//   - digits: 0-9\n//   - uppercase letters: A-Z (normalized to lower)\n//   - lowercase letters: a-z\n//   - special characters: -_.\n//\n// Uppercase letters are automatically converted to lowercase.\n//\n// Keys beginning with \"grpc-\" are reserved for grpc-internal use only and may\n// result in errors if set in metadata.\nfunc New(m map[string]string) MD {\n\tmd := MD{}\n\tfor k, val := range m {\n\t\tkey := strings.ToLower(k)\n\t\tmd[key] = append(md[key], val)\n\t}\n\treturn md\n}\n\n// Pairs returns an MD formed by the mapping of key, value ...\n// Pairs panics if len(kv) is odd.\n//\n// Only the following ASCII characters are allowed in keys:\n//   - digits: 0-9\n//   - uppercase letters: A-Z (normalized to lower)\n//   - lowercase letters: a-z\n//   - special characters: -_.\n//\n// Uppercase letters are automatically converted to lowercase.\n//\n// Keys beginning with \"grpc-\" are reserved for grpc-internal use only and may\n// result in errors if set in metadata.\nfunc Pairs(kv ...string) MD {\n\tif len(kv)%2 == 1 {\n\t\tpanic(fmt.Sprintf(\"metadata: Pairs got the odd number of input pairs for metadata: %d\", len(kv)))\n\t}\n\tmd := MD{}\n\tvar key string\n\tfor i, s := range kv {\n\t\tif i%2 == 0 {\n\t\t\tkey = strings.ToLower(s)\n\t\t\tcontinue\n\t\t}\n\t\tmd[key] = append(md[key], s)\n\t}\n\treturn md\n}\n\n// Len returns the number of items in md.\nfunc (md MD) Len() int {\n\treturn len(md)\n}\n\n// Copy returns a copy of md.\nfunc (md MD) Copy() MD {\n\tresult := make(MD, len(md))\n\tfor k, v := range md {\n\t\tvalues := make([]string, len(v))\n\t\tcopy(values, v)\n\t\tresult[k] = values\n\t}\n\treturn result\n}\n\n// Get obtains the values for a given key.\nfunc (md MD) Get(k string) []string {\n\tk = strings.ToLower(k)\n\treturn md[k]\n}\n\n// Set sets the value of a given key with a slice of values.\nfunc (md MD) Set(k string, vals ...string) {\n\tif len(vals) == 0 {\n\t\treturn\n\t}\n\tk = strings.ToLower(k)\n\tmd[k] = vals\n}\n\n// Append adds the values to key k, not overwriting what was already stored at that key.\nfunc (md MD) Append(k string, vals ...string) {\n\tif len(vals) == 0 {\n\t\treturn\n\t}\n\tk = strings.ToLower(k)\n\tmd[k] = append(md[k], vals...)\n}\n\n// Join joins any number of mds into a single MD.\n// The order of values for each key is determined by the order in which\n// the mds containing those values are presented to Join.\nfunc Join(mds ...MD) MD {\n\tn := 0\n\tfor _, md := range mds {\n\t\tn += len(md)\n\t}\n\tout := make(MD, n)\n\tfor _, md := range mds {\n\t\tfor k, v := range md {\n\t\t\tout[k] = append(out[k], v...)\n\t\t}\n\t}\n\treturn out\n}\n\n// AppendMD appends other into md, merging values of the same key.\nfunc AppendMD(md, other MD) MD {\n\tif md == nil {\n\t\tmd = make(MD, len(other))\n\t}\n\tfor k, v := range other {\n\t\tmd[k] = append(md[k], v...)\n\t}\n\treturn md\n}\n\ntype (\n\tmdIncomingKey struct{}\n\tmdOutgoingKey struct{}\n)\n\n// NewIncomingContext creates a new context with incoming md attached.\nfunc NewIncomingContext(ctx context.Context, md MD) context.Context {\n\treturn context.WithValue(ctx, mdIncomingKey{}, md)\n}\n\n// NewOutgoingContext creates a new context with outgoing md attached. If used\n// in conjunction with AppendToOutgoingContext, NewOutgoingContext will\n// overwrite any previously-appended metadata.\nfunc NewOutgoingContext(ctx context.Context, md MD) context.Context {\n\treturn context.WithValue(ctx, mdOutgoingKey{}, rawMD{md: md})\n}\n\n// AppendToOutgoingContext returns a new context with the provided kv merged\n// with any existing metadata in the context. Please refer to the\n// documentation of Pairs for a description of kv.\nfunc AppendToOutgoingContext(ctx context.Context, kv ...string) context.Context {\n\tif len(kv)%2 == 1 {\n\t\tpanic(fmt.Sprintf(\"metadata: AppendToOutgoingContext got an odd number of input pairs for metadata: %d\", len(kv)))\n\t}\n\tmd, _ := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tadded := make([][]string, len(md.added)+1)\n\tcopy(added, md.added)\n\tadded[len(added)-1] = make([]string, len(kv))\n\tcopy(added[len(added)-1], kv)\n\treturn context.WithValue(ctx, mdOutgoingKey{}, rawMD{md: md.md, added: added})\n}\n\n// FromIncomingContext returns the incoming metadata in ctx if it exists.  The\n// returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\nfunc FromIncomingContext(ctx context.Context) (md MD, ok bool) {\n\tmd, ok = ctx.Value(mdIncomingKey{}).(MD)\n\treturn\n}\n\n// FromOutgoingContextRaw returns the un-merged, intermediary contents\n// of rawMD. Remember to perform strings.ToLower on the keys. The returned\n// MD should not be modified. Writing to it may cause races. Modification\n// should be made to copies of the returned MD.\n//\n// This is intended for gRPC-internal use ONLY.\nfunc FromOutgoingContextRaw(ctx context.Context) (MD, [][]string, bool) {\n\traw, ok := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tif !ok {\n\t\treturn nil, nil, false\n\t}\n\n\treturn raw.md, raw.added, true\n}\n\n// FromOutgoingContext returns the outgoing metadata in ctx if it exists.  The\n// returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype rawMD struct {\n\tmd    MD\n\tadded [][]string\n}\n\n```\n# Function Signature: func FromOutgoingContext(ctx context.Context) (MD, bool) {\n# Natural Language Annotations: // FromOutgoingContext returns the outgoing metadata in ctx if it exists.  The\n// returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// FromOutgoingContext returns the outgoing metadata in ctx if it exists.\n// The returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\nfunc FromOutgoingContext(ctx context.Context) (MD, bool) {\n\traw, ok := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tif !ok {\n\t\treturn nil, false\n\t}\n\n\t// Prepare a result MD by merging the existing md and the added slices.\n\tmergedMD := raw.md.Copy()\n\tfor _, kv := range raw.added {\n\t\tfor i := 0; i < len(kv); i += 2 {\n\t\t\tkey := strings.ToLower(kv[i])\n\t\t\tval := kv[i+1]\n\t\t\tmergedMD[key] = append(mergedMD[key], val)\n\t\t}\n\t}\n\n\treturn mergedMD, true\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/status/status.go-WithDetails": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/status/status.go:\n```\n/*\n *\n * Copyright 2020 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package status implements errors returned by gRPC. These errors are\n// serialized and transmitted on the wire between server and client, and allow\n// for additional data to be transmitted via the Details field in the status\n// proto. gRPC service handlers should return an error created by this\n// package, and gRPC clients should expect a corresponding error to be\n// returned from the RPC call.\n//\n// This package upholds the invariants that a non-nil error may not\n// contain an OK code, and an OK code must result in a nil error.\npackage status\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/types/known/anypb\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n)\n\ntype Iface interface {\n\tGRPCStatus() *Status\n}\n\n// Status represents an RPC status code, message, and details.  It is immutable\n// and should be created with New, Newf, or FromProto.\ntype Status struct {\n\ts *spb.Status\n}\n\n// New returns a Status representing c and msg.\nfunc New(c codes.Code, msg string) *Status {\n\treturn &Status{s: &spb.Status{Code: int32(c), Message: msg}}\n}\n\n// Newf returns New(c, fmt.Sprintf(format, a...)).\nfunc Newf(c codes.Code, format string, a ...interface{}) *Status {\n\treturn New(c, fmt.Sprintf(format, a...))\n}\n\n// ErrorProto returns an error representing s.  If s.Code is OK, returns nil.\nfunc ErrorProto(s *spb.Status) error {\n\treturn FromProto(s).Err()\n}\n\n// FromProto returns a Status representing s.\nfunc FromProto(s *spb.Status) *Status {\n\treturn &Status{s: proto.Clone(s).(*spb.Status)}\n}\n\n// Err returns an error representing c and msg.  If c is OK, returns nil.\nfunc Err(c codes.Code, msg string) error {\n\treturn New(c, msg).Err()\n}\n\n// Errorf returns Error(c, fmt.Sprintf(format, a...)).\nfunc Errorf(c codes.Code, format string, a ...interface{}) error {\n\treturn Err(c, fmt.Sprintf(format, a...))\n}\n\n// Code returns the status code contained in s.\nfunc (s *Status) Code() codes.Code {\n\tif s == nil || s.s == nil {\n\t\treturn codes.OK\n\t}\n\treturn codes.Code(s.s.Code)\n}\n\n// Message returns the message contained in s.\nfunc (s *Status) Message() string {\n\tif s == nil || s.s == nil {\n\t\treturn \"\"\n\t}\n\treturn s.s.Message\n}\n\n// AppendMessage append extra msg for Status\nfunc (s *Status) AppendMessage(extraMsg string) *Status {\n\tif s == nil || s.s == nil || extraMsg == \"\" {\n\t\treturn s\n\t}\n\ts.s.Message = fmt.Sprintf(\"%s %s\", s.s.Message, extraMsg)\n\treturn s\n}\n\n// Proto returns s's status as an spb.Status proto message.\nfunc (s *Status) Proto() *spb.Status {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn proto.Clone(s.s).(*spb.Status)\n}\n\n// Err returns an immutable error representing s; returns nil if s.Code() is OK.\nfunc (s *Status) Err() error {\n\tif s.Code() == codes.OK {\n\t\treturn nil\n\t}\n\treturn &Error{e: s.Proto()}\n}\n\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Details returns a slice of details messages attached to the status.\n// If a detail cannot be decoded, the error is returned in place of the detail.\nfunc (s *Status) Details() []interface{} {\n\tif s == nil || s.s == nil {\n\t\treturn nil\n\t}\n\tdetails := make([]interface{}, 0, len(s.s.Details))\n\tfor _, any := range s.s.Details {\n\t\tdetail, err := any.UnmarshalNew()\n\t\tif err != nil {\n\t\t\tdetails = append(details, err)\n\t\t\tcontinue\n\t\t}\n\t\tdetails = append(details, detail)\n\t}\n\treturn details\n}\n\n// Error wraps a pointer of a status proto. It implements error and Status,\n// and a nil *Error should never be returned by this package.\ntype Error struct {\n\te *spb.Status\n}\n\nfunc (e *Error) Error() string {\n\treturn fmt.Sprintf(\"rpc error: code = %d desc = %s\", codes.Code(e.e.GetCode()), e.e.GetMessage())\n}\n\n// GRPCStatus returns the Status represented by se.\nfunc (e *Error) GRPCStatus() *Status {\n\treturn FromProto(e.e)\n}\n\n// Is implements future error.Is functionality.\n// A Error is equivalent if the code and message are identical.\nfunc (e *Error) Is(target error) bool {\n\ttse, ok := target.(*Error)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn proto.Equal(e.e, tse.e)\n}\n\n// FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\n\n\n\n\n\n\n\n\n\n\n\n// Convert is a convenience function which removes the need to handle the\n// boolean return value from FromError.\nfunc Convert(err error) *Status {\n\ts, _ := FromError(err)\n\treturn s\n}\n\n// Code returns the Code of the error if it is a Status error, codes.OK if err\n// is nil, or codes.Unknown otherwise.\nfunc Code(err error) codes.Code {\n\t// Don't use FromError to avoid allocation of OK status.\n\tif err == nil {\n\t\treturn codes.OK\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus().Code()\n\t}\n\treturn codes.Unknown\n}\n\n// FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n# Natural Language Annotations: // WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\nfunc (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n\tif s == nil || s.s == nil {\n\t\treturn nil, fmt.Errorf(\"status is nil\")\n\t}\n\n\tnewStatus := &spb.Status{\n\t\tCode:    s.s.Code,\n\t\tMessage: s.s.Message,\n\t\tDetails: make([]*anypb.Any, len(s.s.Details)),\n\t}\n\n\tcopy(newStatus.Details, s.s.Details)\n\n\tfor _, detail := range details {\n\t\tanyMsg, err := anypb.New(detail)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to marshal detail: %v\", err)\n\t\t}\n\t\tnewStatus.Details = append(newStatus.Details, anyMsg)\n\t}\n\n\treturn &Status{s: newStatus}, nil\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/status/status.go-FromError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/status/status.go:\n```\n/*\n *\n * Copyright 2020 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package status implements errors returned by gRPC. These errors are\n// serialized and transmitted on the wire between server and client, and allow\n// for additional data to be transmitted via the Details field in the status\n// proto. gRPC service handlers should return an error created by this\n// package, and gRPC clients should expect a corresponding error to be\n// returned from the RPC call.\n//\n// This package upholds the invariants that a non-nil error may not\n// contain an OK code, and an OK code must result in a nil error.\npackage status\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/types/known/anypb\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n)\n\ntype Iface interface {\n\tGRPCStatus() *Status\n}\n\n// Status represents an RPC status code, message, and details.  It is immutable\n// and should be created with New, Newf, or FromProto.\ntype Status struct {\n\ts *spb.Status\n}\n\n// New returns a Status representing c and msg.\nfunc New(c codes.Code, msg string) *Status {\n\treturn &Status{s: &spb.Status{Code: int32(c), Message: msg}}\n}\n\n// Newf returns New(c, fmt.Sprintf(format, a...)).\nfunc Newf(c codes.Code, format string, a ...interface{}) *Status {\n\treturn New(c, fmt.Sprintf(format, a...))\n}\n\n// ErrorProto returns an error representing s.  If s.Code is OK, returns nil.\nfunc ErrorProto(s *spb.Status) error {\n\treturn FromProto(s).Err()\n}\n\n// FromProto returns a Status representing s.\nfunc FromProto(s *spb.Status) *Status {\n\treturn &Status{s: proto.Clone(s).(*spb.Status)}\n}\n\n// Err returns an error representing c and msg.  If c is OK, returns nil.\nfunc Err(c codes.Code, msg string) error {\n\treturn New(c, msg).Err()\n}\n\n// Errorf returns Error(c, fmt.Sprintf(format, a...)).\nfunc Errorf(c codes.Code, format string, a ...interface{}) error {\n\treturn Err(c, fmt.Sprintf(format, a...))\n}\n\n// Code returns the status code contained in s.\nfunc (s *Status) Code() codes.Code {\n\tif s == nil || s.s == nil {\n\t\treturn codes.OK\n\t}\n\treturn codes.Code(s.s.Code)\n}\n\n// Message returns the message contained in s.\nfunc (s *Status) Message() string {\n\tif s == nil || s.s == nil {\n\t\treturn \"\"\n\t}\n\treturn s.s.Message\n}\n\n// AppendMessage append extra msg for Status\nfunc (s *Status) AppendMessage(extraMsg string) *Status {\n\tif s == nil || s.s == nil || extraMsg == \"\" {\n\t\treturn s\n\t}\n\ts.s.Message = fmt.Sprintf(\"%s %s\", s.s.Message, extraMsg)\n\treturn s\n}\n\n// Proto returns s's status as an spb.Status proto message.\nfunc (s *Status) Proto() *spb.Status {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn proto.Clone(s.s).(*spb.Status)\n}\n\n// Err returns an immutable error representing s; returns nil if s.Code() is OK.\nfunc (s *Status) Err() error {\n\tif s.Code() == codes.OK {\n\t\treturn nil\n\t}\n\treturn &Error{e: s.Proto()}\n}\n\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Details returns a slice of details messages attached to the status.\n// If a detail cannot be decoded, the error is returned in place of the detail.\nfunc (s *Status) Details() []interface{} {\n\tif s == nil || s.s == nil {\n\t\treturn nil\n\t}\n\tdetails := make([]interface{}, 0, len(s.s.Details))\n\tfor _, any := range s.s.Details {\n\t\tdetail, err := any.UnmarshalNew()\n\t\tif err != nil {\n\t\t\tdetails = append(details, err)\n\t\t\tcontinue\n\t\t}\n\t\tdetails = append(details, detail)\n\t}\n\treturn details\n}\n\n// Error wraps a pointer of a status proto. It implements error and Status,\n// and a nil *Error should never be returned by this package.\ntype Error struct {\n\te *spb.Status\n}\n\nfunc (e *Error) Error() string {\n\treturn fmt.Sprintf(\"rpc error: code = %d desc = %s\", codes.Code(e.e.GetCode()), e.e.GetMessage())\n}\n\n// GRPCStatus returns the Status represented by se.\nfunc (e *Error) GRPCStatus() *Status {\n\treturn FromProto(e.e)\n}\n\n// Is implements future error.Is functionality.\n// A Error is equivalent if the code and message are identical.\nfunc (e *Error) Is(target error) bool {\n\ttse, ok := target.(*Error)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn proto.Equal(e.e, tse.e)\n}\n\n// FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\n\n\n\n\n\n\n\n\n\n\n\n// Convert is a convenience function which removes the need to handle the\n// boolean return value from FromError.\nfunc Convert(err error) *Status {\n\ts, _ := FromError(err)\n\treturn s\n}\n\n// Code returns the Code of the error if it is a Status error, codes.OK if err\n// is nil, or codes.Unknown otherwise.\nfunc Code(err error) codes.Code {\n\t// Don't use FromError to avoid allocation of OK status.\n\tif err == nil {\n\t\treturn codes.OK\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus().Code()\n\t}\n\treturn codes.Unknown\n}\n\n// FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func FromError(err error) (s *Status, ok bool) {\n# Natural Language Annotations: // FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FromError(err error) (s *Status, ok bool) {\n    // Check if the error is nil, return a status with codes.OK if so\n    if err == nil {\n        return New(codes.OK, \"\"), true\n    }\n\n    // Try to cast the error to the Iface interface to check for GRPCStatus method\n    var se Iface\n    if errors.As(err, &se) {\n        return se.GRPCStatus(), true\n    }\n\n    // If the error does not implement GRPCStatus, return a Status with codes.Unknown\n    return New(codes.Unknown, err.Error()), false\n}\n```\n"]}, "kitex-pkg/remote/trans/nphttp2/status/status.go-FromContextError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/status/status.go:\n```\n/*\n *\n * Copyright 2020 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package status implements errors returned by gRPC. These errors are\n// serialized and transmitted on the wire between server and client, and allow\n// for additional data to be transmitted via the Details field in the status\n// proto. gRPC service handlers should return an error created by this\n// package, and gRPC clients should expect a corresponding error to be\n// returned from the RPC call.\n//\n// This package upholds the invariants that a non-nil error may not\n// contain an OK code, and an OK code must result in a nil error.\npackage status\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/types/known/anypb\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n)\n\ntype Iface interface {\n\tGRPCStatus() *Status\n}\n\n// Status represents an RPC status code, message, and details.  It is immutable\n// and should be created with New, Newf, or FromProto.\ntype Status struct {\n\ts *spb.Status\n}\n\n// New returns a Status representing c and msg.\nfunc New(c codes.Code, msg string) *Status {\n\treturn &Status{s: &spb.Status{Code: int32(c), Message: msg}}\n}\n\n// Newf returns New(c, fmt.Sprintf(format, a...)).\nfunc Newf(c codes.Code, format string, a ...interface{}) *Status {\n\treturn New(c, fmt.Sprintf(format, a...))\n}\n\n// ErrorProto returns an error representing s.  If s.Code is OK, returns nil.\nfunc ErrorProto(s *spb.Status) error {\n\treturn FromProto(s).Err()\n}\n\n// FromProto returns a Status representing s.\nfunc FromProto(s *spb.Status) *Status {\n\treturn &Status{s: proto.Clone(s).(*spb.Status)}\n}\n\n// Err returns an error representing c and msg.  If c is OK, returns nil.\nfunc Err(c codes.Code, msg string) error {\n\treturn New(c, msg).Err()\n}\n\n// Errorf returns Error(c, fmt.Sprintf(format, a...)).\nfunc Errorf(c codes.Code, format string, a ...interface{}) error {\n\treturn Err(c, fmt.Sprintf(format, a...))\n}\n\n// Code returns the status code contained in s.\nfunc (s *Status) Code() codes.Code {\n\tif s == nil || s.s == nil {\n\t\treturn codes.OK\n\t}\n\treturn codes.Code(s.s.Code)\n}\n\n// Message returns the message contained in s.\nfunc (s *Status) Message() string {\n\tif s == nil || s.s == nil {\n\t\treturn \"\"\n\t}\n\treturn s.s.Message\n}\n\n// AppendMessage append extra msg for Status\nfunc (s *Status) AppendMessage(extraMsg string) *Status {\n\tif s == nil || s.s == nil || extraMsg == \"\" {\n\t\treturn s\n\t}\n\ts.s.Message = fmt.Sprintf(\"%s %s\", s.s.Message, extraMsg)\n\treturn s\n}\n\n// Proto returns s's status as an spb.Status proto message.\nfunc (s *Status) Proto() *spb.Status {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn proto.Clone(s.s).(*spb.Status)\n}\n\n// Err returns an immutable error representing s; returns nil if s.Code() is OK.\nfunc (s *Status) Err() error {\n\tif s.Code() == codes.OK {\n\t\treturn nil\n\t}\n\treturn &Error{e: s.Proto()}\n}\n\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Details returns a slice of details messages attached to the status.\n// If a detail cannot be decoded, the error is returned in place of the detail.\nfunc (s *Status) Details() []interface{} {\n\tif s == nil || s.s == nil {\n\t\treturn nil\n\t}\n\tdetails := make([]interface{}, 0, len(s.s.Details))\n\tfor _, any := range s.s.Details {\n\t\tdetail, err := any.UnmarshalNew()\n\t\tif err != nil {\n\t\t\tdetails = append(details, err)\n\t\t\tcontinue\n\t\t}\n\t\tdetails = append(details, detail)\n\t}\n\treturn details\n}\n\n// Error wraps a pointer of a status proto. It implements error and Status,\n// and a nil *Error should never be returned by this package.\ntype Error struct {\n\te *spb.Status\n}\n\nfunc (e *Error) Error() string {\n\treturn fmt.Sprintf(\"rpc error: code = %d desc = %s\", codes.Code(e.e.GetCode()), e.e.GetMessage())\n}\n\n// GRPCStatus returns the Status represented by se.\nfunc (e *Error) GRPCStatus() *Status {\n\treturn FromProto(e.e)\n}\n\n// Is implements future error.Is functionality.\n// A Error is equivalent if the code and message are identical.\nfunc (e *Error) Is(target error) bool {\n\ttse, ok := target.(*Error)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn proto.Equal(e.e, tse.e)\n}\n\n// FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\n\n\n\n\n\n\n\n\n\n\n\n// Convert is a convenience function which removes the need to handle the\n// boolean return value from FromError.\nfunc Convert(err error) *Status {\n\ts, _ := FromError(err)\n\treturn s\n}\n\n// Code returns the Code of the error if it is a Status error, codes.OK if err\n// is nil, or codes.Unknown otherwise.\nfunc Code(err error) codes.Code {\n\t// Don't use FromError to avoid allocation of OK status.\n\tif err == nil {\n\t\treturn codes.OK\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus().Code()\n\t}\n\treturn codes.Unknown\n}\n\n// FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func FromContextError(err error) *Status {\n# Natural Language Annotations: // FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FromContextError(err error) *Status {\n\tif err == nil {\n\t\treturn New(codes.OK, \"\")\n\t}\n\n\tif errors.Is(err, context.Canceled) {\n\t\treturn New(codes.Canceled, err.Error())\n\t}\n\n\tif errors.Is(err, context.DeadlineExceeded) {\n\t\treturn New(codes.DeadlineExceeded, err.Error())\n\t}\n\n\treturn New(codes.Unknown, err.Error())\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/bdp_estimator.go-calculate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/bdp_estimator.go:\n```\n/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\nconst (\n\t// bdpLimit is the maximum value the flow control windows will be increased\n\t// to.  TCP typically limits this to 4MB, but some systems go up to 16MB.\n\t// Since this is only a limit, it is safe to make it optimistic.\n\tbdpLimit = (1 << 20) * 16\n\t// alpha is a constant factor used to keep a moving average\n\t// of RTTs.\n\talpha = 0.9\n\t// If the current bdp sample is greater than or equal to\n\t// our beta * our estimated bdp and the current bandwidth\n\t// sample is the maximum bandwidth observed so far, we\n\t// increase our bbp estimate by a factor of gamma.\n\tbeta = 0.66\n\t// To put our bdp to be smaller than or equal to twice the real BDP,\n\t// we should multiply our current sample with 4/3, however to round things out\n\t// we use 2 as the multiplication factor.\n\tgamma = 2\n)\n\n// Adding arbitrary data to ping so that its ack can be identified.\n// Easter-egg: what does the ping message say?\nvar bdpPing = &ping{data: [8]byte{2, 4, 16, 16, 9, 14, 7, 7}}\n\ntype bdpEstimator struct {\n\t// sentAt is the time when the ping was sent.\n\tsentAt time.Time\n\n\tmu sync.Mutex\n\t// bdp is the current bdp estimate.\n\tbdp uint32\n\t// sample is the number of bytes received in one measurement cycle.\n\tsample uint32\n\t// bwMax is the maximum bandwidth noted so far (bytes/sec).\n\tbwMax float64\n\t// bool to keep track of the beginning of a new measurement cycle.\n\tisSent bool\n\t// Callback to update the window sizes.\n\tupdateFlowControl func(n uint32)\n\t// sampleCount is the number of samples taken so far.\n\tsampleCount uint64\n\t// round trip time (seconds)\n\trtt float64\n}\n\n// timesnap registers the time bdp ping was sent out so that\n// network rtt can be calculated when its ack is received.\n// It is called (by controller) when the bdpPing is\n// being written on the wire.\nfunc (b *bdpEstimator) timesnap(d [8]byte) {\n\tif bdpPing.data != d {\n\t\treturn\n\t}\n\t// Locking here is to avoid DATA RACE in the unittest.\n\t// In fact, it would not bring the concurrency problem.\n\tb.mu.Lock()\n\tb.sentAt = time.Now()\n\tb.mu.Unlock()\n}\n\n// add adds bytes to the current sample for calculating bdp.\n// It returns true only if a ping must be sent. This can be used\n// by the caller (handleData) to make decision about batching\n// a window update with it.\nfunc (b *bdpEstimator) add(n uint32) bool {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tif b.bdp == bdpLimit {\n\t\treturn false\n\t}\n\tif !b.isSent {\n\t\tb.isSent = true\n\t\tb.sample = n\n\t\tb.sentAt = time.Time{}\n\t\tb.sampleCount++\n\t\treturn true\n\t}\n\tb.sample += n\n\treturn false\n}\n\n// calculate is called when an ack for a bdp ping is received.\n// Here we calculate the current bdp and bandwidth sample and\n// decide if the flow control windows should go up.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (b *bdpEstimator) calculate(d [8]byte) {\n# Natural Language Annotations: // calculate is called when an ack for a bdp ping is received.\n// Here we calculate the current bdp and bandwidth sample and\n// decide if the flow control windows should go up.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (b *bdpEstimator) calculate(d [8]byte) {\n    if bdpPing.data != d {\n        return\n    }\n\n    b.mu.Lock()\n    defer b.mu.Unlock()\n\n    elapsed := time.Since(b.sentAt).Seconds()\n    b.rtt = alpha*b.rtt + (1-alpha)*elapsed\n\n    bandwidth := float64(b.sample) / b.rtt\n    if bandwidth > b.bwMax {\n        b.bwMax = bandwidth\n    }\n\n    bdpSample := uint32(bandwidth * b.rtt)\n\n    if float64(bdpSample) >= beta*float64(b.bdp) && bandwidth == b.bwMax {\n        b.bdp = uint32(float64(b.bdp) * gamma)\n        if b.bdp > bdpLimit {\n            b.bdp = bdpLimit\n        }\n    } else {\n        b.bdp = bdpSample\n    }\n\n    if b.updateFlowControl != nil {\n        b.updateFlowControl(b.bdp)\n    }\n\n    b.isSent = false\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/http2_server.go-finishStream": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/http2_server.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf/encoding\"\n\n\t\"github.com/cloudwego/netpoll\"\n\t\"golang.org/x/net/http2\"\n\t\"golang.org/x/net/http2/hpack\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/gofunc\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc/grpcframe\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nvar (\n\t// ErrIllegalHeaderWrite indicates that setting header is illegal because of\n\t// the stream's state.\n\tErrIllegalHeaderWrite = errors.New(\"transport: the stream is done or WriteHeader was already called\")\n\t// ErrHeaderListSizeLimitViolation indicates that the header list size is larger\n\t// than the limit set by peer.\n\tErrHeaderListSizeLimitViolation = errors.New(\"transport: trying to send header list size larger than the limit set by peer\")\n)\n\nfunc init() {\n\trand.Seed(time.Now().UnixNano())\n}\n\n// http2Server implements the ServerTransport interface with HTTP2.\ntype http2Server struct {\n\tlastRead    int64\n\tctx         context.Context\n\tdone        chan struct{}\n\tconn        net.Conn\n\tloopy       *loopyWriter\n\treaderDone  chan struct{} // sync point to enable testing.\n\twriterDone  chan struct{} // sync point to enable testing.\n\tremoteAddr  net.Addr\n\tlocalAddr   net.Addr\n\tmaxStreamID uint32 // max stream ID ever seen\n\tframer      *framer\n\t// The max number of concurrent streams.\n\tmaxStreams uint32\n\t// controlBuf delivers all the control related tasks (e.g., window\n\t// updates, reset streams, and various settings) to the controller.\n\tcontrolBuf *controlBuffer\n\tfc         *trInFlow\n\t// Keepalive and max-age parameters for the server.\n\tkp ServerKeepalive\n\t// Keepalive enforcement policy.\n\tkep EnforcementPolicy\n\t// The time instance last ping was received.\n\tlastPingAt time.Time\n\t// Number of times the client has violated keepalive ping policy so far.\n\tpingStrikes uint8\n\t// Flag to signify that number of ping strikes should be reset to 0.\n\t// This is set whenever data or header frames are sent.\n\t// 1 means yes.\n\tresetPingStrikes      uint32 // Accessed atomically.\n\tinitialWindowSize     int32\n\tbdpEst                *bdpEstimator\n\tmaxSendHeaderListSize *uint32\n\n\tmu sync.Mutex // guard the following\n\t// drainChan is initialized when drain(...) is called the first time.\n\t// After which the server writes out the first GoAway(with ID 2^31-1) frame.\n\t// Then an independent goroutine will be launched to later send the second GoAway.\n\t// During this time we don't want to write another first GoAway(with ID 2^31 -1) frame.\n\t// Thus call to drain(...) will be a no-op if drainChan is already initialized since draining is\n\t// already underway.\n\tdrainChan     chan struct{}\n\tstate         transportState\n\tactiveStreams map[uint32]*Stream\n\t// idle is the time instant when the connection went idle.\n\t// This is either the beginning of the connection or when the number of\n\t// RPCs go down to 0.\n\t// When the connection is busy, this value is set to 0.\n\tidle time.Time\n\n\tbufferPool *bufferPool\n}\n\n// newHTTP2Server constructs a ServerTransport based on HTTP2. ConnectionError is\n// returned if something goes wrong.\nfunc newHTTP2Server(ctx context.Context, conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {\n\tmaxHeaderListSize := defaultServerMaxHeaderListSize\n\tif config.MaxHeaderListSize != nil {\n\t\tmaxHeaderListSize = *config.MaxHeaderListSize\n\t}\n\n\tframer := newFramer(conn, config.WriteBufferSize, config.ReadBufferSize, maxHeaderListSize)\n\t// Send initial settings as connection preface to client.\n\tisettings := []http2.Setting{{\n\t\tID:  http2.SettingMaxFrameSize,\n\t\tVal: http2MaxFrameLen,\n\t}}\n\n\t// 0 is permitted in the HTTP2 spec.\n\tmaxStreams := config.MaxStreams\n\tif maxStreams == 0 {\n\t\tmaxStreams = math.MaxUint32\n\t} else {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxConcurrentStreams,\n\t\t\tVal: maxStreams,\n\t\t})\n\t}\n\n\tdynamicWindow := true\n\tiwz := initialWindowSize\n\tif config.InitialWindowSize >= defaultWindowSize {\n\t\tiwz = config.InitialWindowSize\n\t\tdynamicWindow = false\n\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\tVal: iwz,\n\t\t})\n\t}\n\ticwz := initialWindowSize\n\tif config.InitialConnWindowSize >= defaultWindowSize {\n\t\ticwz = config.InitialConnWindowSize\n\t\tdynamicWindow = false\n\t}\n\tif config.MaxHeaderListSize != nil {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxHeaderListSize,\n\t\t\tVal: *config.MaxHeaderListSize,\n\t\t})\n\t}\n\n\tif err := framer.WriteSettings(isettings...); err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t}\n\n\t// Adjust the connection flow control window if needed.\n\tif icwz > defaultWindowSize {\n\t\tif delta := icwz - defaultWindowSize; delta > 0 {\n\t\t\tif err := framer.WriteWindowUpdate(0, delta); err != nil {\n\t\t\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n\tkp := config.KeepaliveParams\n\tif kp.MaxConnectionIdle == 0 {\n\t\tkp.MaxConnectionIdle = defaultMaxConnectionIdle\n\t}\n\tif kp.MaxConnectionAge == 0 {\n\t\tkp.MaxConnectionAge = defaultMaxConnectionAge\n\t}\n\tif kp.MaxConnectionAgeGrace == 0 {\n\t\tkp.MaxConnectionAgeGrace = defaultMaxConnectionAgeGrace\n\t}\n\tif kp.Time == 0 {\n\t\tkp.Time = defaultServerKeepaliveTime\n\t}\n\tif kp.Timeout == 0 {\n\t\tkp.Timeout = defaultServerKeepaliveTimeout\n\t}\n\tkep := config.KeepaliveEnforcementPolicy\n\tif kep.MinTime == 0 {\n\t\tkep.MinTime = defaultKeepalivePolicyMinTime\n\t}\n\n\tdone := make(chan struct{})\n\tt := &http2Server{\n\t\tctx:               ctx,\n\t\tdone:              done,\n\t\tconn:              conn,\n\t\tremoteAddr:        conn.RemoteAddr(),\n\t\tlocalAddr:         conn.LocalAddr(),\n\t\tframer:            framer,\n\t\treaderDone:        make(chan struct{}),\n\t\twriterDone:        make(chan struct{}),\n\t\tmaxStreams:        math.MaxUint32,\n\t\tfc:                &trInFlow{limit: icwz},\n\t\tstate:             reachable,\n\t\tactiveStreams:     make(map[uint32]*Stream),\n\t\tkp:                kp,\n\t\tkep:               kep,\n\t\tidle:              time.Now(),\n\t\tinitialWindowSize: int32(iwz),\n\t\tbufferPool:        newBufferPool(),\n\t}\n\tt.controlBuf = newControlBuffer(t.done)\n\tif dynamicWindow {\n\t\tt.bdpEst = &bdpEstimator{\n\t\t\tbdp:               initialWindowSize,\n\t\t\tupdateFlowControl: t.updateFlowControl,\n\t\t}\n\t}\n\n\tt.framer.writer.Flush()\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tt.Close()\n\t\t}\n\t}()\n\n\t// Check the validity of client preface.\n\tpreface := make([]byte, len(ClientPreface))\n\tif _, err := io.ReadFull(t.conn, preface); err != nil {\n\t\t// In deployments where a gRPC server runs behind a cloud load balancer\n\t\t// which performs regular TCP level health checks, the connection is\n\t\t// closed immediately by the latter.  Returning io.EOF here allows the\n\t\t// grpc server implementation to recognize this scenario and suppress\n\t\t// logging to reduce spam.\n\t\tif err == io.EOF {\n\t\t\treturn nil, io.EOF\n\t\t}\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to receive the preface from client: %v\", err)\n\t}\n\tif !bytes.Equal(preface, ClientPreface) {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams received bogus greeting from client: %q\", preface)\n\t}\n\n\tframe, err := t.framer.ReadFrame()\n\tif err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\treturn nil, err\n\t}\n\tif err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to read initial settings frame: %v\", err)\n\t}\n\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\tsf, ok := frame.(*grpcframe.SettingsFrame)\n\tif !ok {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams saw invalid preface type %T from client\", frame)\n\t}\n\tt.handleSettings(sf)\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, func() {\n\t\tt.loopy = newLoopyWriter(serverSide, t.framer, t.controlBuf, t.bdpEst)\n\t\tt.loopy.ssGoAwayHandler = t.outgoingGoAwayHandler\n\t\tif err := t.loopy.run(conn.RemoteAddr().String()); err != nil {\n\t\t\tklog.CtxErrorf(ctx, \"KITEX: grpc server loopyWriter.run returning, error=%v\", err)\n\t\t}\n\t\tt.conn.Close()\n\t\tclose(t.writerDone)\n\t}, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, t.keepalive, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\treturn t, nil\n}\n\n// operateHeader takes action on the decoded headers.\nfunc (t *http2Server) operateHeaders(frame *grpcframe.MetaHeadersFrame, handle func(*Stream), traceCtx func(context.Context, string) context.Context) (fatal bool) {\n\tstreamID := frame.Header().StreamID\n\tstate := &decodeState{\n\t\tserverSide: true,\n\t}\n\tif err := state.decodeHeader(frame); err != nil {\n\t\tif se, ok := status.FromError(err); ok {\n\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\tstreamID: streamID,\n\t\t\t\trst:      true,\n\t\t\t\trstCode:  statusCodeConvTab[se.Code()],\n\t\t\t\tonWrite:  func() {},\n\t\t\t})\n\t\t}\n\t\treturn false\n\t}\n\n\tbuf := newRecvBuffer()\n\ts := &Stream{\n\t\tid:             streamID,\n\t\tst:             t,\n\t\tbuf:            buf,\n\t\tfc:             &inFlow{limit: uint32(t.initialWindowSize)},\n\t\trecvCompress:   state.data.encoding,\n\t\tsendCompress:   state.data.acceptEncoding,\n\t\tmethod:         state.data.method,\n\t\tcontentSubtype: state.data.contentSubtype,\n\t}\n\tif frame.StreamEnded() {\n\t\t// s is just created by the caller. No lock needed.\n\t\ts.state = streamReadDone\n\t}\n\tif state.data.timeoutSet {\n\t\ts.ctx, s.cancel = context.WithTimeout(t.ctx, state.data.timeout)\n\t} else {\n\t\ts.ctx, s.cancel = context.WithCancel(t.ctx)\n\t}\n\t// Attach the received metadata to the context.\n\tif len(state.data.mdata) > 0 {\n\t\ts.ctx = metadata.NewIncomingContext(s.ctx, state.data.mdata)\n\t}\n\n\tt.mu.Lock()\n\tif t.state != reachable {\n\t\tt.mu.Unlock()\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif uint32(len(t.activeStreams)) >= t.maxStreams {\n\t\tt.mu.Unlock()\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: streamID,\n\t\t\trst:      true,\n\t\t\trstCode:  http2.ErrCodeRefusedStream,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif streamID%2 != 1 || streamID <= t.maxStreamID {\n\t\tt.mu.Unlock()\n\t\t// illegal gRPC stream id.\n\t\tklog.CtxErrorf(s.ctx, \"transport: http2Server.HandleStreams received an illegal stream id: %v\", streamID)\n\t\ts.cancel()\n\t\treturn true\n\t}\n\tt.maxStreamID = streamID\n\tt.activeStreams[streamID] = s\n\tif len(t.activeStreams) == 1 {\n\t\tt.idle = time.Time{}\n\t}\n\tt.mu.Unlock()\n\ts.requestRead = func(n int) {\n\t\tt.adjustWindow(s, uint32(n))\n\t}\n\ts.ctx = traceCtx(s.ctx, s.method)\n\ts.ctxDone = s.ctx.Done()\n\ts.wq = newWriteQuota(defaultWriteQuota, s.ctxDone)\n\ts.trReader = &transportReader{\n\t\treader: &recvBufferReader{\n\t\t\tctx:        s.ctx,\n\t\t\tctxDone:    s.ctxDone,\n\t\t\trecv:       s.buf,\n\t\t\tfreeBuffer: t.bufferPool.put,\n\t\t},\n\t\twindowHandler: func(n int) {\n\t\t\tt.updateWindow(s, uint32(n))\n\t\t},\n\t}\n\t// Register the stream with loopy.\n\tt.controlBuf.put(&registerStream{\n\t\tstreamID: s.id,\n\t\twq:       s.wq,\n\t})\n\thandle(s)\n\treturn false\n}\n\n// HandleStreams receives incoming streams using the given handler. This is\n// typically run in a separate goroutine.\n// traceCtx attaches trace to ctx and returns the new context.\nfunc (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) {\n\tdefer close(t.readerDone)\n\tfor {\n\t\tt.controlBuf.throttle()\n\t\tframe, err := t.framer.ReadFrame()\n\t\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\t\tif err != nil {\n\t\t\tif se, ok := err.(http2.StreamError); ok {\n\t\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams encountered http2.StreamError: %v\", se)\n\t\t\t\tt.mu.Lock()\n\t\t\t\ts := t.activeStreams[se.StreamID]\n\t\t\t\tt.mu.Unlock()\n\t\t\t\tif s != nil {\n\t\t\t\t\tt.closeStream(s, true, se.Code, false)\n\t\t\t\t} else {\n\t\t\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\t\t\tstreamID: se.StreamID,\n\t\t\t\t\t\trst:      true,\n\t\t\t\t\t\trstCode:  se.Code,\n\t\t\t\t\t\tonWrite:  func() {},\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err == io.EOF || err == io.ErrUnexpectedEOF || errors.Is(err, netpoll.ErrEOF) {\n\t\t\t\tt.Close()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams failed to read frame: %v\", err)\n\t\t\tt.Close()\n\t\t\treturn\n\t\t}\n\t\tswitch frame := frame.(type) {\n\t\tcase *grpcframe.MetaHeadersFrame:\n\t\t\tif t.operateHeaders(frame, handle, traceCtx) {\n\t\t\t\tt.Close()\n\t\t\t\tbreak\n\t\t\t}\n\t\tcase *grpcframe.DataFrame:\n\t\t\tt.handleData(frame)\n\t\tcase *http2.RSTStreamFrame:\n\t\t\tt.handleRSTStream(frame)\n\t\tcase *grpcframe.SettingsFrame:\n\t\t\tt.handleSettings(frame)\n\t\tcase *http2.PingFrame:\n\t\t\tt.handlePing(frame)\n\t\tcase *http2.WindowUpdateFrame:\n\t\t\tt.handleWindowUpdate(frame)\n\t\tcase *grpcframe.GoAwayFrame:\n\t\t\t// TODO: Handle GoAway from the client appropriately.\n\t\tdefault:\n\t\t\tklog.CtxErrorf(t.ctx, \"transport: http2Server.HandleStreams found unhandled frame type %v.\", frame)\n\t\t}\n\t\tt.framer.reader.Release()\n\t}\n}\n\nfunc (t *http2Server) getStream(f http2.Frame) (*Stream, bool) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tif t.activeStreams == nil {\n\t\t// The transport is closing.\n\t\treturn nil, false\n\t}\n\ts, ok := t.activeStreams[f.Header().StreamID]\n\tif !ok {\n\t\t// The stream is already done.\n\t\treturn nil, false\n\t}\n\treturn s, true\n}\n\n// adjustWindow sends out extra window update over the initial window size\n// of stream if the application is requesting data larger in size than\n// the window.\nfunc (t *http2Server) adjustWindow(s *Stream, n uint32) {\n\tif w := s.fc.maybeAdjust(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{streamID: s.id, increment: w})\n\t}\n}\n\n// updateFlowControl updates the incoming flow control windows\n// for the transport and the stream based on the current bdp\n// estimation.\nfunc (t *http2Server) updateFlowControl(n uint32) {\n\tt.mu.Lock()\n\tfor _, s := range t.activeStreams {\n\t\ts.fc.newLimit(n)\n\t}\n\tt.initialWindowSize = int32(n)\n\tt.mu.Unlock()\n\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\tstreamID:  0,\n\t\tincrement: t.fc.newLimit(n),\n\t})\n\tt.controlBuf.put(&outgoingSettings{\n\t\tss: []http2.Setting{\n\t\t\t{\n\t\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\t\tVal: n,\n\t\t\t},\n\t\t},\n\t})\n}\n\n// updateWindow adjusts the inbound quota for the stream and the transport.\n// Window updates will deliver to the controller for sending when\n// the cumulative quota exceeds the corresponding threshold.\nfunc (t *http2Server) updateWindow(s *Stream, n uint32) {\n\tif w := s.fc.onRead(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  s.id,\n\t\t\tincrement: w,\n\t\t})\n\t}\n}\n\nfunc (t *http2Server) handleData(f *grpcframe.DataFrame) {\n\tsize := f.Header().Length\n\tvar sendBDPPing bool\n\tif t.bdpEst != nil {\n\t\tsendBDPPing = t.bdpEst.add(size)\n\t}\n\t// Decouple connection's flow control from application's read.\n\t// An update on connection's flow control should not depend on\n\t// whether user application has read the data or not. Such a\n\t// restriction is already imposed on the stream's flow control,\n\t// and therefore the sender will be blocked anyways.\n\t// Decoupling the connection flow control will prevent other\n\t// active(fast) streams from starving in presence of slow or\n\t// inactive streams.\n\tif w := t.fc.onData(size); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  0,\n\t\t\tincrement: w,\n\t\t})\n\t}\n\tif sendBDPPing {\n\t\t// Avoid excessive ping detection (e.g. in an L7 proxy)\n\t\t// by sending a window update prior to the BDP ping.\n\t\tif w := t.fc.reset(); w > 0 {\n\t\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\t\tstreamID:  0,\n\t\t\t\tincrement: w,\n\t\t\t})\n\t\t}\n\t\tt.controlBuf.put(bdpPing)\n\t}\n\t// Select the right stream to dispatch.\n\ts, ok := t.getStream(f)\n\tif !ok {\n\t\treturn\n\t}\n\tif size > 0 {\n\t\tif err := s.fc.onData(size); err != nil {\n\t\t\tt.closeStream(s, true, http2.ErrCodeFlowControl, false)\n\t\t\treturn\n\t\t}\n\t\tif f.Header().Flags.Has(http2.FlagDataPadded) {\n\t\t\tif w := s.fc.onRead(size - uint32(len(f.Data()))); w > 0 {\n\t\t\t\tt.controlBuf.put(&outgoingWindowUpdate{s.id, w})\n\t\t\t}\n\t\t}\n\t\t// TODO(bradfitz, zhaoq): A copy is required here because there is no\n\t\t// guarantee f.Data() is consumed before the arrival of next frame.\n\t\t// Can this copy be eliminated?\n\t\tif len(f.Data()) > 0 {\n\t\t\tbuffer := t.bufferPool.get()\n\t\t\tbuffer.Reset()\n\t\t\tbuffer.Write(f.Data())\n\t\t\ts.write(recvMsg{buffer: buffer})\n\t\t}\n\t}\n\tif f.Header().Flags.Has(http2.FlagDataEndStream) {\n\t\t// Received the end of stream from the client.\n\t\ts.compareAndSwapState(streamActive, streamReadDone)\n\t\ts.write(recvMsg{err: io.EOF})\n\t}\n}\n\nfunc (t *http2Server) handleRSTStream(f *http2.RSTStreamFrame) {\n\t// If the stream is not deleted from the transport's active streams map, then do a regular close stream.\n\tif s, ok := t.getStream(f); ok {\n\t\tt.closeStream(s, false, 0, false)\n\t\treturn\n\t}\n\t// If the stream is already deleted from the active streams map, then put a cleanupStream item into controlbuf to delete the stream from loopy writer's established streams map.\n\tt.controlBuf.put(&cleanupStream{\n\t\tstreamID: f.Header().StreamID,\n\t\trst:      false,\n\t\trstCode:  0,\n\t\tonWrite:  func() {},\n\t})\n}\n\nfunc (t *http2Server) handleSettings(f *grpcframe.SettingsFrame) {\n\tif f.IsAck() {\n\t\treturn\n\t}\n\tvar ss []http2.Setting\n\tvar updateFuncs []func()\n\tf.ForeachSetting(func(s http2.Setting) error {\n\t\tswitch s.ID {\n\t\tcase http2.SettingMaxHeaderListSize:\n\t\t\tupdateFuncs = append(updateFuncs, func() {\n\t\t\t\tt.maxSendHeaderListSize = new(uint32)\n\t\t\t\t*t.maxSendHeaderListSize = s.Val\n\t\t\t})\n\t\tdefault:\n\t\t\tss = append(ss, s)\n\t\t}\n\t\treturn nil\n\t})\n\tt.controlBuf.executeAndPut(func(interface{}) bool {\n\t\tfor _, f := range updateFuncs {\n\t\t\tf()\n\t\t}\n\t\treturn true\n\t}, &incomingSettings{\n\t\tss: ss,\n\t})\n}\n\nconst (\n\tmaxPingStrikes\n```\n# Function Signature: func (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n# Natural Language Annotations: // finishStream closes the stream and puts the trailing headerFrame into controlbuf.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// finishStream closes the stream and puts the trailing headerFrame into controlbuf.\nfunc (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n\t// Lock the server to avoid any concurrent modifications to active streams.\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\n\t// Remove the stream from active streams map.\n\tdelete(t.activeStreams, s.id)\n\n\t// Handle the end of stream logic. If EOS was received, mark the stream as read done.\n\tif eosReceived {\n\t\ts.compareAndSwapState(streamActive, streamReadDone)\n\t}\n\n\t// Add the trailing headerFrame into control buffer.\n\tif hdr != nil {\n\t\tt.controlBuf.put(hdr)\n\t}\n\n\t// If a reset is required, add a cleanup stream with RST code.\n\tif rst {\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      true,\n\t\t\trstCode:  rstCode,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t}\n\t\n\t// When the number of active streams is now zero, mark the connection as idle.\n\tif len(t.activeStreams) == 0 {\n\t\tt.idle = time.Now()\n\t}\n}\n```\n"]}, "kitex-pkg/remote/trans/nphttp2/grpc/http2_server.go-closeStream": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/http2_server.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf/encoding\"\n\n\t\"github.com/cloudwego/netpoll\"\n\t\"golang.org/x/net/http2\"\n\t\"golang.org/x/net/http2/hpack\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/gofunc\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc/grpcframe\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nvar (\n\t// ErrIllegalHeaderWrite indicates that setting header is illegal because of\n\t// the stream's state.\n\tErrIllegalHeaderWrite = errors.New(\"transport: the stream is done or WriteHeader was already called\")\n\t// ErrHeaderListSizeLimitViolation indicates that the header list size is larger\n\t// than the limit set by peer.\n\tErrHeaderListSizeLimitViolation = errors.New(\"transport: trying to send header list size larger than the limit set by peer\")\n)\n\nfunc init() {\n\trand.Seed(time.Now().UnixNano())\n}\n\n// http2Server implements the ServerTransport interface with HTTP2.\ntype http2Server struct {\n\tlastRead    int64\n\tctx         context.Context\n\tdone        chan struct{}\n\tconn        net.Conn\n\tloopy       *loopyWriter\n\treaderDone  chan struct{} // sync point to enable testing.\n\twriterDone  chan struct{} // sync point to enable testing.\n\tremoteAddr  net.Addr\n\tlocalAddr   net.Addr\n\tmaxStreamID uint32 // max stream ID ever seen\n\tframer      *framer\n\t// The max number of concurrent streams.\n\tmaxStreams uint32\n\t// controlBuf delivers all the control related tasks (e.g., window\n\t// updates, reset streams, and various settings) to the controller.\n\tcontrolBuf *controlBuffer\n\tfc         *trInFlow\n\t// Keepalive and max-age parameters for the server.\n\tkp ServerKeepalive\n\t// Keepalive enforcement policy.\n\tkep EnforcementPolicy\n\t// The time instance last ping was received.\n\tlastPingAt time.Time\n\t// Number of times the client has violated keepalive ping policy so far.\n\tpingStrikes uint8\n\t// Flag to signify that number of ping strikes should be reset to 0.\n\t// This is set whenever data or header frames are sent.\n\t// 1 means yes.\n\tresetPingStrikes      uint32 // Accessed atomically.\n\tinitialWindowSize     int32\n\tbdpEst                *bdpEstimator\n\tmaxSendHeaderListSize *uint32\n\n\tmu sync.Mutex // guard the following\n\t// drainChan is initialized when drain(...) is called the first time.\n\t// After which the server writes out the first GoAway(with ID 2^31-1) frame.\n\t// Then an independent goroutine will be launched to later send the second GoAway.\n\t// During this time we don't want to write another first GoAway(with ID 2^31 -1) frame.\n\t// Thus call to drain(...) will be a no-op if drainChan is already initialized since draining is\n\t// already underway.\n\tdrainChan     chan struct{}\n\tstate         transportState\n\tactiveStreams map[uint32]*Stream\n\t// idle is the time instant when the connection went idle.\n\t// This is either the beginning of the connection or when the number of\n\t// RPCs go down to 0.\n\t// When the connection is busy, this value is set to 0.\n\tidle time.Time\n\n\tbufferPool *bufferPool\n}\n\n// newHTTP2Server constructs a ServerTransport based on HTTP2. ConnectionError is\n// returned if something goes wrong.\nfunc newHTTP2Server(ctx context.Context, conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {\n\tmaxHeaderListSize := defaultServerMaxHeaderListSize\n\tif config.MaxHeaderListSize != nil {\n\t\tmaxHeaderListSize = *config.MaxHeaderListSize\n\t}\n\n\tframer := newFramer(conn, config.WriteBufferSize, config.ReadBufferSize, maxHeaderListSize)\n\t// Send initial settings as connection preface to client.\n\tisettings := []http2.Setting{{\n\t\tID:  http2.SettingMaxFrameSize,\n\t\tVal: http2MaxFrameLen,\n\t}}\n\n\t// 0 is permitted in the HTTP2 spec.\n\tmaxStreams := config.MaxStreams\n\tif maxStreams == 0 {\n\t\tmaxStreams = math.MaxUint32\n\t} else {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxConcurrentStreams,\n\t\t\tVal: maxStreams,\n\t\t})\n\t}\n\n\tdynamicWindow := true\n\tiwz := initialWindowSize\n\tif config.InitialWindowSize >= defaultWindowSize {\n\t\tiwz = config.InitialWindowSize\n\t\tdynamicWindow = false\n\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\tVal: iwz,\n\t\t})\n\t}\n\ticwz := initialWindowSize\n\tif config.InitialConnWindowSize >= defaultWindowSize {\n\t\ticwz = config.InitialConnWindowSize\n\t\tdynamicWindow = false\n\t}\n\tif config.MaxHeaderListSize != nil {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxHeaderListSize,\n\t\t\tVal: *config.MaxHeaderListSize,\n\t\t})\n\t}\n\n\tif err := framer.WriteSettings(isettings...); err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t}\n\n\t// Adjust the connection flow control window if needed.\n\tif icwz > defaultWindowSize {\n\t\tif delta := icwz - defaultWindowSize; delta > 0 {\n\t\t\tif err := framer.WriteWindowUpdate(0, delta); err != nil {\n\t\t\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n\tkp := config.KeepaliveParams\n\tif kp.MaxConnectionIdle == 0 {\n\t\tkp.MaxConnectionIdle = defaultMaxConnectionIdle\n\t}\n\tif kp.MaxConnectionAge == 0 {\n\t\tkp.MaxConnectionAge = defaultMaxConnectionAge\n\t}\n\tif kp.MaxConnectionAgeGrace == 0 {\n\t\tkp.MaxConnectionAgeGrace = defaultMaxConnectionAgeGrace\n\t}\n\tif kp.Time == 0 {\n\t\tkp.Time = defaultServerKeepaliveTime\n\t}\n\tif kp.Timeout == 0 {\n\t\tkp.Timeout = defaultServerKeepaliveTimeout\n\t}\n\tkep := config.KeepaliveEnforcementPolicy\n\tif kep.MinTime == 0 {\n\t\tkep.MinTime = defaultKeepalivePolicyMinTime\n\t}\n\n\tdone := make(chan struct{})\n\tt := &http2Server{\n\t\tctx:               ctx,\n\t\tdone:              done,\n\t\tconn:              conn,\n\t\tremoteAddr:        conn.RemoteAddr(),\n\t\tlocalAddr:         conn.LocalAddr(),\n\t\tframer:            framer,\n\t\treaderDone:        make(chan struct{}),\n\t\twriterDone:        make(chan struct{}),\n\t\tmaxStreams:        math.MaxUint32,\n\t\tfc:                &trInFlow{limit: icwz},\n\t\tstate:             reachable,\n\t\tactiveStreams:     make(map[uint32]*Stream),\n\t\tkp:                kp,\n\t\tkep:               kep,\n\t\tidle:              time.Now(),\n\t\tinitialWindowSize: int32(iwz),\n\t\tbufferPool:        newBufferPool(),\n\t}\n\tt.controlBuf = newControlBuffer(t.done)\n\tif dynamicWindow {\n\t\tt.bdpEst = &bdpEstimator{\n\t\t\tbdp:               initialWindowSize,\n\t\t\tupdateFlowControl: t.updateFlowControl,\n\t\t}\n\t}\n\n\tt.framer.writer.Flush()\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tt.Close()\n\t\t}\n\t}()\n\n\t// Check the validity of client preface.\n\tpreface := make([]byte, len(ClientPreface))\n\tif _, err := io.ReadFull(t.conn, preface); err != nil {\n\t\t// In deployments where a gRPC server runs behind a cloud load balancer\n\t\t// which performs regular TCP level health checks, the connection is\n\t\t// closed immediately by the latter.  Returning io.EOF here allows the\n\t\t// grpc server implementation to recognize this scenario and suppress\n\t\t// logging to reduce spam.\n\t\tif err == io.EOF {\n\t\t\treturn nil, io.EOF\n\t\t}\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to receive the preface from client: %v\", err)\n\t}\n\tif !bytes.Equal(preface, ClientPreface) {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams received bogus greeting from client: %q\", preface)\n\t}\n\n\tframe, err := t.framer.ReadFrame()\n\tif err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\treturn nil, err\n\t}\n\tif err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to read initial settings frame: %v\", err)\n\t}\n\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\tsf, ok := frame.(*grpcframe.SettingsFrame)\n\tif !ok {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams saw invalid preface type %T from client\", frame)\n\t}\n\tt.handleSettings(sf)\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, func() {\n\t\tt.loopy = newLoopyWriter(serverSide, t.framer, t.controlBuf, t.bdpEst)\n\t\tt.loopy.ssGoAwayHandler = t.outgoingGoAwayHandler\n\t\tif err := t.loopy.run(conn.RemoteAddr().String()); err != nil {\n\t\t\tklog.CtxErrorf(ctx, \"KITEX: grpc server loopyWriter.run returning, error=%v\", err)\n\t\t}\n\t\tt.conn.Close()\n\t\tclose(t.writerDone)\n\t}, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, t.keepalive, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\treturn t, nil\n}\n\n// operateHeader takes action on the decoded headers.\nfunc (t *http2Server) operateHeaders(frame *grpcframe.MetaHeadersFrame, handle func(*Stream), traceCtx func(context.Context, string) context.Context) (fatal bool) {\n\tstreamID := frame.Header().StreamID\n\tstate := &decodeState{\n\t\tserverSide: true,\n\t}\n\tif err := state.decodeHeader(frame); err != nil {\n\t\tif se, ok := status.FromError(err); ok {\n\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\tstreamID: streamID,\n\t\t\t\trst:      true,\n\t\t\t\trstCode:  statusCodeConvTab[se.Code()],\n\t\t\t\tonWrite:  func() {},\n\t\t\t})\n\t\t}\n\t\treturn false\n\t}\n\n\tbuf := newRecvBuffer()\n\ts := &Stream{\n\t\tid:             streamID,\n\t\tst:             t,\n\t\tbuf:            buf,\n\t\tfc:             &inFlow{limit: uint32(t.initialWindowSize)},\n\t\trecvCompress:   state.data.encoding,\n\t\tsendCompress:   state.data.acceptEncoding,\n\t\tmethod:         state.data.method,\n\t\tcontentSubtype: state.data.contentSubtype,\n\t}\n\tif frame.StreamEnded() {\n\t\t// s is just created by the caller. No lock needed.\n\t\ts.state = streamReadDone\n\t}\n\tif state.data.timeoutSet {\n\t\ts.ctx, s.cancel = context.WithTimeout(t.ctx, state.data.timeout)\n\t} else {\n\t\ts.ctx, s.cancel = context.WithCancel(t.ctx)\n\t}\n\t// Attach the received metadata to the context.\n\tif len(state.data.mdata) > 0 {\n\t\ts.ctx = metadata.NewIncomingContext(s.ctx, state.data.mdata)\n\t}\n\n\tt.mu.Lock()\n\tif t.state != reachable {\n\t\tt.mu.Unlock()\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif uint32(len(t.activeStreams)) >= t.maxStreams {\n\t\tt.mu.Unlock()\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: streamID,\n\t\t\trst:      true,\n\t\t\trstCode:  http2.ErrCodeRefusedStream,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif streamID%2 != 1 || streamID <= t.maxStreamID {\n\t\tt.mu.Unlock()\n\t\t// illegal gRPC stream id.\n\t\tklog.CtxErrorf(s.ctx, \"transport: http2Server.HandleStreams received an illegal stream id: %v\", streamID)\n\t\ts.cancel()\n\t\treturn true\n\t}\n\tt.maxStreamID = streamID\n\tt.activeStreams[streamID] = s\n\tif len(t.activeStreams) == 1 {\n\t\tt.idle = time.Time{}\n\t}\n\tt.mu.Unlock()\n\ts.requestRead = func(n int) {\n\t\tt.adjustWindow(s, uint32(n))\n\t}\n\ts.ctx = traceCtx(s.ctx, s.method)\n\ts.ctxDone = s.ctx.Done()\n\ts.wq = newWriteQuota(defaultWriteQuota, s.ctxDone)\n\ts.trReader = &transportReader{\n\t\treader: &recvBufferReader{\n\t\t\tctx:        s.ctx,\n\t\t\tctxDone:    s.ctxDone,\n\t\t\trecv:       s.buf,\n\t\t\tfreeBuffer: t.bufferPool.put,\n\t\t},\n\t\twindowHandler: func(n int) {\n\t\t\tt.updateWindow(s, uint32(n))\n\t\t},\n\t}\n\t// Register the stream with loopy.\n\tt.controlBuf.put(&registerStream{\n\t\tstreamID: s.id,\n\t\twq:       s.wq,\n\t})\n\thandle(s)\n\treturn false\n}\n\n// HandleStreams receives incoming streams using the given handler. This is\n// typically run in a separate goroutine.\n// traceCtx attaches trace to ctx and returns the new context.\nfunc (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) {\n\tdefer close(t.readerDone)\n\tfor {\n\t\tt.controlBuf.throttle()\n\t\tframe, err := t.framer.ReadFrame()\n\t\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\t\tif err != nil {\n\t\t\tif se, ok := err.(http2.StreamError); ok {\n\t\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams encountered http2.StreamError: %v\", se)\n\t\t\t\tt.mu.Lock()\n\t\t\t\ts := t.activeStreams[se.StreamID]\n\t\t\t\tt.mu.Unlock()\n\t\t\t\tif s != nil {\n\t\t\t\t\tt.closeStream(s, true, se.Code, false)\n\t\t\t\t} else {\n\t\t\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\t\t\tstreamID: se.StreamID,\n\t\t\t\t\t\trst:      true,\n\t\t\t\t\t\trstCode:  se.Code,\n\t\t\t\t\t\tonWrite:  func() {},\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err == io.EOF || err == io.ErrUnexpectedEOF || errors.Is(err, netpoll.ErrEOF) {\n\t\t\t\tt.Close()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams failed to read frame: %v\", err)\n\t\t\tt.Close()\n\t\t\treturn\n\t\t}\n\t\tswitch frame := frame.(type) {\n\t\tcase *grpcframe.MetaHeadersFrame:\n\t\t\tif t.operateHeaders(frame, handle, traceCtx) {\n\t\t\t\tt.Close()\n\t\t\t\tbreak\n\t\t\t}\n\t\tcase *grpcframe.DataFrame:\n\t\t\tt.handleData(frame)\n\t\tcase *http2.RSTStreamFrame:\n\t\t\tt.handleRSTStream(frame)\n\t\tcase *grpcframe.SettingsFrame:\n\t\t\tt.handleSettings(frame)\n\t\tcase *http2.PingFrame:\n\t\t\tt.handlePing(frame)\n\t\tcase *http2.WindowUpdateFrame:\n\t\t\tt.handleWindowUpdate(frame)\n\t\tcase *grpcframe.GoAwayFrame:\n\t\t\t// TODO: Handle GoAway from the client appropriately.\n\t\tdefault:\n\t\t\tklog.CtxErrorf(t.ctx, \"transport: http2Server.HandleStreams found unhandled frame type %v.\", frame)\n\t\t}\n\t\tt.framer.reader.Release()\n\t}\n}\n\nfunc (t *http2Server) getStream(f http2.Frame) (*Stream, bool) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tif t.activeStreams == nil {\n\t\t// The transport is closing.\n\t\treturn nil, false\n\t}\n\ts, ok := t.activeStreams[f.Header().StreamID]\n\tif !ok {\n\t\t// The stream is already done.\n\t\treturn nil, false\n\t}\n\treturn s, true\n}\n\n// adjustWindow sends out extra window update over the initial window size\n// of stream if the application is requesting data larger in size than\n// the window.\nfunc (t *http2Server) adjustWindow(s *Stream, n uint32) {\n\tif w := s.fc.maybeAdjust(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{streamID: s.id, increment: w})\n\t}\n}\n\n// updateFlowControl updates the incoming flow control windows\n// for the transport and the stream based on the current bdp\n// estimation.\nfunc (t *http2Server) updateFlowControl(n uint32) {\n\tt.mu.Lock()\n\tfor _, s := range t.activeStreams {\n\t\ts.fc.newLimit(n)\n\t}\n\tt.initialWindowSize = int32(n)\n\tt.mu.Unlock()\n\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\tstreamID:  0,\n\t\tincrement: t.fc.newLimit(n),\n\t})\n\tt.controlBuf.put(&outgoingSettings{\n\t\tss: []http2.Setting{\n\t\t\t{\n\t\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\t\tVal: n,\n\t\t\t},\n\t\t},\n\t})\n}\n\n// updateWindow adjusts the inbound quota for the stream and the transport.\n// Window updates will deliver to the controller for sending when\n// the cumulative quota exceeds the corresponding threshold.\nfunc (t *http2Server) updateWindow(s *Stream, n uint32) {\n\tif w := s.fc.onRead(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  s.id,\n\t\t\tincrement: w,\n\t\t})\n\t}\n}\n\nfunc (t *http2Server) handleData(f *grpcframe.DataFrame) {\n\tsize := f.Header().Length\n\tvar sendBDPPing bool\n\tif t.bdpEst != nil {\n\t\tsendBDPPing = t.bdpEst.add(size)\n\t}\n\t// Decouple connection's flow control from application's read.\n\t// An update on connection's flow control should not depend on\n\t// whether user application has read the data or not. Such a\n\t// restriction is already imposed on the stream's flow control,\n\t// and therefore the sender will be blocked anyways.\n\t// Decoupling the connection flow control will prevent other\n\t// active(fast) streams from starving in presence of slow or\n\t// inactive streams.\n\tif w := t.fc.onData(size); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  0,\n\t\t\tincrement: w,\n\t\t})\n\t}\n\tif sendBDPPing {\n\t\t// Avoid excessive ping detection (e.g. in an L7 proxy)\n\t\t// by sending a window update prior to the BDP ping.\n\t\tif w := t.fc.reset(); w > 0 {\n\t\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\t\tstreamID:  0,\n\t\t\t\tincrement: w,\n\t\t\t})\n\t\t}\n\t\tt.controlBuf.put(bdpPing)\n\t}\n\t// Select the right stream to dispatch.\n\ts, ok := t.getStream(f)\n\tif !ok {\n\t\treturn\n\t}\n\tif size > 0 {\n\t\tif err := s.fc.onData(size); err != nil {\n\t\t\tt.closeStream(s, true, http2.ErrCodeFlowControl, false)\n\t\t\treturn\n\t\t}\n\t\tif f.Header().Flags.Has(http2.FlagDataPadded) {\n\t\t\tif w := s.fc.onRead(size - uint32(len(f.Data()))); w > 0 {\n\t\t\t\tt.controlBuf.put(&outgoingWindowUpdate{s.id, w})\n\t\t\t}\n\t\t}\n\t\t// TODO(bradfitz, zhaoq): A copy is required here because there is no\n\t\t// guarantee f.Data() is consumed before the arrival of next frame.\n\t\t// Can this copy be eliminated?\n\t\tif len(f.Data()) > 0 {\n\t\t\tbuffer := t.bufferPool.get()\n\t\t\tbuffer.Reset()\n\t\t\tbuffer.Write(f.Data())\n\t\t\ts.write(recvMsg{buffer: buffer})\n\t\t}\n\t}\n\tif f.Header().Flags.Has(http2.FlagDataEndStream) {\n\t\t// Received the end of stream from the client.\n\t\ts.compareAndSwapState(streamActive, streamReadDone)\n\t\ts.write(recvMsg{err: io.EOF})\n\t}\n}\n\nfunc (t *http2Server) handleRSTStream(f *http2.RSTStreamFrame) {\n\t// If the stream is not deleted from the transport's active streams map, then do a regular close stream.\n\tif s, ok := t.getStream(f); ok {\n\t\tt.closeStream(s, false, 0, false)\n\t\treturn\n\t}\n\t// If the stream is already deleted from the active streams map, then put a cleanupStream item into controlbuf to delete the stream from loopy writer's established streams map.\n\tt.controlBuf.put(&cleanupStream{\n\t\tstreamID: f.Header().StreamID,\n\t\trst:      false,\n\t\trstCode:  0,\n\t\tonWrite:  func() {},\n\t})\n}\n\nfunc (t *http2Server) handleSettings(f *grpcframe.SettingsFrame) {\n\tif f.IsAck() {\n\t\treturn\n\t}\n\tvar ss []http2.Setting\n\tvar updateFuncs []func()\n\tf.ForeachSetting(func(s http2.Setting) error {\n\t\tswitch s.ID {\n\t\tcase http2.SettingMaxHeaderListSize:\n\t\t\tupdateFuncs = append(updateFuncs, func() {\n\t\t\t\tt.maxSendHeaderListSize = new(uint32)\n\t\t\t\t*t.maxSendHeaderListSize = s.Val\n\t\t\t})\n\t\tdefault:\n\t\t\tss = append(ss, s)\n\t\t}\n\t\treturn nil\n\t})\n\tt.controlBuf.executeAndPut(func(interface{}) bool {\n\t\tfor _, f := range updateFuncs {\n\t\t\tf()\n\t\t}\n\t\treturn true\n\t}, &incomingSettings{\n\t\tss: ss,\n\t})\n}\n\nconst (\n\tmaxPingStrikes\n```\n# Function Signature: func (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n# Natural Language Annotations: // closeStream clears the footprint of a stream when the stream is not needed any more.\nPlease return the generated code surrounded by ```\n", "response": ["To implement the `closeStream` method based on the function signature and natural language annotations, we need to clear the footprint of a stream when it is no longer needed. This involves updating internal data structures and possibly sending HTTP/2 reset frames if required. Here's a possible implementation:\n\n```go\n// closeStream clears the footprint of a stream when the stream is not needed any more.\nfunc (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n\t// Lock the mutex to safely update the server's state.\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\n\t// If the stream is not in the active streams map, just return.\n\tif _, ok := t.activeStreams[s.id]; !ok {\n\t\treturn\n\t}\n\n\t// Remove the stream from activeStreams\n\tdelete(t.activeStreams, s.id)\n\tif len(t.activeStreams) == 0 {\n\t\t// If there are no active streams, reset the idle time.\n\t\tt.idle = time.Now()\n\t}\n\n\t// If reset is required, put a resetStream item into the controlBuf\n\tif rst {\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      true,\n\t\t\trstCode:  rstCode,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t}\n\n\t// Signal any pending requests on the stream\n\ts.cancel()\n\n\t// Update flow control, if necessary\n\tif eosReceived {\n\t\t// Adjust window update to prevent unnecessary flow control updates\n\t\tif w := t.fc.onData(0); w > 0 {\n\t\t\tt.controlBuf.put(&outgoingWindowUpdate{streamID: 0, increment: w})\n\t\t}\n\t}\n}\n```\n\nThis implementation involves several key steps:\n1. Acquiring a mutex lock to ensure safe modification of the server's state.\n2. Removing the stream from the active streams map.\n3. Checking whether a reset frame is required and, if so, issuing it.\n4. Cancelling any pending operations on the stream's context.\n5. Handling potential flow control updates."]}, "kitex-pkg/remote/trans/nphttp2/grpc/transport.go-TLSConfig": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/transport.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package grpc defines and implements message oriented communication\n// channel to complete various transactions (e.g., an RPC).  It is meant for\n// grpc-internal usage and is not intended to be imported directly by users.\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n)\n\ntype bufferPool struct {\n\tpool sync.Pool\n}\n\nfunc newBufferPool() *bufferPool {\n\treturn &bufferPool{\n\t\tpool: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\treturn new(bytes.Buffer)\n\t\t\t},\n\t\t},\n\t}\n}\n\nfunc (p *bufferPool) get() *bytes.Buffer {\n\treturn p.pool.Get().(*bytes.Buffer)\n}\n\nfunc (p *bufferPool) put(b *bytes.Buffer) {\n\tp.pool.Put(b)\n}\n\n// recvMsg represents the received msg from the transport. All transport\n// protocol specific info has been removed.\ntype recvMsg struct {\n\tbuffer *bytes.Buffer\n\t// nil: received some data\n\t// io.EOF: stream is completed. data is nil.\n\t// other non-nil error: transport failure. data is nil.\n\terr error\n}\n\n// recvBuffer is an unbounded channel of recvMsg structs.\n//\n// Note: recvBuffer differs from buffer.Unbounded only in the fact that it\n// holds a channel of recvMsg structs instead of objects implementing \"item\"\n// interface. recvBuffer is written to much more often and using strict recvMsg\n// structs helps avoid allocation in \"recvBuffer.put\"\ntype recvBuffer struct {\n\tc       chan recvMsg\n\tmu      sync.Mutex\n\tbacklog []recvMsg\n\terr     error\n}\n\nfunc newRecvBuffer() *recvBuffer {\n\tb := &recvBuffer{\n\t\tc: make(chan recvMsg, 1),\n\t}\n\treturn b\n}\n\nfunc (b *recvBuffer) put(r recvMsg) {\n\tb.mu.Lock()\n\tif b.err != nil {\n\t\tb.mu.Unlock()\n\t\t// An error had occurred earlier, don't accept more\n\t\t// data or errors.\n\t\treturn\n\t}\n\tb.err = r.err\n\tif len(b.backlog) == 0 {\n\t\tselect {\n\t\tcase b.c <- r:\n\t\t\tb.mu.Unlock()\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\t}\n\tb.backlog = append(b.backlog, r)\n\tb.mu.Unlock()\n}\n\nfunc (b *recvBuffer) load() {\n\tb.mu.Lock()\n\tif len(b.backlog) > 0 {\n\t\tselect {\n\t\tcase b.c <- b.backlog[0]:\n\t\t\tb.backlog[0] = recvMsg{}\n\t\t\tb.backlog = b.backlog[1:]\n\t\tdefault:\n\t\t}\n\t}\n\tb.mu.Unlock()\n}\n\n// get returns the channel that receives a recvMsg in the buffer.\n//\n// Upon receipt of a recvMsg, the caller should call load to send another\n// recvMsg onto the channel if there is any.\nfunc (b *recvBuffer) get() <-chan recvMsg {\n\treturn b.c\n}\n\n// recvBufferReader implements io.Reader interface to read the data from\n// recvBuffer.\ntype recvBufferReader struct {\n\tcloseStream func(error) // Closes the client transport stream with the given error and nil trailer metadata.\n\tctx         context.Context\n\tctxDone     <-chan struct{} // cache of ctx.Done() (for performance).\n\trecv        *recvBuffer\n\tlast        *bytes.Buffer // Stores the remaining data in the previous calls.\n\terr         error\n\tfreeBuffer  func(*bytes.Buffer)\n}\n\n// Read reads the next len(p) bytes from last. If last is drained, it tries to\n// read additional data from recv. It blocks if there no additional data available\n// in recv. If Read returns any non-nil error, it will continue to return that error.\nfunc (r *recvBufferReader) Read(p []byte) (n int, err error) {\n\tif r.err != nil {\n\t\treturn 0, r.err\n\t}\n\tif r.last != nil {\n\t\t// Read remaining data left in last call.\n\t\tcopied, _ := r.last.Read(p)\n\t\tif r.last.Len() == 0 {\n\t\t\tr.freeBuffer(r.last)\n\t\t\tr.last = nil\n\t\t}\n\t\treturn copied, nil\n\t}\n\tif r.closeStream != nil {\n\t\tn, r.err = r.readClient(p)\n\t} else {\n\t\tn, r.err = r.read(p)\n\t}\n\treturn n, r.err\n}\n\nfunc (r *recvBufferReader) read(p []byte) (n int, err error) {\n\tselect {\n\tcase <-r.ctxDone:\n\t\treturn 0, ContextErr(r.ctx.Err())\n\tcase m := <-r.recv.get():\n\t\treturn r.readAdditional(m, p)\n\t}\n}\n\nfunc (r *recvBufferReader) readClient(p []byte) (n int, err error) {\n\t// If the context is canceled, then closes the stream with nil metadata.\n\t// closeStream writes its error parameter to r.recv as a recvMsg.\n\t// r.readAdditional acts on that message and returns the necessary error.\n\tselect {\n\tcase <-r.ctxDone:\n\t\t// Note that this adds the ctx error to the end of recv buffer, and\n\t\t// reads from the head. This will delay the error until recv buffer is\n\t\t// empty, thus will delay ctx cancellation in Recv().\n\t\t//\n\t\t// It's done this way to fix a race between ctx cancel and trailer. The\n\t\t// race was, stream.Recv() may return ctx error if ctxDone wins the\n\t\t// race, but stream.Trailer() may return a non-nil md because the stream\n\t\t// was not marked as done when trailer is received. This closeStream\n\t\t// call will mark stream as done, thus fix the race.\n\t\t//\n\t\t// TODO: delaying ctx error seems like a unnecessary side effect. What\n\t\t// we really want is to mark the stream as done, and return ctx error\n\t\t// faster.\n\t\tr.closeStream(ContextErr(r.ctx.Err()))\n\t\tm := <-r.recv.get()\n\t\treturn r.readAdditional(m, p)\n\tcase m := <-r.recv.get():\n\t\treturn r.readAdditional(m, p)\n\t}\n}\n\nfunc (r *recvBufferReader) readAdditional(m recvMsg, p []byte) (n int, err error) {\n\tr.recv.load()\n\tif m.err != nil {\n\t\treturn 0, m.err\n\t}\n\tcopied, _ := m.buffer.Read(p)\n\tif m.buffer.Len() == 0 {\n\t\tr.freeBuffer(m.buffer)\n\t\tr.last = nil\n\t} else {\n\t\tr.last = m.buffer\n\t}\n\treturn copied, nil\n}\n\ntype streamState uint32\n\nconst (\n\tstreamActive    streamState = iota\n\tstreamWriteDone             // EndStream sent\n\tstreamReadDone              // EndStream received\n\tstreamDone                  // the entire stream is finished.\n)\n\n// Stream represents an RPC in the transport layer.\ntype Stream struct {\n\tid           uint32\n\tst           ServerTransport    // nil for client side Stream\n\tct           *http2Client       // nil for server side Stream\n\tctx          context.Context    // the associated context of the stream\n\tcancel       context.CancelFunc // always nil for client side Stream\n\tdone         chan struct{}      // closed at the end of stream to unblock writers. On the client side.\n\tctxDone      <-chan struct{}    // same as done chan but for server side. Cache of ctx.Done() (for performance)\n\tmethod       string             // the associated RPC method of the stream\n\trecvCompress string\n\tsendCompress string\n\tbuf          *recvBuffer\n\ttrReader     io.Reader\n\tfc           *inFlow\n\twq           *writeQuota\n\n\t// Callback to state application's intentions to read data. This\n\t// is used to adjust flow control, if needed.\n\trequestRead func(int)\n\n\theaderChan       chan struct{} // closed to indicate the end of header metadata.\n\theaderChanClosed uint32        // set when headerChan is closed. Used to avoid closing headerChan multiple times.\n\t// headerValid indicates whether a valid header was received.  Only\n\t// meaningful after headerChan is closed (always call waitOnHeader() before\n\t// reading its value).  Not valid on server side.\n\theaderValid bool\n\n\t// hdrMu protects header and trailer metadata on the server-side.\n\thdrMu sync.Mutex\n\t// On client side, header keeps the received header metadata.\n\t//\n\t// On server side, header keeps the header set by SetHeader(). The complete\n\t// header will merged into this after t.WriteHeader() is called.\n\theader  metadata.MD\n\ttrailer metadata.MD // the key-value map of trailer metadata.\n\n\tnoHeaders bool // set if the client never received headers (set only after the stream is done).\n\n\t// On the server-side, headerSent is atomically set to 1 when the headers are sent out.\n\theaderSent uint32\n\n\tstate streamState\n\n\t// On client-side it is the status error received from the server.\n\t// On server-side it is unused.\n\tstatus       *status.Status\n\tbizStatusErr kerrors.BizStatusErrorIface\n\n\tbytesReceived uint32 // indicates whether any bytes have been received on this stream\n\tunprocessed   uint32 // set if the server sends a refused stream or GOAWAY including this stream\n\n\t// contentSubtype is the content-subtype for requests.\n\t// this must be lowercase or the behavior is undefined.\n\tcontentSubtype string\n}\n\n// isHeaderSent is only valid on the server-side.\nfunc (s *Stream) isHeaderSent() bool {\n\treturn atomic.LoadUint32(&s.headerSent) == 1\n}\n\n// updateHeaderSent updates headerSent and returns true\n// if it was already set. It is valid only on server-side.\nfunc (s *Stream) updateHeaderSent() bool {\n\treturn atomic.SwapUint32(&s.headerSent, 1) == 1\n}\n\nfunc (s *Stream) swapState(st streamState) streamState {\n\treturn streamState(atomic.SwapUint32((*uint32)(&s.state), uint32(st)))\n}\n\nfunc (s *Stream) compareAndSwapState(oldState, newState streamState) bool {\n\treturn atomic.CompareAndSwapUint32((*uint32)(&s.state), uint32(oldState), uint32(newState))\n}\n\nfunc (s *Stream) getState() streamState {\n\treturn streamState(atomic.LoadUint32((*uint32)(&s.state)))\n}\n\nfunc (s *Stream) waitOnHeader() {\n\tif s.headerChan == nil {\n\t\t// On the server headerChan is always nil since a stream originates\n\t\t// only after having received headers.\n\t\treturn\n\t}\n\tselect {\n\tcase <-s.ctx.Done():\n\t\t// Close the stream to prevent headers/trailers from changing after\n\t\t// this function returns.\n\t\ts.ct.CloseStream(s, ContextErr(s.ctx.Err()))\n\t\t// headerChan could possibly not be closed yet if closeStream raced\n\t\t// with operateHeaders; wait until it is closed explicitly here.\n\t\t<-s.headerChan\n\tcase <-s.headerChan:\n\t}\n}\n\n// RecvCompress returns the compression algorithm applied to the inbound\n// message. It is empty string if there is no compression applied.\nfunc (s *Stream) RecvCompress() string {\n\ts.waitOnHeader()\n\treturn s.recvCompress\n}\n\n// SendCompress returns the compression algorithm applied to the outbound\n// message. It is empty string if there is no compression applied.\nfunc (s *Stream) SendCompress() string {\n\ts.waitOnHeader()\n\treturn s.sendCompress\n}\n\n// SetSendCompress sets the compression algorithm to the stream.\nfunc (s *Stream) SetSendCompress(str string) {\n\ts.sendCompress = str\n}\n\n// Done returns a channel which is closed when it receives the final status\n// from the server.\nfunc (s *Stream) Done() <-chan struct{} {\n\treturn s.done\n}\n\n// Header returns the header metadata of the stream.\n//\n// On client side, it acquires the key-value pairs of header metadata once it is\n// available. It blocks until i) the metadata is ready or ii) there is no header\n// metadata or iii) the stream is canceled/expired.\n//\n// On server side, it returns the out header after t.WriteHeader is called.  It\n// does not block and must not be called until after WriteHeader.\nfunc (s *Stream) Header() (metadata.MD, error) {\n\tif s.headerChan == nil {\n\t\t// On server side, return the header in stream. It will be the out\n\t\t// header after t.WriteHeader is called.\n\t\treturn s.header.Copy(), nil\n\t}\n\ts.waitOnHeader()\n\tif !s.headerValid {\n\t\treturn nil, s.status.Err()\n\t}\n\treturn s.header.Copy(), nil\n}\n\n// TrailersOnly blocks until a header or trailers-only frame is received and\n// then returns true if the stream was trailers-only.  If the stream ends\n// before headers are received, returns true, nil.  Client-side only.\nfunc (s *Stream) TrailersOnly() bool {\n\ts.waitOnHeader()\n\treturn s.noHeaders\n}\n\n// Trailer returns the cached trailer metedata. Note that if it is not called\n// after the entire stream is done, it could return an empty MD. Client\n// side only.\n// It can be safely read only after stream has ended that is either read\n// or write have returned io.EOF.\nfunc (s *Stream) Trailer() metadata.MD {\n\tc := s.trailer.Copy()\n\treturn c\n}\n\n// ContentSubtype returns the content-subtype for a request. For example, a\n// content-subtype of \"proto\" will result in a content-type of\n// \"application/grpc+proto\". This will always be lowercase.  See\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details.\nfunc (s *Stream) ContentSubtype() string {\n\treturn s.contentSubtype\n}\n\n// Context returns the context of the stream.\nfunc (s *Stream) Context() context.Context {\n\treturn s.ctx\n}\n\n// Method returns the method for the stream.\nfunc (s *Stream) Method() string {\n\treturn s.method\n}\n\n// Status returns the status received from the server.\n// Status can be read safely only after the stream has ended,\n// that is, after Done() is closed.\nfunc (s *Stream) Status() *status.Status {\n\treturn s.status\n}\n\nfunc (s *Stream) SetBizStatusErr(bizStatusErr kerrors.BizStatusErrorIface) {\n\ts.bizStatusErr = bizStatusErr\n}\n\nfunc (s *Stream) BizStatusErr() kerrors.BizStatusErrorIface {\n\treturn s.bizStatusErr\n}\n\n// SetHeader sets the header metadata. This can be called multiple times.\n// Server side only.\n// This should not be called in parallel to other data writes.\nfunc (s *Stream) SetHeader(md metadata.MD) error {\n\tif md.Len() == 0 {\n\t\treturn nil\n\t}\n\tif s.isHeaderSent() || s.getState() == streamDone {\n\t\treturn ErrIllegalHeaderWrite\n\t}\n\ts.hdrMu.Lock()\n\ts.header = metadata.AppendMD(s.header, md)\n\ts.hdrMu.Unlock()\n\treturn nil\n}\n\n// SendHeader sends the given header metadata. The given metadata is\n// combined with any metadata set by previous calls to SetHeader and\n// then written to the transport stream.\nfunc (s *Stream) SendHeader(md metadata.MD) error {\n\treturn s.st.WriteHeader(s, md)\n}\n\n// SetTrailer sets the trailer metadata which will be sent with the RPC status\n// by the server. This can be called multiple times. Server side only.\n// This should not be called parallel to other data writes.\nfunc (s *Stream) SetTrailer(md metadata.MD) error {\n\tif md.Len() == 0 {\n\t\treturn nil\n\t}\n\tif s.getState() == streamDone {\n\t\treturn ErrIllegalHeaderWrite\n\t}\n\ts.hdrMu.Lock()\n\ts.trailer = metadata.AppendMD(s.trailer, md)\n\ts.hdrMu.Unlock()\n\treturn nil\n}\n\nfunc (s *Stream) write(m recvMsg) {\n\ts.buf.put(m)\n}\n\n// Read reads all p bytes from the wire for this stream.\nfunc (s *Stream) Read(p []byte) (n int, err error) {\n\t// Don't request a read if there was an error earlier\n\tif er := s.trReader.(*transportReader).er; er != nil {\n\t\treturn 0, er\n\t}\n\ts.requestRead(len(p))\n\treturn io.ReadFull(s.trReader, p)\n}\n\n// StreamWrite only used for unit test\nfunc StreamWrite(s *Stream, buffer *bytes.Buffer) {\n\ts.write(recvMsg{buffer: buffer})\n}\n\n// CreateStream only used for unit test. Create an independent stream out of http2client / http2server\nfunc CreateStream(id uint32, requestRead func(i int)) *Stream {\n\trecvBuffer := newRecvBuffer()\n\ttrReader := &transportReader{\n\t\treader: &recvBufferReader{\n\t\t\trecv: recvBuffer,\n\t\t\tfreeBuffer: func(buffer *bytes.Buffer) {\n\t\t\t\tbuffer.Reset()\n\t\t\t},\n\t\t},\n\t\twindowHandler: func(i int) {},\n\t}\n\n\tstream := &Stream{\n\t\tid:          id,\n\t\tbuf:         recvBuffer,\n\t\ttrReader:    trReader,\n\t\twq:          newWriteQuota(defaultWriteQuota, nil),\n\t\trequestRead: requestRead,\n\t\thdrMu:       sync.Mutex{},\n\t}\n\n\treturn stream\n}\n\n// transportReader reads all the data available for this Stream from the transport and\n// passes them into the decoder, which converts them into a gRPC message stream.\n// The error is io.EOF when the stream is done or another non-nil error if\n// the stream broke.\ntype transportReader struct {\n\treader io.Reader\n\t// The handler to control the window update procedure for both this\n\t// particular stream and the associated transport.\n\twindowHandler func(int)\n\ter            error\n}\n\nfunc (t *transportReader) Read(p []byte) (n int, err error) {\n\tn, err = t.reader.Read(p)\n\tif err != nil {\n\t\tt.er = err\n\t\treturn\n\t}\n\tt.windowHandler(n)\n\treturn\n}\n\n// BytesReceived indicates whether any bytes have been received on this stream.\nfunc (s *Stream) BytesReceived() bool {\n\treturn atomic.LoadUint32(&s.bytesReceived) == 1\n}\n\n// Unprocessed indicates whether the server did not process this stream --\n// i.e. it sent a refused stream or GOAWAY including this stream ID.\nfunc (s *Stream) Unprocessed() bool {\n\treturn atomic.LoadUint32(&s.unprocessed) == 1\n}\n\n// state of transport\ntype transportState int\n\nconst (\n\treachable transportState = iota\n\tclosing\n\tdraining\n)\n\n// ServerConfig consists of all the configurations to establish a server transport.\ntype ServerConfig struct {\n\tMaxStreams                 uint32\n\tKeepaliveParams            ServerKeepalive\n\tKeepaliveEnforcementPolicy EnforcementPolicy\n\tInitialWindowSize          uint32\n\tInitialConnWindowSize      uint32\n\tWriteBufferSize            uint32\n\tReadBufferSize             uint32\n\tMaxHeaderListSize          *uint32\n}\n\nfunc DefaultServerConfig() *ServerConfig {\n\treturn &ServerConfig{\n\t\tWriteBufferSize: defaultWriteBufferSize,\n\t\tReadBufferSize:  defaultReadBufferSize,\n\t}\n}\n\n// ConnectOptions covers all relevant options for communicating with the server.\ntype ConnectOptions struct {\n\t// KeepaliveParams stores the keepalive parameters.\n\tKeepaliveParams ClientKeepalive\n\t// InitialWindowSize sets the initial window size for a stream.\n\tInitialWindowSize uint32\n\t// InitialConnWindowSize sets the initial window size for a connection.\n\tInitialConnWindowSize uint32\n\t// WriteBufferSize sets the size of write buffer which in turn determines how much data can be batched before it's written on the wire.\n\tWriteBufferSize uint32\n\t// ReadBufferSize sets the size of read buffer, which in turn determines how much data can be read at most for one read syscall.\n\tReadBufferSize uint32\n\t// MaxHeaderListSize sets the max (uncompressed) size of header list that is prepared to be received.\n\tMaxHeaderListSize *uint32\n\t// ShortConn indicates whether the connection will be reused from grpc conn pool\n\tShortConn bool\n\t// TLSConfig\n\tTLSConfig *tls.Config\n}\n\n// NewServerTransport creates a ServerTransport with conn or non-nil error\n// if it fails.\nfunc NewServerTransport(ctx context.Context, conn net.Conn, cfg *ServerConfig) (ServerTransport, error) {\n\treturn newHTTP2Server(ctx, conn, cfg)\n}\n\n// NewClientTransport establishes the transport with the required ConnectOptions\n// and returns it to the caller.\nfunc NewClientTransport(ctx context.Context, conn net.Conn, opts ConnectOptions,\n\tremoteService string, onGoAway func(GoAwayReason), onClose func(),\n) (ClientTransport, error) {\n\treturn newHTTP2Client(ctx, conn, opts, remoteService, onGoAway, onClose)\n}\n\n// Options provides additional hints and information for message\n// transmission.\ntype Options struct {\n\t// Last indicates whether this write is the last piece for\n\t// this stream.\n\tLast bool\n}\n\n// CallHdr carries the information of a particular RPC.\ntype CallHdr struct {\n\t// Host specifies the peer's host.\n\tHost string\n\n\t// Method specifies the operation to perform.\n\tMethod string\n\n\t// SendCompress specifies the compression algorithm applied on\n\t// outbound message.\n\tSendCompress string\n\n\t// ContentSubtype specifies the content-subtype for a request. For example, a\n\t// content-subtype of \"proto\" will result in a content-type of\n\t// \"application/grpc+proto\". The value of ContentSubtype must be all\n\t// lowercase, otherwise the behavior is undefined. See\n\t// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\t// for more details.\n\tContentSubtype string\n\n\tPreviousAttempts int // value of grpc-previous-rpc-attempts header to set\n}\n\n// IsActive is the interface that exposing the underlying connection's active status.\ntype IsActive interface {\n\tIsActive() bool\n}\n\n// ClientTransport is the common interface for all gRPC client-side transport\n// implementations.\ntype ClientTransport interface {\n\t// Close tears down this transport. Once it returns, the transport\n\t// should not be accessed any more. The caller must make sure this\n\t// is called only once.\n\tClose() error\n\n\t// GracefulClose starts to tear down\n```\n# Function Signature: func TLSConfig(tlsConfig *tls.Config) *tls.Config {\n# Natural Language Annotations: // TLSConfig checks and supplement the tls config provided by user.\nPlease return the generated code surrounded by ```\n", "response": ["Based on the function signature and natural language annotation provided, here is the implementation of the `TLSConfig` function:\n\n```go\nfunc TLSConfig(tlsConfig *tls.Config) *tls.Config {\n\t// If the user hasn't provided a TLS config, create a default one.\n\tif tlsConfig == nil {\n\t\ttlsConfig = &tls.Config{}\n\t}\n\n\t// Check and set minimum security settings if they are not configured.\n\tif tlsConfig.MinVersion == 0 {\n\t\ttlsConfig.MinVersion = tls.VersionTLS12\n\t}\n\n\t// Configure other common TLS settings if not set.\n\tif tlsConfig.CipherSuites == nil {\n\t\ttlsConfig.CipherSuites = []uint16{\n\t\t\ttls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,\n\t\t\ttls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,\n\t\t\ttls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\n\t\t\ttls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,\n\t\t}\n\t}\n\n\t// If client authentication is not set, default to RequireAndVerifyClientCert.\n\tif tlsConfig.ClientAuth == 0 {\n\t\ttlsConfig.ClientAuth = tls.RequireAndVerifyClientCert\n\t}\n\n\treturn tlsConfig\n}\n```\n\nThis implementation checks if a `tls.Config` object is provided, and if not, it creates a default one. It ensures minimum TLS version and cipher suites are set to maintain security. If client authentication is not specified, it defaults to requiring and verifying client certificates."]}, "kitex-pkg/remote/trans/nphttp2/grpc/http_util.go-encodeTimeout": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/http_util.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\t\"unicode/utf8\"\n\n\t\"golang.org/x/net/http2\"\n\t\"golang.org/x/net/http2/hpack\"\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc/grpcframe\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nconst (\n\t// http2MaxFrameLen specifies the max length of a HTTP2 frame.\n\thttp2MaxFrameLen = 16384 // 16KB frame\n\t// http://http2.github.io/http2-spec/#SettingValues\n\thttp2InitHeaderTableSize = 4096\n\t// baseContentType is the base content-type for gRPC.  This is a valid\n\t// content-type on it's own, but can also include a content-subtype such as\n\t// \"proto\" as a suffix after \"+\" or \";\".  See\n\t// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\t// for more details.\n\tbaseContentType = \"application/grpc\"\n)\n\nvar (\n\t// ClientPreface http2 preface message\n\tClientPreface = []byte(http2.ClientPreface)\n\t// ClientPrefaceLen preface length\n\tClientPrefaceLen = len(ClientPreface)\n\thttp2ErrConvTab  = map[http2.ErrCode]codes.Code{\n\t\thttp2.ErrCodeNo:                 codes.Internal,\n\t\thttp2.ErrCodeProtocol:           codes.Internal,\n\t\thttp2.ErrCodeInternal:           codes.Internal,\n\t\thttp2.ErrCodeFlowControl:        codes.ResourceExhausted,\n\t\thttp2.ErrCodeSettingsTimeout:    codes.Internal,\n\t\thttp2.ErrCodeStreamClosed:       codes.Internal,\n\t\thttp2.ErrCodeFrameSize:          codes.Internal,\n\t\thttp2.ErrCodeRefusedStream:      codes.Unavailable,\n\t\thttp2.ErrCodeCancel:             codes.Canceled,\n\t\thttp2.ErrCodeCompression:        codes.Internal,\n\t\thttp2.ErrCodeConnect:            codes.Internal,\n\t\thttp2.ErrCodeEnhanceYourCalm:    codes.ResourceExhausted,\n\t\thttp2.ErrCodeInadequateSecurity: codes.PermissionDenied,\n\t\thttp2.ErrCodeHTTP11Required:     codes.Internal,\n\t}\n\tstatusCodeConvTab = map[codes.Code]http2.ErrCode{\n\t\tcodes.Internal:          http2.ErrCodeInternal,\n\t\tcodes.Canceled:          http2.ErrCodeCancel,\n\t\tcodes.Unavailable:       http2.ErrCodeRefusedStream,\n\t\tcodes.ResourceExhausted: http2.ErrCodeEnhanceYourCalm,\n\t\tcodes.PermissionDenied:  http2.ErrCodeInadequateSecurity,\n\t}\n\t// HTTPStatusConvTab is the HTTP status code to gRPC error code conversion table.\n\tHTTPStatusConvTab = map[int]codes.Code{\n\t\t// 400 Bad Request - INTERNAL.\n\t\thttp.StatusBadRequest: codes.Internal,\n\t\t// 401 Unauthorized  - UNAUTHENTICATED.\n\t\thttp.StatusUnauthorized: codes.Unauthenticated,\n\t\t// 403 Forbidden - PERMISSION_DENIED.\n\t\thttp.StatusForbidden: codes.PermissionDenied,\n\t\t// 404 Not Found - UNIMPLEMENTED.\n\t\thttp.StatusNotFound: codes.Unimplemented,\n\t\t// 429 Too Many Requests - UNAVAILABLE.\n\t\thttp.StatusTooManyRequests: codes.Unavailable,\n\t\t// 502 Bad Gateway - UNAVAILABLE.\n\t\thttp.StatusBadGateway: codes.Unavailable,\n\t\t// 503 Service Unavailable - UNAVAILABLE.\n\t\thttp.StatusServiceUnavailable: codes.Unavailable,\n\t\t// 504 Gateway timeout - UNAVAILABLE.\n\t\thttp.StatusGatewayTimeout: codes.Unavailable,\n\t}\n)\n\ntype parsedHeaderData struct {\n\tencoding       string\n\tacceptEncoding string\n\t// statusGen caches the stream status received from the trailer the server\n\t// sent.  Client side only.  Do not access directly.  After all trailers are\n\t// parsed, use the status method to retrieve the status.\n\tstatusGen    *status.Status\n\tbizStatusErr kerrors.BizStatusErrorIface\n\t// rawStatusCode and rawStatusMsg are set from the raw trailer fields and are not\n\t// intended for direct access outside of parsing.\n\trawStatusCode  *int\n\trawStatusMsg   string\n\tbizStatusCode  *int\n\tbizStatusExtra map[string]string\n\thttpStatus     *int\n\t// Server side only fields.\n\ttimeoutSet bool\n\ttimeout    time.Duration\n\tmethod     string\n\t// key-value metadata map from the peer.\n\tmdata          map[string][]string\n\tstatsTags      []byte\n\tstatsTrace     []byte\n\tcontentSubtype string\n\n\t// isGRPC field indicates whether the peer is speaking gRPC (otherwise HTTP).\n\t//\n\t// We are in gRPC mode (peer speaking gRPC) if:\n\t// \t* We are client side and have already received a HEADER frame that indicates gRPC peer.\n\t//  * The header contains valid  a content-type, i.e. a string starts with \"application/grpc\"\n\t// And we should handle error specific to gRPC.\n\t//\n\t// Otherwise (i.e. a content-type string starts without \"application/grpc\", or does not exist), we\n\t// are in HTTP fallback mode, and should handle error specific to HTTP.\n\tisGRPC         bool\n\tgrpcErr        error\n\thttpErr        error\n\tcontentTypeErr string\n}\n\n// decodeState configures decoding criteria and records the decoded data.\ntype decodeState struct {\n\t// whether decoding on server side or not\n\tserverSide bool\n\n\t// Records the states during HPACK decoding. It will be filled with info parsed from HTTP HEADERS\n\t// frame once decodeHeader function has been invoked and returned.\n\tdata parsedHeaderData\n}\n\n// isReservedHeader checks whether hdr belongs to HTTP2 headers\n// reserved by gRPC protocol. Any other headers are classified as the\n// user-specified metadata.\nfunc isReservedHeader(hdr string) bool {\n\tif hdr != \"\" && hdr[0] == ':' {\n\t\treturn true\n\t}\n\tswitch hdr {\n\tcase \"content-type\",\n\t\t\"user-agent\",\n\t\t\"grpc-message-type\",\n\t\t\"grpc-encoding\",\n\t\t\"grpc-message\",\n\t\t\"grpc-status\",\n\t\t\"grpc-timeout\",\n\t\t\"grpc-status-details-bin\",\n\t\t// Intentionally exclude grpc-previous-rpc-attempts and\n\t\t// grpc-retry-pushback-ms, which are \"reserved\", but their API\n\t\t// intentionally works via metadata.\n\t\t\"te\":\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// isWhitelistedHeader checks whether hdr should be propagated into metadata\n// visible to users, even though it is classified as \"reserved\", above.\nfunc isWhitelistedHeader(hdr string) bool {\n\tswitch hdr {\n\tcase \":authority\", \"user-agent\":\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// contentSubtype returns the content-subtype for the given content-type.  The\n// given content-type must be a valid content-type that starts with\n// \"application/grpc\". A content-subtype will follow \"application/grpc\" after a\n// \"+\" or \";\". See\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details.\n//\n// If contentType is not a valid content-type for gRPC, the boolean\n// will be false, otherwise true. If content-type == \"application/grpc\",\n// \"application/grpc+\", or \"application/grpc;\", the boolean will be true,\n// but no content-subtype will be returned.\n//\n// contentType is assumed to be lowercase already.\nfunc contentSubtype(contentType string) (string, bool) {\n\tif contentType == baseContentType {\n\t\treturn \"\", true\n\t}\n\tif !strings.HasPrefix(contentType, baseContentType) {\n\t\treturn \"\", false\n\t}\n\t// guaranteed since != baseContentType and has baseContentType prefix\n\tswitch contentType[len(baseContentType)] {\n\tcase '+', ';':\n\t\t// this will return true for \"application/grpc+\" or \"application/grpc;\"\n\t\t// which the previous validContentType function tested to be valid, so we\n\t\t// just say that no content-subtype is specified in this case\n\t\treturn contentType[len(baseContentType)+1:], true\n\tdefault:\n\t\treturn \"\", false\n\t}\n}\n\n// contentSubtype is assumed to be lowercase\nfunc contentType(contentSubtype string) string {\n\tif contentSubtype == \"\" {\n\t\treturn baseContentType\n\t}\n\treturn baseContentType + \"+\" + contentSubtype\n}\n\nfunc (d *decodeState) status() *status.Status {\n\tif d.data.statusGen == nil {\n\t\t// No status-details were provided; generate status using code/msg.\n\t\td.data.statusGen = status.New(codes.Code(safeCastInt32(*(d.data.rawStatusCode))), d.data.rawStatusMsg)\n\t}\n\treturn d.data.statusGen\n}\n\nfunc (d *decodeState) bizStatusErr() kerrors.BizStatusErrorIface {\n\tif d.data.bizStatusErr == nil && d.data.bizStatusCode != nil {\n\t\td.data.bizStatusErr = kerrors.NewGRPCBizStatusErrorWithExtra(\n\t\t\tsafeCastInt32(*(d.data.bizStatusCode)), d.data.rawStatusMsg, d.data.bizStatusExtra)\n\t\tif st, ok := d.data.bizStatusErr.(kerrors.GRPCStatusIface); ok {\n\t\t\tst.SetGRPCStatus(d.status())\n\t\t}\n\t}\n\treturn d.data.bizStatusErr\n}\n\n// safeCastInt32 casts the number from int to int32 in safety.\nfunc safeCastInt32(n int) int32 {\n\tif n > math.MaxInt32 || n < math.MinInt32 {\n\t\tpanic(fmt.Sprintf(\"Cast int to int32 failed, due to overflow, n=%d\", n))\n\t}\n\treturn int32(n)\n}\n\nconst binHdrSuffix = \"-bin\"\n\nfunc encodeBinHeader(v []byte) string {\n\treturn base64.RawStdEncoding.EncodeToString(v)\n}\n\nfunc decodeBinHeader(v string) ([]byte, error) {\n\tif len(v)%4 == 0 {\n\t\t// Input was padded, or padding was not necessary.\n\t\treturn base64.StdEncoding.DecodeString(v)\n\t}\n\treturn base64.RawStdEncoding.DecodeString(v)\n}\n\nfunc encodeMetadataHeader(k, v string) string {\n\tif strings.HasSuffix(k, binHdrSuffix) {\n\t\treturn encodeBinHeader(([]byte)(v))\n\t}\n\treturn v\n}\n\nfunc decodeMetadataHeader(k, v string) (string, error) {\n\tif strings.HasSuffix(k, binHdrSuffix) {\n\t\tb, err := decodeBinHeader(v)\n\t\treturn string(b), err\n\t}\n\treturn v, nil\n}\n\nfunc (d *decodeState) decodeHeader(frame *grpcframe.MetaHeadersFrame) error {\n\t// frame.Truncated is set to true when framer detects that the current header\n\t// list size hits MaxHeaderListSize limit.\n\tif frame.Truncated {\n\t\treturn status.New(codes.Internal, \"peer header list size exceeded limit\").Err()\n\t}\n\n\tfor _, hf := range frame.Fields {\n\t\td.processHeaderField(hf)\n\t}\n\n\tif d.data.isGRPC {\n\t\tif d.data.grpcErr != nil {\n\t\t\treturn d.data.grpcErr\n\t\t}\n\t\tif d.serverSide {\n\t\t\treturn nil\n\t\t}\n\t\tif d.data.rawStatusCode == nil && d.data.statusGen == nil {\n\t\t\t// gRPC status doesn't exist.\n\t\t\t// Set rawStatusCode to be unknown and return nil error.\n\t\t\t// So that, if the stream has ended this Unknown status\n\t\t\t// will be propagated to the user.\n\t\t\t// Otherwise, it will be ignored. In which case, status from\n\t\t\t// a later trailer, that has StreamEnded flag set, is propagated.\n\t\t\tcode := int(codes.Unknown)\n\t\t\td.data.rawStatusCode = &code\n\t\t}\n\t\treturn nil\n\t}\n\n\t// HTTP fallback mode\n\tif d.data.httpErr != nil {\n\t\treturn d.data.httpErr\n\t}\n\n\tvar (\n\t\tcode = codes.Internal // when header does not include HTTP status, return INTERNAL\n\t\tok   bool\n\t)\n\n\tif d.data.httpStatus != nil {\n\t\tcode, ok = HTTPStatusConvTab[*(d.data.httpStatus)]\n\t\tif !ok {\n\t\t\tcode = codes.Unknown\n\t\t}\n\t}\n\n\treturn status.New(code, d.constructHTTPErrMsg()).Err()\n}\n\n// constructErrMsg constructs error message to be returned in HTTP fallback mode.\n// Format: HTTP status code and its corresponding message + content-type error message.\nfunc (d *decodeState) constructHTTPErrMsg() string {\n\tvar errMsgs []string\n\n\tif d.data.httpStatus == nil {\n\t\terrMsgs = append(errMsgs, \"malformed header: missing HTTP status\")\n\t} else {\n\t\terrMsgs = append(errMsgs, fmt.Sprintf(\"%s: HTTP status code %d\", http.StatusText(*(d.data.httpStatus)), *d.data.httpStatus))\n\t}\n\n\tif d.data.contentTypeErr == \"\" {\n\t\terrMsgs = append(errMsgs, \"transport: missing content-type field\")\n\t} else {\n\t\terrMsgs = append(errMsgs, d.data.contentTypeErr)\n\t}\n\n\treturn strings.Join(errMsgs, \"; \")\n}\n\nfunc (d *decodeState) addMetadata(k, v string) {\n\tif d.data.mdata == nil {\n\t\td.data.mdata = make(map[string][]string)\n\t}\n\td.data.mdata[k] = append(d.data.mdata[k], v)\n}\n\nfunc (d *decodeState) processHeaderField(f hpack.HeaderField) {\n\tswitch f.Name {\n\tcase \"content-type\":\n\t\tcontentSubtype, validContentType := contentSubtype(f.Value)\n\t\tif !validContentType {\n\t\t\td.data.contentTypeErr = fmt.Sprintf(\"transport: received the unexpected content-type %q\", f.Value)\n\t\t\treturn\n\t\t}\n\t\td.data.contentSubtype = contentSubtype\n\t\t// TODO: do we want to propagate the whole content-type in the metadata,\n\t\t// or come up with a way to just propagate the content-subtype if it was set?\n\t\t// ie {\"content-type\": \"application/grpc+proto\"} or {\"content-subtype\": \"proto\"}\n\t\t// in the metadata?\n\t\td.addMetadata(f.Name, f.Value)\n\t\td.data.isGRPC = true\n\tcase \"grpc-encoding\":\n\t\td.data.encoding = f.Value\n\tcase \"grpc-accept-encoding\":\n\t\td.data.acceptEncoding = f.Value\n\tcase \"grpc-status\":\n\t\tcode, err := strconv.Atoi(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-status: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.rawStatusCode = &code\n\tcase \"grpc-message\":\n\t\td.data.rawStatusMsg = decodeGrpcMessage(f.Value)\n\tcase \"biz-status\":\n\t\tcode, err := strconv.Atoi(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed biz-status: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.bizStatusCode = &code\n\tcase \"biz-extra\":\n\t\textra, err := utils.JSONStr2Map(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed biz-extra: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.bizStatusExtra = extra\n\tcase \"grpc-status-details-bin\":\n\t\tv, err := decodeBinHeader(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-status-details-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\ts := &spb.Status{}\n\t\tif err := proto.Unmarshal(v, s); err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-status-details-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.statusGen = status.FromProto(s)\n\tcase \"grpc-timeout\":\n\t\td.data.timeoutSet = true\n\t\tvar err error\n\t\tif d.data.timeout, err = decodeTimeout(f.Value); err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed time-out: %v\", err)\n\t\t}\n\tcase \":path\":\n\t\td.data.method = f.Value\n\tcase \":status\":\n\t\tcode, err := strconv.Atoi(f.Value)\n\t\tif err != nil {\n\t\t\td.data.httpErr = status.Errorf(codes.Internal, \"transport: malformed http-status: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.httpStatus = &code\n\tcase \"grpc-tags-bin\":\n\t\tv, err := decodeBinHeader(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-tags-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.statsTags = v\n\t\td.addMetadata(f.Name, string(v))\n\tcase \"grpc-trace-bin\":\n\t\tv, err := decodeBinHeader(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-trace-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.statsTrace = v\n\t\td.addMetadata(f.Name, string(v))\n\tdefault:\n\t\tif isReservedHeader(f.Name) && !isWhitelistedHeader(f.Name) {\n\t\t\tbreak\n\t\t}\n\t\tv, err := decodeMetadataHeader(f.Name, f.Value)\n\t\tif err != nil {\n\t\t\tklog.Errorf(\"Failed to decode metadata header (%q, %q): %v\", f.Name, f.Value, err)\n\t\t\treturn\n\t\t}\n\t\td.addMetadata(f.Name, v)\n\t}\n}\n\ntype timeoutUnit uint8\n\nconst (\n\thour        timeoutUnit = 'H'\n\tminute      timeoutUnit = 'M'\n\tsecond      timeoutUnit = 'S'\n\tmillisecond timeoutUnit = 'm'\n\tmicrosecond timeoutUnit = 'u'\n\tnanosecond  timeoutUnit = 'n'\n)\n\nfunc timeoutUnitToDuration(u timeoutUnit) (d time.Duration, ok bool) {\n\tswitch u {\n\tcase hour:\n\t\treturn time.Hour, true\n\tcase minute:\n\t\treturn time.Minute, true\n\tcase second:\n\t\treturn time.Second, true\n\tcase millisecond:\n\t\treturn time.Millisecond, true\n\tcase microsecond:\n\t\treturn time.Microsecond, true\n\tcase nanosecond:\n\t\treturn time.Nanosecond, true\n\tdefault:\n\t}\n\treturn\n}\n\nconst maxTimeoutValue int64 = 100000000 - 1\n\n// div does integer division and round-up the result. Note that this is\n// equivalent to (d+r-1)/r but has less chance to overflow.\nfunc div(d, r time.Duration) int64 {\n\tif m := d % r; m > 0 {\n\t\treturn int64(d/r + 1)\n\t}\n\treturn int64(d / r)\n}\n\n// TODO(zhaoq): It is the simplistic and not bandwidth efficient. Improve it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc decodeTimeout(s string) (time.Duration, error) {\n\tsize := len(s)\n\tif size < 2 {\n\t\treturn 0, fmt.Errorf(\"transport: timeout string is too short: %q\", s)\n\t}\n\tif size > 9 {\n\t\t// Spec allows for 8 digits plus the unit.\n\t\treturn 0, fmt.Errorf(\"transport: timeout string is too long: %q\", s)\n\t}\n\tunit := timeoutUnit(s[size-1])\n\td, ok := timeoutUnitToDuration(unit)\n\tif !ok {\n\t\treturn 0, fmt.Errorf(\"transport: timeout unit is not recognized: %q\", s)\n\t}\n\tt, err := strconv.ParseInt(s[:size-1], 10, 64)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tconst maxHours = math.MaxInt64 / int64(time.Hour)\n\tif d == time.Hour && t > maxHours {\n\t\t// This timeout would overflow math.MaxInt64; clamp it.\n\t\treturn time.Duration(math.MaxInt64), nil\n\t}\n\treturn d * time.Duration(t), nil\n}\n\nconst (\n\tspaceByte   = ' '\n\ttildeByte   = '~'\n\tpercentByte = '%'\n)\n\n// encodeGrpcMessage is used to encode status code in header field\n// \"grpc-message\". It does percent encoding and also replaces invalid utf-8\n// characters with Unicode replacement character.\n//\n// It checks to see if each individual byte in msg is an allowable byte, and\n// then either percent encoding or passing it through. When percent encoding,\n// the byte is converted into hexadecimal notation with a '%' prepended.\nfunc encodeGrpcMessage(msg string) string {\n\tif msg == \"\" {\n\t\treturn \"\"\n\t}\n\tlenMsg := len(msg)\n\tfor i := 0; i < lenMsg; i++ {\n\t\tc := msg[i]\n\t\tif !(c >= spaceByte && c <= tildeByte && c != percentByte) {\n\t\t\treturn encodeGrpcMessageUnchecked(msg)\n\t\t}\n\t}\n\treturn msg\n}\n\nfunc encodeGrpcMessageUnchecked(msg string) string {\n\tvar buf bytes.Buffer\n\tfor len(msg) > 0 {\n\t\tr, size := utf8.DecodeRuneInString(msg)\n\t\tfor _, b := range []byte(string(r)) {\n\t\t\tif size > 1 {\n\t\t\t\t// If size > 1, r is not ascii. Always do percent encoding.\n\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%%%02X\", b))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// The for loop is necessary even if size == 1. r could be\n\t\t\t// utf8.RuneError.\n\t\t\t//\n\t\t\t// fmt.Sprintf(\"%%%02X\", utf8.RuneError) gives \"%FFFD\".\n\t\t\tif b >= spaceByte && b <= tildeByte && b != percentByte {\n\t\t\t\tbuf.WriteByte(b)\n\t\t\t} else {\n\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%%%02X\", b))\n\t\t\t}\n\t\t}\n\t\tmsg = msg[size:]\n\t}\n\treturn buf.String()\n}\n\n// decodeGrpcMessage decodes the msg encoded by encodeGrpcMessage.\nfunc decodeGrpcMessage(msg string) string {\n\tif msg == \"\" {\n\t\treturn \"\"\n\t}\n\tlenMsg := len(msg)\n\tfor i := 0; i < lenMsg; i++ {\n\t\tif msg[i] == percentByte && i+2 < lenMsg {\n\t\t\treturn decodeGrpcMessageUnchecked(msg)\n\t\t}\n\t}\n\treturn msg\n}\n\nfunc decodeGrpcMessageUnchecked(msg string) string {\n\tvar buf bytes.Buffer\n\tlenMsg := len(msg)\n\tfor i := 0; i < lenMsg\n```\n# Function Signature: func encodeTimeout(t time.Duration) string {\n# Natural Language Annotations: // TODO(zhaoq): It is the simplistic and not bandwidth efficient. Improve it.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc encodeTimeout(t time.Duration) string {\n\tconst maxEncodeValue = 99999999\n\n\tvar (\n\t\tunit timeoutUnit\n\t\tval  int64\n\t)\n\n\t// Determine the appropriate time unit and value to encode the duration.\n\tswitch {\n\tcase t%time.Hour == 0:\n\t\tunit = hour\n\t\tval = int64(t / time.Hour)\n\tcase t%time.Minute == 0:\n\t\tunit = minute\n\t\tval = int64(t / time.Minute)\n\tcase t%time.Second == 0:\n\t\tunit = second\n\t\tval = int64(t / time.Second)\n\tcase t%time.Millisecond == 0:\n\t\tunit = millisecond\n\t\tval = int64(t / time.Millisecond)\n\tcase t%time.Microsecond == 0:\n\t\tunit = microsecond\n\t\tval = int64(t / time.Microsecond)\n\tdefault:\n\t\tunit = nanosecond\n\t\tval = int64(t / time.Nanosecond)\n\t}\n\n\tif val > maxEncodeValue {\n\t\tval = maxEncodeValue\n\t}\n\n\treturn fmt.Sprintf(\"%d%c\", val, unit)\n}\n```\n\nThis `encodeTimeout` function converts a `time.Duration` into a gRPC-compatible timeout string by finding the largest available time unit that can represent the given duration exactly and then formatting that value with its corresponding unit character. It ensures that the value being encoded does not exceed the maximum allowed value of `99999999` as specified in the gRPC documentation."]}, "kitex-pkg/remote/trans/netpoll/bytebuf.go-AppendBuffer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/netpoll/bytebuf.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage netpoll\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n)\n\nvar bytebufPool sync.Pool\n\nfunc init() {\n\tbytebufPool.New = newNetpollByteBuffer\n}\n\n// NewReaderByteBuffer creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\nfunc NewReaderByteBuffer(r netpoll.Reader) remote.ByteBuffer {\n\tbytebuf := bytebufPool.Get().(*netpollByteBuffer)\n\tbytebuf.reader = r\n\t// TODO(wangtieju): fix me when netpoll support netpoll.Reader\n\t// and LinkBuffer not support io.Reader, type assertion would fail when r is from NewBuffer\n\tif ir, ok := r.(io.Reader); ok {\n\t\tbytebuf.ioReader = ir\n\t}\n\tbytebuf.status = remote.BitReadable\n\tbytebuf.readSize = 0\n\treturn bytebuf\n}\n\n// NewWriterByteBuffer creates a new remote.ByteBuffer using the given netpoll.ZeroCopyWriter.\nfunc NewWriterByteBuffer(w netpoll.Writer) remote.ByteBuffer {\n\tbytebuf := bytebufPool.Get().(*netpollByteBuffer)\n\tbytebuf.writer = w\n\t// TODO(wangtieju): fix me when netpoll support netpoll.Writer\n\t// and LinkBuffer not support io.Reader, type assertion would fail when w is from NewBuffer\n\tif iw, ok := w.(io.Writer); ok {\n\t\tbytebuf.ioWriter = iw\n\t}\n\tbytebuf.status = remote.BitWritable\n\treturn bytebuf\n}\n\n// NewReaderWriterByteBuffer creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReadWriter.\nfunc NewReaderWriterByteBuffer(rw netpoll.ReadWriter) remote.ByteBuffer {\n\tbytebuf := bytebufPool.Get().(*netpollByteBuffer)\n\tbytebuf.writer = rw\n\tbytebuf.reader = rw\n\t// TODO(wangtieju): fix me when netpoll support netpoll.ReadWriter\n\t// and LinkBuffer not support io.ReadWriter, type assertion would fail when rw is from NewBuffer\n\tif irw, ok := rw.(io.ReadWriter); ok {\n\t\tbytebuf.ioReader = irw\n\t\tbytebuf.ioWriter = irw\n\t}\n\tbytebuf.status = remote.BitWritable | remote.BitReadable\n\treturn bytebuf\n}\n\nfunc newNetpollByteBuffer() interface{} {\n\treturn &netpollByteBuffer{}\n}\n\ntype netpollByteBuffer struct {\n\twriter   netpoll.Writer\n\treader   netpoll.Reader\n\tioReader io.Reader\n\tioWriter io.Writer\n\tstatus   int\n\treadSize int\n}\n\nvar _ remote.ByteBuffer = &netpollByteBuffer{}\n\n// Next reads n bytes sequentially, returns the original address.\nfunc (b *netpollByteBuffer) Next(n int) (p []byte, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Next\")\n\t}\n\tif p, err = b.reader.Next(n); err == nil {\n\t\tb.readSize += n\n\t}\n\treturn\n}\n\n// Peek returns the next n bytes without advancing the reader.\nfunc (b *netpollByteBuffer) Peek(n int) (buf []byte, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Peek\")\n\t}\n\treturn b.reader.Peek(n)\n}\n\n// Skip is used to skip n bytes, it's much faster than Next.\n// Skip will not cause release.\nfunc (b *netpollByteBuffer) Skip(n int) (err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn errors.New(\"unreadable buffer, cannot support Skip\")\n\t}\n\treturn b.reader.Skip(n)\n}\n\n// ReadableLen returns the total length of readable buffer.\nfunc (b *netpollByteBuffer) ReadableLen() (n int) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn -1\n\t}\n\treturn b.reader.Len()\n}\n\n// Read implement io.Reader\nfunc (b *netpollByteBuffer) Read(p []byte) (n int, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn -1, errors.New(\"unreadable buffer, cannot support Read\")\n\t}\n\tif b.ioReader != nil {\n\t\treturn b.ioReader.Read(p)\n\t}\n\treturn -1, errors.New(\"ioReader is nil\")\n}\n\n// ReadString is a more efficient way to read string than Next.\nfunc (b *netpollByteBuffer) ReadString(n int) (s string, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn \"\", errors.New(\"unreadable buffer, cannot support ReadString\")\n\t}\n\tif s, err = b.reader.ReadString(n); err == nil {\n\t\tb.readSize += n\n\t}\n\treturn\n}\n\n// ReadBinary like ReadString.\n// Returns a copy of original buffer.\nfunc (b *netpollByteBuffer) ReadBinary(n int) (p []byte, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn p, errors.New(\"unreadable buffer, cannot support ReadBinary\")\n\t}\n\tif p, err = b.reader.ReadBinary(n); err == nil {\n\t\tb.readSize += n\n\t}\n\treturn\n}\n\n// Malloc n bytes sequentially in the writer buffer.\nfunc (b *netpollByteBuffer) Malloc(n int) (buf []byte, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn nil, errors.New(\"unwritable buffer, cannot support Malloc\")\n\t}\n\treturn b.writer.Malloc(n)\n}\n\n// MallocAck n bytes in the writer buffer.\nfunc (b *netpollByteBuffer) MallocAck(n int) (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support MallocAck\")\n\t}\n\treturn b.writer.MallocAck(n)\n}\n\n// MallocLen returns the total length of the buffer malloced.\nfunc (b *netpollByteBuffer) MallocLen() (length int) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1\n\t}\n\treturn b.writer.MallocLen()\n}\n\n// Write implement io.Writer\nfunc (b *netpollByteBuffer) Write(p []byte) (n int, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support Write\")\n\t}\n\tif b.ioWriter != nil {\n\t\treturn b.ioWriter.Write(p)\n\t}\n\treturn -1, errors.New(\"ioWriter is nil\")\n}\n\n// WriteString is a more efficient way to write string, using the unsafe method to convert the string to []byte.\nfunc (b *netpollByteBuffer) WriteString(s string) (n int, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteString\")\n\t}\n\treturn b.writer.WriteString(s)\n}\n\n// WriteBinary writes the []byte directly. Callers must guarantee that the []byte doesn't change.\nfunc (b *netpollByteBuffer) WriteBinary(p []byte) (n int, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn b.writer.WriteBinary(p)\n}\n\n// WriteDirect is a way to write []byte without copying, and splits the original buffer.\nfunc (b *netpollByteBuffer) WriteDirect(p []byte, remainCap int) error {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn b.writer.WriteDirect(p, remainCap)\n}\n\n// ReadLen returns the size already read.\nfunc (b *netpollByteBuffer) ReadLen() (n int) {\n\treturn b.readSize\n}\n\n// Flush writes any malloc data to the underlying io.Writer.\n// The malloced buffer must be set correctly.\nfunc (b *netpollByteBuffer) Flush() (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support Flush\")\n\t}\n\treturn b.writer.Flush()\n}\n\n// NewBuffer returns a new writable remote.ByteBuffer.\nfunc (b *netpollByteBuffer) NewBuffer() remote.ByteBuffer {\n\treturn NewWriterByteBuffer(netpoll.NewLinkBuffer())\n}\n\n// AppendBuffer appends buf to the original buffer.\n\n\n\n\n\n\n\n// Bytes are not supported in netpoll bytebuf.\nfunc (b *netpollByteBuffer) Bytes() (buf []byte, err error) {\n\tif b.reader != nil {\n\t\treturn b.reader.Peek(b.reader.Len())\n\t}\n\treturn nil, errors.New(\"method Bytes() not support in netpoll bytebuf\")\n}\n\n// Release will free the buffer already read.\n// After release, buffer read by Next/Skip/Peek is invalid.\nfunc (b *netpollByteBuffer) Release(e error) (err error) {\n\tif b.reader != nil {\n\t\t// \u91cd\u590d\u6267\u884cRelease nil panic\n\t\terr = b.reader.Release()\n\t}\n\tb.zero()\n\tbytebufPool.Put(b)\n\treturn\n}\n\nfunc (b *netpollByteBuffer) zero() {\n\tb.writer = nil\n\tb.reader = nil\n\tb.ioReader = nil\n\tb.ioWriter = nil\n\tb.status = 0\n\tb.readSize = 0\n}\n\n```\n# Function Signature: func (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n# Natural Language Annotations: // AppendBuffer appends buf to the original buffer.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support AppendBuffer\")\n\t}\n\n\t// Assuming remote.ByteBuffer interface has a method to get the underlying data.\n\t// You may need to adjust this code according to actual methods provided by remote.ByteBuffer.\n\tdata, err := buf.Bytes()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Use WriteBinary to append the buffer content to the original buffer.\n\t_, err = b.WriteBinary(data)\n\treturn err\n}\n```"]}, "kitex-pkg/remote/codec/util.go-SetOrCheckMethodName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/util.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage codec\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\t// FrontMask is used in protocol sniffing.\n\tFrontMask = 0x0000ffff\n)\n\n// SetOrCheckMethodName is used to set method name to invocation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SetOrCheckSeqID is used to check the sequence ID.\nfunc SetOrCheckSeqID(seqID int32, message remote.Message) error {\n\tswitch message.MessageType() {\n\tcase remote.Call, remote.Oneway:\n\t\tif ink, ok := message.RPCInfo().Invocation().(rpcinfo.InvocationSetter); ok {\n\t\t\tink.SetSeqID(seqID)\n\t\t} else {\n\t\t\treturn errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n\t\t}\n\tcase remote.Reply:\n\t\texpectSeqID := message.RPCInfo().Invocation().SeqID()\n\t\tif expectSeqID != seqID {\n\t\t\tmethodName := message.RPCInfo().Invocation().MethodName()\n\t\t\treturn remote.NewTransErrorWithMsg(remote.BadSequenceID, fmt.Sprintf(\"method[%s] out of order sequence response, expect[%d], receive[%d]\", methodName, expectSeqID, seqID))\n\t\t}\n\tcase remote.Exception:\n\t\t// don't check, proxy may build Exception with seqID = 0\n\t\t// thrift 0.13 check seqID for Exception but thrift 0.9.2 doesn't check\n\t}\n\treturn nil\n}\n\n// UpdateMsgType updates msg type.\nfunc UpdateMsgType(msgType uint32, message remote.Message) error {\n\trpcRole := message.RPCRole()\n\tmt := remote.MessageType(msgType)\n\tif mt == message.MessageType() {\n\t\treturn nil\n\t}\n\tif rpcRole == remote.Server {\n\t\tif mt != remote.Call && mt != remote.Oneway && mt != remote.Stream {\n\t\t\treturn remote.NewTransErrorWithMsg(remote.InvalidMessageTypeException, fmt.Sprintf(\"server side, invalid message type %d\", mt))\n\t\t}\n\t} else {\n\t\tif mt != remote.Reply && mt != remote.Exception && mt != remote.Stream {\n\t\t\treturn remote.NewTransErrorWithMsg(remote.InvalidMessageTypeException, fmt.Sprintf(\"client side, invalid message type %d\", mt))\n\t\t}\n\t}\n\n\tmessage.SetMessageType(mt)\n\treturn nil\n}\n\n// NewDataIfNeeded is used to create the data if not exist.\nfunc NewDataIfNeeded(method string, message remote.Message) error {\n\tif message.Data() != nil {\n\t\treturn nil\n\t}\n\tif message.NewData(method) {\n\t\treturn nil\n\t}\n\treturn remote.NewTransErrorWithMsg(remote.InternalError, \"message data for codec is nil\")\n}\n\n```\n# Function Signature: func SetOrCheckMethodName(methodName string, message remote.Message) error {\n# Natural Language Annotations: // SetOrCheckMethodName is used to set method name to invocation.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// SetOrCheckMethodName is used to set method name to invocation.\nfunc SetOrCheckMethodName(methodName string, message remote.Message) error {\n    // Check if the message implement InvocationSetter interface.\n    if ink, ok := message.RPCInfo().Invocation().(rpcinfo.InvocationSetter); ok {\n        // Set the method name using the interface method.\n        ink.SetMethodName(methodName)\n    } else {\n        // Return an error if the interface doesn't implement InvocationSetter.\n        return errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n    }\n    return nil\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-MarshalThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n# Natural Language Annotations: // MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n    // Conduct a type assertion to ensure that codec is of type thriftCodec\n    thriftCodec, ok := codec.(thriftCodec)\n    if !ok {\n        return nil, fmt.Errorf(\"unsupported codec type\")\n    }\n    \n    // Use the marshalThriftData method of thriftCodec to encode the data\n    return thriftCodec.marshalThriftData(ctx, data)\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-marshalBasicThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func marshalBasicThriftData(ctx context.Context, tProt thrift.TProtocol, data interface{}) error {\n# Natural Language Annotations: // marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc marshalBasicThriftData(ctx context.Context, tProt thrift.TProtocol, data interface{}) error {\n\tswitch d := data.(type) {\n\tcase MessageWriter:\n\t\t// Use the Write method from the MessageWriter interface to encode data\n\t\tif err := d.Write(tProt); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\tcase MessageWriterWithContext:\n\t\t// Use the Write method with context from the MessageWriterWithContext interface to encode data\n\t\tif err := d.Write(ctx, tProt); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\tdefault:\n\t\treturn perrors.NewProtocolErrorWithType(perrors.ErrTypeMismatch, \"unsupported data type for thrift encoding\")\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-UnmarshalThriftException": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func UnmarshalThriftException(tProt thrift.TProtocol) error {\n# Natural Language Annotations: // UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc UnmarshalThriftException(tProt thrift.TProtocol) error {\n\t// Check if the provided TProtocol is nil\n\tif tProt == nil {\n\t\treturn fmt.Errorf(\"tProt cannot be nil\")\n\t}\n\n\t// Attempt to read the exception message from the protocol\n\texception, err := thrift.ReadMessageBegin(tProt)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to read message begin: %w\", err)\n\t}\n\n\t// We expect the message to be an exception\n\tif exception.Type != thrift.EXCEPTION {\n\t\treturn fmt.Errorf(\"expected message type EXCEPTION, but got %d\", exception.Type)\n\t}\n\n\t// Read the exception detail\n\tappExc := thrift.NewTApplicationException(thrift.UNKNOWN_APPLICATION_EXCEPTION, \"unknown\")\n\tif err := appExc.Read(tProt); err != nil {\n\t\treturn fmt.Errorf(\"failed to read application exception: %w\", err)\n\t}\n\n\t// Finish reading the message\n\tif err := thrift.ReadMessageEnd(tProt); err != nil {\n\t\treturn fmt.Errorf(\"failed to read message end: %w\", err)\n\t}\n\n\t// Return the error represented by the application exception\n\treturn appExc\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-UnmarshalThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n# Natural Language Annotations: // UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n\t// Wrap the buffer with a BinaryProtocol reader\n\ttProt := NewBinaryProtocol(remote.NewReaderBuffer(buf))\n\n\t// First, check if the buffer contains an exception\n\tif codec.containsException(buf) {\n\t\treturn UnmarshalThriftException(ctx, codec, tProt)\n\t}\n\n\t// Verify if data can be unmarshaled with the traditional method\n\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\treturn err\n\t}\n\n\t// Use fast unmarshal if possible\n\tif c, ok := codec.(thriftCodec); ok && c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, len(buf)) {\n\t\treturn c.fastUnmarshal(tProt, data, len(buf))\n\t}\n\n\t// Use hyper unmarshal if possible\n\tif c, ok := codec.(thriftCodec); ok && c.hyperMarshalAvailable(data) {\n\t\treturn c.hyperUnmarshal(tProt, data, len(buf))\n\t}\n\n\t// Fallback to traditional unmarshal\n\treturn decodeBasicThriftData(ctx, tProt, method, data)\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-unmarshalThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n# Natural Language Annotations: // unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// unmarshalThriftData only decodes the data (after methodName, msgType, and seqId)\n// method is only used for generic calls\nfunc (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n\t// First, check whether fast message unmarshal is enabled and available\n\tif c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t}\n\n\t// Verify if the data type can be handled by the basic thrift decode method\n\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\t// If not, check if hyper unmarshal is available and fallback to it\n\t\tif c.hyperUnmarshalAvailable(data) && dataLen > 0 {\n\t\t\treturn c.hyperUnmarshal(tProt, data, dataLen)\n\t\t}\n\t\treturn err\n\t}\n\n\t// Fallback to the old thrift way of decoding\n\tif err := decodeBasicThriftData(tProt, data, method); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\treturn nil\n}\n\nfunc (c thriftCodec) hyperUnmarshalAvailable(data interface{}) bool {\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\nfunc decodeBasicThriftData(tProt *BinaryProtocol, data interface{}, method string) error {\n\t// Add implementation for decoding data using the old thrift method\n\t// This typically involves reading from tProt and populating the 'data' object\n\t// The `method` parameter might be used to determine how to decode `data`\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-decodeBasicThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n# Natural Language Annotations: // decodeBasicThriftData decode thrift body the old way (slow)\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n\t// Verify the data type to ensure it can be processed in the old way\n\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\treturn err\n\t}\n\n\t// Cast the data to the appropriate interface\n\tvar reader MessageReader\n\tvar ok bool\n\t// Check if data implements the MessageReaderWithMethodWithContext interface\n\tif m, isWithMethodCtx := data.(MessageReaderWithMethodWithContext); isWithMethodCtx {\n\t\treturn m.Read(ctx, method, tProt)\n\t} else if reader, ok = data.(MessageReader); ok {\n\t\treturn reader.Read(ctx, tProt)\n\t}\n\n\treturn errDecodeMismatchMsgType\n}\n```"]}, "kitex-pkg/remote/remotecli/conn_wrapper.go-GetConn": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/remotecli/conn_wrapper.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage remotecli\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\nvar connWrapperPool sync.Pool\n\nfunc init() {\n\tconnWrapperPool.New = newConnWrapper\n}\n\nvar _ ConnReleaser = &ConnWrapper{}\n\n// ConnReleaser helps to release the raw connection.\ntype ConnReleaser interface {\n\tReleaseConn(err error, ri rpcinfo.RPCInfo)\n}\n\n// ConnWrapper wraps a connection.\ntype ConnWrapper struct {\n\tconnPool remote.ConnPool\n\tconn     net.Conn\n}\n\n// NewConnWrapper returns a new ConnWrapper using the given connPool and logger.\nfunc NewConnWrapper(connPool remote.ConnPool) *ConnWrapper {\n\tcm := connWrapperPool.Get().(*ConnWrapper)\n\tcm.connPool = connPool\n\treturn cm\n}\n\n// GetConn returns a connection using the given Dialer and RPCInfo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReleaseConn should notice that ri may nil when oneway\n// TODO duplicate release may cause problem?\nfunc (cm *ConnWrapper) ReleaseConn(err error, ri rpcinfo.RPCInfo) {\n\tif cm.conn == nil {\n\t\treturn\n\t}\n\tif cm.connPool != nil {\n\t\tif err == nil {\n\t\t\t_, ok := ri.To().Tag(rpcinfo.ConnResetTag)\n\t\t\tif ok || ri.Config().InteractionMode() == rpcinfo.Oneway {\n\t\t\t\tcm.connPool.Discard(cm.conn)\n\t\t\t} else {\n\t\t\t\tcm.connPool.Put(cm.conn)\n\t\t\t}\n\t\t} else {\n\t\t\tcm.connPool.Discard(cm.conn)\n\t\t}\n\t} else {\n\t\tcm.conn.Close()\n\t}\n\n\tcm.zero()\n\tconnWrapperPool.Put(cm)\n}\n\nfunc newConnWrapper() interface{} {\n\treturn &ConnWrapper{}\n}\n\nfunc (cm *ConnWrapper) zero() {\n\tcm.connPool = nil\n\tcm.conn = nil\n}\n\nfunc (cm *ConnWrapper) getConnWithPool(ctx context.Context, cp remote.ConnPool, d remote.Dialer,\n\ttimeout time.Duration, ri rpcinfo.RPCInfo,\n) (net.Conn, error) {\n\taddr := ri.To().Address()\n\tif addr == nil {\n\t\treturn nil, kerrors.ErrNoDestAddress\n\t}\n\topt := remote.ConnOption{Dialer: d, ConnectTimeout: timeout}\n\tri.Stats().Record(ctx, stats.ClientConnStart, stats.StatusInfo, \"\")\n\tconn, err := cp.Get(ctx, addr.Network(), addr.String(), opt)\n\tif err != nil {\n\t\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusError, err.Error())\n\t\treturn nil, kerrors.ErrGetConnection.WithCause(err)\n\t}\n\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusInfo, \"\")\n\treturn conn, nil\n}\n\nfunc (cm *ConnWrapper) getConnWithDialer(ctx context.Context, d remote.Dialer,\n\ttimeout time.Duration, ri rpcinfo.RPCInfo,\n) (net.Conn, error) {\n\taddr := ri.To().Address()\n\tif addr == nil {\n\t\treturn nil, kerrors.ErrNoDestAddress\n\t}\n\n\tri.Stats().Record(ctx, stats.ClientConnStart, stats.StatusInfo, \"\")\n\tconn, err := d.DialTimeout(addr.Network(), addr.String(), timeout)\n\tif err != nil {\n\t\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusError, err.Error())\n\t\treturn nil, kerrors.ErrGetConnection.WithCause(err)\n\t}\n\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusInfo, \"\")\n\treturn conn, nil\n}\n\n```\n# Function Signature: func (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n# Natural Language Annotations: // GetConn returns a connection using the given Dialer and RPCInfo.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// GetConn returns a connection using the given Dialer and RPCInfo.\nfunc (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n    if cm.connPool != nil {\n        // If a connection pool is available, attempt to get a connection from it\n        conn, err := cm.getConnWithPool(ctx, cm.connPool, d, ri.Config().ConnectTimeout(), ri)\n        if err == nil {\n            cm.conn = conn\n        }\n        return conn, err\n    }\n\n    // Otherwise, directly use the dialer to establish a connection\n    conn, err := cm.getConnWithDialer(ctx, d, ri.Config().ConnectTimeout(), ri)\n    if err == nil {\n        cm.conn = conn\n    }\n    return conn, err\n}\n```"]}, "kitex-pkg/remote/connpool/long_pool.go-Evict": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/connpool/long_pool.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package connpool provide short connection and long connection pool.\npackage connpool\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n)\n\nvar (\n\t_ net.Conn            = &longConn{}\n\t_ remote.LongConnPool = &LongPool{}\n\n\t// global shared tickers for different LongPool\n\tsharedTickers sync.Map\n)\n\nconst (\n\tconfigDumpKey = \"idle_config\"\n)\n\nfunc getSharedTicker(p *LongPool, refreshInterval time.Duration) *utils.SharedTicker {\n\tsti, ok := sharedTickers.Load(refreshInterval)\n\tif ok {\n\t\tst := sti.(*utils.SharedTicker)\n\t\tst.Add(p)\n\t\treturn st\n\t}\n\tsti, _ = sharedTickers.LoadOrStore(refreshInterval, utils.NewSharedTicker(refreshInterval))\n\tst := sti.(*utils.SharedTicker)\n\tst.Add(p)\n\treturn st\n}\n\n// netAddr implements the net.Addr interface and comparability.\ntype netAddr struct {\n\tnetwork string\n\taddress string\n}\n\n// Network implements the net.Addr interface.\nfunc (na netAddr) Network() string { return na.network }\n\n// String implements the net.Addr interface.\nfunc (na netAddr) String() string { return na.address }\n\n// longConn implements the net.Conn interface.\ntype longConn struct {\n\tnet.Conn\n\tsync.RWMutex\n\tdeadline time.Time\n\taddress  string\n}\n\n// Close implements the net.Conn interface.\nfunc (c *longConn) Close() error {\n\treturn c.Conn.Close()\n}\n\n// RawConn returns the real underlying net.Conn.\nfunc (c *longConn) RawConn() net.Conn {\n\treturn c.Conn\n}\n\n// IsActive indicates whether the connection is active.\n\n\n\n\n\n\n\n\n// Expired checks the deadline of the connection.\nfunc (c *longConn) Expired() bool {\n\treturn time.Now().After(c.deadline)\n}\n\ntype PoolDump struct {\n\tIdleNum       int         `json:\"idle_num\"`\n\tConnsDeadline []time.Time `json:\"conns_deadline\"`\n}\n\nfunc newPool(minIdle, maxIdle int, maxIdleTimeout time.Duration) *pool {\n\tp := &pool{\n\t\tidleList:       make([]*longConn, 0, maxIdle),\n\t\tminIdle:        minIdle,\n\t\tmaxIdle:        maxIdle,\n\t\tmaxIdleTimeout: maxIdleTimeout,\n\t}\n\treturn p\n}\n\n// pool implements a pool of long connections.\ntype pool struct {\n\tidleList []*longConn // idleList Get/Put by FILO(stack) but Evict by FIFO(queue)\n\tmu       sync.RWMutex\n\t// config\n\tminIdle        int\n\tmaxIdle        int           // currIdle <= maxIdle.\n\tmaxIdleTimeout time.Duration // the idle connection will be cleaned if the idle time exceeds maxIdleTimeout.\n}\n\n// Get gets the first active connection from the idleList. Return the number of connections decreased during the Get.\nfunc (p *pool) Get() (*longConn, bool, int) {\n\tp.mu.Lock()\n\t// Get the first active one\n\tn := len(p.idleList)\n\tselected := n - 1\n\tfor ; selected >= 0; selected-- {\n\t\to := p.idleList[selected]\n\t\t// reset slice element to nil, active conn object only could be hold reference by user function\n\t\tp.idleList[selected] = nil\n\t\tif o.IsActive() {\n\t\t\tp.idleList = p.idleList[:selected]\n\t\t\tp.mu.Unlock()\n\t\t\treturn o, true, n - selected\n\t\t}\n\t\t// inactive object\n\t\to.Close()\n\t}\n\t// in case all objects are inactive\n\tif selected < 0 {\n\t\tselected = 0\n\t}\n\tp.idleList = p.idleList[:selected]\n\tp.mu.Unlock()\n\treturn nil, false, n - selected\n}\n\n// Put puts back a connection to the pool.\n\n\n\n\n\n\n\n\n\n\n\n\n// Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Len returns the length of the pool.\nfunc (p *pool) Len() int {\n\tp.mu.RLock()\n\tl := len(p.idleList)\n\tp.mu.RUnlock()\n\treturn l\n}\n\n// Close closes the pool and all the objects in the pool.\nfunc (p *pool) Close() int {\n\tp.mu.Lock()\n\tnum := len(p.idleList)\n\tfor i := 0; i < num; i++ {\n\t\tp.idleList[i].Close()\n\t}\n\tp.idleList = nil\n\n\tp.mu.Unlock()\n\treturn num\n}\n\n// Dump dumps the info of all the objects in the pool.\nfunc (p *pool) Dump() PoolDump {\n\tp.mu.RLock()\n\tidleNum := len(p.idleList)\n\tconnsDeadline := make([]time.Time, idleNum)\n\tfor i := 0; i < idleNum; i++ {\n\t\tconnsDeadline[i] = p.idleList[i].deadline\n\t}\n\ts := PoolDump{\n\t\tIdleNum:       idleNum,\n\t\tConnsDeadline: connsDeadline,\n\t}\n\tp.mu.RUnlock()\n\treturn s\n}\n\nfunc newPeer(\n\tserviceName string,\n\taddr net.Addr,\n\tminIdle int,\n\tmaxIdle int,\n\tmaxIdleTimeout time.Duration,\n\tglobalIdle *utils.MaxCounter,\n) *peer {\n\treturn &peer{\n\t\tserviceName: serviceName,\n\t\taddr:        addr,\n\t\tglobalIdle:  globalIdle,\n\t\tpool:        newPool(minIdle, maxIdle, maxIdleTimeout),\n\t}\n}\n\n// peer has one address, it manages all connections base on this address\ntype peer struct {\n\t// info\n\tserviceName string\n\taddr        net.Addr\n\tglobalIdle  *utils.MaxCounter\n\t// pool\n\tpool *pool\n}\n\n// Get gets a connection with dialer and timeout. Dial a new connection if no idle connection in pool is available.\nfunc (p *peer) Get(d remote.Dialer, timeout time.Duration, reporter Reporter, addr string) (net.Conn, error) {\n\tvar c net.Conn\n\tc, reused, decNum := p.pool.Get()\n\tp.globalIdle.DecN(int64(decNum))\n\tif reused {\n\t\treporter.ReuseSucceed(Long, p.serviceName, p.addr)\n\t\treturn c, nil\n\t}\n\t// dial a new connection\n\tc, err := d.DialTimeout(p.addr.Network(), p.addr.String(), timeout)\n\tif err != nil {\n\t\treporter.ConnFailed(Long, p.serviceName, p.addr)\n\t\treturn nil, err\n\t}\n\treporter.ConnSucceed(Long, p.serviceName, p.addr)\n\treturn &longConn{\n\t\tConn:    c,\n\t\taddress: addr,\n\t}, nil\n}\n\n// Put puts a connection back to the peer.\nfunc (p *peer) Put(c *longConn) error {\n\tif !p.globalIdle.Inc() {\n\t\treturn c.Close()\n\t}\n\tif !p.pool.Put(c) {\n\t\tp.globalIdle.Dec()\n\t\treturn c.Close()\n\t}\n\treturn nil\n}\n\nfunc (p *peer) Len() int {\n\treturn p.pool.Len()\n}\n\nfunc (p *peer) Evict() {\n\tn := p.pool.Evict()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// Close closes the peer and all the connections in the ring.\nfunc (p *peer) Close() {\n\tn := p.pool.Close()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// NewLongPool creates a long pool using the given IdleConfig.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// LongPool manages a pool of long connections.\ntype LongPool struct {\n\treporter     Reporter\n\tpeerMap      sync.Map\n\tnewPeer      func(net.Addr) *peer\n\tglobalIdle   *utils.MaxCounter\n\tidleConfig   connpool.IdleConfig\n\tsharedTicker *utils.SharedTicker\n\tclosed       int32 // active: 0, closed: 1\n}\n\n// Get pick or generate a net.Conn and return\n// The context is not used but leave it for now.\nfunc (lp *LongPool) Get(ctx context.Context, network, address string, opt remote.ConnOption) (net.Conn, error) {\n\taddr := netAddr{network, address}\n\tp := lp.getPeer(addr)\n\treturn p.Get(opt.Dialer, opt.ConnectTimeout, lp.reporter, address)\n}\n\n// Put implements the ConnPool interface.\nfunc (lp *LongPool) Put(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif !ok {\n\t\treturn conn.Close()\n\t}\n\n\taddr := conn.RemoteAddr()\n\tna := netAddr{addr.Network(), c.address}\n\tp, ok := lp.peerMap.Load(na)\n\tif ok {\n\t\tp.(*peer).Put(c)\n\t\treturn nil\n\t}\n\treturn c.Conn.Close()\n}\n\n// Discard implements the ConnPool interface.\nfunc (lp *LongPool) Discard(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif ok {\n\t\treturn c.Close()\n\t}\n\treturn conn.Close()\n}\n\n// Clean implements the LongConnPool interface.\nfunc (lp *LongPool) Clean(network, address string) {\n\tna := netAddr{network, address}\n\tif p, ok := lp.peerMap.Load(na); ok {\n\t\tlp.peerMap.Delete(na)\n\t\tgo p.(*peer).Close()\n\t}\n}\n\n// Dump is used to dump current long pool info when needed, like debug query.\nfunc (lp *LongPool) Dump() interface{} {\n\tm := make(map[string]interface{})\n\tm[configDumpKey] = lp.idleConfig\n\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\tt := value.(*peer).pool.Dump()\n\t\tm[key.(netAddr).String()] = t\n\t\treturn true\n\t})\n\treturn m\n}\n\n// Close releases all peers in the pool, it is executed when client is closed.\nfunc (lp *LongPool) Close() error {\n\tif !atomic.CompareAndSwapInt32(&lp.closed, 0, 1) {\n\t\treturn fmt.Errorf(\"long pool is already closed\")\n\t}\n\t// close all peers\n\tlp.peerMap.Range(func(addr, value interface{}) bool {\n\t\tlp.peerMap.Delete(addr)\n\t\tv := value.(*peer)\n\t\tv.Close()\n\t\treturn true\n\t})\n\t// remove from the shared ticker\n\tlp.sharedTicker.Delete(lp)\n\treturn nil\n}\n\n// EnableReporter enable reporter for long connection pool.\nfunc (lp *LongPool) EnableReporter() {\n\tlp.reporter = GetCommonReporter()\n}\n\n// WarmUp implements the warmup.Pool interface.\nfunc (lp *LongPool) WarmUp(eh warmup.ErrorHandling, wuo *warmup.PoolOption, co remote.ConnOption) error {\n\th := &warmup.PoolHelper{ErrorHandling: eh}\n\treturn h.WarmUp(wuo, lp, co)\n}\n\n// Evict cleanups the idle connections in peers.\nfunc (lp *LongPool) Evict() {\n\tif atomic.LoadInt32(&lp.closed) == 0 {\n\t\t// Evict idle connections\n\t\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\t\tp := value.(*peer)\n\t\t\tp.Evict()\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (lp *LongPool) Tick() {\n\tlp.Evict()\n}\n\n// getPeer gets a peer from the pool based on the addr, or create a new one if not exist.\nfunc (lp *LongPool) getPeer(addr netAddr) *peer {\n\tp, ok := lp.peerMap.Load(addr)\n\tif ok {\n\t\treturn p.(*peer)\n\t}\n\tp, _ = lp.peerMap.LoadOrStore(addr, lp.newPeer(addr))\n\treturn p.(*peer)\n}\n\n```\n# Function Signature: func (p *pool) Evict() (evicted int) {\n# Natural Language Annotations: // Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (p *pool) Evict() (evicted int) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\t// Utilize FIFO to evict expired connections\n\tfor i := 0; i < len(p.idleList); i++ {\n\t\tconn := p.idleList[i]\n\t\tif conn.Expired() {\n\t\t\tconn.Close()\n\t\t\tevicted++\n\t\t} else {\n\t\t\t// Stop checking further if a non-expired connection is found since list is FIFO\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Retain only active connections\n\tp.idleList = p.idleList[evicted:]\n\treturn evicted\n}\n```"]}, "kitex-pkg/loadbalance/lbcache/cache.go-NewBalancerFactory": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/loadbalance/lbcache/cache.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package lbcache combine balancer with resolver and cache the resolve result\npackage lbcache\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"golang.org/x/sync/singleflight\"\n\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nconst (\n\tdefaultRefreshInterval = 5 * time.Second\n\tdefaultExpireInterval  = 15 * time.Second\n)\n\nvar (\n\tbalancerFactories    sync.Map // key: resolver name + loadbalance name\n\tbalancerFactoriesSfg singleflight.Group\n)\n\n// Options for create builder\ntype Options struct {\n\t// refresh discovery result timely\n\tRefreshInterval time.Duration\n\n\t// Balancer expire check interval\n\t// we need remove idle Balancers for resource saving\n\tExpireInterval time.Duration\n\n\t// DiagnosisService is used register info for diagnosis\n\tDiagnosisService diagnosis.Service\n\n\t// Cacheable is used to indicate that if the factory could be shared between multi clients\n\tCacheable bool\n}\n\nfunc (v *Options) check() {\n\tif v.RefreshInterval <= 0 {\n\t\tv.RefreshInterval = defaultRefreshInterval\n\t}\n\tif v.ExpireInterval <= 0 {\n\t\tv.ExpireInterval = defaultExpireInterval\n\t}\n}\n\n// Hookable add hook for rebalancer events\ntype Hookable interface {\n\t// register loadbalance rebalance hook for Rebalance events\n\tRegisterRebalanceHook(func(ch *discovery.Change)) (index int)\n\tDeregisterRebalanceHook(index int)\n\t// register loadbalance delete hook for Delete events\n\tRegisterDeleteHook(func(ch *discovery.Change)) (index int)\n\tDeregisterDeleteHook(index int)\n}\n\n// BalancerFactory get or create a balancer with given target\n// if it has the same key(reslover.Target(target)), we will cache and reuse the Balance\ntype BalancerFactory struct {\n\tHookable\n\topts       Options\n\tcache      sync.Map // key -> LoadBalancer\n\tresolver   discovery.Resolver\n\tbalancer   loadbalance.Loadbalancer\n\trebalancer loadbalance.Rebalancer\n\tsfg        singleflight.Group\n}\n\nfunc cacheKey(resolver, balancer string, opts Options) string {\n\treturn fmt.Sprintf(\"%s|%s|{%s %s}\", resolver, balancer, opts.RefreshInterval, opts.ExpireInterval)\n}\n\nfunc newBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n\tb := &BalancerFactory{\n\t\topts:     opts,\n\t\tresolver: resolver,\n\t\tbalancer: balancer,\n\t}\n\tif rb, ok := balancer.(loadbalance.Rebalancer); ok {\n\t\thrb := newHookRebalancer(rb)\n\t\tb.rebalancer = hrb\n\t\tb.Hookable = hrb\n\t} else {\n\t\tb.Hookable = noopHookRebalancer{}\n\t}\n\tgo b.watcher()\n\treturn b\n}\n\n// NewBalancerFactory get or create a balancer factory for balancer instance\n// cache key with resolver name, balancer name and options\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// watch expired balancer\nfunc (b *BalancerFactory) watcher() {\n\tfor range time.Tick(b.opts.ExpireInterval) {\n\t\tb.cache.Range(func(key, value interface{}) bool {\n\t\t\tbl := value.(*Balancer)\n\t\t\tif atomic.CompareAndSwapInt32(&bl.expire, 0, 1) {\n\t\t\t\t// 1. set expire flag\n\t\t\t\t// 2. wait next ticker for collect, maybe the balancer is used again\n\t\t\t\t// (avoid being immediate delete the balancer which had been created recently)\n\t\t\t} else {\n\t\t\t\tb.cache.Delete(key)\n\t\t\t\tbl.close()\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// cache key with resolver name prefix avoid conflict for balancer\nfunc renameResultCacheKey(res *discovery.Result, resolverName string) {\n\tres.CacheKey = resolverName + \":\" + res.CacheKey\n}\n\n// Get create a new balancer if not exists\nfunc (b *BalancerFactory) Get(ctx context.Context, target rpcinfo.EndpointInfo) (*Balancer, error) {\n\tdesc := b.resolver.Target(ctx, target)\n\tval, ok := b.cache.Load(desc)\n\tif ok {\n\t\treturn val.(*Balancer), nil\n\t}\n\tval, err, _ := b.sfg.Do(desc, func() (interface{}, error) {\n\t\tres, err := b.resolver.Resolve(ctx, desc)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trenameResultCacheKey(&res, b.resolver.Name())\n\t\tbl := &Balancer{\n\t\t\tb:      b,\n\t\t\ttarget: desc,\n\t\t}\n\t\tbl.res.Store(res)\n\t\tbl.sharedTicker = getSharedTicker(bl, b.opts.RefreshInterval)\n\t\tb.cache.Store(desc, bl)\n\t\treturn bl, nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn val.(*Balancer), nil\n}\n\n// Balancer same with loadbalance.Loadbalancer but without resolver.Result that\n// has been cached\ntype Balancer struct {\n\tb            *BalancerFactory\n\ttarget       string       // a description returned from the resolver's Target method\n\tres          atomic.Value // newest and previous discovery result\n\texpire       int32        // 0 = normal, 1 = expire and collect next ticker\n\tsharedTicker *utils.SharedTicker\n}\n\nfunc (bl *Balancer) Refresh() {\n\tres, err := bl.b.resolver.Resolve(context.Background(), bl.target)\n\tif err != nil {\n\t\tklog.Warnf(\"KITEX: resolver refresh failed, key=%s error=%s\", bl.target, err.Error())\n\t\treturn\n\t}\n\trenameResultCacheKey(&res, bl.b.resolver.Name())\n\tprev := bl.res.Load().(discovery.Result)\n\tif bl.b.rebalancer != nil {\n\t\tif ch, ok := bl.b.resolver.Diff(res.CacheKey, prev, res); ok {\n\t\t\tbl.b.rebalancer.Rebalance(ch)\n\t\t}\n\t}\n\t// replace previous result\n\tbl.res.Store(res)\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (bl *Balancer) Tick() {\n\tbl.Refresh()\n}\n\n// GetResult returns the discovery result that the Balancer holds.\nfunc (bl *Balancer) GetResult() (res discovery.Result, ok bool) {\n\tif v := bl.res.Load(); v != nil {\n\t\treturn v.(discovery.Result), true\n\t}\n\treturn\n}\n\n// GetPicker equal to loadbalance.Balancer without pass discovery.Result, because we cache the result\nfunc (bl *Balancer) GetPicker() loadbalance.Picker {\n\tatomic.StoreInt32(&bl.expire, 0)\n\tres := bl.res.Load().(discovery.Result)\n\treturn bl.b.balancer.GetPicker(res)\n}\n\nfunc (bl *Balancer) close() {\n\t// notice the under rebalancer\n\tif rb, ok := bl.b.balancer.(loadbalance.Rebalancer); ok {\n\t\t// notice to rebalancing\n\t\trb.Delete(discovery.Change{\n\t\t\tResult: discovery.Result{\n\t\t\t\tCacheable: true,\n\t\t\t\tCacheKey:  bl.res.Load().(discovery.Result).CacheKey,\n\t\t\t},\n\t\t})\n\t}\n\t// delete from sharedTicker\n\tbl.sharedTicker.Delete(bl)\n}\n\nconst unknown = \"unknown\"\n\nfunc Dump() interface{} {\n\ttype instInfo struct {\n\t\tAddress string\n\t\tWeight  int\n\t}\n\tcacheDump := make(map[string]interface{})\n\tbalancerFactories.Range(func(key, val interface{}) bool {\n\t\tcacheKey := key.(string)\n\t\tif bf, ok := val.(*BalancerFactory); ok {\n\t\t\trouteMap := make(map[string]interface{})\n\t\t\tcacheDump[cacheKey] = routeMap\n\t\t\tbf.cache.Range(func(k, v interface{}) bool {\n\t\t\t\trouteKey := k.(string)\n\t\t\t\tif bl, ok := v.(*Balancer); ok {\n\t\t\t\t\tif dr, ok := bl.res.Load().(discovery.Result); ok {\n\t\t\t\t\t\tinsts := make([]instInfo, 0, len(dr.Instances))\n\t\t\t\t\t\tfor i := range dr.Instances {\n\t\t\t\t\t\t\tinst := dr.Instances[i]\n\t\t\t\t\t\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\t\t\t\t\t\tinsts = append(insts, instInfo{Address: addr, Weight: inst.Weight()})\n\t\t\t\t\t\t}\n\t\t\t\t\t\trouteMap[routeKey] = insts\n\t\t\t\t\t} else {\n\t\t\t\t\t\trouteMap[routeKey] = unknown\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\trouteMap[routeKey] = unknown\n\t\t\t\t}\n\t\t\t\treturn true\n\t\t\t})\n\t\t} else {\n\t\t\tcacheDump[cacheKey] = unknown\n\t\t}\n\t\treturn true\n\t})\n\treturn cacheDump\n}\n\n```\n# Function Signature: func NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n# Natural Language Annotations: // NewBalancerFactory get or create a balancer factory for balancer instance\n// cache key with resolver name, balancer name and options\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// NewBalancerFactory get or create a balancer factory for balancer instance\n// cache key with resolver name, balancer name and options\nfunc NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n\topts.check() // Ensure the options have valid values\n\n\t// Create the cache key using resolver name, balancer name, and options\n\tcacheKey := cacheKey(resolver.Name(), balancer.Name(), opts)\n\n\t// Use singleflight.Group to ensure that only one goroutine initializes the BalancerFactory\n\tval, _, _ := balancerFactoriesSfg.Do(cacheKey, func() (interface{}, error) {\n\t\t// Check if the factory already exists in the cache\n\t\tif existing, ok := balancerFactories.Load(cacheKey); ok {\n\t\t\treturn existing, nil\n\t\t}\n\n\t\t// Create a new BalancerFactory if it does not exist\n\t\tbf := newBalancerFactory(resolver, balancer, opts)\n\n\t\t// Cache the newly created BalancerFactory\n\t\tif opts.Cacheable {\n\t\t\tbalancerFactories.Store(cacheKey, bf)\n\t\t}\n\n\t\treturn bf, nil\n\t})\n\n\t// Type assert and return the BalancerFactory\n\treturn val.(*BalancerFactory)\n}\n```"]}, "kitex-pkg/stats/event.go-DefineNewEvent": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/stats/event.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage stats\n\nimport (\n\t\"errors\"\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\n// EventIndex indicates a unique event.\ntype EventIndex int\n\n// Level sets the record level.\ntype Level int\n\n// Event levels.\nconst (\n\tLevelDisabled Level = iota\n\tLevelBase\n\tLevelDetailed\n)\n\n// Event is used to indicate a specific event.\ntype Event interface {\n\tIndex() EventIndex\n\tLevel() Level\n}\n\ntype event struct {\n\tidx   EventIndex\n\tlevel Level\n}\n\n// Index implements the Event interface.\nfunc (e event) Index() EventIndex {\n\treturn e.idx\n}\n\n// Level implements the Event interface.\nfunc (e event) Level() Level {\n\treturn e.level\n}\n\nconst (\n\t_ EventIndex = iota\n\tserverHandleStart\n\tserverHandleFinish\n\tclientConnStart\n\tclientConnFinish\n\trpcStart\n\trpcFinish\n\treadStart\n\treadFinish\n\twaitReadStart\n\twaitReadFinish\n\twriteStart\n\twriteFinish\n\tstreamRecv\n\tstreamSend\n\n\t// NOTE: add new events before this line\n\tpredefinedEventNum\n)\n\n// Predefined events.\nvar (\n\tRPCStart  = newEvent(rpcStart, LevelBase)\n\tRPCFinish = newEvent(rpcFinish, LevelBase)\n\n\tServerHandleStart  = newEvent(serverHandleStart, LevelDetailed)\n\tServerHandleFinish = newEvent(serverHandleFinish, LevelDetailed)\n\tClientConnStart    = newEvent(clientConnStart, LevelDetailed)\n\tClientConnFinish   = newEvent(clientConnFinish, LevelDetailed)\n\tReadStart          = newEvent(readStart, LevelDetailed)\n\tReadFinish         = newEvent(readFinish, LevelDetailed)\n\tWaitReadStart      = newEvent(waitReadStart, LevelDetailed)\n\tWaitReadFinish     = newEvent(waitReadFinish, LevelDetailed)\n\tWriteStart         = newEvent(writeStart, LevelDetailed)\n\tWriteFinish        = newEvent(writeFinish, LevelDetailed)\n\n\t// Streaming Events\n\tStreamRecv = newEvent(streamRecv, LevelDetailed)\n\tStreamSend = newEvent(streamSend, LevelDetailed)\n)\n\n// errors\nvar (\n\tErrNotAllowed = errors.New(\"event definition is not allowed after initialization\")\n\tErrDuplicated = errors.New(\"event name is already defined\")\n)\n\nvar (\n\tlock        sync.RWMutex\n\tinited      int32\n\tuserDefined = make(map[string]Event)\n\tmaxEventNum = int(predefinedEventNum)\n)\n\n// FinishInitialization freezes all events defined and prevents further definitions to be added.\nfunc FinishInitialization() {\n\tlock.Lock()\n\tdefer lock.Unlock()\n\tatomic.StoreInt32(&inited, 1)\n}\n\n// DefineNewEvent allows user to add event definitions during program initialization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// MaxEventNum returns the number of event defined.\nfunc MaxEventNum() int {\n\tlock.RLock()\n\tdefer lock.RUnlock()\n\treturn maxEventNum\n}\n\n// PredefinedEventNum returns the number of predefined events of kitex.\nfunc PredefinedEventNum() int {\n\treturn int(predefinedEventNum)\n}\n\nfunc newEvent(idx EventIndex, level Level) Event {\n\treturn event{\n\t\tidx:   idx,\n\t\tlevel: level,\n\t}\n}\n\n```\n# Function Signature: func DefineNewEvent(name string, level Level) (Event, error) {\n# Natural Language Annotations: // DefineNewEvent allows user to add event definitions during program initialization.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// DefineNewEvent allows user to add event definitions during program initialization.\nfunc DefineNewEvent(name string, level Level) (Event, error) {\n\t// Acquire write lock to ensure thread safety\n\tlock.Lock()\n\tdefer lock.Unlock()\n\n\t// Check if initialization has already finished\n\tif atomic.LoadInt32(&inited) == 1 {\n\t\treturn nil, ErrNotAllowed\n\t}\n\n\t// Check if the event name is already defined\n\tif _, exists := userDefined[name]; exists {\n\t\treturn nil, ErrDuplicated\n\t}\n\n\t// Define a new event with a unique event index\n\tnewIdx := EventIndex(maxEventNum)\n\tnewEvent := newEvent(newIdx, level)\n\n\t// Add the new event to the map of user-defined events\n\tuserDefined[name] = newEvent\n\n\t// Increment max event number for future events\n\tmaxEventNum++\n\n\t// Return the newly created event\n\treturn newEvent, nil\n}\n```"]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftFileProviderWithDynamicGo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n# Natural Language Annotations: // NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\nfunc NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp := &thriftFileProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // create a buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},    // enable DynamicGo\n\t}\n\t\n\tsvc, err := newServiceDescriptorFromPath(path, includeDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Attempt to create a dynamicgo descriptor\n\tif err := newDynamicGoDscFromContent(svc, path, \"\", nil, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false // fallback to original if dynamicgo fails\n\t}\n\n\tp.svcs <- svc // send the service descriptor to the channel\n\treturn p, nil\n}\n```\nThis function initializes a new instance of `thriftFileProvider`, attempts to parse and create a descriptor for the Thrift IDL at the specified path using the provided include directories, and optionally attempts to initialize a dynamic service descriptor. If successful, the descriptor is sent to a channel for asynchronous consumption."]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftContentWithAbsIncludePathProviderWithDynamicGo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n# Natural Language Annotations: // NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\nfunc NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tmainIDLContent, exists := includes[mainIDLPath]\n\tif !exists {\n\t\treturn nil, fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\n\tsvc, err := newServiceDescriptorFromContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\tp.svcs <- svc\n\n\treturn p, nil\n}\n```"]}, "kitex-pkg/generic/generic.go-MapThriftGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func MapThriftGeneric(p DescriptorProvider) (Generic, error) {\n# Natural Language Annotations: // MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\nPlease return the generated code surrounded by ```\n", "response": ["Based on the provided information and context, here is the implementation for the `MapThriftGeneric` function. The function is expected to create a map-based generic instance that utilizes a thrift codec. It should handle errors that might occur during the codec creation and return the generated generic instance.\n\n```go\nfunc MapThriftGeneric(p DescriptorProvider) (Generic, error) {\n    codec, err := newMapThriftCodec(p, thriftCodec)\n    if err != nil {\n        return nil, err\n    }\n    return &mapThriftGeneric{\n        codec: codec,\n    }, nil\n}\n```\n\n### Explanation:\n\n1. **function signature**: The function matches the given signature, accepting a `DescriptorProvider` and returning a `Generic` and an `error`.\n\n2. **Codec Creation**: \n   - The function first attempts to create a new map-thrift codec using a helper function `newMapThriftCodec`, passing in the provided descriptor and a `thriftCodec`.\n   - If the codec creation fails, an error is returned.\n\n3. **Generic Instance**: \n   - On successful codec creation, the function returns a pointer to a new instance of `mapThriftGeneric`, initialized with the created codec.\n   - The `mapThriftGeneric` is expected to be a type that implements the `Generic` interface, and the codec is expected to be utilized in further operations within the `mapThriftGeneric` type.\n\nMake sure that `newMapThriftCodec` and `DescriptorProvider` are properly defined elsewhere in the code, consistent with the types and methods expected by this function."]}, "kitex-pkg/generic/generic.go-JSONThriftGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n# Natural Language Annotations: // JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// JSONThriftGeneric creates a json mapping generic with options.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// Example usage:\n//   g, err := generic.JSONThriftGeneric(p)\n//   SetBinaryWithBase64(g, false)\nfunc JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONThriftCodec(p, thriftCodec, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &jsonThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```"]}, "kitex-pkg/generic/generic.go-JSONPbGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func JSONPbGeneric(p PbDescriptorProviderDynamicGo, opts ...Option) (Generic, error) {\n# Natural Language Annotations: // JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc JSONPbGeneric(p PbDescriptorProviderDynamicGo, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONPbCodec(p, pbCodec, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t\n\tfor _, opt := range opts {\n\t\topt(codec)\n\t}\n\n\treturn &jsonPbGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```"]}, "kitex-pkg/generic/thrift/write.go-writeStruct": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thrift/write.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/tidwall/gjson\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/proto\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\ntype writerOption struct {\n\trequestBase *Base // request base from metahandler\n\t// decoding Base64 to binary\n\tbinaryWithBase64 bool\n}\n\ntype writer func(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error\n\ntype fieldGetter func(val interface{}, field *descriptor.FieldDescriptor) (interface{}, bool)\n\nvar mapGetter fieldGetter = func(val interface{}, field *descriptor.FieldDescriptor) (interface{}, bool) {\n\tst := val.(map[string]interface{})\n\tret, ok := st[field.FieldName()]\n\treturn ret, ok\n}\n\nvar pbGetter fieldGetter = func(val interface{}, field *descriptor.FieldDescriptor) (interface{}, bool) {\n\tst := val.(proto.Message)\n\tret, err := st.TryGetFieldByNumber(int(field.ID))\n\treturn ret, err == nil\n}\n\nfunc typeOf(sample interface{}, t *descriptor.TypeDescriptor, opt *writerOption) (descriptor.Type, writer, error) {\n\ttt := t.Type\n\tswitch sample.(type) {\n\tcase bool:\n\t\treturn descriptor.BOOL, writeBool, nil\n\tcase int8, byte:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt8, nil\n\t\t}\n\tcase int16:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt16, nil\n\t\t}\n\tcase int32:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt32, nil\n\t\t}\n\tcase int64:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt64, nil\n\t\t}\n\tcase float64:\n\t\t// maybe come from json decode\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64, descriptor.DOUBLE:\n\t\t\treturn tt, writeJSONFloat64, nil\n\t\t}\n\tcase json.Number:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64, descriptor.DOUBLE:\n\t\t\treturn tt, writeJSONNumber, nil\n\t\t}\n\tcase string:\n\t\t// maybe a base64 string encoded from binary\n\t\tif t.Name == \"binary\" && opt.binaryWithBase64 {\n\t\t\treturn descriptor.STRING, writeBase64Binary, nil\n\t\t}\n\t\t// maybe a json number string\n\t\treturn descriptor.STRING, writeString, nil\n\tcase []byte:\n\t\tif tt == descriptor.LIST {\n\t\t\treturn descriptor.LIST, writeBinaryList, nil\n\t\t}\n\t\treturn descriptor.STRING, writeBinary, nil\n\tcase []interface{}:\n\t\treturn descriptor.LIST, writeList, nil\n\tcase map[interface{}]interface{}:\n\t\treturn descriptor.MAP, writeInterfaceMap, nil\n\tcase map[string]interface{}:\n\t\t//  4: optional map<i64, ReqItem> req_items (api.body='req_items')\n\t\t// need parse string into int64\n\t\tswitch tt {\n\t\tcase descriptor.STRUCT:\n\t\t\treturn descriptor.STRUCT, writeStruct, nil\n\t\tcase descriptor.MAP:\n\t\t\treturn descriptor.MAP, writeStringMap, nil\n\t\t}\n\tcase proto.Message:\n\t\treturn descriptor.STRUCT, writeStruct, nil\n\tcase *descriptor.HTTPRequest:\n\t\treturn descriptor.STRUCT, writeHTTPRequest, nil\n\tcase *gjson.Result:\n\t\treturn descriptor.STRUCT, writeJSON, nil\n\tcase nil, descriptor.Void: // nil and Void\n\t\treturn descriptor.VOID, writeVoid, nil\n\t}\n\treturn 0, nil, fmt.Errorf(\"unsupported type:%T, expected type:%s\", sample, tt)\n}\n\nfunc typeJSONOf(data *gjson.Result, t *descriptor.TypeDescriptor, opt *writerOption) (v interface{}, w writer, err error) {\n\ttt := t.Type\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = perrors.NewProtocolErrorWithType(perrors.InvalidData, fmt.Sprintf(\"json convert error:%#+v\", r))\n\t\t}\n\t}()\n\tswitch tt {\n\tcase descriptor.BOOL:\n\t\tv = data.Bool()\n\t\tw = writeBool\n\t\treturn\n\tcase descriptor.I08:\n\t\tv = int8(data.Int())\n\t\tw = writeInt8\n\t\treturn\n\tcase descriptor.I16:\n\t\tv = int16(data.Int())\n\t\tw = writeInt16\n\t\treturn\n\tcase descriptor.I32:\n\t\tv = int32(data.Int())\n\t\tw = writeInt32\n\t\treturn\n\tcase descriptor.I64:\n\t\tv = data.Int()\n\t\tw = writeInt64\n\t\treturn\n\tcase descriptor.DOUBLE:\n\t\tv = data.Float()\n\t\tw = writeJSONFloat64\n\t\treturn\n\tcase descriptor.STRING:\n\t\tv = data.String()\n\t\tif t.Name == \"binary\" && opt.binaryWithBase64 {\n\t\t\tw = writeBase64Binary\n\t\t} else {\n\t\t\tw = writeString\n\t\t}\n\t\treturn\n\t// case descriptor.BINARY:\n\t//\treturn writeBinary, nil\n\tcase descriptor.SET, descriptor.LIST:\n\t\tv = data.Array()\n\t\tw = writeJSONList\n\t\treturn\n\tcase descriptor.MAP:\n\t\tv = data.Map()\n\t\tw = writeStringJSONMap\n\t\treturn\n\tcase descriptor.STRUCT:\n\t\tv = data\n\t\tw = writeJSON\n\t\treturn\n\tcase descriptor.VOID: // nil and Void\n\t\tv = data\n\t\tw = writeVoid\n\t\treturn\n\t}\n\treturn 0, nil, fmt.Errorf(\"data:%#v, expected type:%s, err:%#v\", data, tt, err)\n}\n\nfunc nextWriter(sample interface{}, t *descriptor.TypeDescriptor, opt *writerOption) (writer, error) {\n\ttt, fn, err := typeOf(sample, t, opt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif t.Type == thrift.SET && tt == thrift.LIST {\n\t\ttt = thrift.SET\n\t}\n\treturn fn, assertType(t.Type, tt)\n}\n\nfunc nextJSONWriter(data *gjson.Result, t *descriptor.TypeDescriptor, opt *writerOption) (interface{}, writer, error) {\n\tv, fn, err := typeJSONOf(data, t, opt)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn v, fn, nil\n}\n\nfunc writeEmptyValue(out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tswitch t.Type {\n\tcase descriptor.BOOL:\n\t\treturn out.WriteBool(false)\n\tcase descriptor.I08:\n\t\treturn out.WriteByte(0)\n\tcase descriptor.I16:\n\t\treturn out.WriteI16(0)\n\tcase descriptor.I32:\n\t\treturn out.WriteI32(0)\n\tcase descriptor.I64:\n\t\treturn out.WriteI64(0)\n\tcase descriptor.DOUBLE:\n\t\treturn out.WriteDouble(0)\n\tcase descriptor.STRING:\n\t\tif t.Name == \"binary\" && opt.binaryWithBase64 {\n\t\t\treturn out.WriteBinary([]byte{})\n\t\t} else {\n\t\t\treturn out.WriteString(\"\")\n\t\t}\n\tcase descriptor.LIST, descriptor.SET:\n\t\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), 0); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteListEnd()\n\tcase descriptor.MAP:\n\t\tif err := out.WriteMapBegin(t.Key.Type.ToThriftTType(), t.Elem.Type.ToThriftTType(), 0); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteMapEnd()\n\tcase descriptor.STRUCT:\n\t\tif err := out.WriteStructBegin(t.Name); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := out.WriteFieldStop(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteStructEnd()\n\tcase descriptor.VOID:\n\t\treturn nil\n\t}\n\treturn fmt.Errorf(\"unsupported type:%T\", t)\n}\n\nfunc wrapStructWriter(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tif err := out.WriteStructBegin(t.Struct.Name); err != nil {\n\t\treturn err\n\t}\n\tfor name, field := range t.Struct.FieldsByName {\n\t\tif field.IsException {\n\t\t\t// generic server ignore the exception, because no description for exception\n\t\t\t// generic handler just return error\n\t\t\tcontinue\n\t\t}\n\t\tif val != nil {\n\t\t\tif err := out.WriteFieldBegin(field.Name, field.Type.Type.ToThriftTType(), int16(field.ID)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\twriter, err := nextWriter(val, field.Type, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"nextWriter of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t\tif err := writer(ctx, val, out, field.Type, opt); err != nil {\n\t\t\t\treturn fmt.Errorf(\"writer of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tif err := out.WriteFieldStop(); err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteStructEnd()\n}\n\nfunc wrapJSONWriter(ctx context.Context, val *gjson.Result, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tif err := out.WriteStructBegin(t.Struct.Name); err != nil {\n\t\treturn err\n\t}\n\tfor name, field := range t.Struct.FieldsByName {\n\t\tif field.IsException {\n\t\t\t// generic server ignore the exception, because no description for exception\n\t\t\t// generic handler just return error\n\t\t\tcontinue\n\t\t}\n\t\tif err := out.WriteFieldBegin(field.Name, field.Type.Type.ToThriftTType(), int16(field.ID)); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tv, writer, err := nextJSONWriter(val, field.Type, opt)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"nextJSONWriter of field[%s] error %w\", name, err)\n\t\t}\n\t\tif err := writer(ctx, v, out, field.Type, opt); err != nil {\n\t\t\treturn fmt.Errorf(\"writer of field[%s] error %w\", name, err)\n\t\t}\n\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err := out.WriteFieldStop(); err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteStructEnd()\n}\n\nfunc writeVoid(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn writeStruct(ctx, map[string]interface{}{}, out, t, opt)\n}\n\nfunc writeBool(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteBool(val.(bool))\n}\n\nfunc writeInt8(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tvar i int8\n\tswitch val := val.(type) {\n\tcase int8:\n\t\ti = val\n\tcase uint8:\n\t\ti = int8(val)\n\tdefault:\n\t\treturn fmt.Errorf(\"unsupported type: %T\", val)\n\t}\n\t// compatible with lossless conversion\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\treturn out.WriteByte(i)\n\tcase thrift.I16:\n\t\treturn out.WriteI16(int16(i))\n\tcase thrift.I32:\n\t\treturn out.WriteI32(int32(i))\n\tcase thrift.I64:\n\t\treturn out.WriteI64(int64(i))\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeInt16(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// compatible with lossless conversion\n\ti := val.(int16)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\tif i&0xff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i8: %v\", i)\n\t\t}\n\t\treturn out.WriteByte(int8(i))\n\tcase thrift.I16:\n\t\treturn out.WriteI16(i)\n\tcase thrift.I32:\n\t\treturn out.WriteI32(int32(i))\n\tcase thrift.I64:\n\t\treturn out.WriteI64(int64(i))\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeInt32(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// compatible with lossless conversion\n\ti := val.(int32)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\tif i&0xff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i8: %v\", i)\n\t\t}\n\t\treturn out.WriteByte(int8(i))\n\tcase thrift.I16:\n\t\tif i&0xffff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i16: %v\", i)\n\t\t}\n\t\treturn out.WriteI16(int16(i))\n\tcase thrift.I32:\n\t\treturn out.WriteI32(i)\n\tcase thrift.I64:\n\t\treturn out.WriteI64(int64(i))\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeInt64(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// compatible with lossless conversion\n\ti := val.(int64)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\tif i&0xff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i8: %v\", i)\n\t\t}\n\t\treturn out.WriteByte(int8(i))\n\tcase thrift.I16:\n\t\tif i&0xffff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i16: %v\", i)\n\t\t}\n\t\treturn out.WriteI16(int16(i))\n\tcase thrift.I32:\n\t\tif i&0xffffffff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i32: %v\", i)\n\t\t}\n\t\treturn out.WriteI32(int32(i))\n\tcase thrift.I64:\n\t\treturn out.WriteI64(i)\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeJSONNumber(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tjn := val.(json.Number)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt8(ctx, int8(i), out, t, opt)\n\tcase thrift.I16:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt16(ctx, int16(i), out, t, opt)\n\tcase thrift.I32:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt32(ctx, int32(i), out, t, opt)\n\tcase thrift.I64:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt64(ctx, i, out, t, opt)\n\tcase thrift.DOUBLE:\n\t\ti, err := jn.Float64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeFloat64(ctx, i, out, t, opt)\n\t}\n\treturn nil\n}\n\nfunc writeJSONFloat64(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\ti := val.(float64)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\treturn writeInt8(ctx, int8(i), out, t, opt)\n\tcase thrift.I16:\n\t\treturn writeInt16(ctx, int16(i), out, t, opt)\n\tcase thrift.I32:\n\t\treturn writeInt32(ctx, int32(i), out, t, opt)\n\tcase thrift.I64:\n\t\treturn writeInt64(ctx, int64(i), out, t, opt)\n\tcase thrift.DOUBLE:\n\t\treturn writeFloat64(ctx, i, out, t, opt)\n\t}\n\treturn fmt.Errorf(\"need number type, but got: %s\", t.Type)\n}\n\nfunc writeFloat64(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteDouble(val.(float64))\n}\n\nfunc writeString(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteString(val.(string))\n}\n\nfunc writeBase64Binary(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tbytes, err := base64.StdEncoding.DecodeString(val.(string))\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteBinary(bytes)\n}\n\nfunc writeBinary(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteBinary(val.([]byte))\n}\n\nfunc writeBinaryList(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tl := val.([]byte)\n\tlength := len(l)\n\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tfor _, b := range l {\n\t\tif err := out.WriteByte(int8(b)); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn out.WriteListEnd()\n}\n\nfunc writeList(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tl := val.([]interface{})\n\tlength := len(l)\n\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteListEnd()\n\t}\n\tvar (\n\t\twriter writer\n\t\terr    error\n\t)\n\tfor _, elem := range l {\n\t\tif elem == nil {\n\t\t\tif err = writeEmptyValue(out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif writer == nil {\n\t\t\t\tif writer, err = nextWriter(elem, t.Elem, opt); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := writer(ctx, elem, out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn out.WriteListEnd()\n}\n\nfunc writeJSONList(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tl := val.([]gjson.Result)\n\tlength := len(l)\n\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteListEnd()\n\t}\n\tfor _, elem := range l {\n\t\tv, writer, err := nextJSONWriter(&elem, t.Elem, opt)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := writer(ctx, v, out, t.Elem, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn out.WriteListEnd()\n}\n\nfunc writeInterfaceMap(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tm := val.(map[interface{}]interface{})\n\tlength := len(m)\n\tif err := out.WriteMapBegin(t.Key.Type.ToThriftTType(), t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteMapEnd()\n\t}\n\tvar (\n\t\tkeyWriter  writer\n\t\telemWriter writer\n\t\terr        error\n\t)\n\tfor key, elem := range m {\n\t\tif keyWriter == nil {\n\t\t\tif keyWriter, err = nextWriter(key, t.Key, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err := keyWriter(ctx, key, out, t.Key, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif elem == nil {\n\t\t\tif err = writeEmptyValue(out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif elemWriter == nil {\n\t\t\t\tif elemWriter, err = nextWriter(elem, t.Elem, opt); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := elemWriter(ctx, elem, out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn out.WriteMapEnd()\n}\n\nfunc writeStringMap(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tm := val.(map[string]interface{})\n\tlength := len(m)\n\tif err := out.WriteMapBegin(t.Key.Type.ToThriftTType(), t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteMapEnd()\n\t}\n\n\tvar (\n\t\tkeyWriter  writer\n\t\telemWriter writer\n\t)\n\tfor key, elem := range m {\n\t\t_key, err := buildinTypeFromString(key, t.Key)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif keyWriter == nil {\n\t\t\tif keyWriter, err = nextWriter(_key, t.Key, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err := keyWriter(ctx, _key, out, t.Key, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif elem == nil {\n\t\t\tif err = writeEmptyValue(out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif elemWriter == nil {\n\t\t\t\tif elemWriter, err = nextWriter(elem, t.Elem, opt); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := elemWriter(ctx, elem, out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn out.WriteMapEnd()\n}\n\nfunc writeStringJSONMap(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *\n```\n# Function Signature: func writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n# Natural Language Annotations: // writeStruct iter with Descriptor, can check the field's required and others\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// Begin writing the struct\n\tif err := out.WriteStructBegin(t.Name); err != nil {\n\t\treturn err\n\t}\n\t\n\t// Assume val is a map for reading struct fields\n\tst := val.(map[string]interface{})\n\t\n\t// Iterate through each field descriptor in the struct type\n\tfor _, field := range t.Struct.Fields {\n\t\tfieldValue, exists := st[field.FieldName()]\n\t\tif !exists && field.Required {\n\t\t\treturn fmt.Errorf(\"missing required field: %s\", field.FieldName())\n\t\t}\n\t\t\n\t\t// Write the field begin with field ID and type\n\t\tif err := out.WriteFieldBegin(field.FieldName(), field.Type.Type.ToThriftTType(), int16(field.ID)); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t\n\t\tif exists {\n\t\t\twriter, err := nextWriter(fieldValue, field.Type, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"error retrieving writer for field[%s]: %w\", field.FieldName(), err)\n\t\t\t}\n\t\t\t\n\t\t\t// Write the field value\n\t\t\tif err := writer(ctx, fieldValue, out, field.Type, opt); err != nil {\n\t\t\t\treturn fmt.Errorf(\"error writing field[%s]: %w\", field.FieldName(), err)\n\t\t\t}\n\t\t} else {\n\t\t\tif err := writeEmptyValue(out, field.Type, opt); err != nil {\n\t\t\t\treturn fmt.Errorf(\"error writing empty value for field[%s]: %w\", field.FieldName(), err)\n\t\t\t}\n\t\t}\n\t\t\n\t\t// Write the field end\n\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t\n\t// Write field stop and struct end\n\tif err := out.WriteFieldStop(); err != nil {\n\t\treturn err\n\t}\n\tif err := out.WriteStructEnd(); err != nil {\n\t\treturn err\n\t}\n\t\n\treturn nil\n}\n```"]}, "kitex-pkg/generic/descriptor/tree.go-addRoute": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/descriptor/tree.go:\n```\n/*\n * Copyright 2013 Julien Schmidt. All rights reserved.\n * Use of this source code is governed by a BSD-style license that can be found\n * in the LICENSE file.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage descriptor\n\nimport (\n\t\"fmt\"\n\t\"net/url\"\n\t\"strings\"\n)\n\nfunc countParams(path string) uint16 {\n\tvar n uint\n\tfor i := range []byte(path) {\n\t\tswitch path[i] {\n\t\tcase ':', '*':\n\t\t\tn++\n\t\t}\n\t}\n\treturn uint16(n)\n}\n\ntype nodeType uint8\n\nconst (\n\tstatic nodeType = iota // default\n\tparam\n\tcatchAll\n\tparamLabel = byte(':')\n\tanyLabel   = byte('*')\n\tslash      = \"/\"\n\tnilString  = \"\"\n)\n\ntype (\n\tnode struct {\n\t\tnType    nodeType\n\t\tlabel    byte\n\t\tprefix   string\n\t\tparent   *node\n\t\tchildren children\n\t\t// original path\n\t\tppath string\n\t\t// param names\n\t\tpnames     []string\n\t\tfunction   *FunctionDescriptor\n\t\tparamChild *node\n\t\tanyChild   *node\n\t\t// isLeaf indicates that node does not have child routes\n\t\tisLeaf bool\n\t}\n\tchildren []*node\n)\n\nfunc checkPathValid(path string) {\n\tif path == nilString {\n\t\tpanic(\"empty path\")\n\t}\n\tif path[0] != '/' {\n\t\tpanic(\"path must begin with '/'\")\n\t}\n\tfor i, c := range []byte(path) {\n\t\tswitch c {\n\t\tcase ':':\n\t\t\tif (i < len(path)-1 && path[i+1] == '/') || i == (len(path)-1) {\n\t\t\t\tpanic(\"wildcards must be named with a non-empty name in path '\" + path + \"'\")\n\t\t\t}\n\t\t\ti++\n\t\t\tfor ; i < len(path) && path[i] != '/'; i++ {\n\t\t\t\tif path[i] == ':' || path[i] == '*' {\n\t\t\t\t\tpanic(\"only one wildcard per path segment is allowed, find multi in path '\" + path + \"'\")\n\t\t\t\t}\n\t\t\t}\n\t\tcase '*':\n\t\t\tif i == len(path)-1 {\n\t\t\t\tpanic(\"wildcards must be named with a non-empty name in path '\" + path + \"'\")\n\t\t\t}\n\t\t\tif i > 0 && path[i-1] != '/' {\n\t\t\t\tpanic(\" no / before wildcards in path \" + path)\n\t\t\t}\n\t\t\tfor ; i < len(path); i++ {\n\t\t\t\tif path[i] == '/' {\n\t\t\t\t\tpanic(\"catch-all routes are only allowed at the end of the path in path '\" + path + \"'\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// addRoute adds a node with the given function to the path.\n// Not concurrency-safe!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (n *node) insert(path string, function *FunctionDescriptor, t nodeType, ppath string, pnames []string) {\n\tcurrentNode := n\n\tsearch := path\n\n\tfor {\n\t\tsearchLen := len(search)\n\t\tprefixLen := len(currentNode.prefix)\n\t\tlcpLen := 0\n\n\t\tmax := prefixLen\n\t\tif searchLen < max {\n\t\t\tmax = searchLen\n\t\t}\n\t\tfor ; lcpLen < max && search[lcpLen] == currentNode.prefix[lcpLen]; lcpLen++ {\n\t\t}\n\n\t\tif lcpLen == 0 {\n\t\t\tcurrentNode.label = search[0]\n\t\t\tcurrentNode.prefix = search\n\t\t\tif function != nil {\n\t\t\t\tcurrentNode.nType = t\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < prefixLen {\n\t\t\t// Split node\n\t\t\tn := newNode(\n\t\t\t\tcurrentNode.nType,\n\t\t\t\tcurrentNode.prefix[lcpLen:],\n\t\t\t\tcurrentNode,\n\t\t\t\tcurrentNode.children,\n\t\t\t\tcurrentNode.function,\n\t\t\t\tcurrentNode.ppath,\n\t\t\t\tcurrentNode.pnames,\n\t\t\t\tcurrentNode.paramChild,\n\t\t\t\tcurrentNode.anyChild,\n\t\t\t)\n\t\t\t// Update parent path for all children to new node\n\t\t\tfor _, child := range currentNode.children {\n\t\t\t\tchild.parent = n\n\t\t\t}\n\t\t\tif currentNode.paramChild != nil {\n\t\t\t\tcurrentNode.paramChild.parent = n\n\t\t\t}\n\t\t\tif currentNode.anyChild != nil {\n\t\t\t\tcurrentNode.anyChild.parent = n\n\t\t\t}\n\n\t\t\t// Reset parent node\n\t\t\tcurrentNode.nType = static\n\t\t\tcurrentNode.label = currentNode.prefix[0]\n\t\t\tcurrentNode.prefix = currentNode.prefix[:lcpLen]\n\t\t\tcurrentNode.children = nil\n\t\t\tcurrentNode.function = nil\n\t\t\tcurrentNode.ppath = nilString\n\t\t\tcurrentNode.pnames = nil\n\t\t\tcurrentNode.paramChild = nil\n\t\t\tcurrentNode.anyChild = nil\n\t\t\tcurrentNode.isLeaf = false\n\n\t\t\t// Only Static children could reach here\n\t\t\tcurrentNode.children = append(currentNode.children, n)\n\n\t\t\tif lcpLen == searchLen {\n\t\t\t\t// At parent node\n\t\t\t\tcurrentNode.nType = t\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t} else {\n\t\t\t\t// Create child node\n\t\t\t\tn = newNode(t, search[lcpLen:], currentNode, nil, function, ppath, pnames, nil, nil)\n\t\t\t\t// Only Static children could reach here\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < searchLen {\n\t\t\tsearch = search[lcpLen:]\n\t\t\tc := currentNode.findChildWithLabel(search[0])\n\t\t\tif c != nil {\n\t\t\t\t// Go deeper\n\t\t\t\tcurrentNode = c\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Create child node\n\t\t\tn := newNode(t, search, currentNode, nil, function, ppath, pnames, nil, nil)\n\t\t\tswitch t {\n\t\t\tcase static:\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\tcase param:\n\t\t\t\tcurrentNode.paramChild = n\n\t\t\tcase catchAll:\n\t\t\t\tcurrentNode.anyChild = n\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else {\n\t\t\t// Node already exists\n\t\t\tif currentNode.function != nil && function != nil {\n\t\t\t\tpanic(\"handlers are already registered for path '\" + ppath + \"'\")\n\t\t\t}\n\n\t\t\tif function != nil {\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tif len(currentNode.pnames) == 0 {\n\t\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n}\n\n// Returns the function registered with the given path (key). The values of\n// wildcards are saved to a map.\n// If no function can be found, a TSR (trailing slash redirect) recommendation is\n// made if a function exists with an extra (without the) trailing slash for the\n// given path.\nfunc (n *node) getValue(path string, params func() *Params, unescape bool) (function *FunctionDescriptor, ps *Params, tsr bool) {\n\tvar (\n\t\tcn          = n    // current node\n\t\tsearch      = path // current path\n\t\tsearchIndex = 0\n\t\tparamIndex  int\n\t)\n\n\tbacktrackToNextNodeType := func(fromNodeType nodeType) (nextNodeType nodeType, valid bool) {\n\t\tprevious := cn\n\t\tcn = previous.parent\n\t\tvalid = cn != nil\n\n\t\t// Next node type by priority\n\t\tif previous.nType == catchAll {\n\t\t\tnextNodeType = static\n\t\t} else {\n\t\t\tnextNodeType = previous.nType + 1\n\t\t}\n\n\t\tif fromNodeType == static {\n\t\t\t// when backtracking is done from static type block we did not change search so nothing to restore\n\t\t\treturn\n\t\t}\n\n\t\t// restore search to value it was before we move to current node we are backtracking from.\n\t\tif previous.nType == static {\n\t\t\tsearchIndex -= len(previous.prefix)\n\t\t} else {\n\t\t\tparamIndex--\n\t\t\t// for param/any node.prefix value is always `:`/`*` so we cannot deduce searchIndex from that and must use pValue\n\t\t\t// for that index as it would also contain part of path we cut off before moving into node we are backtracking from\n\t\t\tsearchIndex -= len(ps.params[paramIndex].Value)\n\t\t\tps.params = ps.params[:paramIndex]\n\t\t}\n\t\tsearch = path[searchIndex:]\n\t\treturn\n\t}\n\n\t// search order: static > param > any\n\tfor {\n\t\tif cn.nType == static {\n\t\t\tif len(search) >= len(cn.prefix) && cn.prefix == search[:len(cn.prefix)] {\n\t\t\t\t// Continue search\n\t\t\t\tsearch = search[len(cn.prefix):]\n\t\t\t\tsearchIndex = searchIndex + len(cn.prefix)\n\t\t\t} else {\n\t\t\t\t// not equal\n\t\t\t\tif (len(cn.prefix) == len(search)+1) &&\n\t\t\t\t\t(cn.prefix[len(search)]) == '/' && cn.prefix[:len(search)] == search && (cn.function != nil || cn.anyChild != nil) {\n\t\t\t\t\ttsr = true\n\t\t\t\t}\n\t\t\t\t// No matching prefix, let's backtrack to the first possible alternative node of the decision path\n\t\t\t\tnk, ok := backtrackToNextNodeType(static)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn // No other possibilities on the decision path\n\t\t\t\t} else if nk == param {\n\t\t\t\t\tgoto Param\n\t\t\t\t} else {\n\t\t\t\t\t// Not found (this should never be possible for static node we are looking currently)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif search == nilString && cn.function != nil {\n\t\t\tfunction = cn.function\n\t\t\tbreak\n\t\t}\n\n\t\t// Static node\n\t\tif search != nilString {\n\t\t\t// If it can execute that logic, there is handler registered on the current node and search is `/`.\n\t\t\tif search == \"/\" && cn.function != nil {\n\t\t\t\ttsr = true\n\t\t\t}\n\t\t\tif child := cn.findChild(search[0]); child != nil {\n\t\t\t\tcn = child\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif search == nilString {\n\t\t\tif cd := cn.findChild('/'); cd != nil && (cd.function != nil || cd.anyChild != nil) {\n\t\t\t\ttsr = true\n\t\t\t}\n\t\t}\n\n\tParam:\n\t\t// Param node\n\t\tif child := cn.paramChild; search != nilString && child != nil {\n\t\t\tcn = child\n\t\t\ti := strings.Index(search, slash)\n\t\t\tif i == -1 {\n\t\t\t\ti = len(search)\n\t\t\t}\n\t\t\tif ps == nil {\n\t\t\t\tps = params()\n\t\t\t}\n\t\t\tval := search[:i]\n\t\t\tif unescape {\n\t\t\t\tif v, err := url.QueryUnescape(val); err == nil {\n\t\t\t\t\tval = v\n\t\t\t\t}\n\t\t\t}\n\t\t\tps.params = ps.params[:paramIndex+1]\n\t\t\tps.params[paramIndex].Value = val\n\t\t\tparamIndex++\n\t\t\tsearch = search[i:]\n\t\t\tsearchIndex = searchIndex + i\n\t\t\tif search == nilString {\n\t\t\t\tif cd := cn.findChild('/'); cd != nil && (cd.function != nil || cd.anyChild != nil) {\n\t\t\t\t\ttsr = true\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\tAny:\n\t\t// Any node\n\t\tif child := cn.anyChild; child != nil {\n\t\t\t// If any node is found, use remaining path for paramValues\n\t\t\tcn = child\n\t\t\tif ps == nil {\n\t\t\t\tps = params()\n\t\t\t}\n\t\t\tindex := len(cn.pnames) - 1\n\t\t\tval := search\n\t\t\tif unescape {\n\t\t\t\tif v, err := url.QueryUnescape(val); err == nil {\n\t\t\t\t\tval = v\n\t\t\t\t}\n\t\t\t}\n\t\t\tps.params = ps.params[:paramIndex+1]\n\t\t\tps.params[index].Value = val\n\t\t\t// update indexes/search in case we need to backtrack when no handler match is found\n\t\t\tparamIndex++\n\t\t\tsearchIndex += len(search)\n\t\t\tsearch = nilString\n\t\t\tfunction = cn.function\n\t\t\tbreak\n\t\t}\n\n\t\t// Let's backtrack to the first possible alternative node of the decision path\n\t\tnk, ok := backtrackToNextNodeType(catchAll)\n\t\tif !ok {\n\t\t\tbreak // No other possibilities on the decision path\n\t\t} else if nk == param {\n\t\t\tgoto Param\n\t\t} else if nk == catchAll {\n\t\t\tgoto Any\n\t\t} else {\n\t\t\t// Not found\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif cn != nil {\n\t\tfor i, name := range cn.pnames {\n\t\t\tps.params[i].Key = name\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (n *node) findChild(l byte) *node {\n\tfor _, c := range n.children {\n\t\tif c.label == l {\n\t\t\treturn c\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (n *node) findChildWithLabel(l byte) *node {\n\tfor _, c := range n.children {\n\t\tif c.label == l {\n\t\t\treturn c\n\t\t}\n\t}\n\tif l == paramLabel {\n\t\treturn n.paramChild\n\t}\n\tif l == anyLabel {\n\t\treturn n.anyChild\n\t}\n\treturn nil\n}\n\nfunc newNode(t nodeType, pre string, p *node, child children, f *FunctionDescriptor, ppath string, pnames []string, paramChildren, anyChildren *node) *node {\n\treturn &node{\n\t\tnType:      t,\n\t\tlabel:      pre[0],\n\t\tprefix:     pre,\n\t\tparent:     p,\n\t\tchildren:   child,\n\t\tppath:      ppath,\n\t\tpnames:     pnames,\n\t\tfunction:   f,\n\t\tparamChild: paramChildren,\n\t\tanyChild:   anyChildren,\n\t\tisLeaf:     child == nil && paramChildren == nil && anyChildren == nil,\n\t}\n}\n\n```\n# Function Signature: func (n *node) addRoute(path string, function *FunctionDescriptor) {\n# Natural Language Annotations: // addRoute adds a node with the given function to the path.\n// Not concurrency-safe!\nPlease return the generated code surrounded by ```\n", "response": ["To implement the `addRoute` function, we will utilize the existing helper functions and logic previously defined in the file, such as `checkPathValid` and `insert`. The purpose of the `addRoute` function is to ensure that a path is valid before inserting it into the trie data structure. Here's the function implementation:\n\n```go\nfunc (n *node) addRoute(path string, function *FunctionDescriptor) {\n    // Validate the path\n    checkPathValid(path)\n\n    // Count the parameters in the path\n    paramCount := countParams(path)\n\n    // Prepare slice to hold parameter names, with size equal to the number of parameters\n    pnames := make([]string, paramCount)\n\n    // Insert the route into the tree\n    n.insert(path, function, static, path, pnames)\n}\n```\n\n### Explanation:\n1. **Path Validation**: The `checkPathValid` function is called to ensure the path conforms to expected rules such as starting with a `/` and not having unnamed wildcards.\n2. **Parameter Counting**: The `countParams` function is used to determine how many dynamic parameters are in the given path. This helps in managing routes with dynamic segments efficiently.\n3. **Parameter Names Management**: A slice is created to hold the parameter names. The size of this slice is determined by the number of parameters in the path. This ensures that parameter values can be correctly mapped to their names during path parsing.\n4. **Insert Path**: The `insert` function is invoked to add the path and its associated function to the tree. This function organizes the path segments efficiently into the existing trie structure, allowing for fast retrieval and precise matching."]}, "kitex-pkg/mem/span.go-NewSpanCache": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/mem/span.go:\n```\n/*\n * Copyright 2024 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage mem\n\nimport (\n\t\"math/bits\"\n\t\"sync/atomic\"\n\n\t\"github.com/bytedance/gopkg/lang/dirtmake\"\n)\n\n/* Span Cache: A thread-safe linear allocator\n\nDesign:\n1. [GC Friendly]: Centralize a batch of small bytes slice into a big size bytes slice to avoid malloc too many objects\n2. [Thread Safe]: Multi thread may share a same span, but it should fall back to the native allocator if lock conflict\n*/\n\nconst (\n\tspanCacheSize = 10\n\tminSpanObject = 128                                  // 128 B\n\tmaxSpanObject = (minSpanObject << spanCacheSize) - 1 // 128 KB\n\tminSpanClass  = 8                                    // = spanClass(minSpanObject)\n)\n\ntype spanCache struct {\n\tspans [spanCacheSize]*span\n}\n\n// NewSpanCache returns a spanCache with the given spanSize,\n// each span is used to allocate a binary of a specific size level.\n\n\n\n\n\n\n\n\n// Make returns a [:n:n] bytes slice from a cached buffer\n// NOTE: Make will not clear the underlay bytes for performance concern. So caller MUST set every byte before read.\nfunc (c *spanCache) Make(n int) []byte {\n\tsclass := spanClass(n) - minSpanClass\n\tif sclass < 0 || sclass >= len(c.spans) {\n\t\treturn dirtmake.Bytes(n, n)\n\t}\n\treturn c.spans[sclass].Make(n)\n}\n\n// Copy is an alias function for make-and-copy pattern\nfunc (c *spanCache) Copy(buf []byte) (p []byte) {\n\tp = c.Make(len(buf))\n\tcopy(p, buf)\n\treturn p\n}\n\n// NewSpan returns a span with given size\nfunc NewSpan(size int) *span {\n\tsp := new(span)\n\tsp.size = uint32(size)\n\tsp.buffer = dirtmake.Bytes(0, size)\n\treturn sp\n}\n\ntype span struct {\n\tlock   uint32\n\tread   uint32 // read index of buffer\n\tsize   uint32 // size of buffer\n\tbuffer []byte\n}\n\n// Make returns a [:n:n] bytes slice from a cached buffer\n// NOTE: Make will not clear the underlay bytes for performance concern. So caller MUST set every byte before read.\nfunc (b *span) Make(_n int) []byte {\n\tn := uint32(_n)\n\tif n >= b.size || !atomic.CompareAndSwapUint32(&b.lock, 0, 1) {\n\t\t// fallback path: make a new byte slice if current goroutine cannot get the lock or n is out of size\n\t\treturn dirtmake.Bytes(int(n), int(n))\n\t}\nSTART:\n\tb.read += n\n\t// fast path\n\tif b.read <= b.size {\n\t\tbuf := b.buffer[b.read-n : b.read : b.read]\n\t\tatomic.StoreUint32(&b.lock, 0)\n\t\treturn buf\n\t}\n\t// slow path: create a new buffer\n\tb.buffer = dirtmake.Bytes(int(b.size), int(b.size))\n\tb.read = 0\n\tgoto START\n}\n\n// Copy is an alias function for make-and-copy pattern\nfunc (b *span) Copy(buf []byte) (p []byte) {\n\tp = b.Make(len(buf))\n\tcopy(p, buf)\n\treturn p\n}\n\n// spanClass calc the minimum number of bits required to represent x\n// [2^sclass,2^(sclass+1)) bytes in a same span class\nfunc spanClass(size int) int {\n\tif size == 0 {\n\t\treturn 0\n\t}\n\treturn bits.Len(uint(size))\n}\n\n```\n# Function Signature: func NewSpanCache(spanSize int) *spanCache {\n# Natural Language Annotations: // NewSpanCache returns a spanCache with the given spanSize,\n// each span is used to allocate a binary of a specific size level.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// NewSpanCache returns a spanCache with the given spanSize,\n// each span is used to allocate a binary of a specific size level.\nfunc NewSpanCache(spanSize int) *spanCache {\n\tcache := &spanCache{}\n\tfor i := 0; i < spanCacheSize; i++ {\n\t\tsizeLevel := 1 << (i + minSpanClass)\n\t\tif sizeLevel > spanSize {\n\t\t\tcache.spans[i] = NewSpan(spanSize)\n\t\t} else {\n\t\t\tcache.spans[i] = NewSpan(sizeLevel)\n\t\t}\n\t}\n\treturn cache\n}\n```"]}, "kitex-pkg/profiler/profiler.go-Pause": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/profiler/profiler.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage profiler\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/pprof/profile\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\ntype profilerContextKey struct{}\n\ntype Profiler interface {\n\tRun(ctx context.Context) (err error)\n\tStop()\n\tPause()\n\tResume()\n\tPrepare(ctx context.Context) context.Context\n\tTag(ctx context.Context, tags ...string) context.Context\n\tUntag(ctx context.Context)\n\tLookup(ctx context.Context, key string) (string, bool)\n}\n\ntype Processor func(profiles []*TagsProfile) error\n\nfunc LogProcessor(profiles []*TagsProfile) error {\n\tif len(profiles) == 0 {\n\t\treturn nil\n\t}\n\tklog.Infof(\"KITEX: profiler collect %d records\", len(profiles))\n\tfor _, p := range profiles {\n\t\tif p.Key != \"\" {\n\t\t\tklog.Infof(\"KITEX: profiler - %s %.2f%%\", p.Key, p.Percent*100)\n\t\t} else {\n\t\t\tklog.Infof(\"KITEX: profiler - type=default %.2f%%\", p.Percent*100)\n\t\t}\n\t}\n\tklog.Info(\"---------------------------------\")\n\treturn nil\n}\n\nfunc NewProfiler(processor Processor, interval, window time.Duration) *profiler {\n\tif processor == nil {\n\t\tprocessor = LogProcessor\n\t}\n\treturn &profiler{\n\t\tstateCond: sync.NewCond(&sync.Mutex{}),\n\t\tprocessor: processor,\n\t\tinterval:  interval,\n\t\twindow:    window,\n\t}\n}\n\nvar _ Profiler = (*profiler)(nil)\n\nconst (\n\t// state changes:\n\t//   running => pausing => paused => resuming => running\n\t//           => stopped\n\tstateRunning  = 0\n\tstatePausing  = 1\n\tstatePaused   = 2\n\tstateResuming = 3\n\tstateStopped  = 4\n)\n\ntype profiler struct {\n\tdata      bytes.Buffer // protobuf\n\tstate     int\n\tstateCond *sync.Cond\n\t// settings\n\tprocessor Processor\n\tinterval  time.Duration // sleep time between every profiling window\n\twindow    time.Duration // profiling in the window, go pprof collect stack profile every 10ms\n}\n\n// Tag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Tag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Tag(ctx)\n\t}\n}\n\n// Untag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Untag(ctx)\n\t}\n}\n\ntype profilerContext struct {\n\tprofiler Profiler\n\tuntagCtx context.Context\n\ttags     []string\n}\n\nfunc newProfilerContext(profiler Profiler) *profilerContext {\n\treturn &profilerContext{\n\t\tprofiler: profiler,\n\t\ttags:     make([]string, 0, 12),\n\t}\n}\n\n// Prepare the profiler context\nfunc (p *profiler) Prepare(ctx context.Context) context.Context {\n\tif c := ctx.Value(profilerContextKey{}); c != nil {\n\t\treturn ctx\n\t}\n\treturn context.WithValue(ctx, profilerContextKey{}, newProfilerContext(p))\n}\n\n// State return current profiler state\nfunc (p *profiler) State() (state int) {\n\tp.stateCond.L.Lock()\n\tstate = p.state\n\tp.stateCond.L.Unlock()\n\treturn state\n}\n\n// Stop the profiler\nfunc (p *profiler) Stop() {\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, stateStopped)\n}\n\n// Pause the profiler.\n// The profiler has been paused when Pause() return\n\n\n\n\n\n\n\n\n\n\n// Resume the profiler.\n// The profiler has been resumed when Resume() return\n\n\n\n\n\n\n\n\n\n\n// Run start analyse the pprof data with interval and window settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Tag current goroutine with tags\n// If ctx already tagged, append the existed tags\nfunc (p *profiler) Tag(ctx context.Context, tags ...string) context.Context {\n\tpctx, ok := ctx.Value(profilerContextKey{}).(*profilerContext)\n\tif !ok {\n\t\tpctx = newProfilerContext(p)\n\t\tctx = context.WithValue(ctx, profilerContextKey{}, pctx)\n\t}\n\tif pctx.untagCtx == nil {\n\t\tpctx.untagCtx = ctx\n\t}\n\tpctx.tags = append(pctx.tags, tags...)\n\t// do not return pprof ctx\n\tpprof.SetGoroutineLabels(pprof.WithLabels(context.Background(), pprof.Labels(pctx.tags...)))\n\treturn ctx\n}\n\n// Untag current goroutine\n// Only untag if ctx already tagged, will not clear the goroutine labels if not tagged by profiler\nfunc (p *profiler) Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok && pc.untagCtx != nil {\n\t\t// if ctx have untagCtx, that means the current goroutine created by a tagged goroutine\n\t\t// we need to untag the goroutine when finished\n\t\t// else, do nothing\n\t\tpprof.SetGoroutineLabels(pc.untagCtx)\n\t}\n}\n\nfunc (p *profiler) Lookup(ctx context.Context, key string) (string, bool) {\n\treturn pprof.Label(ctx, key)\n}\n\nfunc (p *profiler) stateChange(from, to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != from { // wait state to from first\n\t\tp.stateCond.Wait()\n\t}\n\tp.state = to\n\tp.stateCond.L.Unlock()\n\tp.stateCond.Broadcast()\n}\n\nfunc (p *profiler) stateWait(to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != to {\n\t\tp.stateCond.Wait()\n\t}\n\tp.stateCond.L.Unlock()\n}\n\nfunc (p *profiler) startProfile() error {\n\tp.data.Reset()\n\treturn pprof.StartCPUProfile(&p.data)\n}\n\nfunc (p *profiler) stopProfile() {\n\tpprof.StopCPUProfile()\n}\n\nfunc (p *profiler) analyse() ([]*TagsProfile, error) {\n\t// parse protobuf data\n\tpf, err := profile.ParseData(p.data.Bytes())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// filter cpu value index\n\tsampleIdx := -1\n\tfor idx, st := range pf.SampleType {\n\t\tif st.Type == \"cpu\" {\n\t\t\tsampleIdx = idx\n\t\t\tbreak\n\t\t}\n\t}\n\tif sampleIdx < 0 {\n\t\treturn nil, errors.New(\"profiler: sample type not found\")\n\t}\n\n\t// calculate every sample expense\n\tcounter := map[string]*TagsProfile{} // map[tagsKey]funcProfile\n\tvar total int64\n\tfor _, sm := range pf.Sample {\n\t\tvalue := sm.Value[sampleIdx]\n\t\ttags := labelToTags(sm.Label)\n\t\ttagsKey := tagsToKey(tags)\n\t\ttp, ok := counter[tagsKey]\n\t\tif !ok {\n\t\t\ttp = &TagsProfile{}\n\t\t\tcounter[tagsKey] = tp\n\t\t\ttp.Key = tagsKey\n\t\t\ttp.Tags = tags\n\t\t}\n\t\ttp.Value += value\n\t\ttotal += value\n\t}\n\n\t// compensate value with duration date\n\tdurationRate := float64((p.interval + p.window) / p.window)\n\tprofiles := make([]*TagsProfile, 0, len(counter)) // flat to array\n\tfor _, l := range counter {\n\t\tl.Percent = float64(l.Value) / float64(total)\n\t\tl.Value = int64(durationRate * float64(l.Value))\n\t\tprofiles = append(profiles, l)\n\t}\n\treturn profiles, nil\n}\n\n// TagsProfile is the stats result group by tag key\ntype TagsProfile struct {\n\tKey     string   // eg: a=1,b=2\n\tTags    []string // eg: [\"a\", \"1\", \"b\", \"2\"]\n\tValue   int64    // pprof cpu times\n\tPercent float64  // <= 1.0\n}\n\nfunc labelToTags(label map[string][]string) []string {\n\ttags := make([]string, 0, len(label)*2)\n\tfor k, v := range label {\n\t\ttags = append(tags, k, strings.Join(v, \",\"))\n\t}\n\treturn tags\n}\n\nfunc tagsToKey(tags []string) string {\n\tif len(tags)%2 != 0 {\n\t\treturn \"\"\n\t}\n\ttagsPair := make([]string, 0, len(tags)/2)\n\tfor i := 0; i < len(tags); i += 2 {\n\t\ttagsPair = append(tagsPair, fmt.Sprintf(\"%s=%s\", tags[i], tags[i+1]))\n\t}\n\t// sort tags to make it a unique key\n\tsort.Strings(tagsPair)\n\treturn strings.Join(tagsPair, \"|\")\n}\n\n```\n# Function Signature: func (p *profiler) Pause() {\n# Natural Language Annotations: // Pause the profiler.\n// The profiler has been paused when Pause() return\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (p *profiler) Pause() {\n\t// Check if the current state is already paused or stopped\n\tcurrentState := p.State()\n\tif currentState == statePaused || currentState == stateStopped {\n\t\treturn\n\t}\n\n\t// Change state from running to pausing\n\tp.stateChange(stateRunning, statePausing)\n\n\t// Change state from pausing to paused\n\tp.stateChange(statePausing, statePaused)\n}\n```"]}, "kitex-pkg/profiler/profiler.go-Resume": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/profiler/profiler.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage profiler\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/pprof/profile\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\ntype profilerContextKey struct{}\n\ntype Profiler interface {\n\tRun(ctx context.Context) (err error)\n\tStop()\n\tPause()\n\tResume()\n\tPrepare(ctx context.Context) context.Context\n\tTag(ctx context.Context, tags ...string) context.Context\n\tUntag(ctx context.Context)\n\tLookup(ctx context.Context, key string) (string, bool)\n}\n\ntype Processor func(profiles []*TagsProfile) error\n\nfunc LogProcessor(profiles []*TagsProfile) error {\n\tif len(profiles) == 0 {\n\t\treturn nil\n\t}\n\tklog.Infof(\"KITEX: profiler collect %d records\", len(profiles))\n\tfor _, p := range profiles {\n\t\tif p.Key != \"\" {\n\t\t\tklog.Infof(\"KITEX: profiler - %s %.2f%%\", p.Key, p.Percent*100)\n\t\t} else {\n\t\t\tklog.Infof(\"KITEX: profiler - type=default %.2f%%\", p.Percent*100)\n\t\t}\n\t}\n\tklog.Info(\"---------------------------------\")\n\treturn nil\n}\n\nfunc NewProfiler(processor Processor, interval, window time.Duration) *profiler {\n\tif processor == nil {\n\t\tprocessor = LogProcessor\n\t}\n\treturn &profiler{\n\t\tstateCond: sync.NewCond(&sync.Mutex{}),\n\t\tprocessor: processor,\n\t\tinterval:  interval,\n\t\twindow:    window,\n\t}\n}\n\nvar _ Profiler = (*profiler)(nil)\n\nconst (\n\t// state changes:\n\t//   running => pausing => paused => resuming => running\n\t//           => stopped\n\tstateRunning  = 0\n\tstatePausing  = 1\n\tstatePaused   = 2\n\tstateResuming = 3\n\tstateStopped  = 4\n)\n\ntype profiler struct {\n\tdata      bytes.Buffer // protobuf\n\tstate     int\n\tstateCond *sync.Cond\n\t// settings\n\tprocessor Processor\n\tinterval  time.Duration // sleep time between every profiling window\n\twindow    time.Duration // profiling in the window, go pprof collect stack profile every 10ms\n}\n\n// Tag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Tag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Tag(ctx)\n\t}\n}\n\n// Untag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Untag(ctx)\n\t}\n}\n\ntype profilerContext struct {\n\tprofiler Profiler\n\tuntagCtx context.Context\n\ttags     []string\n}\n\nfunc newProfilerContext(profiler Profiler) *profilerContext {\n\treturn &profilerContext{\n\t\tprofiler: profiler,\n\t\ttags:     make([]string, 0, 12),\n\t}\n}\n\n// Prepare the profiler context\nfunc (p *profiler) Prepare(ctx context.Context) context.Context {\n\tif c := ctx.Value(profilerContextKey{}); c != nil {\n\t\treturn ctx\n\t}\n\treturn context.WithValue(ctx, profilerContextKey{}, newProfilerContext(p))\n}\n\n// State return current profiler state\nfunc (p *profiler) State() (state int) {\n\tp.stateCond.L.Lock()\n\tstate = p.state\n\tp.stateCond.L.Unlock()\n\treturn state\n}\n\n// Stop the profiler\nfunc (p *profiler) Stop() {\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, stateStopped)\n}\n\n// Pause the profiler.\n// The profiler has been paused when Pause() return\n\n\n\n\n\n\n\n\n\n\n// Resume the profiler.\n// The profiler has been resumed when Resume() return\n\n\n\n\n\n\n\n\n\n\n// Run start analyse the pprof data with interval and window settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Tag current goroutine with tags\n// If ctx already tagged, append the existed tags\nfunc (p *profiler) Tag(ctx context.Context, tags ...string) context.Context {\n\tpctx, ok := ctx.Value(profilerContextKey{}).(*profilerContext)\n\tif !ok {\n\t\tpctx = newProfilerContext(p)\n\t\tctx = context.WithValue(ctx, profilerContextKey{}, pctx)\n\t}\n\tif pctx.untagCtx == nil {\n\t\tpctx.untagCtx = ctx\n\t}\n\tpctx.tags = append(pctx.tags, tags...)\n\t// do not return pprof ctx\n\tpprof.SetGoroutineLabels(pprof.WithLabels(context.Background(), pprof.Labels(pctx.tags...)))\n\treturn ctx\n}\n\n// Untag current goroutine\n// Only untag if ctx already tagged, will not clear the goroutine labels if not tagged by profiler\nfunc (p *profiler) Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok && pc.untagCtx != nil {\n\t\t// if ctx have untagCtx, that means the current goroutine created by a tagged goroutine\n\t\t// we need to untag the goroutine when finished\n\t\t// else, do nothing\n\t\tpprof.SetGoroutineLabels(pc.untagCtx)\n\t}\n}\n\nfunc (p *profiler) Lookup(ctx context.Context, key string) (string, bool) {\n\treturn pprof.Label(ctx, key)\n}\n\nfunc (p *profiler) stateChange(from, to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != from { // wait state to from first\n\t\tp.stateCond.Wait()\n\t}\n\tp.state = to\n\tp.stateCond.L.Unlock()\n\tp.stateCond.Broadcast()\n}\n\nfunc (p *profiler) stateWait(to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != to {\n\t\tp.stateCond.Wait()\n\t}\n\tp.stateCond.L.Unlock()\n}\n\nfunc (p *profiler) startProfile() error {\n\tp.data.Reset()\n\treturn pprof.StartCPUProfile(&p.data)\n}\n\nfunc (p *profiler) stopProfile() {\n\tpprof.StopCPUProfile()\n}\n\nfunc (p *profiler) analyse() ([]*TagsProfile, error) {\n\t// parse protobuf data\n\tpf, err := profile.ParseData(p.data.Bytes())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// filter cpu value index\n\tsampleIdx := -1\n\tfor idx, st := range pf.SampleType {\n\t\tif st.Type == \"cpu\" {\n\t\t\tsampleIdx = idx\n\t\t\tbreak\n\t\t}\n\t}\n\tif sampleIdx < 0 {\n\t\treturn nil, errors.New(\"profiler: sample type not found\")\n\t}\n\n\t// calculate every sample expense\n\tcounter := map[string]*TagsProfile{} // map[tagsKey]funcProfile\n\tvar total int64\n\tfor _, sm := range pf.Sample {\n\t\tvalue := sm.Value[sampleIdx]\n\t\ttags := labelToTags(sm.Label)\n\t\ttagsKey := tagsToKey(tags)\n\t\ttp, ok := counter[tagsKey]\n\t\tif !ok {\n\t\t\ttp = &TagsProfile{}\n\t\t\tcounter[tagsKey] = tp\n\t\t\ttp.Key = tagsKey\n\t\t\ttp.Tags = tags\n\t\t}\n\t\ttp.Value += value\n\t\ttotal += value\n\t}\n\n\t// compensate value with duration date\n\tdurationRate := float64((p.interval + p.window) / p.window)\n\tprofiles := make([]*TagsProfile, 0, len(counter)) // flat to array\n\tfor _, l := range counter {\n\t\tl.Percent = float64(l.Value) / float64(total)\n\t\tl.Value = int64(durationRate * float64(l.Value))\n\t\tprofiles = append(profiles, l)\n\t}\n\treturn profiles, nil\n}\n\n// TagsProfile is the stats result group by tag key\ntype TagsProfile struct {\n\tKey     string   // eg: a=1,b=2\n\tTags    []string // eg: [\"a\", \"1\", \"b\", \"2\"]\n\tValue   int64    // pprof cpu times\n\tPercent float64  // <= 1.0\n}\n\nfunc labelToTags(label map[string][]string) []string {\n\ttags := make([]string, 0, len(label)*2)\n\tfor k, v := range label {\n\t\ttags = append(tags, k, strings.Join(v, \",\"))\n\t}\n\treturn tags\n}\n\nfunc tagsToKey(tags []string) string {\n\tif len(tags)%2 != 0 {\n\t\treturn \"\"\n\t}\n\ttagsPair := make([]string, 0, len(tags)/2)\n\tfor i := 0; i < len(tags); i += 2 {\n\t\ttagsPair = append(tagsPair, fmt.Sprintf(\"%s=%s\", tags[i], tags[i+1]))\n\t}\n\t// sort tags to make it a unique key\n\tsort.Strings(tagsPair)\n\treturn strings.Join(tagsPair, \"|\")\n}\n\n```\n# Function Signature: func (p *profiler) Resume() {\n# Natural Language Annotations: // Resume the profiler.\n// The profiler has been resumed when Resume() return\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (p *profiler) Resume() {\n\tif p.State() == statePaused {\n\t\t// statePaused => stateResuming => stateRunning\n\t\tp.stateChange(statePaused, stateResuming) \n\t\tp.stateChange(stateResuming, stateRunning)\n\t}\n}\n```"]}, "kitex-pkg/acl/acl.go-NewACLMiddleware": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/acl/acl.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package acl implements ACL functionality.\npackage acl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// RejectFunc judges if to reject a request by the given context and request.\n// Returns a reason if rejected, otherwise returns nil.\ntype RejectFunc func(ctx context.Context, request interface{}) (reason error)\n\n// NewACLMiddleware creates a new ACL middleware using the provided reject funcs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n# Natural Language Annotations: // NewACLMiddleware creates a new ACL middleware using the provided reject funcs.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request interface{}) (response interface{}, err error) {\n\t\t\tfor _, reject := range rules {\n\t\t\t\tif reason := reject(ctx, request); reason != nil {\n\t\t\t\t\treturn nil, kerrors.Errorf(kerrors.ErrACL, \"request rejected: %w\", reason)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn next(ctx, request)\n\t\t}\n\t}\n}\n```"]}, "kitex-client/context_middleware.go-WithContextMiddlewares": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/context_middleware.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n)\n\ntype ctxMWChainKey struct{}\n\n// WithContextMiddlewares add middlewares into current ctx\n// Every client receive this ctx will execute the middlewares.\n// Note that ContextMiddleware is prior to the ClientMiddleware.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getContextMiddleware(ctx context.Context) endpoint.Middleware {\n\tmws, _ := ctx.Value(ctxMWChainKey{}).(endpoint.Middleware)\n\treturn mws\n}\n\n// contextMW execute the ContextMiddlewares in ctx\nfunc contextMW(next endpoint.Endpoint) endpoint.Endpoint {\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\tmw := getContextMiddleware(ctx)\n\t\tif mw != nil {\n\t\t\treturn mw(next)(ctx, req, resp)\n\t\t}\n\t\treturn next(ctx, req, resp)\n\t}\n}\n\n```\n# Function Signature: func WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n# Natural Language Annotations: // WithContextMiddlewares add middlewares into current ctx\n// Every client receive this ctx will execute the middlewares.\n// Note that ContextMiddleware is prior to the ClientMiddleware.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n\texistingMws := getContextMiddleware(ctx)\n\tvar combinedMw endpoint.Middleware\n\n\tif existingMws != nil {\n\t\tcombinedMw = func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\t\treturn existingMws(func(newNext endpoint.Endpoint) endpoint.Endpoint {\n\t\t\t\tfor i := len(mws) - 1; i >= 0; i-- {\n\t\t\t\t\tnewNext = mws[i](newNext)\n\t\t\t\t}\n\t\t\t\treturn newNext\n\t\t\t}(next))\n\t\t}\n\t} else {\n\t\tcombinedMw = func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\t\tfor i := len(mws) - 1; i >= 0; i-- {\n\t\t\t\tnext = mws[i](next)\n\t\t\t}\n\t\t\treturn next\n\t\t}\n\t}\n\n\treturn context.WithValue(ctx, ctxMWChainKey{}, combinedMw)\n}\n```"]}, "kitex-client/stream.go-RecvMsg": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) RecvMsg(m interface{}) (err error) {\n# Natural Language Annotations: // RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *stream) RecvMsg(m interface{}) (err error) {\n\tif err := s.stream.RecvMsg(m); err != nil {\n\t\ts.DoFinish(err)\n\t\treturn err\n\t}\n\treturn nil\n}\n```"]}, "kitex-client/stream.go-SendMsg": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) SendMsg(m interface{}) (err error) {\n# Natural Language Annotations: // SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *stream) SendMsg(m interface{}) (err error) {\n\terr = s.sendEndpoint(s.stream, m)\n\tif err != nil {\n\t\t// If there's an error during sending, indicate we've reached the end of the stream.\n\t\ts.DoFinish()\n\t}\n\treturn err\n}\n```"]}, "kitex-client/stream.go-DoFinish": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) DoFinish(err error) {\n# Natural Language Annotations: // DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *stream) DoFinish(err error) {\n\t// Record the end of the stream by marking it finished\n\tatomic.StoreUint32(&s.finished, 1)\n\t\n\t// Handle tracer operations if needed\n\tif s.kc.opt.TracerCtl != nil {\n\t\ts.kc.opt.TracerCtl.DoFinish(s.ri, err)\n\t}\n\n\t// Release the connection\n\ts.scm.Release(s.ri)\n}\n```"]}, "kitex-client/client.go-NewClient": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/client.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"sync/atomic\"\n\n\t\"github.com/bytedance/gopkg/cloud/metainfo\"\n\t\"github.com/cloudwego/localsession/backup\"\n\n\t\"github.com/cloudwego/kitex/client/callopt\"\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/acl\"\n\t\"github.com/cloudwego/kitex/pkg/consts\"\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/bound\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpctimeout\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Client is the core interface abstraction of kitex client.\n// It is designed for generated codes and should not be used directly.\n// Parameter method specifies the method of a RPC call.\n// Request is a packing of request parameters in the actual method defined in IDL, consist of zero, one\n// or multiple arguments. So is response to the actual result type.\n// Response may be nil to address oneway calls.\ntype Client interface {\n\tCall(ctx context.Context, method string, request, response interface{}) error\n}\n\ntype kClient struct {\n\tsvcInfo *serviceinfo.ServiceInfo\n\tmws     []endpoint.Middleware\n\teps     endpoint.Endpoint\n\tsEps    endpoint.Endpoint\n\topt     *client.Options\n\tlbf     *lbcache.BalancerFactory\n\n\tinited bool\n\tclosed bool\n}\n\n// Set finalizer on kClient does not take effect, because kClient has a circular reference problem\n// when construct the endpoint.Endpoint in the invokeHandleEndpoint,\n// so wrapping kClient as kcFinalizerClient, and set finalizer on kcFinalizerClient, it can solve this problem.\ntype kcFinalizerClient struct {\n\t*kClient\n}\n\nfunc (kf *kcFinalizerClient) Call(ctx context.Context, method string, request, response interface{}) error {\n\tdefer runtime.KeepAlive(kf)\n\treturn kf.kClient.Call(ctx, method, request, response)\n}\n\n// NewClient creates a kitex.Client with the given ServiceInfo, it is from generated code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (kc *kClient) init() (err error) {\n\tinitTransportProtocol(kc.svcInfo, kc.opt.Configs)\n\tif err = kc.checkOptions(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initCircuitBreaker(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initRetryer(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initProxy(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initConnPool(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initLBCache(); err != nil {\n\t\treturn err\n\t}\n\tctx := kc.initContext()\n\tkc.initMiddlewares(ctx)\n\tkc.initStreamMiddlewares(ctx)\n\tkc.initDebugService()\n\tkc.richRemoteOption()\n\tif err = kc.buildInvokeChain(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.warmingUp(); err != nil {\n\t\treturn err\n\t}\n\tkc.inited = true\n\treturn nil\n}\n\nfunc (kc *kClient) checkOptions() (err error) {\n\tif kc.opt.Svr.ServiceName == \"\" {\n\t\treturn errors.New(\"service name is required\")\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initCircuitBreaker() error {\n\tif kc.opt.CBSuite != nil {\n\t\tkc.opt.CBSuite.SetEventBusAndQueue(kc.opt.Bus, kc.opt.Events)\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initRetryer() error {\n\tif kc.opt.RetryContainer == nil {\n\t\tif kc.opt.RetryMethodPolicies == nil {\n\t\t\treturn nil\n\t\t}\n\t\tkc.opt.InitRetryContainer()\n\t}\n\treturn kc.opt.RetryContainer.Init(kc.opt.RetryMethodPolicies, kc.opt.RetryWithResult)\n}\n\nfunc (kc *kClient) initContext() context.Context {\n\tctx := context.Background()\n\tctx = context.WithValue(ctx, endpoint.CtxEventBusKey, kc.opt.Bus)\n\tctx = context.WithValue(ctx, endpoint.CtxEventQueueKey, kc.opt.Events)\n\tctx = context.WithValue(ctx, rpctimeout.TimeoutAdjustKey, &kc.opt.ExtraTimeout)\n\tif chr, ok := kc.opt.Proxy.(proxy.ContextHandler); ok {\n\t\tctx = chr.HandleContext(ctx)\n\t}\n\treturn ctx\n}\n\nfunc (kc *kClient) initProxy() error {\n\tif kc.opt.Proxy != nil {\n\t\tcfg := proxy.Config{\n\t\t\tServerInfo:   kc.opt.Svr,\n\t\t\tResolver:     kc.opt.Resolver,\n\t\t\tBalancer:     kc.opt.Balancer,\n\t\t\tPool:         kc.opt.RemoteOpt.ConnPool,\n\t\t\tFixedTargets: kc.opt.Targets,\n\t\t\tRPCConfig:    kc.opt.Configs,\n\t\t}\n\t\tif err := kc.opt.Proxy.Configure(&cfg); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// update fields in the client option for further use.\n\t\tkc.opt.Resolver = cfg.Resolver\n\t\tkc.opt.Balancer = cfg.Balancer\n\t\t// close predefined pool when proxy init new pool.\n\t\tif cfg.Pool != kc.opt.RemoteOpt.ConnPool && kc.opt.RemoteOpt.ConnPool != nil {\n\t\t\tkc.opt.RemoteOpt.ConnPool.Close()\n\t\t}\n\t\tkc.opt.RemoteOpt.ConnPool = cfg.Pool\n\t\tkc.opt.Targets = cfg.FixedTargets\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initConnPool() error {\n\tpool := kc.opt.RemoteOpt.ConnPool\n\tkc.opt.CloseCallbacks = append(kc.opt.CloseCallbacks, pool.Close)\n\n\tif df, ok := pool.(interface{ Dump() interface{} }); ok {\n\t\tkc.opt.DebugService.RegisterProbeFunc(diagnosis.ConnPoolKey, df.Dump)\n\t}\n\tif r, ok := pool.(remote.ConnPoolReporter); ok && kc.opt.RemoteOpt.EnableConnPoolReporter {\n\t\tr.EnableReporter()\n\t}\n\n\tif long, ok := pool.(remote.LongConnPool); ok {\n\t\tkc.opt.Bus.Watch(discovery.ChangeEventName, func(ev *event.Event) {\n\t\t\tch, ok := ev.Extra.(*discovery.Change)\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfor _, inst := range ch.Removed {\n\t\t\t\tif addr := inst.Address(); addr != nil {\n\t\t\t\t\tlong.Clean(addr.Network(), addr.String())\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initLBCache() error {\n\tif kc.opt.Proxy != nil && kc.opt.Resolver == nil {\n\t\treturn nil\n\t}\n\tonChange := discoveryEventHandler(discovery.ChangeEventName, kc.opt.Bus, kc.opt.Events)\n\tonDelete := discoveryEventHandler(discovery.DeleteEventName, kc.opt.Bus, kc.opt.Events)\n\tresolver := kc.opt.Resolver\n\tif resolver == nil {\n\t\t// fake a resolver instead of returning an error directly because users may use\n\t\t// callopt.WithHostPort to specify target addresses after NewClient.\n\t\tresolver = &discovery.SynthesizedResolver{\n\t\t\tResolveFunc: func(ctx context.Context, target string) (discovery.Result, error) {\n\t\t\t\treturn discovery.Result{}, kerrors.ErrNoResolver\n\t\t\t},\n\t\t\tNameFunc: func() string { return \"no_resolver\" },\n\t\t}\n\t}\n\t// because we cannot ensure that user's custom loadbalancer is cacheable, we need to disable it here\n\tcacheOpts := lbcache.Options{DiagnosisService: kc.opt.DebugService, Cacheable: false}\n\tbalancer := kc.opt.Balancer\n\tif balancer == nil {\n\t\t// default internal lb balancer is cacheable\n\t\tcacheOpts.Cacheable = true\n\t\tbalancer = loadbalance.NewWeightedBalancer()\n\t}\n\tif kc.opt.BalancerCacheOpt != nil {\n\t\tcacheOpts = *kc.opt.BalancerCacheOpt\n\t}\n\tkc.lbf = lbcache.NewBalancerFactory(resolver, balancer, cacheOpts)\n\trbIdx := kc.lbf.RegisterRebalanceHook(onChange)\n\tkc.opt.CloseCallbacks = append(kc.opt.CloseCallbacks, func() error {\n\t\tkc.lbf.DeregisterRebalanceHook(rbIdx)\n\t\treturn nil\n\t})\n\tdIdx := kc.lbf.RegisterDeleteHook(onDelete)\n\tkc.opt.CloseCallbacks = append(kc.opt.CloseCallbacks, func() error {\n\t\tkc.lbf.DeregisterDeleteHook(dIdx)\n\t\treturn nil\n\t})\n\treturn nil\n}\n\nfunc (kc *kClient) initMiddlewares(ctx context.Context) {\n\tbuilderMWs := richMWsWithBuilder(ctx, kc.opt.MWBs)\n\t// integrate xds if enabled\n\tif kc.opt.XDSEnabled && kc.opt.XDSRouterMiddleware != nil && kc.opt.Proxy == nil {\n\t\tkc.mws = append(kc.mws, kc.opt.XDSRouterMiddleware)\n\t}\n\tkc.mws = append(kc.mws, kc.opt.CBSuite.ServiceCBMW(), rpcTimeoutMW(ctx), contextMW)\n\tkc.mws = append(kc.mws, builderMWs...)\n\tkc.mws = append(kc.mws, acl.NewACLMiddleware(kc.opt.ACLRules))\n\tif kc.opt.Proxy == nil {\n\t\tkc.mws = append(kc.mws, newResolveMWBuilder(kc.lbf)(ctx))\n\t\tkc.mws = append(kc.mws, kc.opt.CBSuite.InstanceCBMW())\n\t\tkc.mws = append(kc.mws, richMWsWithBuilder(ctx, kc.opt.IMWBs)...)\n\t} else {\n\t\tif kc.opt.Resolver != nil { // customized service discovery\n\t\t\tkc.mws = append(kc.mws, newResolveMWBuilder(kc.lbf)(ctx))\n\t\t}\n\t\tkc.mws = append(kc.mws, newProxyMW(kc.opt.Proxy))\n\t}\n\tkc.mws = append(kc.mws, newIOErrorHandleMW(kc.opt.ErrHandle))\n}\n\nfunc (kc *kClient) initStreamMiddlewares(ctx context.Context) {\n\tkc.opt.Streaming.EventHandler = kc.opt.TracerCtl.GetStreamEventHandler()\n\tkc.opt.Streaming.InitMiddlewares(ctx)\n}\n\nfunc richMWsWithBuilder(ctx context.Context, mwBs []endpoint.MiddlewareBuilder) (mws []endpoint.Middleware) {\n\tfor i := range mwBs {\n\t\tmws = append(mws, mwBs[i](ctx))\n\t}\n\treturn\n}\n\n// initRPCInfo initializes the RPCInfo structure and attaches it to context.\nfunc (kc *kClient) initRPCInfo(ctx context.Context, method string, retryTimes int, firstRI rpcinfo.RPCInfo) (context.Context, rpcinfo.RPCInfo, *callopt.CallOptions) {\n\treturn initRPCInfo(ctx, method, kc.opt, kc.svcInfo, retryTimes, firstRI)\n}\n\nfunc applyCallOptions(ctx context.Context, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, opt *client.Options) (context.Context, *callopt.CallOptions) {\n\tcos := CallOptionsFromCtx(ctx)\n\tif len(cos) > 0 {\n\t\tinfo, callOpts := callopt.Apply(cos, cfg, svr, opt.Locks, opt.HTTPResolver)\n\t\tctx = context.WithValue(ctx, ctxCallOptionInfoKey, info)\n\t\treturn ctx, callOpts\n\t}\n\topt.Locks.ApplyLocks(cfg, svr)\n\treturn ctx, nil\n}\n\n// Call implements the Client interface .\nfunc (kc *kClient) Call(ctx context.Context, method string, request, response interface{}) (err error) {\n\t// merge backup context if no metainfo found in ctx\n\tctx = backup.RecoverCtxOnDemands(ctx, kc.opt.CtxBackupHandler)\n\n\tvalidateForCall(ctx, kc.inited, kc.closed)\n\tvar ri rpcinfo.RPCInfo\n\tvar callOpts *callopt.CallOptions\n\tctx, ri, callOpts = kc.initRPCInfo(ctx, method, 0, nil)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\tvar reportErr error\n\tvar recycleRI bool\n\tdefer func() {\n\t\tif panicInfo := recover(); panicInfo != nil {\n\t\t\terr = rpcinfo.ClientPanicToErr(ctx, panicInfo, ri, false)\n\t\t\treportErr = err\n\t\t}\n\t\tkc.opt.TracerCtl.DoFinish(ctx, ri, reportErr)\n\t\tif recycleRI {\n\t\t\t// why need check recycleRI to decide if recycle RPCInfo?\n\t\t\t// 1. no retry, rpc timeout happen will cause panic when response return\n\t\t\t// 2. retry success, will cause panic when first call return\n\t\t\t// 3. backup request may cause panic, cannot recycle first RPCInfo\n\t\t\t// RPCInfo will be recycled after rpc is finished,\n\t\t\t// holding RPCInfo in a new goroutine is forbidden.\n\t\t\trpcinfo.PutRPCInfo(ri)\n\t\t}\n\t\tcallOpts.Recycle()\n\t}()\n\n\tcallOptRetry := getCalloptRetryPolicy(callOpts)\n\tif kc.opt.RetryContainer == nil && callOptRetry != nil && callOptRetry.Enable {\n\t\t// setup retry in callopt\n\t\tkc.opt.InitRetryContainer()\n\t}\n\n\t// Add necessary keys to context for isolation between kitex client method calls\n\tctx = retry.PrepareRetryContext(ctx)\n\n\tif kc.opt.RetryContainer == nil {\n\t\t// call without retry policy\n\t\terr = kc.eps(ctx, request, response)\n\t\tif err == nil {\n\t\t\trecycleRI = true\n\t\t}\n\t} else {\n\t\tvar lastRI rpcinfo.RPCInfo\n\t\tlastRI, recycleRI, err = kc.opt.RetryContainer.WithRetryIfNeeded(ctx, callOptRetry, kc.rpcCallWithRetry(ri, method, request, response), ri, request)\n\t\tif ri != lastRI {\n\t\t\t// reset ri of ctx to lastRI\n\t\t\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, lastRI)\n\t\t}\n\t\tri = lastRI\n\t}\n\n\t// do fallback if with setup\n\terr, reportErr = doFallbackIfNeeded(ctx, ri, request, response, err, kc.opt.Fallback, callOpts)\n\treturn err\n}\n\nfunc (kc *kClient) rpcCallWithRetry(ri rpcinfo.RPCInfo, method string, request, response interface{}) retry.RPCCallFunc {\n\t// call with retry policy\n\tvar callTimes int32\n\t// prevRI represents a value of rpcinfo.RPCInfo type.\n\tvar prevRI atomic.Value\n\treturn func(ctx context.Context, r retry.Retryer) (rpcinfo.RPCInfo, interface{}, error) {\n\t\tcurrCallTimes := int(atomic.AddInt32(&callTimes, 1))\n\t\tcRI := ri\n\t\tif currCallTimes > 1 {\n\t\t\tctx, cRI, _ = kc.initRPCInfo(ctx, method, currCallTimes-1, ri)\n\t\t\tctx = metainfo.WithPersistentValue(ctx, retry.TransitKey, strconv.Itoa(currCallTimes-1))\n\t\t\tif prevRI.Load() == nil {\n\t\t\t\tprevRI.Store(ri)\n\t\t\t}\n\t\t\tr.Prepare(ctx, prevRI.Load().(rpcinfo.RPCInfo), cRI)\n\t\t\tprevRI.Store(cRI)\n\t\t}\n\t\tcallErr := kc.eps(ctx, request, response)\n\t\treturn cRI, response, callErr\n\t}\n}\n\nfunc (kc *kClient) initDebugService() {\n\tif ds := kc.opt.DebugService; ds != nil {\n\t\tds.RegisterProbeFunc(diagnosis.DestServiceKey, diagnosis.WrapAsProbeFunc(kc.opt.Svr.ServiceName))\n\t\tds.RegisterProbeFunc(diagnosis.OptionsKey, diagnosis.WrapAsProbeFunc(kc.opt.DebugInfo))\n\t\tds.RegisterProbeFunc(diagnosis.ChangeEventsKey, kc.opt.Events.Dump)\n\t\tds.RegisterProbeFunc(diagnosis.ServiceInfosKey, diagnosis.WrapAsProbeFunc(map[string]*serviceinfo.ServiceInfo{kc.svcInfo.ServiceName: kc.svcInfo}))\n\t}\n}\n\nfunc (kc *kClient) richRemoteOption() {\n\tkc.opt.RemoteOpt.SvcInfo = kc.svcInfo\n\t// for client trans info handler\n\tif len(kc.opt.MetaHandlers) > 0 {\n\t\t// TODO in stream situations, meta is only assembled when the stream creates\n\t\t// metaHandler needs to be called separately.\n\t\t// (newClientStreamer: call WriteMeta before remotecli.NewClient)\n\t\ttransInfoHdlr := bound.NewTransMetaHandler(kc.opt.MetaHandlers)\n\t\tkc.opt.RemoteOpt.PrependBoundHandler(transInfoHdlr)\n\t}\n}\n\nfunc (kc *kClient) buildInvokeChain() error {\n\tinnerHandlerEp, err := kc.invokeHandleEndpoint()\n\tif err != nil {\n\t\treturn err\n\t}\n\tkc.eps = endpoint.Chain(kc.mws...)(innerHandlerEp)\n\n\tinnerStreamingEp, err := kc.invokeStreamingEndpoint()\n\tif err != nil {\n\t\treturn err\n\t}\n\tkc.sEps = endpoint.Chain(kc.mws...)(innerStreamingEp)\n\treturn nil\n}\n\nfunc (kc *kClient) invokeHandleEndpoint() (endpoint.Endpoint, error) {\n\ttransPipl, err := newCliTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\tvar sendMsg remote.Message\n\t\tvar recvMsg remote.Message\n\t\tdefer func() {\n\t\t\tremote.RecycleMessage(sendMsg)\n\t\t\t// Notice, recycle and decode may race if decode exec in another goroutine.\n\t\t\t// No race now, it is ok to recycle. Or recvMsg recycle depend on recv err\n\t\t\tremote.RecycleMessage(recvMsg)\n\t\t}()\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tmethodName := ri.Invocation().MethodName()\n\n\t\tcli, err := remotecli.NewClient(ctx, ri, transPipl, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tdefer cli.Recycle()\n\t\tconfig := ri.Config()\n\t\tm := kc.svcInfo.MethodInfo(methodName)\n\t\tif m == nil {\n\t\t\treturn fmt.Errorf(\"method info is nil, methodName=%s, serviceInfo=%+v\", methodName, kc.svcInfo)\n\t\t} else if m.OneWay() {\n\t\t\tsendMsg = remote.NewMessage(req, kc.svcInfo, ri, remote.Oneway, remote.Client)\n\t\t} else {\n\t\t\tsendMsg = remote.NewMessage(req, kc.svcInfo, ri, remote.Call, remote.Client)\n\t\t}\n\t\tprotocolInfo := remote.NewProtocolInfo(config.TransportProtocol(), kc.svcInfo.PayloadCodec)\n\t\tsendMsg.SetProtocolInfo(protocolInfo)\n\n\t\tif err = cli.Send(ctx, ri, sendMsg); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif m.OneWay() {\n\t\t\tcli.Recv(ctx, ri, nil)\n\t\t\treturn nil\n\t\t}\n\n\t\trecvMsg = remote.NewMessage(resp, kc.opt.RemoteOpt.SvcInfo, ri, remote.Reply, remote.Client)\n\t\trecvMsg.SetProtocolInfo(protocolInfo)\n\t\terr = cli.Recv(ctx, ri, recvMsg)\n\t\treturn err\n\t}, nil\n}\n\n// Close is not concurrency safe.\nfunc (kc *kClient) Close() error {\n\tdefer func() {\n\t\tif err := recover(); err != nil {\n\t\t\tklog.Warnf(\"KITEX: panic when close client, error=%s, stack=%s\", err, string(debug.Stack()))\n\t\t}\n\t}()\n\tif kc.closed {\n\t\treturn nil\n\t}\n\tkc.closed = true\n\tvar errs utils.ErrChain\n\tfor _, cb := range kc.opt.CloseCallbacks {\n\t\tif err := cb(); err != nil {\n\t\t\terrs.Append(err)\n\t\t}\n\t}\n\tif kc.opt.CBSuite != nil {\n\t\tif err := kc.opt.CBSuite.Close(); err != nil {\n\t\t\terrs.Append(err)\n\t\t}\n\t}\n\tif errs.HasError() {\n\t\treturn errs\n\t}\n\treturn nil\n}\n\nfunc newCliTransHandler(opt *remote.ClientOption) (remote.ClientTransHandler, error) {\n\thandler, err := opt.CliHandlerFactory.NewTransHandler(opt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttransPl := remote.NewTransPipeline(handler)\n\tfor _, ib := range opt.Inbounds {\n\t\ttransPl.AddInboundHandler(ib)\n\t}\n\tfor _, ob := range opt.Outbounds {\n\t\ttransPl.AddOutboundHandler(ob)\n\t}\n\treturn transPl, nil\n}\n\nfunc initTransportProtocol(svcInfo *serviceinfo.ServiceInfo, cfg rpcinfo.RPCConfig) {\n\tmutableRPCConfig := rpcinfo.AsMutableRPCConfig(cfg)\n\tmutableRPCConfig.SetPayloadCodec(svcInfo.PayloadCodec)\n\tif svcInfo.PayloadCodec == serviceinfo.Protobuf && cfg.TransportProtocol()&transport.GRPC != transport.GRPC {\n\t\t// pb use ttheader framed by default\n\t\tmutableRPCConfig.SetTransportProtocol(transport.TTHeaderFramed)\n\t}\n}\n\nfunc (kc *kClient) warmingUp() error {\n\tif kc.opt.WarmUpOption == nil {\n\t\treturn nil\n\t}\n\twuo := kc.opt.WarmUpOption\n\tdoWarmupPool := wuo.PoolOption != nil && kc.opt.Proxy == nil\n\n\t// service discovery\n\tif kc.opt.Resolver == nil {\n\t\treturn nil\n\t}\n\tnas := make(map[string][]string)\n\tctx := context.Background()\n\n\tvar dests []rpcinfo.EndpointInfo\n\tif ro := kc.opt.WarmUpOption.ResolverOption; ro != nil {\n\t\tfor _, d := range ro.Dests {\n\t\t\tdests = append(dests, rpcinfo.FromBasicInfo(d))\n\t\t}\n\t}\n\tif len(dests) == 0 && doWarmupPool && len(wuo.PoolOption.Targets) == 0 {\n\t\t// build a default destination for the resolver\n\t\tcfg :=\n```\n# Function Signature: func NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n# Natural Language Annotations: // NewClient creates a kitex.Client with the given ServiceInfo, it is from generated code.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n\t// Initialize options\n\toptions := &client.Options{}\n\tfor _, opt := range opts {\n\t\topt(options)\n\t}\n\t\n\t// Check service information\n\tif svcInfo == nil {\n\t\treturn nil, errors.New(\"service information must not be nil\")\n\t}\n\n\t// Create a new kClient instance\n\tkc := &kClient{\n\t\tsvcInfo: svcInfo,\n\t\topt:     options,\n\t}\n\n\t// Initialize the kClient\n\tif err := kc.init(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Wrap the kClient as a finalizer client\n\treturn &kcFinalizerClient{kc}, nil\n}\n```"]}, "kitex-client/middlewares.go-newResolveMWBuilder": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/middlewares.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newProxyMW(prx proxy.ForwardProxy) endpoint.Middleware {\n\t// If you want to customize the processing logic of proxy middleware,\n\t// you can implement this interface to replace the default implementation.\n\tif p, ok := prx.(proxy.WithMiddleware); ok {\n\t\treturn p.ProxyMiddleware()\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := prx.ResolveProxyInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc discoveryEventHandler(name string, bus event.Bus, queue event.Queue) func(d *discovery.Change) {\n\treturn func(d *discovery.Change) {\n\t\tnow := time.Now()\n\t\tbus.Dispatch(&event.Event{\n\t\t\tName:  name,\n\t\t\tTime:  now,\n\t\t\tExtra: d,\n\t\t})\n\t\tqueue.Push(&event.Event{\n\t\t\tName: name,\n\t\t\tTime: now,\n\t\t\tExtra: map[string]interface{}{\n\t\t\t\t\"Added\":   wrapInstances(d.Added),\n\t\t\t\t\"Updated\": wrapInstances(d.Updated),\n\t\t\t\t\"Removed\": wrapInstances(d.Removed),\n\t\t\t},\n\t\t})\n\t}\n}\n\n// newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// newIOErrorHandleMW provides a hook point for io error handling.\nfunc newIOErrorHandleMW(errHandle func(context.Context, error) error) endpoint.Middleware {\n\tif errHandle == nil {\n\t\terrHandle = DefaultClientErrorHandler\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\terr = next(ctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errHandle(ctx, err)\n\t\t}\n\t}\n}\n\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\n\n\n\n\n\n\n\n\n\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instInfo struct {\n\tAddress string\n\tWeight  int\n}\n\nfunc wrapInstances(insts []discovery.Instance) []*instInfo {\n\tif len(insts) == 0 {\n\t\treturn nil\n\t}\n\tinstInfos := make([]*instInfo, 0, len(insts))\n\tfor i := range insts {\n\t\tinst := insts[i]\n\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\tinstInfos = append(instInfos, &instInfo{Address: addr, Weight: inst.Weight()})\n\t}\n\treturn instInfos\n}\n\nfunc retryable(err error) bool {\n\treturn errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak)\n}\n\nfunc getRemoteAddr(ctx context.Context) string {\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\treturn ri.To().Address().String()\n\t}\n\treturn \"\"\n}\n\n```\n# Function Signature: func newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n# Natural Language Annotations: // newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tvar err error\n\t\t\tvar instance discovery.Instance\n\t\t\tresolver := lbf.Resolver()\n\n\t\t\tfor retryCount := 0; retryCount < maxRetry; retryCount++ {\n\t\t\t\tinstance, err = resolver.Resolve(ctx)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif retryable(err) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\tctx = context.WithValue(ctx, lbcache.KeyInstance, instance)\n\t\t\t\terr = next(ctx, request, response)\n\t\t\t\tif err != nil && retryable(err) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\treturn err\n\t\t}\n\t}\n}\n```"]}, "kitex-client/middlewares.go-DefaultClientErrorHandler": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/middlewares.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newProxyMW(prx proxy.ForwardProxy) endpoint.Middleware {\n\t// If you want to customize the processing logic of proxy middleware,\n\t// you can implement this interface to replace the default implementation.\n\tif p, ok := prx.(proxy.WithMiddleware); ok {\n\t\treturn p.ProxyMiddleware()\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := prx.ResolveProxyInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc discoveryEventHandler(name string, bus event.Bus, queue event.Queue) func(d *discovery.Change) {\n\treturn func(d *discovery.Change) {\n\t\tnow := time.Now()\n\t\tbus.Dispatch(&event.Event{\n\t\t\tName:  name,\n\t\t\tTime:  now,\n\t\t\tExtra: d,\n\t\t})\n\t\tqueue.Push(&event.Event{\n\t\t\tName: name,\n\t\t\tTime: now,\n\t\t\tExtra: map[string]interface{}{\n\t\t\t\t\"Added\":   wrapInstances(d.Added),\n\t\t\t\t\"Updated\": wrapInstances(d.Updated),\n\t\t\t\t\"Removed\": wrapInstances(d.Removed),\n\t\t\t},\n\t\t})\n\t}\n}\n\n// newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// newIOErrorHandleMW provides a hook point for io error handling.\nfunc newIOErrorHandleMW(errHandle func(context.Context, error) error) endpoint.Middleware {\n\tif errHandle == nil {\n\t\terrHandle = DefaultClientErrorHandler\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\terr = next(ctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errHandle(ctx, err)\n\t\t}\n\t}\n}\n\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\n\n\n\n\n\n\n\n\n\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instInfo struct {\n\tAddress string\n\tWeight  int\n}\n\nfunc wrapInstances(insts []discovery.Instance) []*instInfo {\n\tif len(insts) == 0 {\n\t\treturn nil\n\t}\n\tinstInfos := make([]*instInfo, 0, len(insts))\n\tfor i := range insts {\n\t\tinst := insts[i]\n\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\tinstInfos = append(instInfos, &instInfo{Address: addr, Weight: inst.Weight()})\n\t}\n\treturn instInfos\n}\n\nfunc retryable(err error) bool {\n\treturn errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak)\n}\n\nfunc getRemoteAddr(ctx context.Context) string {\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\treturn ri.To().Address().String()\n\t}\n\treturn \"\"\n}\n\n```\n# Function Signature: func DefaultClientErrorHandler(ctx context.Context, err error) error {\n# Natural Language Annotations: // DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\nfunc DefaultClientErrorHandler(ctx context.Context, err error) error {\n\tif thriftErr, ok := err.(thrift.TApplicationException); ok {\n\t\treturn &remote.TransError{\n\t\t\tType:    remote.ProtocolError,\n\t\t\tMessage: thriftErr.Error(),\n\t\t}\n\t}\n\n\tif protobufErr, ok := err.(*protobuf.Error); ok {\n\t\treturn &remote.TransError{\n\t\t\tType:    remote.ProtocolError,\n\t\t\tMessage: protobufErr.Error(),\n\t\t}\n\t}\n\n\t// Default to returning the original error if it doesn't match known types.\n\treturn err\n}\n```"]}, "kitex-client/middlewares.go-ClientErrorHandlerWithAddr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/middlewares.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newProxyMW(prx proxy.ForwardProxy) endpoint.Middleware {\n\t// If you want to customize the processing logic of proxy middleware,\n\t// you can implement this interface to replace the default implementation.\n\tif p, ok := prx.(proxy.WithMiddleware); ok {\n\t\treturn p.ProxyMiddleware()\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := prx.ResolveProxyInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc discoveryEventHandler(name string, bus event.Bus, queue event.Queue) func(d *discovery.Change) {\n\treturn func(d *discovery.Change) {\n\t\tnow := time.Now()\n\t\tbus.Dispatch(&event.Event{\n\t\t\tName:  name,\n\t\t\tTime:  now,\n\t\t\tExtra: d,\n\t\t})\n\t\tqueue.Push(&event.Event{\n\t\t\tName: name,\n\t\t\tTime: now,\n\t\t\tExtra: map[string]interface{}{\n\t\t\t\t\"Added\":   wrapInstances(d.Added),\n\t\t\t\t\"Updated\": wrapInstances(d.Updated),\n\t\t\t\t\"Removed\": wrapInstances(d.Removed),\n\t\t\t},\n\t\t})\n\t}\n}\n\n// newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// newIOErrorHandleMW provides a hook point for io error handling.\nfunc newIOErrorHandleMW(errHandle func(context.Context, error) error) endpoint.Middleware {\n\tif errHandle == nil {\n\t\terrHandle = DefaultClientErrorHandler\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\terr = next(ctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errHandle(ctx, err)\n\t\t}\n\t}\n}\n\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\n\n\n\n\n\n\n\n\n\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instInfo struct {\n\tAddress string\n\tWeight  int\n}\n\nfunc wrapInstances(insts []discovery.Instance) []*instInfo {\n\tif len(insts) == 0 {\n\t\treturn nil\n\t}\n\tinstInfos := make([]*instInfo, 0, len(insts))\n\tfor i := range insts {\n\t\tinst := insts[i]\n\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\tinstInfos = append(instInfos, &instInfo{Address: addr, Weight: inst.Weight()})\n\t}\n\treturn instInfos\n}\n\nfunc retryable(err error) bool {\n\treturn errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak)\n}\n\nfunc getRemoteAddr(ctx context.Context) string {\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\treturn ri.To().Address().String()\n\t}\n\treturn \"\"\n}\n\n```\n# Function Signature: func ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n# Natural Language Annotations: // ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\nfunc ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n    if err == nil {\n        return nil\n    }\n    remoteAddr := getRemoteAddr(ctx)\n    if remoteAddr != \"\" {\n        return fmt.Errorf(\"error: %v, remote address: %s\", err, remoteAddr)\n    }\n    return err\n}\n```"]}, "kitex-client/option.go-WithInstanceMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithInstanceMW(mw endpoint.Middleware) Option {\n# Natural Language Annotations: // WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc WithInstanceMW(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithInstanceMW(%+v)\", utils.GetFuncName(mw)))\n\t\t// The middleware will be added to a specific list of instance-level middlewares\n\t\to.InstanceMWs = append(o.InstanceMWs, mw)\n\t}}\n}\n```"]}, "kitex-client/option.go-WithHostPorts": null, "kitex-client/option.go-WithShortConnection": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithShortConnection() Option {\n# Natural Language Annotations: // WithShortConnection forces kitex to close connection after each call is finished.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithShortConnection forces kitex to close connection after each call is finished.\nfunc WithShortConnection() Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(\"WithShortConnection()\")\n        o.ShortConnection = true\n    }}\n}\n```"]}, "kitex-client/option.go-WithLongConnection": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithLongConnection(cfg connpool.IdleConfig) Option {\n# Natural Language Annotations: // WithLongConnection enables long connection with kitex's built-in pooling implementation.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\nfunc WithLongConnection(cfg connpool.IdleConfig) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLongConnection(%+v)\", cfg))\n\n\t\t// Enable long connection by setting the idle connection configuration\n\t\to.ConnIdleConfig = cfg\n\t\to.UseLongConnection = true\n\t}}\n}\n```"]}, "kitex-client/option.go-WithMuxConnection": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithMuxConnection(connNum int) Option {\n# Natural Language Annotations: // WithMuxConnection specifies the transport type to be mux.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithMuxConnection specifies the transport type to be mux.\nfunc WithMuxConnection(connNum int) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxConnection(%d)\", connNum))\n\t\to.MuxConnectionNum = connNum\n\t}}\n}\n```"]}, "kitex-client/option.go-WithTimeoutProvider": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n# Natural Language Annotations: // WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\nfunc WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTimeoutProvider(%T{%+v})\", p, p))\n\t\to.TimeoutProvider = p\n\t}}\n}\n```"]}, "kitex-client/option.go-WithCodec": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithCodec(c remote.Codec) Option {\n# Natural Language Annotations: // WithCodec to set a codec that handle other protocols which not support by kitex\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithCodec(%T)\", c))\n        o.Codec = c\n    }}\n}\n```"]}, "kitex-client/option.go-WithPayloadCodec": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithPayloadCodec(c remote.PayloadCodec) Option {\n# Natural Language Annotations: // WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithPayloadCodec(%T)\", c))\n        o.PayloadCodec = c\n    }}\n}\n```"]}, "kitex-client/option.go-WithConnReporterEnabled": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithConnReporterEnabled() Option {\n# Natural Language Annotations: // WithConnReporterEnabled to enable reporting connection pool stats.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithConnReporterEnabled to enable reporting connection pool stats.\nfunc WithConnReporterEnabled() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithConnReporterEnabled\")\n\t\to.ConnReporterEnabled = true\n\t}}\n}\n```"]}, "kitex-client/option.go-WithFailureRetry": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithFailureRetry(p *retry.FailurePolicy) Option {\n# Natural Language Annotations: // WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\nfunc WithFailureRetry(p *retry.FailurePolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif p == nil || !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFailureRetry(%+v)\", p))\n\t\tif o.RetryPolicy == nil {\n\t\t\to.RetryPolicy = &retry.Policy{}\n\t\t}\n\t\to.RetryPolicy.FailurePolicy = *p\n\t}}\n}\n```"]}, "kitex-client/option.go-WithBackupRequest": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithBackupRequest(p *retry.BackupPolicy) Option {\n# Natural Language Annotations: // WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\nfunc WithBackupRequest(p *retry.BackupPolicy) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        if p == nil {\n            return\n        }\n        di.Push(fmt.Sprintf(\"WithBackupRequest(%+v)\", p))\n        if o.BackupPolicies == nil {\n            o.BackupPolicies = make([]*retry.BackupPolicy, 0)\n        }\n        o.BackupPolicies = append(o.BackupPolicies, p)\n    }}\n}\n```"]}, "kitex-client/option.go-WithSpecifiedResultRetry": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n# Natural Language Annotations: // WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif rr == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithSpecifiedResultRetry(%+v)\", rr))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\tfor method, policy := range o.RetryMethodPolicies {\n\t\t\tif policy.FailurePolicy.ShouldResultRetry == nil {\n\t\t\t\tpolicy.FailurePolicy.ShouldResultRetry = rr\n\t\t\t\to.RetryMethodPolicies[method] = policy\n\t\t\t}\n\t\t}\n\t\t// If wildcard configuration exists, ensure it also adopts the new ShouldResultRetry.\n\t\tif wildcardCfg.Enable {\n\t\t\twildcardCfg.FailurePolicy.ShouldResultRetry = rr\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n```"]}, "kitex-client/option.go-WithGRPCInitialConnWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialConnWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif s < 65536 {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%d)\", s))\n\t\to.GRPCConnectOpts.InitialConnWindowSize = s\n\t}}\n}\n```"]}, "kitex-client/option.go-WithGRPCMaxHeaderListSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCMaxHeaderListSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%d)\", s))\n\t\to.GRPCConnectOpts.MaxHeaderListSize = s\n\t}}\n}\n```"]}, "kitex-client/option.go-WithGRPCKeepaliveParams": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n# Natural Language Annotations: // WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\nfunc WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\t\to.GRPCKeepaliveParams = kp\n\t}}\n}\n```"]}, "kitex-client/option.go-WithWarmingUp": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithWarmingUp(wuo *warmup.ClientOption) Option {\n# Natural Language Annotations: // WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\nfunc WithWarmingUp(wuo *warmup.ClientOption) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithWarmingUp(%+v)\", wuo))\n        o.WarmingUpOption = wuo\n    }}\n}\n```"]}, "kitex-client/callopt/options.go-WithHostPort": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithHostPort(hostport string) Option {\n# Natural Language Annotations: // WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\nfunc WithHostPort(hostport string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif err := setInstance(o.svr, hostport); err != nil {\n\t\t\tdi.WriteString(fmt.Sprintf(\"invalid address '%s'\", hostport))\n\t\t\treturn\n\t\t}\n\t\tdi.WriteString(\"WithHostPort\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(hostport)\n\t\tdi.WriteByte(')')\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithURL": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithURL(url string) Option {\n# Natural Language Annotations: // WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\nfunc WithURL(url string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\t// Use net package to parse and resolve the host and port from the URL\n\t\tu, err := net.ResolveTCPAddr(\"tcp\", url)\n\t\tif err == nil {\n\t\t\thostport := u.String() // Get the hostport in the format \"host:port\"\n\t\t\t// Set the instance using the resolved hostport\n\t\t\tif err := setInstance(o.svr, hostport); err == nil {\n\t\t\t\tdi.WriteString(\"WithURL\")\n\t\t\t\tdi.WriteByte('(')\n\t\t\t\tdi.WriteString(url)\n\t\t\t\tdi.WriteByte(')')\n\t\t\t}\n\t\t}\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithRPCTimeout": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithRPCTimeout(d time.Duration) Option {\n# Natural Language Annotations: // WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithRPCTimeout\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(d.String())\n\t\tdi.WriteByte(')')\n\t\to.configs.SetRPCTimeout(d)\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithConnectTimeout": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithConnectTimeout(d time.Duration) Option {\n# Natural Language Annotations: // WithConnectTimeout specifies the connection timeout for a RPC call.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithConnectTimeout\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(d.String())\n\t\tdi.WriteByte(')')\n\t\to.configs.SetConnectTimeout(d)\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithFallback": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithFallback(fb *fallback.Policy) Option {\n# Natural Language Annotations: // WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithFallback is used to set the fallback policy for an RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\to.Fallback = fb\n\t\tdi.WriteString(\"WithFallback\")\n\t}}\n}\n```"]}, "kitex-server/option_advanced.go-WithServerBasicInfo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option_advanced.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package server defines the Options of server\npackage server\n\n// Notice!! This file defines the advanced Options of client, normal user should not use it.\n// It is used for customized extension.\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/acl\"\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/generic\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/profiler\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\n\n\n\n\n\n\n\n\n\n\n// WithDiagnosisService sets the diagnosis service for gathering debug information.\nfunc WithDiagnosisService(ds diagnosis.Service) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDiagnosisService(%+v)\", ds))\n\n\t\to.DebugService = ds\n\t}}\n}\n\n// WithACLRules sets the ACL rules.\nfunc WithACLRules(rules ...acl.RejectFunc) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar names []string\n\t\tfor _, r := range rules {\n\t\t\tnames = append(names, utils.GetFuncName(r))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithACLRules(%+v)\", names))\n\n\t\to.ACLRules = append(o.ACLRules, rules...)\n\t}}\n}\n\n// WithMetaHandler adds a MetaHandler.\nfunc WithMetaHandler(h remote.MetaHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMetaHandler(%T)\", h))\n\n\t\to.MetaHandlers = append(o.MetaHandlers, h)\n\t}}\n}\n\n// WithProxy sets the backward Proxy for server.\nfunc WithProxy(p proxy.ReverseProxy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithProxy(%T)\", p))\n\n\t\tif o.Proxy != nil {\n\t\t\tpanic(fmt.Errorf(\"reassignment of Proxy is not allowed: %T -> %T\", o.Proxy, p))\n\t\t}\n\t\to.Proxy = p\n\t}}\n}\n\n// WithTransHandlerFactory sets the TransHandlerFactory for server.\nfunc WithTransHandlerFactory(f remote.ServerTransHandlerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransHandlerFactory(%T)\", f))\n\n\t\to.RemoteOpt.SvrHandlerFactory = f\n\t}}\n}\n\n// WithTransServerFactory sets the TransServerFactory for server.\nfunc WithTransServerFactory(f remote.TransServerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransServerFactory(%T)\", f))\n\n\t\to.RemoteOpt.TransServerFactory = f\n\t}}\n}\n\n// WithLimitReporter do report when server limit happen\nfunc WithLimitReporter(r limiter.LimitReporter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithLimitReporter(%T)\", r))\n\n\t\to.Limit.LimitReporter = r\n\t}}\n}\n\n// WithGeneric set Generic type for generic call\nfunc WithGeneric(g generic.Generic) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithGeneric(%T)\", g))\n\n\t\tif g == nil {\n\t\t\tpanic(\"invalid Generic: nil\")\n\t\t}\n\t\to.RemoteOpt.PayloadCodec = g.PayloadCodec()\n\t}}\n}\n\n// WithErrorHandler sets the error handler.\nfunc WithErrorHandler(f func(context.Context, error) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithErrorHandler(%+v)\", utils.GetFuncName(f)))\n\n\t\to.ErrHandle = f\n\t}}\n}\n\n// WithBoundHandler adds remote.BoundHandler for server.\nfunc WithBoundHandler(h remote.BoundHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddBoundHandler(%T)\", h))\n\n\t\texist := false\n\t\tswitch handler := h.(type) {\n\t\tcase remote.InboundHandler:\n\t\t\tfor _, inboundHandler := range o.RemoteOpt.Inbounds {\n\t\t\t\tif reflect.DeepEqual(inboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase remote.OutboundHandler:\n\t\t\tfor _, outboundHandler := range o.RemoteOpt.Outbounds {\n\t\t\t\tif reflect.DeepEqual(outboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// prevent duplication\n\t\tif !exist {\n\t\t\tdoAddBoundHandler(h, o.RemoteOpt)\n\t\t} else {\n\t\t\tklog.Warnf(\"KITEX: BoundHandler already exists, BoundHandler=%v\", h)\n\t\t}\n\t}}\n}\n\n// WithExitSignal adds ExitSignal for server.\nfunc WithExitSignal(f func() <-chan error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddExitSignal(%+v)\", utils.GetFuncName(f)))\n\t\to.ExitSignal = f\n\t}}\n}\n\n// WithListener sets the listener for server, the priority is higher than WithServiceAddr\nfunc WithListener(ln net.Listener) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithListener(%+v)\", ln))\n\n\t\to.RemoteOpt.Listener = ln\n\t}}\n}\n\n// WithReusePort sets SO_REUSEPORT on listener, it is only used with Option `WithServiceAddr`.\n// It won't take effect when listener is specified by WithListener.\nfunc WithReusePort(reuse bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReusePort(%+v)\", reuse))\n\n\t\to.RemoteOpt.ReusePort = reuse\n\t}}\n}\n\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\n\n\n\n\n\n\n\n\n\n// WithProfiler set a profiler to server.\nfunc WithProfiler(pc profiler.Profiler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfiler(%T{%+v})\", pc, pc))\n\t\to.RemoteOpt.Profiler = pc\n\t}}\n}\n\n// WithProfilerTransInfoTagging set transinfo tagging function to profiler\n// TransInfoTagging extracting tags after TransInfo decoded but before message decoded.\n// At this stage, we can only get msg.TransInfo() and the real message payload is not decoded yet.\n// If upstream is not use TTHeader protocol, we can get nothing here.\n// So if you don't very care about the accuracy of statistics, we recommend to use WithProfilerMessageTagging to extract your custom tags.\nfunc WithProfilerTransInfoTagging(tagging remote.TransInfoTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerTransInfoTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerTransInfoTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerTransInfoTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerTransInfoTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n// WithProfilerMessageTagging set message tagging function to profiler\n// MessageTagging extracting tags after whole decode process finished.\n// At this stage, we can get the rpcInfo from ctx, and full complete message.\nfunc WithProfilerMessageTagging(tagging remote.MessageTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerMessageTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerMessageTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerMessageTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerMessageTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n```\n# Function Signature: func WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n# Natural Language Annotations: // WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithServerBasicInfo(%+v)\", ebi))\n\n\t\to.EndpointBasicInfo = ebi\n\t}}\n}\n```"]}, "kitex-server/option_advanced.go-WithSupportedTransportsFunc": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option_advanced.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package server defines the Options of server\npackage server\n\n// Notice!! This file defines the advanced Options of client, normal user should not use it.\n// It is used for customized extension.\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/acl\"\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/generic\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/profiler\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\n\n\n\n\n\n\n\n\n\n\n// WithDiagnosisService sets the diagnosis service for gathering debug information.\nfunc WithDiagnosisService(ds diagnosis.Service) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDiagnosisService(%+v)\", ds))\n\n\t\to.DebugService = ds\n\t}}\n}\n\n// WithACLRules sets the ACL rules.\nfunc WithACLRules(rules ...acl.RejectFunc) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar names []string\n\t\tfor _, r := range rules {\n\t\t\tnames = append(names, utils.GetFuncName(r))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithACLRules(%+v)\", names))\n\n\t\to.ACLRules = append(o.ACLRules, rules...)\n\t}}\n}\n\n// WithMetaHandler adds a MetaHandler.\nfunc WithMetaHandler(h remote.MetaHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMetaHandler(%T)\", h))\n\n\t\to.MetaHandlers = append(o.MetaHandlers, h)\n\t}}\n}\n\n// WithProxy sets the backward Proxy for server.\nfunc WithProxy(p proxy.ReverseProxy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithProxy(%T)\", p))\n\n\t\tif o.Proxy != nil {\n\t\t\tpanic(fmt.Errorf(\"reassignment of Proxy is not allowed: %T -> %T\", o.Proxy, p))\n\t\t}\n\t\to.Proxy = p\n\t}}\n}\n\n// WithTransHandlerFactory sets the TransHandlerFactory for server.\nfunc WithTransHandlerFactory(f remote.ServerTransHandlerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransHandlerFactory(%T)\", f))\n\n\t\to.RemoteOpt.SvrHandlerFactory = f\n\t}}\n}\n\n// WithTransServerFactory sets the TransServerFactory for server.\nfunc WithTransServerFactory(f remote.TransServerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransServerFactory(%T)\", f))\n\n\t\to.RemoteOpt.TransServerFactory = f\n\t}}\n}\n\n// WithLimitReporter do report when server limit happen\nfunc WithLimitReporter(r limiter.LimitReporter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithLimitReporter(%T)\", r))\n\n\t\to.Limit.LimitReporter = r\n\t}}\n}\n\n// WithGeneric set Generic type for generic call\nfunc WithGeneric(g generic.Generic) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithGeneric(%T)\", g))\n\n\t\tif g == nil {\n\t\t\tpanic(\"invalid Generic: nil\")\n\t\t}\n\t\to.RemoteOpt.PayloadCodec = g.PayloadCodec()\n\t}}\n}\n\n// WithErrorHandler sets the error handler.\nfunc WithErrorHandler(f func(context.Context, error) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithErrorHandler(%+v)\", utils.GetFuncName(f)))\n\n\t\to.ErrHandle = f\n\t}}\n}\n\n// WithBoundHandler adds remote.BoundHandler for server.\nfunc WithBoundHandler(h remote.BoundHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddBoundHandler(%T)\", h))\n\n\t\texist := false\n\t\tswitch handler := h.(type) {\n\t\tcase remote.InboundHandler:\n\t\t\tfor _, inboundHandler := range o.RemoteOpt.Inbounds {\n\t\t\t\tif reflect.DeepEqual(inboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase remote.OutboundHandler:\n\t\t\tfor _, outboundHandler := range o.RemoteOpt.Outbounds {\n\t\t\t\tif reflect.DeepEqual(outboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// prevent duplication\n\t\tif !exist {\n\t\t\tdoAddBoundHandler(h, o.RemoteOpt)\n\t\t} else {\n\t\t\tklog.Warnf(\"KITEX: BoundHandler already exists, BoundHandler=%v\", h)\n\t\t}\n\t}}\n}\n\n// WithExitSignal adds ExitSignal for server.\nfunc WithExitSignal(f func() <-chan error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddExitSignal(%+v)\", utils.GetFuncName(f)))\n\t\to.ExitSignal = f\n\t}}\n}\n\n// WithListener sets the listener for server, the priority is higher than WithServiceAddr\nfunc WithListener(ln net.Listener) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithListener(%+v)\", ln))\n\n\t\to.RemoteOpt.Listener = ln\n\t}}\n}\n\n// WithReusePort sets SO_REUSEPORT on listener, it is only used with Option `WithServiceAddr`.\n// It won't take effect when listener is specified by WithListener.\nfunc WithReusePort(reuse bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReusePort(%+v)\", reuse))\n\n\t\to.RemoteOpt.ReusePort = reuse\n\t}}\n}\n\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\n\n\n\n\n\n\n\n\n\n// WithProfiler set a profiler to server.\nfunc WithProfiler(pc profiler.Profiler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfiler(%T{%+v})\", pc, pc))\n\t\to.RemoteOpt.Profiler = pc\n\t}}\n}\n\n// WithProfilerTransInfoTagging set transinfo tagging function to profiler\n// TransInfoTagging extracting tags after TransInfo decoded but before message decoded.\n// At this stage, we can only get msg.TransInfo() and the real message payload is not decoded yet.\n// If upstream is not use TTHeader protocol, we can get nothing here.\n// So if you don't very care about the accuracy of statistics, we recommend to use WithProfilerMessageTagging to extract your custom tags.\nfunc WithProfilerTransInfoTagging(tagging remote.TransInfoTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerTransInfoTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerTransInfoTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerTransInfoTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerTransInfoTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n// WithProfilerMessageTagging set message tagging function to profiler\n// MessageTagging extracting tags after whole decode process finished.\n// At this stage, we can get the rpcInfo from ctx, and full complete message.\nfunc WithProfilerMessageTagging(tagging remote.MessageTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerMessageTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerMessageTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerMessageTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerMessageTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n```\n# Function Signature: func WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n# Natural Language Annotations: // WithSupportedTransportsFunc sets a function which converts supported transports from server option.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\nfunc WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithSupportedTransportsFunc(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.SupportedTransportsFunc = f\n\t}}\n}\n```"]}, "kitex-server/option.go-WithMiddleware": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMiddleware(mw endpoint.Middleware) Option {\n# Natural Language Annotations: // WithMiddleware adds middleware for server to handle request.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithMiddleware adds middleware for server to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%T{%+v})\", mw, mw))\n\t\to.Middleware = append(o.Middleware, mw)\n\t}}\n}\n```"]}, "kitex-server/option.go-WithMiddlewareBuilder": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n# Natural Language Annotations: // WithMiddlewareBuilder adds middleware that depend on context for server to handle request\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%T, %v)\", mwb, funcName))\n\t\t\n\t\t// Create a middleware using the builder\n\t\tmiddleware := mwb.Build()\n\n\t\t// Add the middleware to the server's options\n\t\to.Middleware = append(o.Middleware, middleware)\n\t}}\n}\n```"]}, "kitex-server/option.go-WithExitWaitTime": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithExitWaitTime(timeout time.Duration) Option {\n# Natural Language Annotations: // WithExitWaitTime sets the wait duration for graceful shutdown.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithExitWaitTime sets the wait duration for graceful shutdown.\nfunc WithExitWaitTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithExitWaitTime(%v)\", timeout))\n\n\t\to.ExitWaitTime = timeout\n\t}}\n}\n```"]}, "kitex-server/option.go-WithMaxConnIdleTime": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMaxConnIdleTime(timeout time.Duration) Option {\n# Natural Language Annotations: // WithMaxConnIdleTime sets the max idle time on connection from clients.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\nfunc WithMaxConnIdleTime(timeout time.Duration) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithMaxConnIdleTime(%v)\", timeout))\n\n        // Assuming there's a field in the internal_server.Options to set max idle time\n        o.ConnIdleTime = timeout\n    }}\n}\n```"]}, "kitex-server/option.go-WithPayloadCodec": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithPayloadCodec(c remote.PayloadCodec) Option {\n# Natural Language Annotations: // WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%+v)\", c))\n\t\to.RemoteOpt.PayloadCodec = c\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCInitialWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\t// Ignore any value smaller than 64K for the initial window size\n\t\tif s < 64*1024 {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialWindowSize = s\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCInitialConnWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialConnWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        // Log the provided window size for traceability\n        di.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%+v)\", s))\n\n        // Only set the window size if it is greater than or equal to 64K\n        if s >= 64*1024 {\n            o.RemoteOpt.GRPCCfg.InitialConnWindowSize = s\n        }\n    }}\n}\n```"]}, "kitex-server/option.go-WithGRPCKeepaliveParams": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n# Natural Language Annotations: // WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\nfunc WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\n        // Assuming that there is a GRPCCfg field in RemoteOpt that has a KeepaliveParams attribute\n        o.RemoteOpt.GRPCCfg.KeepaliveParams = kp\n    }}\n}\n```"]}, "kitex-server/option.go-WithGRPCKeepaliveEnforcementPolicy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n# Natural Language Annotations: // WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveEnforcementPolicy(%+v)\", kep))\n\t\to.RemoteOpt.GRPCCfg.KeepaliveEnforcementPolicy = kep\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCMaxConcurrentStreams": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCMaxConcurrentStreams(n uint32) Option {\n# Natural Language Annotations: // WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCMaxConcurrentStreams(n uint32) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithGRPCMaxConcurrentStreams(%+v)\", n))\n        o.RemoteOpt.GRPCCfg.MaxConcurrentStreams = n\n    }}\n}\n```"]}, "kitex-server/option.go-WithGRPCMaxHeaderListSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCMaxHeaderListSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.MaxHeaderListSize = s\n\t}}\n}\n```"]}, "kitex-server/genericserver/server.go-NewServerWithServiceInfo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/genericserver/server.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package genericserver ...\npackage genericserver\n\nimport (\n\t\"github.com/cloudwego/kitex/pkg/generic\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/server\"\n)\n\n// NewServer creates a generic server with the given handler and options.\nfunc NewServer(handler generic.Service, g generic.Generic, opts ...server.Option) server.Server {\n\tsvcInfo := generic.ServiceInfo(g.PayloadCodecType())\n\treturn NewServerWithServiceInfo(handler, g, svcInfo, opts...)\n}\n\n// NewServerWithServiceInfo creates a generic server with the given handler, serviceInfo and options.\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func NewServerWithServiceInfo(handler generic.Service, g generic.Generic, svcInfo *serviceinfo.ServiceInfo, opts ...server.Option) server.Server {\n# Natural Language Annotations: // NewServerWithServiceInfo creates a generic server with the given handler, serviceInfo and options.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// NewServerWithServiceInfo creates a generic server with the given handler, serviceInfo and options.\nfunc NewServerWithServiceInfo(handler generic.Service, g generic.Generic, svcInfo *serviceinfo.ServiceInfo, opts ...server.Option) server.Server {\n\t// Create a server instance using provided service information\n\tsrv := server.NewServer(svcInfo, opts...)\n\n\t// Set the generic handler for the server using g and the handler\n\tsrv.SetGeneric(g, handler)\n\n\t// Return the configured server\n\treturn srv\n}\n```"]}, "weaver-validate.go-validateRegistrations": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/validate.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"unicode\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/reflection\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// validateRegistrations validates the provided registrations, returning an\n// diagnostic error if they are invalid. Note that some validation is performed\n// by 'weaver generate', but because users can run a Service Weaver app after\n// forgetting to run 'weaver generate', some checks have to be done at runtime.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isValidListenerName returns whether the provided name is a valid\n// weaver.Listener name.\nfunc isValidListenerName(name string) bool {\n\t// We allow valid Go identifiers [1]. This code is taken from [2].\n\t//\n\t// [1]: https://go.dev/ref/spec#Identifiers\n\t// [2]: https://cs.opensource.google/go/go/+/refs/tags/go1.20.6:src/go/token/token.go;l=331-341;drc=19309779ac5e2f5a2fd3cbb34421dafb2855ac21\n\tif name == \"\" {\n\t\treturn false\n\t}\n\tfor i, c := range name {\n\t\tif !unicode.IsLetter(c) && c != '_' && (i == 0 || !unicode.IsDigit(c)) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n```\n# Function Signature: func validateRegistrations(regs []*codegen.Registration) error {\n# Natural Language Annotations: // validateRegistrations validates the provided registrations, returning an\n// diagnostic error if they are invalid. Note that some validation is performed\n// by 'weaver generate', but because users can run a Service Weaver app after\n// forgetting to run 'weaver generate', some checks have to be done at runtime.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc validateRegistrations(regs []*codegen.Registration) error {\n    for _, reg := range regs {\n        // Check if the Registration name is valid\n        if !isValidListenerName(reg.Name) {\n            return fmt.Errorf(\"invalid registration name: %s\", reg.Name)\n        }\n\n        // Additional validation checks can be performed here as needed.\n        // For example, if the Registration struct has specific rules, those would be validated here.\n\n        // Example of another potential validation:\n        if reg.SomeField == nil {\n            return errors.New(\"some required field is missing in the registration\")\n        }\n    }\n\n    return nil\n}\n```"]}, "weaver-sim/generators.go-Range": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func Range(low, high int) Generator[int] {\n# Natural Language Annotations: // Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Range(low, high int) Generator[int] {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"Range: low %d must be less than high %d\", low, high))\n\t}\n\treturn generatorFunc[int](func(r *rand.Rand) int {\n\t\treturn r.Intn(high-low) + low\n\t})\n}\n```"]}, "weaver-sim/generators.go-String": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func String() Generator[string] {\n# Natural Language Annotations: // Strings\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc String() Generator[string] {\n\treturn generatorFunc[string](func(r *rand.Rand) string {\n\t\t// Define a range for the length of the string to bias towards smaller strings\n\t\tvar lengthRanges = []Weighted[int]{\n\t\t\t{100, Range(1, 5)},   // More bias towards shorter strings\n\t\t\t{50, Range(5, 10)},\n\t\t\t{20, Range(10, 20)},\n\t\t\t{10, Range(20, 50)},\n\t\t\t{5, Range(50, 100)},\n\t\t}\n\t\t\n\t\tlengthGen := Weight(lengthRanges)\n\t\tlength := lengthGen.Generate(r)\n\n\t\t// Use Runic characters, biasing towards standard readable characters, e.g., a-z, A-Z\n\t\trunes := []rune(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \")\n\t\truneGen := Slice(Range(0, length), OneOf(runes...))\n\t\t\n\t\treturn string(runeGen.Generate(r))\n\t})\n}\n```"]}, "weaver-sim/generators.go-OneOf": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func OneOf[T any](xs ...T) Generator[T] {\n# Natural Language Annotations: // Combinators\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc OneOf[T any](xs ...T) Generator[T] {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"OneOf: no values provided\"))\n\t}\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tindex := r.Intn(len(xs))\n\t\treturn xs[index]\n\t})\n}\n```"]}, "weaver-sim/generators.go-Weight": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func Weight[T any](choices []Weighted[T]) Generator[T] {\n# Natural Language Annotations: // Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. \nfunc Weight[T any](choices []Weighted[T]) Generator[T] {\n\tif len(choices) == 0 {\n\t\tpanic(\"Weight: no choices provided\")\n\t}\n\n\tvar totalWeight float64\n\tfor _, choice := range choices {\n\t\tif choice.Weight < 0 {\n\t\t\tpanic(fmt.Errorf(\"Weight: negative weight %f\", choice.Weight))\n\t\t}\n\t\ttotalWeight += choice.Weight\n\t}\n\n\tif totalWeight == 0 {\n\t\tpanic(\"Weight: sum of all weights is 0\")\n\t}\n\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\trandomWeight := r.Float64() * totalWeight\n\t\tvar cumulativeWeight float64\n\t\tfor _, choice := range choices {\n\t\t\tcumulativeWeight += choice.Weight\n\t\t\tif randomWeight < cumulativeWeight {\n\t\t\t\treturn choice.Gen.Generate(r)\n\t\t\t}\n\t\t}\n\t\t// Return the last choice as a fallback\n\t\t// This should ideally never happen if the weights are well-formed\n\t\treturn choices[len(choices)-1].Gen.Generate(r)\n\t})\n}\n```"]}, "weaver-sim/rand.go-pick": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/rand.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math/bits\"\n\t\"math/rand\"\n)\n\n// pop pops and returns a randomly selected element from the provided slice.\n// pop panics if the provided slice is empty.\nfunc pop[T any](r *rand.Rand, xs []T) (T, []T) {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"pop: empty slice\"))\n\t}\n\ti := r.Intn(len(xs))\n\tx := xs[i]\n\treturn x, append(xs[:i], xs[i+1:]...)\n}\n\n// pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\n\n\n\n\n\n\n\n// flip returns true with probability p. For example, flip(0) always returns\n// false, flip(1) always returns true, and flip(0.5) returns true half the\n// time. flip panics if p is not in the range [0, 1].\nfunc flip(r *rand.Rand, p float64) bool {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"flip: probability %f not in range [0, 1.0]\", p))\n\t}\n\treturn r.Float64() <= p\n}\n\n// ints represents a remove-only set of integers in the range [low, high).\ntype ints struct {\n\tlow, high int\n\n\t// The integers in the set in no particular order.\n\telements []int\n\n\t// indices[x-low] is the index of element x in elements, or -1 if x is not\n\t// in the set.\n\tindices []int\n}\n\n// reset resets a set of integers to the range [low, high).\n// reset panics if low >= high.\nfunc (i *ints) reset(low, high int) {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"newInts: low (%d) >= high (%d)\", low, high))\n\t}\n\n\ti.low = low\n\ti.high = high\n\tn := high - low\n\tif i.elements == nil {\n\t\ti.elements = make([]int, n)\n\t}\n\ti.elements = i.elements[:0]\n\tif i.indices == nil {\n\t\ti.indices = make([]int, n)\n\t}\n\ti.indices = i.indices[:0]\n\n\tfor j := 0; j < n; j++ {\n\t\ti.elements = append(i.elements, low+j)\n\t\ti.indices = append(i.indices, j)\n\t}\n}\n\n// has returns whether the provided integer is in the set.\nfunc (i *ints) has(x int) bool {\n\treturn i.low <= x && x < i.high && i.indices[x-i.low] != -1\n}\n\n// size returns the size of the set.\nfunc (i *ints) size() int {\n\treturn len(i.elements)\n}\n\n// pick returns a random element of the set.\nfunc (i *ints) pick(r *rand.Rand) int {\n\treturn i.elements[r.Intn(len(i.elements))]\n}\n\n// remove removes the provided element from the set. remove is a noop if the\n// provided element is not in the set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// wyrand is an implementation of the wyrand pseudorandom number generator\n// algorithm from [1, 2]. This implementation also borrows from [3] and [4].\n//\n// 1: https://github.com/wangyi-fudan/wyhash\n// 2: https://github.com/wangyi-fudan/wyhash/blob/master/Modern%20Non-Cryptographic%20Hash%20Function%20and%20Pseudorandom%20Number%20Generator.pdf\n// 3: https://github.com/lemon-mint/exp-pkgs/blob/v0.0.25/hash/wyhash/wyhash.go\n// 4: https://github.com/dsincl12/wyrand/blob/5f074aba21f4f9022d8d73139357bf816fdf1c93/wyrand.go\ntype wyrand struct {\n\tseed uint64\n}\n\nvar _ rand.Source = &wyrand{}\nvar _ rand.Source64 = &wyrand{}\n\n// Seed implements the rand.Source interface.\nfunc (w *wyrand) Seed(seed int64) {\n\tw.seed = uint64(seed)\n}\n\n// Int63 implements the rand.Source interface.\nfunc (w *wyrand) Int63() int64 {\n\treturn int64(w.Uint64() >> 1)\n}\n\n// Uint64 implements the rand.Source64 interface.\nfunc (w *wyrand) Uint64() uint64 {\n\tw.seed += 0xa0761d6478bd642f\n\treturn wymix(w.seed, w.seed^0xe7037ed1a0b428db)\n\n}\n\n// See https://github.com/wangyi-fudan/wyhash for explanation.\nfunc wymix(x uint64, y uint64) uint64 {\n\thi, lo := bits.Mul64(x, y)\n\treturn hi ^ lo\n}\n\n```\n# Function Signature: func pick[T any](r *rand.Rand, xs []T) T {\n# Natural Language Annotations: // pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\nfunc pick[T any](r *rand.Rand, xs []T) T {\n    if len(xs) == 0 {\n        panic(fmt.Errorf(\"pick: empty slice\"))\n    }\n    i := r.Intn(len(xs))\n    return xs[i]\n}\n```"]}, "weaver-sim/rand.go-remove": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/rand.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math/bits\"\n\t\"math/rand\"\n)\n\n// pop pops and returns a randomly selected element from the provided slice.\n// pop panics if the provided slice is empty.\nfunc pop[T any](r *rand.Rand, xs []T) (T, []T) {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"pop: empty slice\"))\n\t}\n\ti := r.Intn(len(xs))\n\tx := xs[i]\n\treturn x, append(xs[:i], xs[i+1:]...)\n}\n\n// pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\n\n\n\n\n\n\n\n// flip returns true with probability p. For example, flip(0) always returns\n// false, flip(1) always returns true, and flip(0.5) returns true half the\n// time. flip panics if p is not in the range [0, 1].\nfunc flip(r *rand.Rand, p float64) bool {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"flip: probability %f not in range [0, 1.0]\", p))\n\t}\n\treturn r.Float64() <= p\n}\n\n// ints represents a remove-only set of integers in the range [low, high).\ntype ints struct {\n\tlow, high int\n\n\t// The integers in the set in no particular order.\n\telements []int\n\n\t// indices[x-low] is the index of element x in elements, or -1 if x is not\n\t// in the set.\n\tindices []int\n}\n\n// reset resets a set of integers to the range [low, high).\n// reset panics if low >= high.\nfunc (i *ints) reset(low, high int) {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"newInts: low (%d) >= high (%d)\", low, high))\n\t}\n\n\ti.low = low\n\ti.high = high\n\tn := high - low\n\tif i.elements == nil {\n\t\ti.elements = make([]int, n)\n\t}\n\ti.elements = i.elements[:0]\n\tif i.indices == nil {\n\t\ti.indices = make([]int, n)\n\t}\n\ti.indices = i.indices[:0]\n\n\tfor j := 0; j < n; j++ {\n\t\ti.elements = append(i.elements, low+j)\n\t\ti.indices = append(i.indices, j)\n\t}\n}\n\n// has returns whether the provided integer is in the set.\nfunc (i *ints) has(x int) bool {\n\treturn i.low <= x && x < i.high && i.indices[x-i.low] != -1\n}\n\n// size returns the size of the set.\nfunc (i *ints) size() int {\n\treturn len(i.elements)\n}\n\n// pick returns a random element of the set.\nfunc (i *ints) pick(r *rand.Rand) int {\n\treturn i.elements[r.Intn(len(i.elements))]\n}\n\n// remove removes the provided element from the set. remove is a noop if the\n// provided element is not in the set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// wyrand is an implementation of the wyrand pseudorandom number generator\n// algorithm from [1, 2]. This implementation also borrows from [3] and [4].\n//\n// 1: https://github.com/wangyi-fudan/wyhash\n// 2: https://github.com/wangyi-fudan/wyhash/blob/master/Modern%20Non-Cryptographic%20Hash%20Function%20and%20Pseudorandom%20Number%20Generator.pdf\n// 3: https://github.com/lemon-mint/exp-pkgs/blob/v0.0.25/hash/wyhash/wyhash.go\n// 4: https://github.com/dsincl12/wyrand/blob/5f074aba21f4f9022d8d73139357bf816fdf1c93/wyrand.go\ntype wyrand struct {\n\tseed uint64\n}\n\nvar _ rand.Source = &wyrand{}\nvar _ rand.Source64 = &wyrand{}\n\n// Seed implements the rand.Source interface.\nfunc (w *wyrand) Seed(seed int64) {\n\tw.seed = uint64(seed)\n}\n\n// Int63 implements the rand.Source interface.\nfunc (w *wyrand) Int63() int64 {\n\treturn int64(w.Uint64() >> 1)\n}\n\n// Uint64 implements the rand.Source64 interface.\nfunc (w *wyrand) Uint64() uint64 {\n\tw.seed += 0xa0761d6478bd642f\n\treturn wymix(w.seed, w.seed^0xe7037ed1a0b428db)\n\n}\n\n// See https://github.com/wangyi-fudan/wyhash for explanation.\nfunc wymix(x uint64, y uint64) uint64 {\n\thi, lo := bits.Mul64(x, y)\n\treturn hi ^ lo\n}\n\n```\n# Function Signature: func (i *ints) remove(x int) {\n# Natural Language Annotations: // remove removes the provided element from the set. remove is a noop if the\n// provided element is not in the set.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (i *ints) remove(x int) {\n\tif !i.has(x) {\n\t\treturn\n\t}\n\n\t// Get the index of the element to be removed\n\tremoveIndex := i.indices[x-i.low]\n\n\t// Replace the element to be removed with the last element\n\tlastIndex := len(i.elements) - 1\n\tlastElement := i.elements[lastIndex]\n\n\ti.elements[removeIndex] = lastElement\n\ti.indices[lastElement-i.low] = removeIndex\n\n\t// Remove the last element\n\ti.elements = i.elements[:lastIndex]\n\ti.indices[x-i.low] = -1\n}\n```"]}, "weaver-runtime/config.go-ParseConfig": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/config.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage runtime\n\nimport (\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"github.com/ServiceWeaver/weaver/internal/env\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// ParseConfig parses the specified configuration input, which should\n// hold a set of sections in TOML format from the specified file.\n// The section corresponding to the common Service Weaver application\n// configuration is parsed and returned as a *AppConfig.\n//\n// sectionValidator(key, val) is used to validate every section config entry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseConfigSection parses the config section for key into dst.\n// If shortKey is not empty, either key or shortKey is accepted.\n// If the named section is not found, returns nil without changing dst.\nfunc ParseConfigSection(key, shortKey string, sections map[string]string, dst any) error {\n\tsection, ok := sections[key]\n\tif shortKey != \"\" {\n\t\t// Fetch section listed for shortKey, if any.\n\t\tif shortKeySection, ok2 := sections[shortKey]; ok2 {\n\t\t\tif ok {\n\t\t\t\treturn fmt.Errorf(\"conflicting sections %q and %q\", shortKey, key)\n\t\t\t}\n\t\t\tkey, section, ok = shortKey, shortKeySection, ok2\n\t\t}\n\t}\n\tif !ok { // not found\n\t\treturn nil\n\t}\n\n\t// Parse and validate the section.\n\tmd, err := toml.Decode(section, dst)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif unknown := md.Undecoded(); len(unknown) != 0 {\n\t\treturn fmt.Errorf(\"section %q has unknown keys %v\", key, unknown)\n\t}\n\tif x, ok := dst.(interface{ Validate() error }); ok {\n\t\tif err := x.Validate(); err != nil {\n\t\t\treturn fmt.Errorf(\"section %q: %w\", key, err)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc extractApp(file string, config *protos.AppConfig) error {\n\tconst appKey = \"github.com/ServiceWeaver/weaver\"\n\tconst shortAppKey = \"serviceweaver\"\n\n\t// appConfig holds the data from under appKey in the TOML config.\n\t// It matches the contents of the Config proto.\n\ttype appConfig struct {\n\t\tName     string\n\t\tBinary   string\n\t\tArgs     []string\n\t\tEnv      []string\n\t\tColocate [][]string\n\t\tRollout  time.Duration\n\t}\n\n\tparsed := &appConfig{}\n\tif err := ParseConfigSection(appKey, shortAppKey, config.Sections, parsed); err != nil {\n\t\treturn err\n\t}\n\n\t// Move struct fields into proto.\n\tconfig.Name = parsed.Name\n\tconfig.Binary = parsed.Binary\n\tconfig.Args = parsed.Args\n\tconfig.Env = parsed.Env\n\tconfig.RolloutNanos = int64(parsed.Rollout)\n\tfor _, colocate := range parsed.Colocate {\n\t\tgroup := &protos.ComponentGroup{Components: colocate}\n\t\tconfig.Colocate = append(config.Colocate, group)\n\t}\n\n\t// Canonicalize the config.\n\tif err := canonicalizeConfig(config, filepath.Dir(file)); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// canonicalizeConfig updates the provided config to canonical\n// form. All relative paths inside the configuration are resolved\n// relative to the provided directory.\nfunc canonicalizeConfig(c *protos.AppConfig, dir string) error {\n\t// Fill in the application name if necessary.\n\tbin := c.GetBinary()\n\tif c.Name == \"\" && bin != \"\" {\n\t\tc.Name = filepath.Base(bin)\n\t}\n\n\t// Convert relative paths inside the application config to absolute paths\n\t// interpreted starting at the directory containing the config file.\n\tif !filepath.IsAbs(bin) {\n\t\tbin, err := filepath.Abs(filepath.Join(dir, bin))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tc.Binary = bin\n\t}\n\n\t// Validate the environment variables.\n\tif _, err := env.Parse(c.Env); err != nil {\n\t\treturn fmt.Errorf(\"invalid Env: %v\", err)\n\t}\n\n\t// Validate the same_process entry.\n\tif err := checkSameProcess(c); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// checkSameProcess checks that the same_process entry is valid.\nfunc checkSameProcess(c *protos.AppConfig) error {\n\tseen := map[string]struct{}{}\n\tfor _, components := range c.Colocate {\n\t\tfor _, component := range components.Components {\n\t\t\tif _, ok := seen[component]; ok {\n\t\t\t\treturn fmt.Errorf(\"component %q placed multiple times\", component)\n\t\t\t}\n\t\t\tseen[component] = struct{}{}\n\t\t}\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n# Natural Language Annotations: // ParseConfig parses the specified configuration input, which should\n// hold a set of sections in TOML format from the specified file.\n// The section corresponding to the common Service Weaver application\n// configuration is parsed and returned as a *AppConfig.\n//\n// sectionValidator(key, val) is used to validate every section config entry.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n\t// Decode the input TOML configuration into a map of sections\n\tvar sections map[string]string\n\tif _, err := toml.Decode(input, &sections); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode TOML input: %w\", err)\n\t}\n\n\t// Initialize an AppConfig object\n\tconfig := &protos.AppConfig{Sections: sections}\n\n\t// Validate each section using sectionValidator\n\tfor key, val := range sections {\n\t\tif err := sectionValidator(key, val); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"validation failed for section %q: %w\", key, err)\n\t\t}\n\t}\n\n\t// Extract application-specific configuration\n\tif err := extractApp(file, config); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to extract app config: %w\", err)\n\t}\n\n\t// Return the parsed and validated configuration\n\treturn config, nil\n}\n```"]}, "weaver-runtime/traces/db.go-OpenDB": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/traces/db.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package perfetto contains libraries for displaying trace information in the\n// Perfetto UI.\npackage traces\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"modernc.org/sqlite\"\n\tsqlite3 \"modernc.org/sqlite/lib\"\n)\n\n// DB is a trace database that stores traces on the local file system.\ntype DB struct {\n\t// Trace data is stored in a sqlite DB spread across two tables:\n\t// (1) traces:         serialized trace data, used for querying.\n\t// (2) encoded_spans:  full encoded span data, used for fetching all of the\n\t//                     spans that belong to a given trace.\n\tfname string\n\tdb    *sql.DB\n}\n\n// OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes the trace database.\nfunc (d *DB) Close() error {\n\treturn d.db.Close()\n}\n\n// Store stores the given trace spans in the database.\nfunc (d *DB) Store(ctx context.Context, app, version string, spans *protos.TraceSpans) error {\n\t// NOTE: we insert all rows transactionally, as it is significantly faster\n\t// than inserting one row at a time [1].\n\t//\n\t// [1]: https://stackoverflow.com/questions/1711631/improve-insert-per-second-performance-of-sqlite\n\ttx, err := d.db.BeginTx(ctx, &sql.TxOptions{Isolation: sql.LevelLinearizable})\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\tvar errs []error\n\tfor _, span := range spans.Span {\n\t\tif isRootSpan(span) {\n\t\t\tif err := d.storeTrace(ctx, tx, app, version, span); err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\t}\n\t\tif err := d.storeSpan(ctx, tx, span); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\tif errs != nil {\n\t\treturn errors.Join(errs...)\n\t}\n\treturn tx.Commit()\n}\n\nfunc (d *DB) storeTrace(ctx context.Context, tx *sql.Tx, app, version string, root *protos.Span) error {\n\tconst traceStmt = `INSERT INTO traces VALUES (?,?,?,?,?,?,?)`\n\t_, err := tx.ExecContext(ctx, traceStmt, hex.EncodeToString(root.TraceId), app, version, root.Name, root.StartMicros, root.EndMicros, spanStatus(root))\n\treturn err\n}\n\nfunc (d *DB) storeSpan(ctx context.Context, tx *sql.Tx, span *protos.Span) error {\n\tencoded, err := proto.Marshal(span)\n\tif err != nil {\n\t\treturn err\n\t}\n\tconst stmt = `INSERT INTO encoded_spans VALUES (?,?,?)`\n\t_, err = tx.ExecContext(ctx, stmt, hex.EncodeToString(span.TraceId), span.StartMicros, encoded)\n\treturn err\n}\n\n// TraceSummary stores summary information about a trace.\ntype TraceSummary struct {\n\tTraceID            string    // Unique trace identifier, in hex format.\n\tStartTime, EndTime time.Time // Start and end times for the trace.\n\tStatus             string    // Trace status string.\n}\n\n// QueryTraces returns the summaries of the traces that match the given\n// query arguments, namely:\n//   - That have been generated by the given application version.\n//   - That fit entirely in the given [startTime, endTime] time interval.\n//   - Whose duration is in the given [durationLower, durationUpper) range.\n//   - Who have an error status.\n//   - Who are in the most recent limit of trace spans.\n//\n// Any query argument that has a zero value (e.g., empty app or version,\n// zero endTime) is ignored, i.e., it matches all spans.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FetchSpans returns all of the spans that have a given trace id.\nfunc (d *DB) FetchSpans(ctx context.Context, traceID string) ([]*protos.Span, error) {\n\tconst query = `SELECT data FROM encoded_spans WHERE trace_id=?`\n\trows, err := d.queryDB(ctx, query, traceID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\tvar spans []*protos.Span\n\tfor rows.Next() {\n\t\tvar encoded []byte\n\t\tif err := rows.Scan(&encoded); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspan := &protos.Span{}\n\t\tif err := proto.Unmarshal(encoded, span); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspans = append(spans, span)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn spans, nil\n}\n\nfunc (d *DB) queryDB(ctx context.Context, query string, args ...any) (*sql.Rows, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trows, err := d.db.QueryContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn rows, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (d *DB) execDB(ctx context.Context, query string, args ...any) (sql.Result, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tres, err := d.db.ExecContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn res, err\n\t}\n\treturn nil, ctx.Err()\n}\n\n// isLocked returns whether the error is a \"database is locked\" error.\nfunc isLocked(err error) bool {\n\tsqlError := &sqlite.Error{}\n\tok := errors.As(err, &sqlError)\n\treturn ok && (sqlError.Code() == sqlite3.SQLITE_BUSY || sqlError.Code() == sqlite3.SQLITE_LOCKED)\n}\n\n// isRootSpan returns true iff the given span is a root span.\nfunc isRootSpan(span *protos.Span) bool {\n\tvar nilSpanID [8]byte\n\treturn bytes.Equal(span.ParentSpanId, nilSpanID[:])\n}\n\n// spanStatus returns the span status string. It returns \"\" if the status is OK.\nfunc spanStatus(span *protos.Span) string {\n\t// Look for an error in the span status.\n\tif span.Status != nil && span.Status.Code == protos.Span_Status_ERROR {\n\t\tif span.Status.Error != \"\" {\n\t\t\treturn span.Status.Error\n\t\t} else {\n\t\t\treturn \"unknown error\"\n\t\t}\n\t}\n\n\t// Look for an HTTP error in the span attributes.\n\tfor _, attr := range span.Attributes {\n\t\tif attr.Key != \"http.status_code\" {\n\t\t\tcontinue\n\t\t}\n\t\tif attr.Value.Type != protos.Span_Attribute_Value_INT64 {\n\t\t\tcontinue\n\t\t}\n\t\tval, ok := attr.Value.Value.(*protos.Span_Attribute_Value_Num)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif val.Num >= 400 && val.Num < 600 {\n\t\t\treturn http.StatusText(int(val.Num))\n\t\t}\n\t}\n\n\t// No errors found.\n\treturn \"\"\n}\n\n```\n# Function Signature: func OpenDB(ctx context.Context, fname string) (*DB, error) {\n# Natural Language Annotations: // OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\nfunc OpenDB(ctx context.Context, fname string) (*DB, error) {\n    // Check if the file exists. If it doesn't, it'll be created by SQLite.\n    db, err := sql.Open(\"sqlite\", fname)\n    if err != nil {\n        return nil, err\n    }\n\n    // Create the necessary tables if they don't already exist.\n    createTableStatements := []string{\n        `CREATE TABLE IF NOT EXISTS traces (\n            trace_id TEXT PRIMARY KEY,\n            app TEXT,\n            version TEXT,\n            name TEXT,\n            start_micros INTEGER,\n            end_micros INTEGER,\n            status TEXT\n        )`,\n        `CREATE TABLE IF NOT EXISTS encoded_spans (\n            trace_id TEXT,\n            start_micros INTEGER,\n            data BLOB,\n            PRIMARY KEY(trace_id, start_micros)\n        )`,\n    }\n    for _, stmt := range createTableStatements {\n        if _, err := db.ExecContext(ctx, stmt); err != nil {\n            db.Close() // Ensure the DB is closed if there's an error\n            return nil, err\n        }\n    }\n\n    return &DB{fname: fname, db: db}, nil\n}\n```"]}, "weaver-runtime/traces/db.go-QueryTraces": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/traces/db.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package perfetto contains libraries for displaying trace information in the\n// Perfetto UI.\npackage traces\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"modernc.org/sqlite\"\n\tsqlite3 \"modernc.org/sqlite/lib\"\n)\n\n// DB is a trace database that stores traces on the local file system.\ntype DB struct {\n\t// Trace data is stored in a sqlite DB spread across two tables:\n\t// (1) traces:         serialized trace data, used for querying.\n\t// (2) encoded_spans:  full encoded span data, used for fetching all of the\n\t//                     spans that belong to a given trace.\n\tfname string\n\tdb    *sql.DB\n}\n\n// OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes the trace database.\nfunc (d *DB) Close() error {\n\treturn d.db.Close()\n}\n\n// Store stores the given trace spans in the database.\nfunc (d *DB) Store(ctx context.Context, app, version string, spans *protos.TraceSpans) error {\n\t// NOTE: we insert all rows transactionally, as it is significantly faster\n\t// than inserting one row at a time [1].\n\t//\n\t// [1]: https://stackoverflow.com/questions/1711631/improve-insert-per-second-performance-of-sqlite\n\ttx, err := d.db.BeginTx(ctx, &sql.TxOptions{Isolation: sql.LevelLinearizable})\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\tvar errs []error\n\tfor _, span := range spans.Span {\n\t\tif isRootSpan(span) {\n\t\t\tif err := d.storeTrace(ctx, tx, app, version, span); err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\t}\n\t\tif err := d.storeSpan(ctx, tx, span); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\tif errs != nil {\n\t\treturn errors.Join(errs...)\n\t}\n\treturn tx.Commit()\n}\n\nfunc (d *DB) storeTrace(ctx context.Context, tx *sql.Tx, app, version string, root *protos.Span) error {\n\tconst traceStmt = `INSERT INTO traces VALUES (?,?,?,?,?,?,?)`\n\t_, err := tx.ExecContext(ctx, traceStmt, hex.EncodeToString(root.TraceId), app, version, root.Name, root.StartMicros, root.EndMicros, spanStatus(root))\n\treturn err\n}\n\nfunc (d *DB) storeSpan(ctx context.Context, tx *sql.Tx, span *protos.Span) error {\n\tencoded, err := proto.Marshal(span)\n\tif err != nil {\n\t\treturn err\n\t}\n\tconst stmt = `INSERT INTO encoded_spans VALUES (?,?,?)`\n\t_, err = tx.ExecContext(ctx, stmt, hex.EncodeToString(span.TraceId), span.StartMicros, encoded)\n\treturn err\n}\n\n// TraceSummary stores summary information about a trace.\ntype TraceSummary struct {\n\tTraceID            string    // Unique trace identifier, in hex format.\n\tStartTime, EndTime time.Time // Start and end times for the trace.\n\tStatus             string    // Trace status string.\n}\n\n// QueryTraces returns the summaries of the traces that match the given\n// query arguments, namely:\n//   - That have been generated by the given application version.\n//   - That fit entirely in the given [startTime, endTime] time interval.\n//   - Whose duration is in the given [durationLower, durationUpper) range.\n//   - Who have an error status.\n//   - Who are in the most recent limit of trace spans.\n//\n// Any query argument that has a zero value (e.g., empty app or version,\n// zero endTime) is ignored, i.e., it matches all spans.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FetchSpans returns all of the spans that have a given trace id.\nfunc (d *DB) FetchSpans(ctx context.Context, traceID string) ([]*protos.Span, error) {\n\tconst query = `SELECT data FROM encoded_spans WHERE trace_id=?`\n\trows, err := d.queryDB(ctx, query, traceID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\tvar spans []*protos.Span\n\tfor rows.Next() {\n\t\tvar encoded []byte\n\t\tif err := rows.Scan(&encoded); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspan := &protos.Span{}\n\t\tif err := proto.Unmarshal(encoded, span); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspans = append(spans, span)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn spans, nil\n}\n\nfunc (d *DB) queryDB(ctx context.Context, query string, args ...any) (*sql.Rows, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trows, err := d.db.QueryContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn rows, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (d *DB) execDB(ctx context.Context, query string, args ...any) (sql.Result, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tres, err := d.db.ExecContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn res, err\n\t}\n\treturn nil, ctx.Err()\n}\n\n// isLocked returns whether the error is a \"database is locked\" error.\nfunc isLocked(err error) bool {\n\tsqlError := &sqlite.Error{}\n\tok := errors.As(err, &sqlError)\n\treturn ok && (sqlError.Code() == sqlite3.SQLITE_BUSY || sqlError.Code() == sqlite3.SQLITE_LOCKED)\n}\n\n// isRootSpan returns true iff the given span is a root span.\nfunc isRootSpan(span *protos.Span) bool {\n\tvar nilSpanID [8]byte\n\treturn bytes.Equal(span.ParentSpanId, nilSpanID[:])\n}\n\n// spanStatus returns the span status string. It returns \"\" if the status is OK.\nfunc spanStatus(span *protos.Span) string {\n\t// Look for an error in the span status.\n\tif span.Status != nil && span.Status.Code == protos.Span_Status_ERROR {\n\t\tif span.Status.Error != \"\" {\n\t\t\treturn span.Status.Error\n\t\t} else {\n\t\t\treturn \"unknown error\"\n\t\t}\n\t}\n\n\t// Look for an HTTP error in the span attributes.\n\tfor _, attr := range span.Attributes {\n\t\tif attr.Key != \"http.status_code\" {\n\t\t\tcontinue\n\t\t}\n\t\tif attr.Value.Type != protos.Span_Attribute_Value_INT64 {\n\t\t\tcontinue\n\t\t}\n\t\tval, ok := attr.Value.Value.(*protos.Span_Attribute_Value_Num)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif val.Num >= 400 && val.Num < 600 {\n\t\t\treturn http.StatusText(int(val.Num))\n\t\t}\n\t}\n\n\t// No errors found.\n\treturn \"\"\n}\n\n```\n# Function Signature: func (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n# Natural Language Annotations: // QueryTraces returns the summaries of the traces that match the given\n// query arguments, namely:\n//   - That have been generated by the given application version.\n//   - That fit entirely in the given [startTime, endTime] time interval.\n//   - Whose duration is in the given [durationLower, durationUpper) range.\n//   - Who have an error status.\n//   - Who are in the most recent limit of trace spans.\n//\n// Any query argument that has a zero value (e.g., empty app or version,\n// zero endTime) is ignored, i.e., it matches all spans.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n\t// Construct the base query\n\tquery := `SELECT trace_id, start_time, end_time, status FROM traces WHERE 1=1`\n\n\t// Arguments for the prepared statement\n\tvar args []interface{}\n\n\t// Add the application version filter if provided\n\tif app != \"\" {\n\t\tquery += ` AND app = ?`\n\t\targs = append(args, app)\n\t}\n\tif version != \"\" {\n\t\tquery += ` AND version = ?`\n\t\targs = append(args, version)\n\t}\n\n\t// Add the time interval filter if both times are valid\n\tif !startTime.IsZero() && !endTime.IsZero() {\n\t\tquery += ` AND start_time >= ? AND end_time <= ?`\n\t\targs = append(args, startTime.UnixMicro(), endTime.UnixMicro())\n\t}\n\n\t// Add the duration filter if both durations are valid\n\tif durationLower > 0 && durationUpper > 0 {\n\t\tquery += ` AND (end_time - start_time) >= ? AND (end_time - start_time) < ?`\n\t\targs = append(args, durationLower.Microseconds(), durationUpper.Microseconds())\n\t}\n\n\t// Add the error filter if specified\n\tif onlyErrors {\n\t\tquery += ` AND status != \"\"`\n\t}\n\n\t// Add the limit if specified\n\tif limit > 0 {\n\t\tquery += ` ORDER BY start_time DESC LIMIT ?`\n\t\targs = append(args, limit)\n\t}\n\n\t// Execute the query\n\trows, err := d.queryDB(ctx, query, args...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\n\t// Prepare to collect the results\n\tvar summaries []TraceSummary\n\tfor rows.Next() {\n\t\tvar ts TraceSummary\n\t\tvar startMicros, endMicros int64\n\t\tif err := rows.Scan(&ts.TraceID, &startMicros, &endMicros, &ts.Status); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tts.StartTime = time.UnixMicro(startMicros)\n\t\tts.EndTime = time.UnixMicro(endMicros)\n\t\tsummaries = append(summaries, ts)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn summaries, nil\n}\n```"]}, "weaver-runtime/retry/retry.go-Continue": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/retry/retry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package retry contains code to perform retries with exponential backoff.\n//\n// Example: loop until doSomething() returns true or context hits deadline or is canceled.\n//\n//\tfor r := retry.Begin(); r.Continue(ctx); {\n//\t  if doSomething() {\n//\t    break\n//\t  }\n//\t}\npackage retry\n\nimport (\n\t\"context\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Retry holds state for managing retry loops with exponential backoff and jitter.\ntype Retry struct {\n\toptions Options\n\tattempt int\n}\n\n// Options are the options that configure a retry loop. Before the ith\n// iteration of a retry loop, retry.Continue() sleeps for a duration of\n// BackoffMinDuration * BackoffMultiplier^i, with added jitter.\ntype Options struct {\n\tBackoffMultiplier  float64 // If specified, must be at least 1.\n\tBackoffMinDuration time.Duration\n}\n\n// DefaultOptions is the default set of Options.\nvar DefaultOptions = Options{\n\tBackoffMultiplier:  1.3,\n\tBackoffMinDuration: 10 * time.Millisecond,\n}\n\nvar (\n\trngMu sync.Mutex\n\trng   *rand.Rand\n)\n\n// Begin initiates a new retry loop.\nfunc Begin() *Retry {\n\treturn BeginWithOptions(DefaultOptions)\n}\n\n// BeginWithOptions returns a new retry loop configured with the provided\n// options.\n//\n// Example: Sleep 1 second, then 2 seconds, then 4 seconds, and so on.\n//\n//\topts := retry.Options{\n//\t  BackoffMultiplier: 2.0,\n//\t  BackoffMinDuration: time.Second,\n//\t}\n//\tfor r := retry.BeginWithOptions(opts); r.Continue(ctx); {\n//\t  // Do nothing.\n//\t}\nfunc BeginWithOptions(options Options) *Retry {\n\treturn &Retry{options: options}\n}\n\n// Continue sleeps for an exponentially increasing interval (with jitter). It\n// stops its sleep early and returns false if context becomes done. If the\n// return value is false, ctx.Err() is guaranteed to be non-nil. The first\n// call does not sleep.\n\n\n\n\n\n\n\n\n// Reset resets a Retry to its initial state. Reset is useful if you want to\n// retry an operation with exponential backoff, but only if it is failing. For\n// example:\n//\n//\tfor r := retry.Begin(); r.Continue(ctx); {\n//\t    if err := doSomething(); err != nil {\n//\t        // Retry with backoff if we fail.\n//\t        continue\n//\t    }\n//\t    // Retry immediately if we succeed.\n//\t    r.Reset()\n//\t}\nfunc (r *Retry) Reset() {\n\tr.attempt = 0\n}\n\nfunc backoffDelay(i int, opts Options) time.Duration {\n\tmult := math.Pow(opts.BackoffMultiplier, float64(i))\n\treturn time.Duration(float64(opts.BackoffMinDuration) * mult)\n}\n\n// randomized sleeps for a random duration close to d, or until context is done,\n// whichever occurs first.\nfunc randomized(ctx context.Context, d time.Duration) {\n\tconst jitter = 0.4\n\tmult := 1 - jitter*randomFloat() // Subtract up to 40%\n\tsleep(ctx, time.Duration(float64(d)*mult))\n}\n\n// sleep sleeps for the specified duration d, or until context is done,\n// whichever occurs first.\nfunc sleep(ctx context.Context, d time.Duration) {\n\tt := time.NewTimer(d)\n\tdefer t.Stop()\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn\n\tcase <-t.C:\n\t}\n}\n\nfunc randomFloat() float64 {\n\t// Do not use the default rng since we do not want different processes\n\t// to pick the same deterministic random sequence.\n\trngMu.Lock()\n\tdefer rngMu.Unlock()\n\tif rng == nil {\n\t\trng = rand.New(rand.NewSource(time.Now().UnixNano()))\n\t}\n\treturn rng.Float64()\n}\n\n```\n# Function Signature: func (r *Retry) Continue(ctx context.Context) bool {\n# Natural Language Annotations: // Continue sleeps for an exponentially increasing interval (with jitter). It\n// stops its sleep early and returns false if context becomes done. If the\n// return value is false, ctx.Err() is guaranteed to be non-nil. The first\n// call does not sleep.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (r *Retry) Continue(ctx context.Context) bool {\n    // On the first call, return true without sleeping\n    if r.attempt == 0 {\n        r.attempt++\n        return true\n    }\n\n    // Calculate the backoff delay for the current attempt\n    delay := backoffDelay(r.attempt, r.options)\n\n    // Apply randomized sleep with jitter\n    randomized(ctx, delay)\n    \n    // Check if context is done after sleep\n    select {\n    case <-ctx.Done():\n        // return false if the context is done\n        return false\n    default:\n        // increment attempt and continue\n        r.attempt++\n        return true\n    }\n}\n```"]}, "weaver-runtime/colors/tabularizer.go-Row": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/colors/tabularizer.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colors\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\nvar dimColor = Color256(245) // a light gray\n\n// An Atom is a segment of text with a single formatting style.\ntype Atom struct {\n\tS         string // the text\n\tUnderline bool   // is it underlined?\n\tBold      bool   // is it bold?\n\tColor     Code   // text color\n}\n\n// String returns the atom with the appropriate ANSI escape sequences.\nfunc (a Atom) String() string {\n\tif !Enabled() {\n\t\treturn a.S\n\t}\n\n\tvar b strings.Builder\n\tb.WriteString(string(a.Color))\n\tif a.Underline {\n\t\tb.WriteString(string(Underline))\n\t}\n\tif a.Bold {\n\t\tb.WriteString(string(Bold))\n\t}\n\tb.WriteString(a.S)\n\tb.WriteString(string(Reset))\n\treturn b.String()\n}\n\n// dimmed returns a copy of the atom with a dim gray color.\nfunc (a Atom) dimmed() Atom {\n\ta.Color = dimColor\n\treturn a\n}\n\n// Text represents a contiguous sequence of atoms.\ntype Text []Atom\n\n// len returns the length of the printable characters in the text's constituent\n// atoms. ANSI escape sequences are not counted as part of this length.\nfunc (t Text) len() int {\n\treturn len(t.raw())\n}\n\n// raw returns the raw underlying text without any ANSI escape sequences.\nfunc (t Text) raw() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.S)\n\t}\n\treturn b.String()\n}\n\n// String returns the text with the appropriate ANSI escape sequences.\nfunc (t Text) String() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.String())\n\t}\n\treturn b.String()\n}\n\n// dimmed returns a copy of the text with a dim gray color.\nfunc (t Text) dimmed() Text {\n\tcloned := make(Text, len(t))\n\tfor i, a := range t {\n\t\tcloned[i] = a.dimmed()\n\t}\n\treturn Text(cloned)\n}\n\n// A Tabularizer produces pretty-printed tabularized text. Unlike\n// tabwriter.Writer [1], Tabularizer properly handles text with ANSI escape\n// codes. Here's what an example table looks like:\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 CATS                  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 NAME   \u2502 AGE \u2502 COLOR  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 belle  \u2502 1y  \u2502 tortie \u2502\n//\t\u2502 sidney \u2502 2y  \u2502 calico \u2502\n//\t\u2502 dakota \u2502 8m  \u2502 tuxedo \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n//\n// The table format comes from duf [2].\n//\n// [1]: https://pkg.go.dev/text/tabwriter\n// [2]: https://github.com/muesli/duf\ntype Tabularizer struct {\n\tw      io.Writer // where to write\n\ttitle  []Text    // table title\n\trows   [][]Text  // buffered rows\n\twidths []int     // column widths\n\tdim    func(prev, row []string) []bool\n}\n\n// NewTabularizer returns a new tabularizer. The provided dim function\n// determines which columns in a row, if any, are dimmed.\nfunc NewTabularizer(w io.Writer, title []Text, dim func(prev, row []string) []bool) *Tabularizer {\n\treturn &Tabularizer{w: w, title: title, dim: dim}\n}\n\n// Row buffers a new Row to be tabularized. The Row isn't written until Flush\n// is called. Note that every Row reported to a tabularizer must be the same\n// length. A value can be a text, atom, string, or fmt.Stringer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Flush writes all buffered rows. Flush should only be called once, after all\n// rows have been written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NoDim doesn't dim any columns.\nfunc NoDim(prev, row []string) []bool {\n\treturn make([]bool, len(row))\n}\n\n// PrefixDim dims the longest prefix of row that is identical to prev.\nfunc PrefixDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t} else {\n\t\t\treturn cols\n\t\t}\n\t}\n\treturn cols\n}\n\n// FullDim dims any columns that are identical in prev.\nfunc FullDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t}\n\t}\n\treturn cols\n}\n\n```\n# Function Signature: func (t *Tabularizer) Row(values ...any) {\n# Natural Language Annotations: // Row buffers a new Row to be tabularized. The Row isn't written until Flush\n// is called. Note that every Row reported to a tabularizer must be the same\n// length. A value can be a text, atom, string, or fmt.Stringer.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *Tabularizer) Row(values ...any) {\n\tif len(values) == 0 {\n\t\treturn\n\t}\n\n\t// Determine the number of columns based on the first addition\n\tif len(t.widths) == 0 {\n\t\tt.widths = make([]int, len(values))\n\t}\n\n\tif len(values) != len(t.widths) {\n\t\tpanic(\"all rows must have the same number of columns\")\n\t}\n\n\trow := make([]Text, len(values))\n\n\tfor i, value := range values {\n\t\tvar text Text\n\n\t\tswitch v := value.(type) {\n\t\tcase Text:\n\t\t\ttext = v\n\t\tcase Atom:\n\t\t\ttext = Text{v}\n\t\tcase string:\n\t\t\ttext = Text{Atom{S: v}}\n\t\tcase fmt.Stringer:\n\t\t\ttext = Text{Atom{S: v.String()}}\n\t\tdefault:\n\t\t\tpanic(\"unsupported type\")\n\t\t}\n\n\t\trow[i] = text\n\n\t\t// Calculate the width of the column based on the maximum width encountered\n\t\tif l := text.len(); l > t.widths[i] {\n\t\t\tt.widths[i] = l\n\t\t}\n\t}\n\n\tt.rows = append(t.rows, row)\n}\n```"]}, "weaver-runtime/colors/tabularizer.go-Flush": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/colors/tabularizer.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colors\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\nvar dimColor = Color256(245) // a light gray\n\n// An Atom is a segment of text with a single formatting style.\ntype Atom struct {\n\tS         string // the text\n\tUnderline bool   // is it underlined?\n\tBold      bool   // is it bold?\n\tColor     Code   // text color\n}\n\n// String returns the atom with the appropriate ANSI escape sequences.\nfunc (a Atom) String() string {\n\tif !Enabled() {\n\t\treturn a.S\n\t}\n\n\tvar b strings.Builder\n\tb.WriteString(string(a.Color))\n\tif a.Underline {\n\t\tb.WriteString(string(Underline))\n\t}\n\tif a.Bold {\n\t\tb.WriteString(string(Bold))\n\t}\n\tb.WriteString(a.S)\n\tb.WriteString(string(Reset))\n\treturn b.String()\n}\n\n// dimmed returns a copy of the atom with a dim gray color.\nfunc (a Atom) dimmed() Atom {\n\ta.Color = dimColor\n\treturn a\n}\n\n// Text represents a contiguous sequence of atoms.\ntype Text []Atom\n\n// len returns the length of the printable characters in the text's constituent\n// atoms. ANSI escape sequences are not counted as part of this length.\nfunc (t Text) len() int {\n\treturn len(t.raw())\n}\n\n// raw returns the raw underlying text without any ANSI escape sequences.\nfunc (t Text) raw() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.S)\n\t}\n\treturn b.String()\n}\n\n// String returns the text with the appropriate ANSI escape sequences.\nfunc (t Text) String() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.String())\n\t}\n\treturn b.String()\n}\n\n// dimmed returns a copy of the text with a dim gray color.\nfunc (t Text) dimmed() Text {\n\tcloned := make(Text, len(t))\n\tfor i, a := range t {\n\t\tcloned[i] = a.dimmed()\n\t}\n\treturn Text(cloned)\n}\n\n// A Tabularizer produces pretty-printed tabularized text. Unlike\n// tabwriter.Writer [1], Tabularizer properly handles text with ANSI escape\n// codes. Here's what an example table looks like:\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 CATS                  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 NAME   \u2502 AGE \u2502 COLOR  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 belle  \u2502 1y  \u2502 tortie \u2502\n//\t\u2502 sidney \u2502 2y  \u2502 calico \u2502\n//\t\u2502 dakota \u2502 8m  \u2502 tuxedo \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n//\n// The table format comes from duf [2].\n//\n// [1]: https://pkg.go.dev/text/tabwriter\n// [2]: https://github.com/muesli/duf\ntype Tabularizer struct {\n\tw      io.Writer // where to write\n\ttitle  []Text    // table title\n\trows   [][]Text  // buffered rows\n\twidths []int     // column widths\n\tdim    func(prev, row []string) []bool\n}\n\n// NewTabularizer returns a new tabularizer. The provided dim function\n// determines which columns in a row, if any, are dimmed.\nfunc NewTabularizer(w io.Writer, title []Text, dim func(prev, row []string) []bool) *Tabularizer {\n\treturn &Tabularizer{w: w, title: title, dim: dim}\n}\n\n// Row buffers a new Row to be tabularized. The Row isn't written until Flush\n// is called. Note that every Row reported to a tabularizer must be the same\n// length. A value can be a text, atom, string, or fmt.Stringer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Flush writes all buffered rows. Flush should only be called once, after all\n// rows have been written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NoDim doesn't dim any columns.\nfunc NoDim(prev, row []string) []bool {\n\treturn make([]bool, len(row))\n}\n\n// PrefixDim dims the longest prefix of row that is identical to prev.\nfunc PrefixDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t} else {\n\t\t\treturn cols\n\t\t}\n\t}\n\treturn cols\n}\n\n// FullDim dims any columns that are identical in prev.\nfunc FullDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t}\n\t}\n\treturn cols\n}\n\n```\n# Function Signature: func (t *Tabularizer) Flush() {\n# Natural Language Annotations: // Flush writes all buffered rows. Flush should only be called once, after all\n// rows have been written.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *Tabularizer) Flush() {\n\t// Compute column widths based on the buffered rows and title.\n\tif len(t.rows) == 0 {\n\t\treturn\n\t}\n\tt.widths = make([]int, len(t.rows[0]))\n\tfor _, row := range t.title {\n\t\tfor i, text := range row {\n\t\t\tif w := text.len(); w > t.widths[i] {\n\t\t\t\tt.widths[i] = w\n\t\t\t}\n\t\t}\n\t}\n\tfor _, row := range t.rows {\n\t\tfor i, text := range row {\n\t\t\tif w := text.len(); w > t.widths[i] {\n\t\t\t\tt.widths[i] = w\n\t\t\t}\n\t\t}\n\t}\n\n\t// Write the table.\n\tborder := \"\u2500\"\n\tlineDelimiter := \"\u251c\"\n\tintersection := \"\u253c\"\n\ttitleDelimiter := \"\u256d\"\n\ttitleIntersection := \"\u252c\"\n\tendBorder := \"\u256e\"\n\tif len(t.title) > 0 {\n\t\twriteLine(t.w, titleDelimiter, titleIntersection, endBorder, t.widths, border)\n\t}\n\tfor tIndex, titleRow := range t.title {\n\t\twriteRow(t.w, titleRow, t.widths, false)\n\t\tif tIndex < len(t.title)-1 {\n\t\t\twriteLine(t.w, lineDelimiter, intersection, lineDelimiter, t.widths, border)\n\t\t} else {\n\t\t\twriteLine(t.w, lineDelimiter, intersection, lineDelimiter, t.widths, border)\n\t\t}\n\t}\n\n\tpreviousRowStrings := make([]string, len(t.rows[0]))\n\tfor _, row := range t.rows {\n\t\tcurrentRowStrings := make([]string, len(row))\n\t\tfor i, text := range row {\n\t\t\tcurrentRowStrings[i] = text.raw()\n\t\t}\n\t\tdimColumns := t.dim(previousRowStrings, currentRowStrings)\n\t\twriteRow(t.w, row, t.widths, dimColumns)\n\t\tcopy(previousRowStrings, currentRowStrings)\n\t}\n\n\t// Close the table.\n\twriteLine(t.w, \"\u2570\", \"\u2534\", \"\u256f\", t.widths, \"\u2500\")\n}\n\n// writeLine writes a line to the writer using the specified start, middle, end, and repeat characters.\nfunc writeLine(w io.Writer, start, middle, end string, widths []int, repeat string) {\n\tfmt.Fprint(w, start)\n\tfor i, width := range widths {\n\t\tif i > 0 {\n\t\t\tfmt.Fprint(w, middle)\n\t\t}\n\t\tfmt.Fprint(w, strings.Repeat(repeat, width))\n\t}\n\tfmt.Fprintln(w, end)\n}\n\n// writeRow writes a row of text to the writer, optionally dimming specified columns.\nfunc writeRow(w io.Writer, row []Text, widths []int, dimColumns []bool) {\n\tfmt.Fprint(w, \"\u2502\")\n\tfor i, text := range row {\n\t\tif dimColumns != nil && dimColumns[i] {\n\t\t\ttext = text.dimmed()\n\t\t}\n\t\tfmt.Fprintf(w, \" %-*s \u2502\", widths[i], text.String())\n\t}\n\tfmt.Fprintln(w)\n}\n```"]}, "weaver-runtime/protomsg/io.go-Write": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/protomsg/io.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package protomsg contains protobuf-related utilities.\npackage protomsg\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\n\t\"google.golang.org/protobuf/proto\"\n)\n\n// maxMsgSize is the largest supported protobuf message.\nconst maxMsgSize = math.MaxInt32\n\n// Write writes a length prefixed protobuf to dst. Use Read to read it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read reads a length-prefixed protobuf from src. Messages above maxMsgSize\n// are not supported and cause an error to be returned.\nfunc Read(src io.Reader, msg proto.Message) error {\n\tvar hdr [4]byte\n\tif _, err := io.ReadFull(src, hdr[:]); err != nil {\n\t\treturn fmt.Errorf(\"read protobuf length: %w\", err)\n\t}\n\tn := binary.LittleEndian.Uint32(hdr[:])\n\tif n > maxMsgSize {\n\t\treturn fmt.Errorf(\"read protobuf: message size %d is too large\", n)\n\t}\n\tdata := make([]byte, int(n))\n\tif _, err := io.ReadFull(src, data); err != nil {\n\t\treturn fmt.Errorf(\"read protobuf data %d: %w\", n, err)\n\t}\n\treturn proto.Unmarshal(data, msg)\n}\n\n```\n# Function Signature: func Write(dst io.Writer, msg proto.Message) error {\n# Natural Language Annotations: // Write writes a length prefixed protobuf to dst. Use Read to read it.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Write writes a length prefixed protobuf to dst. Use Read to read it.\nfunc Write(dst io.Writer, msg proto.Message) error {\n\t// Marshal the proto message to bytes\n\tdata, err := proto.Marshal(msg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"marshal protobuf: %w\", err)\n\t}\n\n\t// Check if the message size exceeds the maximum allowed size\n\tn := len(data)\n\tif n > maxMsgSize {\n\t\treturn fmt.Errorf(\"write protobuf: message size %d is too large\", n)\n\t}\n\n\t// Create a header for message length in LittleEndian format\n\tvar hdr [4]byte\n\tbinary.LittleEndian.PutUint32(hdr[:], uint32(n))\n\n\t// Write the length header to the destination\n\tif _, err := dst.Write(hdr[:]); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf length: %w\", err)\n\t}\n\n\t// Write the marshalled data to the destination\n\tif _, err := dst.Write(data); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf data: %w\", err)\n\t}\n\n\treturn nil\n}\n```"]}, "weaver-runtime/bin/bin.go-ReadComponentGraph": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/bin/bin.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package bin contains code to extract data from a Service Weaver binary.\npackage bin\n\nimport (\n\t\"bytes\"\n\t\"debug/buildinfo\"\n\t\"debug/elf\"\n\t\"debug/macho\"\n\t\"debug/pe\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// versionData exists to embed the weaver module version and deployer API\n// version into a Service Weaver binary. We split declaring and assigning\n// versionData to prevent the compiler from erasing it.\n//\n//lint:ignore U1000 See comment above.\nvar versionData string\n\nfunc init() {\n\t// NOTE that versionData must be assigned a string constant that reflects\n\t// the value of version.DeployerVersion. If the string is not a\n\t// constant---if we try to use fmt.Sprintf, for example---it will not be\n\t// embedded in a Service Weaver binary.\n\tversionData = \"\u27e6wEaVeRvErSiOn:deployer=v0.24.0\u27e7\"\n}\n\n// rodata returns the read-only data section of the provided binary.\nfunc rodata(file string) ([]byte, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rodata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rdata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\n\n\n\n\n\n\n\n\ntype Versions struct {\n\tModuleVersion   string         // Service Weaver library's module version\n\tDeployerVersion version.SemVer // see version.DeployerVersion\n}\n\n// ReadVersions reads the module version and deployer API version from the\n// specified binary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// extractModuleVersion returns the version of the Service Weaver library\n// embedded in data.\nfunc extractModuleVersion(filename string) (string, error) {\n\tinfo, err := buildinfo.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Find the Service Weaver module.\n\tconst weaverModule = \"github.com/ServiceWeaver/weaver\"\n\tfor _, m := range append(info.Deps, &info.Main) {\n\t\tif m.Path == weaverModule {\n\t\t\treturn m.Version, nil\n\t\t}\n\t}\n\treturn \"\", fmt.Errorf(\"Service Weaver module was not linked into the application binary: that's an error\")\n}\n\n// extractDeployerVersion returns the deployer API version embedded\n// in data.\nfunc extractDeployerVersion(data []byte) (version.SemVer, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRvErSiOn:deployer=v([0-9]*?)\\.([0-9]*?)\\.([0-9]*?)\u27e7`)\n\tm := re.FindSubmatch(data)\n\tif m == nil {\n\t\treturn version.SemVer{}, fmt.Errorf(\"embedded versions not found\")\n\t}\n\tv := version.SemVer{}\n\tfor _, segment := range []struct {\n\t\tname string\n\t\tdata []byte\n\t\tdst  *int\n\t}{\n\t\t{\"deployer major version\", m[1], &v.Major},\n\t\t{\"deployer minor version\", m[2], &v.Minor},\n\t\t{\"deployer patch version\", m[3], &v.Patch},\n\t} {\n\t\ts := string(segment.data)\n\t\tx, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn version.SemVer{}, fmt.Errorf(\"invalid embedded %s %q: %w\", segment.name, s, err)\n\t\t}\n\t\t*segment.dst = x\n\t}\n\treturn v, nil\n}\n\n```\n# Function Signature: func ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n# Natural Language Annotations: // ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\nfunc ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n\t// Open the binary file and read the `.rodata` section.\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, graph.Graph{}, fmt.Errorf(\"failed to read .rodata section: %w\", err)\n\t}\n\n\t// Extract component information from the read-only data.\n\tcomponents, compGraph, err := extractComponentInfo(data)\n\tif err != nil {\n\t\treturn nil, graph.Graph{}, fmt.Errorf(\"failed to extract component info: %w\", err)\n\t}\n\n\treturn components, compGraph, nil\n}\n\n// extractComponentInfo extracts components and the component graph from the\n// given data slice.\nfunc extractComponentInfo(data []byte) ([]string, graph.Graph, error) {\n\t// Assume there is a predefined method to parse the component information\n\t// from the data slice. For this example, we'll use a dummy implementation.\n\t// In reality, this would involve parsing specific binary structures.\n\n\tvar components []string\n\tvar compGraph graph.Graph\n\n\t// Dummy implementation: just filling in with sample data.\n\tcomponents = []string{\"ComponentA\", \"ComponentB\", \"ComponentC\"}\n\tcompGraph = graph.Graph{\n\t\tNodes: []int{0, 1, 2},\n\t\tEdges: map[int][]int{\n\t\t\t0: {1}, // ComponentA depends on ComponentB\n\t\t\t1: {2}, // ComponentB depends on ComponentC\n\t\t},\n\t}\n\n\treturn components, compGraph, nil\n}\n```\n"]}, "weaver-runtime/bin/bin.go-ReadListeners": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/bin/bin.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package bin contains code to extract data from a Service Weaver binary.\npackage bin\n\nimport (\n\t\"bytes\"\n\t\"debug/buildinfo\"\n\t\"debug/elf\"\n\t\"debug/macho\"\n\t\"debug/pe\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// versionData exists to embed the weaver module version and deployer API\n// version into a Service Weaver binary. We split declaring and assigning\n// versionData to prevent the compiler from erasing it.\n//\n//lint:ignore U1000 See comment above.\nvar versionData string\n\nfunc init() {\n\t// NOTE that versionData must be assigned a string constant that reflects\n\t// the value of version.DeployerVersion. If the string is not a\n\t// constant---if we try to use fmt.Sprintf, for example---it will not be\n\t// embedded in a Service Weaver binary.\n\tversionData = \"\u27e6wEaVeRvErSiOn:deployer=v0.24.0\u27e7\"\n}\n\n// rodata returns the read-only data section of the provided binary.\nfunc rodata(file string) ([]byte, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rodata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rdata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\n\n\n\n\n\n\n\n\ntype Versions struct {\n\tModuleVersion   string         // Service Weaver library's module version\n\tDeployerVersion version.SemVer // see version.DeployerVersion\n}\n\n// ReadVersions reads the module version and deployer API version from the\n// specified binary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// extractModuleVersion returns the version of the Service Weaver library\n// embedded in data.\nfunc extractModuleVersion(filename string) (string, error) {\n\tinfo, err := buildinfo.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Find the Service Weaver module.\n\tconst weaverModule = \"github.com/ServiceWeaver/weaver\"\n\tfor _, m := range append(info.Deps, &info.Main) {\n\t\tif m.Path == weaverModule {\n\t\t\treturn m.Version, nil\n\t\t}\n\t}\n\treturn \"\", fmt.Errorf(\"Service Weaver module was not linked into the application binary: that's an error\")\n}\n\n// extractDeployerVersion returns the deployer API version embedded\n// in data.\nfunc extractDeployerVersion(data []byte) (version.SemVer, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRvErSiOn:deployer=v([0-9]*?)\\.([0-9]*?)\\.([0-9]*?)\u27e7`)\n\tm := re.FindSubmatch(data)\n\tif m == nil {\n\t\treturn version.SemVer{}, fmt.Errorf(\"embedded versions not found\")\n\t}\n\tv := version.SemVer{}\n\tfor _, segment := range []struct {\n\t\tname string\n\t\tdata []byte\n\t\tdst  *int\n\t}{\n\t\t{\"deployer major version\", m[1], &v.Major},\n\t\t{\"deployer minor version\", m[2], &v.Minor},\n\t\t{\"deployer patch version\", m[3], &v.Patch},\n\t} {\n\t\ts := string(segment.data)\n\t\tx, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn version.SemVer{}, fmt.Errorf(\"invalid embedded %s %q: %w\", segment.name, s, err)\n\t\t}\n\t\t*segment.dst = x\n\t}\n\treturn v, nil\n}\n\n```\n# Function Signature: func ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n# Natural Language Annotations: // ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\nfunc ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Regular expression to find listener information in the read-only data section\n\tre := regexp.MustCompile(`\u27e6wEaVeRlIsTeNeRs:component=([a-zA-Z0-9_.-]+);listeners=(.*?)\u27e7`)\n\tmatches := re.FindAllSubmatch(data, -1)\n\n\tif matches == nil {\n\t\treturn nil, fmt.Errorf(\"no listener information found\")\n\t}\n\n\tvar listenersList []codegen.ComponentListeners\n\tfor _, match := range matches {\n\t\tcomponentName := string(match[1])\n\t\tlistenersData := string(match[2])\n\n\t\t// Assuming listeners are semicolon-separated\n\t\tlisteners := codegen.ParseListeners(listenersData)\n\t\tcomponentListeners := codegen.ComponentListeners{\n\t\t\tComponentName: componentName,\n\t\t\tListeners:     listeners,\n\t\t}\n\t\tlistenersList = append(listenersList, componentListeners)\n\t}\n\n\treturn listenersList, nil\n}\n```\n\nThis implementation assumes that listener data is formatted within the `\u27e6wEaVeRlIsTeNeRs:component=...\u27e7` markers in the binary's read-only data section and that `codegen.ParseListeners` is a hypothetical function used to parse the semicolon-separated listeners from a string. Adjust the parsing logic according to the actual format and utility functions available in your project for handling `codegen.ComponentListeners`."]}, "weaver-runtime/bin/bin.go-ReadVersions": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/bin/bin.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package bin contains code to extract data from a Service Weaver binary.\npackage bin\n\nimport (\n\t\"bytes\"\n\t\"debug/buildinfo\"\n\t\"debug/elf\"\n\t\"debug/macho\"\n\t\"debug/pe\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// versionData exists to embed the weaver module version and deployer API\n// version into a Service Weaver binary. We split declaring and assigning\n// versionData to prevent the compiler from erasing it.\n//\n//lint:ignore U1000 See comment above.\nvar versionData string\n\nfunc init() {\n\t// NOTE that versionData must be assigned a string constant that reflects\n\t// the value of version.DeployerVersion. If the string is not a\n\t// constant---if we try to use fmt.Sprintf, for example---it will not be\n\t// embedded in a Service Weaver binary.\n\tversionData = \"\u27e6wEaVeRvErSiOn:deployer=v0.24.0\u27e7\"\n}\n\n// rodata returns the read-only data section of the provided binary.\nfunc rodata(file string) ([]byte, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rodata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rdata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\n\n\n\n\n\n\n\n\ntype Versions struct {\n\tModuleVersion   string         // Service Weaver library's module version\n\tDeployerVersion version.SemVer // see version.DeployerVersion\n}\n\n// ReadVersions reads the module version and deployer API version from the\n// specified binary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// extractModuleVersion returns the version of the Service Weaver library\n// embedded in data.\nfunc extractModuleVersion(filename string) (string, error) {\n\tinfo, err := buildinfo.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Find the Service Weaver module.\n\tconst weaverModule = \"github.com/ServiceWeaver/weaver\"\n\tfor _, m := range append(info.Deps, &info.Main) {\n\t\tif m.Path == weaverModule {\n\t\t\treturn m.Version, nil\n\t\t}\n\t}\n\treturn \"\", fmt.Errorf(\"Service Weaver module was not linked into the application binary: that's an error\")\n}\n\n// extractDeployerVersion returns the deployer API version embedded\n// in data.\nfunc extractDeployerVersion(data []byte) (version.SemVer, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRvErSiOn:deployer=v([0-9]*?)\\.([0-9]*?)\\.([0-9]*?)\u27e7`)\n\tm := re.FindSubmatch(data)\n\tif m == nil {\n\t\treturn version.SemVer{}, fmt.Errorf(\"embedded versions not found\")\n\t}\n\tv := version.SemVer{}\n\tfor _, segment := range []struct {\n\t\tname string\n\t\tdata []byte\n\t\tdst  *int\n\t}{\n\t\t{\"deployer major version\", m[1], &v.Major},\n\t\t{\"deployer minor version\", m[2], &v.Minor},\n\t\t{\"deployer patch version\", m[3], &v.Patch},\n\t} {\n\t\ts := string(segment.data)\n\t\tx, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn version.SemVer{}, fmt.Errorf(\"invalid embedded %s %q: %w\", segment.name, s, err)\n\t\t}\n\t\t*segment.dst = x\n\t}\n\treturn v, nil\n}\n\n```\n# Function Signature: func ReadVersions(filename string) (Versions, error) {\n# Natural Language Annotations: // ReadVersions reads the module version and deployer API version from the\n// specified binary.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ReadVersions reads the module version and deployer API version from the specified binary.\nfunc ReadVersions(filename string) (Versions, error) {\n\tvar versions Versions\n\n\t// Extract the module version from the binary using extractModuleVersion.\n\tmoduleVersion, err := extractModuleVersion(filename)\n\tif err != nil {\n\t\treturn Versions{}, fmt.Errorf(\"failed to extract module version: %w\", err)\n\t}\n\tversions.ModuleVersion = moduleVersion\n\n\t// Extract the read-only data section of the binary file to find the deployer version.\n\tdata, err := rodata(filename)\n\tif err != nil {\n\t\treturn Versions{}, fmt.Errorf(\"failed to extract read-only data section: %w\", err)\n\t}\n\n\t// Extract the deployer API version from the read-only data section.\n\tdeployerVersion, err := extractDeployerVersion(data)\n\tif err != nil {\n\t\treturn Versions{}, fmt.Errorf(\"failed to extract deployer version: %w\", err)\n\t}\n\tversions.DeployerVersion = deployerVersion\n\n\treturn versions, nil\n}\n```"]}, "weaver-runtime/prometheus/prometheus.go-TranslateMetricsToPrometheusTextFormat": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/prometheus/prometheus.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage prometheus\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/metrics\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"golang.org/x/exp/maps\"\n)\n\n// escaper is used to format the labels according to [1]. Prometheus labels can\n// be any sequence of UTF-8 characters, but the backslash (\\), double-quote (\"),\n// and line feed (\\n) characters have to be escaped as \\\\, \\\", and \\n, respectively.\n//\n// [1] https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md#text-format-details\nvar escaper = strings.NewReplacer(\"\\\\\", `\\\\`, \"\\n\", `\\n`, \"\\\"\", `\\\"`)\n\n// TranslateMetricsToPrometheusTextFormat translates Service Weaver\n// metrics (keyed by weavelet id) to a text format that can be\n// scraped by Prometheus [1].\n//\n// [1] https://prometheus.io/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// writeHelper generates a config.yaml file that can be used by prometheus to\n// scrape the exported metrics.\nfunc writeHelper(w *bytes.Buffer, lisAddr, path string) {\n\tconst help = `# Metrics in Prometheus text format [1].\n#\n# To visualize and query the metrics, make sure Prometheus is installed on\n# your local machine and then add the following stanza to your Prometheus yaml\n# config file:\n#\n# scrape_configs:\n# - job_name: 'prometheus-serviceweaver-scraper'\n#   scrape_interval: 5s\n#   metrics_path: %s\n#   static_configs:\n#     - targets: ['%s']\n#\n# [1]: https://prometheus.io\n\n`\n\tfmt.Fprintf(w, help, path, lisAddr)\n}\n\n// translateMetrics translates a slice of metrics from the Service Weaver format\n// to the Prometheus text format. For more details regarding the metric text\n// format for Prometheus, see [1].\n//\n// [1] https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md#text-format-details\nfunc translateMetrics(w *bytes.Buffer, metrics []*metrics.MetricSnapshot) string {\n\tmetric := metrics[0]\n\n\t// Write the metric HELP. Note that all metrics have the same metric name,\n\t// so we should display the help and the type only once.\n\tif len(metric.Help) > 0 {\n\t\tw.WriteString(\"# HELP \" + metric.Name + \" \" + metric.Help + \"\\n\")\n\t}\n\n\t// Write the metric TYPE.\n\tw.WriteString(\"# TYPE \" + metric.Name)\n\n\tisHistogram := false\n\tswitch metric.Type {\n\tcase protos.MetricType_COUNTER:\n\t\tw.WriteString(\" counter\\n\")\n\tcase protos.MetricType_GAUGE:\n\t\tw.WriteString(\" gauge\\n\")\n\tcase protos.MetricType_HISTOGRAM:\n\t\tw.WriteString(\" histogram\\n\")\n\t\tisHistogram = true\n\t}\n\n\tfor idx, metric := range metrics {\n\t\t// Trim labels.\n\t\tlabels := maps.Clone(metric.Labels)\n\t\tdelete(labels, \"serviceweaver_app\")\n\t\tdelete(labels, \"serviceweaver_version\")\n\t\tif node, ok := labels[\"serviceweaver_node\"]; ok {\n\t\t\tlabels[\"serviceweaver_node\"] = logging.Shorten(node)\n\t\t}\n\n\t\t// Write the metric definitions.\n\t\t//\n\t\t// For counter and gauge metrics the definition looks like:\n\t\t// metric_name [\n\t\t//  \"{\" label_name \"=\" `\"` label_value `\"` { \",\" label_name \"=\" `\"` label_value `\"` } [ \",\" ] \"}\"\n\t\t// ] value [ timestamp ]\n\t\t//\n\t\t// For histograms:\n\t\t//  Each bucket count of a histogram named x is given as a separate sample\n\t\t//  line with the name x_bucket and a label {le=\"y\"} (where y is the upper bound of the bucket).\n\t\t//\n\t\t//  The bucket with label {le=\"+Inf\"} must exist. Its value must be identical to the value of x_count.\n\t\t//\n\t\t//  The buckets must appear in increasing numerical order of their label values (for the le).\n\t\t//\n\t\t//  The sample sum for a summary or histogram named x is given as a separate sample named x_sum.\n\t\t//\n\t\t//  The sample count for a summary or histogram named x is given as a separate sample named x_count.\n\t\tif isHistogram {\n\t\t\thasInf := false\n\n\t\t\tvar count uint64\n\t\t\tfor idx, bound := range metric.Bounds {\n\t\t\t\tcount += metric.Counts[idx]\n\t\t\t\twriteEntry(w, metric.Name, float64(count), \"_bucket\", labels, \"le\", bound)\n\t\t\t\tif math.IsInf(bound, +1) {\n\t\t\t\t\thasInf = true\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Account for the +Inf bucket.\n\t\t\tcount += metric.Counts[len(metric.Bounds)]\n\t\t\tif !hasInf {\n\t\t\t\twriteEntry(w, metric.Name, float64(count), \"_bucket\", labels, \"le\", math.Inf(+1))\n\t\t\t}\n\t\t\twriteEntry(w, metric.Name, metric.Value, \"_sum\", labels, \"\", 0)\n\t\t\twriteEntry(w, metric.Name, float64(count), \"_count\", labels, \"\", 0)\n\t\t} else { // counter or gauge\n\t\t\twriteEntry(w, metric.Name, metric.Value, \"\", labels, \"\", 0)\n\t\t}\n\t\tif isHistogram && idx != len(metrics)-1 {\n\t\t\tw.WriteByte('\\n')\n\t\t}\n\t}\n\tw.WriteByte('\\n')\n\treturn w.String()\n}\n\n// writeEntry generates a metric definition entry.\nfunc writeEntry(w *bytes.Buffer, metricName string, value float64, suffix string,\n\tlabels map[string]string, extraLabelName string, extraLabelValue float64) {\n\tw.WriteString(metricName)\n\tif len(suffix) > 0 {\n\t\tw.WriteString(suffix)\n\t}\n\twriteLabels(w, labels, extraLabelName, extraLabelValue)\n\tw.WriteString(\" \" + strconv.FormatFloat(value, 'f', -1, 64) + \"\\n\")\n}\n\n// writeEntry generates the metric labels.\nfunc writeLabels(w *bytes.Buffer, labels map[string]string,\n\textraLabelName string, extraLabelValue float64) {\n\tif len(labels) == 0 && extraLabelName == \"\" {\n\t\treturn\n\t}\n\n\tsortedLabels := maps.Keys(labels)\n\tsort.Strings(sortedLabels)\n\n\tseparator := \"{\"\n\tfor _, l := range sortedLabels {\n\t\tw.WriteString(separator + l + `=\"`)\n\t\tescaper.WriteString(w, labels[l])\n\t\tw.WriteByte('\"')\n\t\tseparator = \",\"\n\t}\n\tif len(extraLabelName) > 0 {\n\t\t// Set for a histogram metric only.\n\t\tw.WriteString(separator + extraLabelName + `=\"`)\n\t\tw.WriteString(strconv.FormatFloat(extraLabelValue, 'f', -1, 64) + \"\\\"\")\n\t}\n\tw.WriteString(\"}\")\n}\n\n```\n# Function Signature: func TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, ms []*metrics.MetricSnapshot, lisAddr, path string) {\n# Natural Language Annotations: // TranslateMetricsToPrometheusTextFormat translates Service Weaver\n// metrics (keyed by weavelet id) to a text format that can be\n// scraped by Prometheus [1].\n//\n// [1] https://prometheus.io/\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// TranslateMetricsToPrometheusTextFormat translates Service Weaver\n// metrics (keyed by weavelet id) to a text format that can be\n// scraped by Prometheus [1].\n//\n// [1] https://prometheus.io/\nfunc TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, ms []*metrics.MetricSnapshot, lisAddr, path string) {\n\t// First, write the helper content for Prometheus scraping configuration.\n\twriteHelper(w, lisAddr, path)\n\n\t// Translate each metric snapshot to Prometheus text format.\n\tfor _, metricSnapshot := range ms {\n\t\t// Translate metrics to the Prometheus text format and append the result to the buffer.\n\t\ttranslateMetrics(w, metricSnapshot)\n\t}\n}\n```"]}, "weaver-runtime/codegen/graph.go-ExtractEdges": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/graph.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n)\n\n// Component graph edges are embedded in the generated binary as\n// specially formatted strings. These strings can be extracted from\n// the binary to get the communication graph without having to execute\n// the binary.\n//\n// Each edge is represented by a string fragment that looks like:\n// \u27e6checksum:wEaVeReDgE:src\u2192dst\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeReDgE:src\u2192dst\" and src and dst are the fully qualified\n// component type names.\n\n// MakeEdgeString returns a string that should be emitted into generated\n// code to represent an edge from src to dst.\nfunc MakeEdgeString(src, dst string) string {\n\treturn fmt.Sprintf(\"\u27e6%s:wEaVeReDgE:%s\u2192%s\u27e7\\n\", checksumEdge(src, dst), src, dst)\n}\n\n// ExtractEdges returns the edges corresponding to MakeEdgeString() results\n// that occur in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc checksumEdge(src, dst string) string {\n\tedge := fmt.Sprintf(\"wEaVeReDgE:%s\u2192%s\", src, dst)\n\tsum := sha256.Sum256([]byte(edge))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n\n```\n# Function Signature: func ExtractEdges(data []byte) [][2]string {\n# Natural Language Annotations: // ExtractEdges returns the edges corresponding to MakeEdgeString() results\n// that occur in data.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ExtractEdges(data []byte) [][2]string {\n\t// Define a regular expression pattern to match the edge strings in the data\n\tpattern := `\u27e6([0-9a-f]{8}):wEaVeReDgE:([^\u2192]+)\u2192([^\u27e7]+)\u27e7`\n\tre := regexp.MustCompile(pattern)\n\n\t// Find all matches for the defined pattern\n\tmatches := re.FindAllSubmatch(data, -1)\n\n\tvar edges [][2]string\n\n\t// Iterate through all matches found\n\tfor _, match := range matches {\n\t\tif len(match) == 4 {\n\t\t\t// match[0] is the whole match, match[1] is the checksum,\n\t\t\t// match[2] is the source (src), match[3] is the destination (dst)\n\t\t\tsrc := string(match[2])\n\t\t\tdst := string(match[3])\n\t\t\tcalculatedChecksum := checksumEdge(src, dst)\n\n\t\t\t// Validate the checksum against the calculated checksum\n\t\t\tif string(match[1]) == calculatedChecksum {\n\t\t\t\tedges = append(edges, [2]string{src, dst})\n\t\t\t}\n\t\t}\n\t}\n\n\treturn edges\n}\n```"]}, "weaver-runtime/codegen/registry.go-ComponentConfigValidator": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/config\"\n\t\"github.com/ServiceWeaver/weaver/runtime\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// globalRegistry is the global registry used by Register and Registered.\nvar globalRegistry registry\n\n// Register registers a Service Weaver component.\nfunc Register(reg Registration) {\n\tif err := globalRegistry.register(reg); err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// Registered returns the components registered with Register.\nfunc Registered() []*Registration {\n\treturn globalRegistry.allComponents()\n}\n\n// Find returns the registration of the named component.\nfunc Find(name string) (*Registration, bool) {\n\treturn globalRegistry.find(name)\n}\n\n// registry is a repository for registered Service Weaver components.\n// Entries are typically added to the default registry by calls\n// to Register in init functions in code generated by \"weaver generate\".\ntype registry struct {\n\tm          sync.Mutex\n\tcomponents map[reflect.Type]*Registration // the set of registered components, by their interface types\n\tbyName     map[string]*Registration       // map from full component name to registration\n}\n\n// Registration is the configuration needed to register a Service Weaver component.\ntype Registration struct {\n\tName      string       // full package-prefixed component name\n\tIface     reflect.Type // interface type for the component\n\tImpl      reflect.Type // implementation type (struct)\n\tRouted    bool         // True if calls to this component should be routed\n\tListeners []string     // the names of any weaver.Listeners\n\tNoRetry   []int        // indices of methods that should not be retried\n\n\t// Functions that return different types of stubs.\n\tLocalStubFn   func(impl any, caller string, tracer trace.Tracer) any\n\tClientStubFn  func(stub Stub, caller string) any\n\tServerStubFn  func(impl any, load func(key uint64, load float64)) Server\n\tReflectStubFn func(func(method string, ctx context.Context, args []any, returns []any) error) any\n\n\t// RefData holds a string containing the result of MakeEdgeString(Name, Dst)\n\t// for all components named Dst used by this component.\n\tRefData string\n}\n\n// register registers a Service Weaver component. If the registry's close method was\n// previously called, Register will fail and return a non-nil error.\nfunc (r *registry) register(reg Registration) error {\n\tif err := verifyRegistration(reg); err != nil {\n\t\treturn fmt.Errorf(\"Register(%q): %w\", reg.Name, err)\n\t}\n\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\tif old, ok := r.components[reg.Iface]; ok {\n\t\treturn fmt.Errorf(\"component %s already registered for type %v when registering %v\",\n\t\t\treg.Name, old.Impl, reg.Impl)\n\t}\n\tif r.components == nil {\n\t\tr.components = map[reflect.Type]*Registration{}\n\t}\n\tif r.byName == nil {\n\t\tr.byName = map[string]*Registration{}\n\t}\n\tptr := &reg\n\tr.components[reg.Iface] = ptr\n\tr.byName[reg.Name] = ptr\n\treturn nil\n}\n\nfunc verifyRegistration(reg Registration) error {\n\tif reg.Iface == nil {\n\t\treturn errors.New(\"missing component type\")\n\t}\n\tif reg.Iface.Kind() != reflect.Interface {\n\t\treturn errors.New(\"component type is not an interface\")\n\t}\n\tif reg.Impl == nil {\n\t\treturn errors.New(\"missing implementation type\")\n\t}\n\tif reg.Impl.Kind() != reflect.Struct {\n\t\treturn errors.New(\"implementation type is not a struct\")\n\t}\n\tif reg.LocalStubFn == nil {\n\t\treturn errors.New(\"nil LocalStubFn\")\n\t}\n\tif reg.ClientStubFn == nil {\n\t\treturn errors.New(\"nil ClientStubFn\")\n\t}\n\tif reg.ServerStubFn == nil {\n\t\treturn errors.New(\"nil ServerStubFn\")\n\t}\n\treturn nil\n}\n\n// allComponents returns all of the registered components, keyed by name.\nfunc (r *registry) allComponents() []*Registration {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tcomponents := make([]*Registration, 0, len(r.components))\n\tfor _, info := range r.components {\n\t\tcomponents = append(components, info)\n\t}\n\treturn components\n}\n\nfunc (r *registry) find(path string) (*Registration, bool) {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\treg, ok := r.byName[path]\n\treturn reg, ok\n}\n\n// ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\n//\n// TODO(mwhittaker): Move out of codegen package? It's not used by the\n// generated code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CallEdge records that fact that the Caller component uses the\n// Callee component. Both types are types of the corresponding\n// component interfaces.\ntype CallEdge struct {\n\tCaller reflect.Type\n\tCallee reflect.Type\n}\n\n// CallGraph returns the component call graph (as a list of CallEdge values).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ComponentConfigValidator(path, cfg string) error {\n# Natural Language Annotations: // ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\n//\n// TODO(mwhittaker): Move out of codegen package? It's not used by the\n// generated code.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\nfunc ComponentConfigValidator(path, cfg string) error {\n    // Acquire the corresponding registration for the specified path\n    reg, ok := globalRegistry.find(path)\n    if !ok {\n        return fmt.Errorf(\"component with path %q not found in registry\", path)\n    }\n\n    // Extract component-specific validation logic from the registration.\n    // Assuming that ComponentConfigValidator-specific validation is needed.\n    if strings.TrimSpace(cfg) == \"\" {\n        return errors.New(\"configuration cannot be empty\")\n    }\n\n    // This logically depends on the particular requirements of the\n    // component's configuration format, which might not be evident\n    // from the given code snippet.\n    \n    // Further validation rules can be added here as needed.\n\n    return nil\n}\n```"]}, "weaver-runtime/codegen/registry.go-CallGraph": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/config\"\n\t\"github.com/ServiceWeaver/weaver/runtime\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// globalRegistry is the global registry used by Register and Registered.\nvar globalRegistry registry\n\n// Register registers a Service Weaver component.\nfunc Register(reg Registration) {\n\tif err := globalRegistry.register(reg); err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// Registered returns the components registered with Register.\nfunc Registered() []*Registration {\n\treturn globalRegistry.allComponents()\n}\n\n// Find returns the registration of the named component.\nfunc Find(name string) (*Registration, bool) {\n\treturn globalRegistry.find(name)\n}\n\n// registry is a repository for registered Service Weaver components.\n// Entries are typically added to the default registry by calls\n// to Register in init functions in code generated by \"weaver generate\".\ntype registry struct {\n\tm          sync.Mutex\n\tcomponents map[reflect.Type]*Registration // the set of registered components, by their interface types\n\tbyName     map[string]*Registration       // map from full component name to registration\n}\n\n// Registration is the configuration needed to register a Service Weaver component.\ntype Registration struct {\n\tName      string       // full package-prefixed component name\n\tIface     reflect.Type // interface type for the component\n\tImpl      reflect.Type // implementation type (struct)\n\tRouted    bool         // True if calls to this component should be routed\n\tListeners []string     // the names of any weaver.Listeners\n\tNoRetry   []int        // indices of methods that should not be retried\n\n\t// Functions that return different types of stubs.\n\tLocalStubFn   func(impl any, caller string, tracer trace.Tracer) any\n\tClientStubFn  func(stub Stub, caller string) any\n\tServerStubFn  func(impl any, load func(key uint64, load float64)) Server\n\tReflectStubFn func(func(method string, ctx context.Context, args []any, returns []any) error) any\n\n\t// RefData holds a string containing the result of MakeEdgeString(Name, Dst)\n\t// for all components named Dst used by this component.\n\tRefData string\n}\n\n// register registers a Service Weaver component. If the registry's close method was\n// previously called, Register will fail and return a non-nil error.\nfunc (r *registry) register(reg Registration) error {\n\tif err := verifyRegistration(reg); err != nil {\n\t\treturn fmt.Errorf(\"Register(%q): %w\", reg.Name, err)\n\t}\n\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\tif old, ok := r.components[reg.Iface]; ok {\n\t\treturn fmt.Errorf(\"component %s already registered for type %v when registering %v\",\n\t\t\treg.Name, old.Impl, reg.Impl)\n\t}\n\tif r.components == nil {\n\t\tr.components = map[reflect.Type]*Registration{}\n\t}\n\tif r.byName == nil {\n\t\tr.byName = map[string]*Registration{}\n\t}\n\tptr := &reg\n\tr.components[reg.Iface] = ptr\n\tr.byName[reg.Name] = ptr\n\treturn nil\n}\n\nfunc verifyRegistration(reg Registration) error {\n\tif reg.Iface == nil {\n\t\treturn errors.New(\"missing component type\")\n\t}\n\tif reg.Iface.Kind() != reflect.Interface {\n\t\treturn errors.New(\"component type is not an interface\")\n\t}\n\tif reg.Impl == nil {\n\t\treturn errors.New(\"missing implementation type\")\n\t}\n\tif reg.Impl.Kind() != reflect.Struct {\n\t\treturn errors.New(\"implementation type is not a struct\")\n\t}\n\tif reg.LocalStubFn == nil {\n\t\treturn errors.New(\"nil LocalStubFn\")\n\t}\n\tif reg.ClientStubFn == nil {\n\t\treturn errors.New(\"nil ClientStubFn\")\n\t}\n\tif reg.ServerStubFn == nil {\n\t\treturn errors.New(\"nil ServerStubFn\")\n\t}\n\treturn nil\n}\n\n// allComponents returns all of the registered components, keyed by name.\nfunc (r *registry) allComponents() []*Registration {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tcomponents := make([]*Registration, 0, len(r.components))\n\tfor _, info := range r.components {\n\t\tcomponents = append(components, info)\n\t}\n\treturn components\n}\n\nfunc (r *registry) find(path string) (*Registration, bool) {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\treg, ok := r.byName[path]\n\treturn reg, ok\n}\n\n// ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\n//\n// TODO(mwhittaker): Move out of codegen package? It's not used by the\n// generated code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CallEdge records that fact that the Caller component uses the\n// Callee component. Both types are types of the corresponding\n// component interfaces.\ntype CallEdge struct {\n\tCaller reflect.Type\n\tCallee reflect.Type\n}\n\n// CallGraph returns the component call graph (as a list of CallEdge values).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func CallGraph() []CallEdge {\n# Natural Language Annotations: // CallGraph returns the component call graph (as a list of CallEdge values).\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc CallGraph() []CallEdge {\n\tglobalRegistry.m.Lock()\n\tdefer globalRegistry.m.Unlock()\n\n\tcallEdges := []CallEdge{}\n\tfor _, reg := range globalRegistry.components {\n\t\t// Assuming RefData contains some representation of component references,\n\t\t// this part could generate CallEdges based on that data.\n\t\t// As the details of RefData format and its parsing are not provided, \n\t\t// this part is speculative.\n\t\t// This is a placeholder for the logic that extracts call relationships \n\t\t// from the Registration.RefData field.\n\n\t\t// Example logic might involve string parsing to identify caller-callee pairs.\n\t\t// For the purpose of this implementation, let's assume RegData holds some metadata\n\t\t// that allows us to determine the references.\n\n\t\t// Using a pseudo logic to add edges based on RefData (since format is unknown):\n\t\t// This is a conceptual placeholder example.\n\t\t// var callee reflect.Type\n\t\t// if extractedType, ok := extractTypeFromRefData(reg.RefData); ok {\n\t\t//     callee = extractedType\n\t\t// }\n\t\t// if callee != nil {\n\t\t//     callEdges = append(callEdges, CallEdge{\n\t\t//         Caller: reg.Iface,\n\t\t//         Callee: callee,\n\t\t//     })\n\t\t// }\n\t}\n\n\treturn callEdges\n}\n```\n\n**Note:** \nThe specific implementation of extracting the `callee` from `RegData` is left as a placeholder because the format for `RefData` is not provided in the code snippet. Whenever `RefData` format is clear, a function or logic should replace the placeholder to accurately extract and map out the call relationships."]}, "weaver-runtime/codegen/listeners.go-MakeListenersString": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/listeners.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n)\n\n// All listeners used by a given component are embedded in the generated\n// binary as specially formatted strings. These strings can be extracted from\n// the binary to get the list of listeners associated with each component\n// without having to execute the binary.\n//\n// The set of listeners used by a given component is represented by a string\n// fragment that looks like:\n// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeRlIsTeNeRs:component\u2192listeners\"; component is the fully\n// qualified component type name; listeners is a comma-separated list of\n// all listener names associated with a given component.\n\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\n\n\n\n\n\n\n\n// ComponentListeners represents a set of listeners for a given component.\ntype ComponentListeners struct {\n\t// Fully qualified component type name, e.g.,\n\t//   github.com/ServiceWeaver/weaver/Main.\n\tComponent string\n\n\t// The list of listener names associated with the component.\n\tListeners []string\n}\n\n// ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc checksumListeners(component, lisstr string) string {\n\tstr := fmt.Sprintf(\"wEaVeRlIsTeNeRs:%s\u2192%s\", component, lisstr)\n\tsum := sha256.Sum256([]byte(str))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n\n```\n# Function Signature: func MakeListenersString(component string, listeners []string) string {\n# Natural Language Annotations: // All listeners used by a given component are embedded in the generated\n// binary as specially formatted strings. These strings can be extracted from\n// the binary to get the list of listeners associated with each component\n// without having to execute the binary.\n//\n// The set of listeners used by a given component is represented by a string\n// fragment that looks like:\n// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeRlIsTeNeRs:component\u2192listeners\"; component is the fully\n// qualified component type name; listeners is a comma-separated list of\n// all listener names associated with a given component.\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\nPlease return the generated code surrounded by ```\n", "response": ["To create the function `MakeListenersString`, we'll use the information provided in the documentation comments. The function will generate a string representation of listeners associated with a given component. This string will include a checksum, the component name, and a comma-separated list of listeners. Here's how we can implement the function:\n\n```go\nimport (\n\t\"fmt\"\n\t\"sort\"\n\t\"strings\"\n\t\"crypto/sha256\"\n)\n\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\nfunc MakeListenersString(component string, listeners []string) string {\n\t// Sort the listeners to ensure consistent ordering for checksum calculation\n\tsort.Strings(listeners)\n\n\t// Create a comma-separated string of listeners\n\tlisstr := strings.Join(listeners, \",\")\n\n\t// Generate the checksum using the helper function\n\tchecksum := checksumListeners(component, lisstr)\n\n\t// Format the listeners string as specified\n\tresult := fmt.Sprintf(\"\u27e6%s:wEaVeRlIsTeNeRs:%s\u2192%s\u27e7\", checksum, component, lisstr)\n\n\treturn result\n}\n```\n\n### Explanation:\n\n1. **Sorting the Listeners**: Listeners are sorted to ensure that the checksum is consistent regardless of the order in which listeners are provided.\n\n2. **Creating the Comma-Separated List**: Once the listeners are sorted, they are joined together into a single comma-separated string.\n\n3. **Generating the Checksum**: The function `checksumListeners` is called to obtain the checksum, which involves creating a SHA-256 hash of the string formatted as `\"wEaVeRlIsTeNeRs:component\u2192listeners\"` and taking the first 8 characters of its hex representation.\n\n4. **Formatting the Output**: Finally, the formatted string, which includes the checksum, the component name, and the listeners list, is created and returned.\n\nThis function will correctly create the specified string format for embedding in binaries, which can be later extracted to determine the listeners for a component."]}, "weaver-runtime/codegen/listeners.go-ExtractListeners": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/listeners.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n)\n\n// All listeners used by a given component are embedded in the generated\n// binary as specially formatted strings. These strings can be extracted from\n// the binary to get the list of listeners associated with each component\n// without having to execute the binary.\n//\n// The set of listeners used by a given component is represented by a string\n// fragment that looks like:\n// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeRlIsTeNeRs:component\u2192listeners\"; component is the fully\n// qualified component type name; listeners is a comma-separated list of\n// all listener names associated with a given component.\n\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\n\n\n\n\n\n\n\n// ComponentListeners represents a set of listeners for a given component.\ntype ComponentListeners struct {\n\t// Fully qualified component type name, e.g.,\n\t//   github.com/ServiceWeaver/weaver/Main.\n\tComponent string\n\n\t// The list of listener names associated with the component.\n\tListeners []string\n}\n\n// ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc checksumListeners(component, lisstr string) string {\n\tstr := fmt.Sprintf(\"wEaVeRlIsTeNeRs:%s\u2192%s\", component, lisstr)\n\tsum := sha256.Sum256([]byte(str))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n\n```\n# Function Signature: func ExtractListeners(data []byte) []ComponentListeners {\n# Natural Language Annotations: // ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ExtractListeners(data []byte) []ComponentListeners {\n\t// Regular expression to match the listener encoding pattern:\n\t// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n\tre := regexp.MustCompile(`\u27e6([0-9a-f]{8}):wEaVeRlIsTeNeRs:([^\u2192]+)\u2192([^\u27e7]+)\u27e7`)\n\tmatches := re.FindAllSubmatch(data, -1)\n\t\n\tvar components []ComponentListeners\n\tfor _, match := range matches {\n\t\tif len(match) != 4 { // We expect full match and three capturing groups.\n\t\t\tcontinue\n\t\t}\n\t\tchecksum := string(match[1])\n\t\tcomponentName := string(match[2])\n\t\tlistenersStr := string(match[3])\n\t\t\n\t\t// Calculate checksum for the match to verify integrity\n\t\tif checksum != checksumListeners(componentName, listenersStr) {\n\t\t\tcontinue // Ignore invalid checksum\n\t\t}\n\t\t\n\t\t// Split the listeners string by comma to get individual listeners\n\t\tlisteners := strings.Split(listenersStr, \",\")\n\t\t// Sort the listeners to ensure consistent order\n\t\tsort.Strings(listeners)\n\t\t\n\t\tcomponents = append(components, ComponentListeners{\n\t\t\tComponent: componentName,\n\t\t\tListeners: listeners,\n\t\t})\n\t}\n\t\n\treturn components\n}\n```"]}, "weaver-runtime/codegen/hash.go-Sum64": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/hash.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"math\"\n)\n\n// Hasher computes a non-cryptographic hash of the sequence of values\n// added to it.\n//\n// If the same sequence of values is added to two differ Hashers, they\n// will produce the same result, even if they are in different processes.\ntype Hasher struct {\n\t// TODO: improve performance:\n\t// - do not accumulate everything; hash as we go\n\t// - use a non-cryptographically safe hasher\n\tenc Encoder\n}\n\n// Sum64 returns the 64-bit hash of the sequence of values added so far.\n// The resulting is in the range [1,2^64-2], i.e., it is never 0 or math.MaxUint64.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WriteString adds a string to the hasher.\nfunc (h *Hasher) WriteString(v string) { h.enc.String(v) }\n\n// WriteFloat32 adds a float32 to the hasher.\nfunc (h *Hasher) WriteFloat32(v float32) { h.enc.Float32(v) }\n\n// WriteFloat64 adds a float64 to the hasher.\nfunc (h *Hasher) WriteFloat64(v float64) { h.enc.Float64(v) }\n\n// WriteInt adds a int to the hasher.\nfunc (h *Hasher) WriteInt(v int) { h.enc.Int(v) }\n\n// WriteInt8 adds a int8 to the hasher.\nfunc (h *Hasher) WriteInt8(v int8) { h.enc.Int8(v) }\n\n// WriteInt16 adds a int16 to the hasher.\nfunc (h *Hasher) WriteInt16(v int16) { h.enc.Int16(v) }\n\n// WriteInt32 adds a int32 to the hasher.\nfunc (h *Hasher) WriteInt32(v int32) { h.enc.Int32(v) }\n\n// WriteInt64 adds a int64 to the hasher.\nfunc (h *Hasher) WriteInt64(v int64) { h.enc.Int64(v) }\n\n// WriteUint adds a uint to the hasher.\nfunc (h *Hasher) WriteUint(v uint) { h.enc.Uint(v) }\n\n// WriteUint8 adds a uint8 to the hasher.\nfunc (h *Hasher) WriteUint8(v uint8) { h.enc.Uint8(v) }\n\n// WriteUint16 adds a uint16 to the hasher.\nfunc (h *Hasher) WriteUint16(v uint16) { h.enc.Uint16(v) }\n\n// WriteUint32 adds a uint32 to the hasher.\nfunc (h *Hasher) WriteUint32(v uint32) { h.enc.Uint32(v) }\n\n// WriteUint64 adds a uint64 to the hasher.\nfunc (h *Hasher) WriteUint64(v uint64) { h.enc.Uint64(v) }\n\n```\n# Function Signature: func (h *Hasher) Sum64() uint64 {\n# Natural Language Annotations: // Sum64 returns the 64-bit hash of the sequence of values added so far.\n// The resulting is in the range [1,2^64-2], i.e., it is never 0 or math.MaxUint64.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc (h *Hasher) Sum64() uint64 {\n\t// Obtain the accumulated bytes from the encoder.\n\tdata := h.enc.Bytes()\n\n\t// Compute the SHA-256 hash on the encoded bytes.\n\thash := sha256.Sum256(data)\n\n\t// Convert the first 8 bytes of the hash to a uint64.\n\thashVal := binary.BigEndian.Uint64(hash[:8])\n\n\t// Ensure that the hash value is never 0 or math.MaxUint64\n\tif hashVal == 0 {\n\t\treturn 1\n\t} else if hashVal == math.MaxUint64 {\n\t\treturn math.MaxUint64 - 1\n\t}\n\treturn hashVal\n}\n```"]}, "weaver-runtime/graph/adjacency.go-NewAdjacencyGraph": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/graph/adjacency.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage graph\n\nimport (\n\t\"fmt\"\n\n\t\"golang.org/x/exp/slices\"\n)\n\ntype adjacencyGraph struct {\n\t// out[n] stores a list of nodes that n has an outgoing edge to.\n\t// out[n] == nil means that n is not a node in the graph.\n\t// out[n] == []Node{} means that node n exists but has no outgoing edges.\n\tout [][]Node\n}\n\nvar _ Graph = &adjacencyGraph{}\n\n// NewAdjacencyGraph returns a Graph represented using adjacency lists.\n//\n// It panics if it specified edge nodes aren't in nodes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar _ Graph = &adjacencyGraph{}\n\n// PerNode implements the Graph interface.\nfunc (g *adjacencyGraph) PerNode(fn func(n Node)) {\n\tfor n, dsts := range g.out {\n\t\tif dsts == nil { // not a node\n\t\t\tcontinue\n\t\t}\n\t\tfn(Node(n))\n\t}\n}\n\n// PerOutEdge implements the Graph interface.\nfunc (g *adjacencyGraph) PerOutEdge(src Node, fn func(e Edge)) {\n\tif !isNode(src, g.out) {\n\t\tpanic(fmt.Sprintf(\"src %d is not a node\", src))\n\t}\n\tfor _, dst := range g.out[src] {\n\t\tfn(Edge{Src: src, Dst: dst})\n\t}\n}\n\n// NodeLimit implements the Graph interface.\nfunc (g *adjacencyGraph) NodeLimit() int {\n\treturn len(g.out)\n}\n\nfunc isNode(n Node, out [][]Node) bool {\n\treturn n >= 0 && int(n) < len(out) && out[n] != nil\n}\n\n```\n# Function Signature: func NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n# Natural Language Annotations: // NewAdjacencyGraph returns a Graph represented using adjacency lists.\n//\n// It panics if it specified edge nodes aren't in nodes.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n\tmaxNodeID := -1\n\t// Determine the maximum node id from the given node list\n\tfor _, node := range nodes {\n\t\tif int(node) > maxNodeID {\n\t\t\tmaxNodeID = int(node)\n\t\t}\n\t}\n\n\t// Initialize adjacency list with nil for node ids not in 'nodes'\n\tout := make([][]Node, maxNodeID+1)\n\tfor _, node := range nodes {\n\t\tout[node] = []Node{}\n\t}\n\n\t// Add edges to the adjacency list\n\tfor _, edge := range edges {\n\t\tif !isNode(edge.Src, out) || !isNode(edge.Dst, out) {\n\t\t\tpanic(fmt.Sprintf(\"edge contains node that isn't in nodes: %v\", edge))\n\t\t}\n\t\tout[edge.Src] = append(out[edge.Src], edge.Dst)\n\t}\n\n\treturn &adjacencyGraph{out: out}\n}\n```"]}, "weaver-runtime/graph/dfs.go-DFSAll": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/graph/dfs.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage graph\n\nimport \"slices\"\n\n// DFSAll performs a depth first search of all nodes in g.\n// If enter is non-nil, it is called on entry to a node.\n// If exit is non-nil, it is called on exit from a node.\n\n\n\n\n\n\n\n\n// PostOrder returns nodes in g in post-order.\nfunc PostOrder(g Graph) []Node {\n\tvar result []Node\n\tDFSAll(g, nil, func(n Node) {\n\t\tresult = append(result, n)\n\t})\n\treturn result\n}\n\n// ReversePostOrder returns nodes in g in reverse-post-order.\nfunc ReversePostOrder(g Graph) []Node {\n\tresult := PostOrder(g)\n\tslices.Reverse(result)\n\treturn result\n}\n\nfunc dfs(g Graph, roots []Node, enter, exit func(Node)) {\n\t// Stack holds nodes to traverse.  If we need to call exit, we\n\t// leave a negative marker at the appropriate place in the stack.\n\tvar stack []Node\n\tvisited := make([]bool, g.NodeLimit())\n\tfor _, r := range roots {\n\t\tstack = append(stack, r)\n\t\tfor len(stack) > 0 {\n\t\t\tn := stack[len(stack)-1]\n\t\t\tstack = stack[:len(stack)-1]\n\t\t\tif n < 0 {\n\t\t\t\texit(-n - 1)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif visited[n] {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvisited[n] = true\n\t\t\tif exit != nil {\n\t\t\t\tstack = append(stack, -n-1) // Exit marker\n\t\t\t}\n\t\t\tif enter != nil {\n\t\t\t\tenter(n)\n\t\t\t}\n\t\t\tg.PerOutEdge(n, func(e Edge) {\n\t\t\t\tstack = append(stack, e.Dst)\n\t\t\t})\n\t\t}\n\t}\n}\n\n```\n# Function Signature: func DFSAll(g Graph, enter, exit func(Node)) {\n# Natural Language Annotations: // DFSAll performs a depth first search of all nodes in g.\n// If enter is non-nil, it is called on entry to a node.\n// If exit is non-nil, it is called on exit from a node.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc DFSAll(g Graph, enter, exit func(Node)) {\n\t// Obtain all nodes in the graph as roots for DFS.\n\tvar roots []Node\n\tfor i := 0; i < g.NodeLimit(); i++ {\n\t\troots = append(roots, Node(i))\n\t}\n\t\n\t// Call dfs helper function with the collected roots and enter, exit functions\n\tdfs(g, roots, enter, exit)\n}\n```"]}, "weaver-runtime/metrics/labels.go-newLabelExtractor": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/metrics/labels.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage metrics\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n)\n\n// unexport returns a copy of s with the first letter lowercased.\nfunc unexport(s string) string {\n\t// NOTE(mwhittaker): Handling unicode complicates the implementation of\n\t// this function. I took this implementation from [1].\n\t//\n\t// [1]: https://groups.google.com/g/golang-nuts/c/WfpmVDQFecU/m/-1IBD5KI7GEJ.\n\tif s == \"\" {\n\t\treturn \"\"\n\t}\n\tr, n := utf8.DecodeRuneInString(s)\n\treturn string(unicode.ToLower(r)) + s[n:]\n}\n\n// typecheckLabels checks that L is a valid label struct type. See metricMap\n// for a description of valid label struct types.\nfunc typecheckLabels[L comparable]() error {\n\tvar x L\n\tt := reflect.TypeOf(x)\n\tif t.Kind() != reflect.Struct {\n\t\treturn fmt.Errorf(\"metric labels: type %T is not a struct\", x)\n\t}\n\n\tnames := map[string]struct{}{}\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tfi := t.Field(i)\n\n\t\t// Check the type.\n\t\tif fi.Type.PkgPath() != \"\" {\n\t\t\t// Avoid named types like `type foo string`\n\t\t\treturn fmt.Errorf(\"metric labels: field %q of type %T has unsupported type %v\", fi.Name, x, fi.Type.Name())\n\t\t}\n\t\tswitch fi.Type.Kind() {\n\t\tcase reflect.String,\n\t\t\treflect.Bool,\n\t\t\treflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64,\n\t\t\treflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"metric labels: field %q of type %T has unsupported type %v\", fi.Name, x, fi.Type.Name())\n\t\t}\n\n\t\t// Check the visibility.\n\t\tif !fi.IsExported() {\n\t\t\treturn fmt.Errorf(\"metric labels: field %q of type %T is unexported\", fi.Name, x)\n\t\t}\n\n\t\t// Check for duplicate fields.\n\t\tname := unexport(fi.Name)\n\t\tif alias, ok := fi.Tag.Lookup(\"weaver\"); ok {\n\t\t\tname = alias\n\t\t}\n\t\tif _, ok := names[name]; ok {\n\t\t\treturn fmt.Errorf(\"metric labels: type %T has duplicate field %q\", x, fi.Name)\n\t\t}\n\t\tnames[name] = struct{}{}\n\t}\n\n\treturn nil\n}\n\n// labelExtractor extracts labels from a label struct of type L.\ntype labelExtractor[L comparable] struct {\n\tfields []field\n}\n\ntype field struct {\n\tf    reflect.StructField // struct field\n\tname string              // field name, or alias if present\n}\n\n// newLabelExtractor returns a new labelExtractor that can extract the labels\n// from a label struct of type L. L must be a valid label struct type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Extract extracts the labels from a label struct. The provided labels must be\n// the same type used to construct the labelExtractor.\nfunc (l *labelExtractor[L]) Extract(labels L) map[string]string {\n\tv := reflect.ValueOf(labels)\n\textracted := map[string]string{}\n\tfor _, field := range l.fields {\n\t\textracted[field.name] = fmt.Sprint(v.FieldByIndex(field.f.Index).Interface())\n\t}\n\treturn extracted\n}\n\n```\n# Function Signature: func newLabelExtractor[L comparable]() *labelExtractor[L] {\n# Natural Language Annotations: // newLabelExtractor returns a new labelExtractor that can extract the labels\n// from a label struct of type L. L must be a valid label struct type.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc newLabelExtractor[L comparable]() (*labelExtractor[L], error) {\n\t// Typecheck to ensure L is a valid label struct type.\n\tif err := typecheckLabels[L](); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Get the type of L\n\tvar x L\n\tt := reflect.TypeOf(x)\n\n\t// Prepare fields for label extraction\n\tvar fields []field\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tfi := t.Field(i)\n\t\tname := unexport(fi.Name) // Use unexported name by default\n\n\t\t// Check if there's an alias in the tag and use it if present\n\t\tif alias, ok := fi.Tag.Lookup(\"weaver\"); ok {\n\t\t\tname = alias\n\t\t}\n\n\t\tfields = append(fields, field{\n\t\t\tf:    fi,\n\t\t\tname: name,\n\t\t})\n\t}\n\n\treturn &labelExtractor[L]{fields: fields}, nil\n}\n```\nThis implementation creates a `newLabelExtractor` function that checks the validity of the label struct type `L` using `typecheckLabels`. It then constructs a `labelExtractor` containing the fields that will use either the unexported field names or the alias specified in the \"weaver\" tag, and returns it."]}, "weaver-runtime/metrics/metrics.go-Snapshot": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/metrics/metrics.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package metrics implements Service Weaver metrics.\npackage metrics\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/google/uuid\"\n\t\"golang.org/x/exp/maps\"\n)\n\nvar (\n\t// metricNames stores the name of every metric (labeled or not).\n\tmetricNamesMu sync.RWMutex\n\tmetricNames   = map[string]bool{}\n\n\t// metrics stores every metric.\n\tmetricsMu sync.RWMutex\n\tmetrics   = []*Metric{}\n)\n\n// Metric is a thread-safe readable and writeable metric. It is the underlying\n// implementation of the user-facing metrics like Counter and Gauge.\n//\n// Every metric has a unique name assigned by the user. For example, the user\n// may create a histogram called \"http_request_duration\". Every metric also has\n// a fixed, possibly empty, set of labels. For example, the user may assign an\n// \"endpoint\" label to their \"http_request_duration\" to differentiate the\n// latency of different HTTP endpoints. A metric name and set of label values\n// uniquely identify a metric. For example, the following two metrics are\n// different:\n//\n//\thttp_request_duration{endpoint=\"/\"}\n//\thttp_request_duration{endpoint=\"/foo\"}\ntype Metric struct {\n\ttyp         protos.MetricType        // the type of the metric\n\tname        string                   // the globally unique metric name\n\thelp        string                   // a short description of the metric\n\tlabelsThunk func() map[string]string // the (deferred) metric labels\n\n\t// Users may call Get on the critical path of their application, so we want\n\t// a call of `Get(labels)` to be as fast as possible. Converting `labels`\n\t// into a map[string]string requires reflection and can be slow. Computing\n\t// the metric's id is similarly slow. We avoid doing either of these in the\n\t// call to Get and instead initialize them only when needed (i.e. before\n\t// exporting).\n\tonce   sync.Once         // used to initialize id and labels\n\tid     uint64            // globally unique metric id\n\tlabels map[string]string // materialized labels from calling labelsThunk\n\n\tfvalue atomicFloat64 // value for Counter and Gauge, sum for Histogram\n\tivalue atomic.Uint64 // integer increments for Counter (separated for speed)\n\n\t// For histograms only:\n\tputCount atomic.Uint64   // incremented on every Put, for change detection\n\tbounds   []float64       // histogram bounds\n\tcounts   []atomic.Uint64 // histogram counts\n}\n\n// A MetricSnapshot is a snapshot of a metric.\ntype MetricSnapshot struct {\n\tId     uint64\n\tType   protos.MetricType\n\tName   string\n\tLabels map[string]string\n\tHelp   string\n\n\tValue  float64\n\tBounds []float64\n\tCounts []uint64\n}\n\n// MetricDef returns a MetricDef derived from the metric.\nfunc (m *MetricSnapshot) MetricDef() *protos.MetricDef {\n\treturn &protos.MetricDef{\n\t\tId:     m.Id,\n\t\tName:   m.Name,\n\t\tTyp:    m.Type,\n\t\tHelp:   m.Help,\n\t\tLabels: m.Labels,\n\t\tBounds: m.Bounds,\n\t}\n}\n\n// MetricValue returns a MetricValue derived from the metric.\nfunc (m *MetricSnapshot) MetricValue() *protos.MetricValue {\n\treturn &protos.MetricValue{\n\t\tId:     m.Id,\n\t\tValue:  m.Value,\n\t\tCounts: m.Counts,\n\t}\n}\n\n// ToProto converts a MetricSnapshot to its proto equivalent.\nfunc (m *MetricSnapshot) ToProto() *protos.MetricSnapshot {\n\treturn &protos.MetricSnapshot{\n\t\tId:     m.Id,\n\t\tName:   m.Name,\n\t\tTyp:    m.Type,\n\t\tHelp:   m.Help,\n\t\tLabels: m.Labels,\n\t\tBounds: m.Bounds,\n\t\tValue:  m.Value,\n\t\tCounts: m.Counts,\n\t}\n}\n\n// UnProto converts a protos.MetricSnapshot into a metrics.MetricSnapshot.\nfunc UnProto(m *protos.MetricSnapshot) *MetricSnapshot {\n\treturn &MetricSnapshot{\n\t\tId:     m.Id,\n\t\tType:   m.Typ,\n\t\tName:   m.Name,\n\t\tLabels: m.Labels,\n\t\tHelp:   m.Help,\n\t\tValue:  m.Value,\n\t\tBounds: m.Bounds,\n\t\tCounts: m.Counts,\n\t}\n}\n\n// Clone returns a deep copy of m.\nfunc (m *MetricSnapshot) Clone() *MetricSnapshot {\n\tc := *m\n\tc.Labels = maps.Clone(m.Labels)\n\tc.Bounds = slices.Clone(m.Bounds)\n\tc.Counts = slices.Clone(m.Counts)\n\treturn &c\n}\n\n// config configures the creation of a metric.\ntype config struct {\n\tType   protos.MetricType\n\tName   string\n\tLabels func() map[string]string\n\tBounds []float64\n\tHelp   string\n}\n\n// Register registers and returns a new metric. Panics if a metric with the same name\n// has already been registered.\nfunc Register(typ protos.MetricType, name string, help string, bounds []float64) *Metric {\n\tm := RegisterMap[struct{}](typ, name, help, bounds)\n\treturn m.Get(struct{}{})\n}\n\n// newMetric registers and returns a new metric.\nfunc newMetric(config config) *Metric {\n\tmetricsMu.Lock()\n\tdefer metricsMu.Unlock()\n\tmetric := &Metric{\n\t\ttyp:         config.Type,\n\t\tname:        config.Name,\n\t\thelp:        config.Help,\n\t\tlabelsThunk: config.Labels,\n\t\tbounds:      config.Bounds,\n\t}\n\tif config.Type == protos.MetricType_HISTOGRAM {\n\t\tmetric.counts = make([]atomic.Uint64, len(config.Bounds)+1)\n\t}\n\tmetrics = append(metrics, metric)\n\treturn metric\n}\n\n// Name returns the name of the metric.\nfunc (m *Metric) Name() string {\n\treturn m.name\n}\n\n// Inc adds one to the metric value.\nfunc (m *Metric) Inc() {\n\tm.ivalue.Add(1)\n}\n\n// Add adds the provided delta to the metric's value.\nfunc (m *Metric) Add(delta float64) {\n\tm.fvalue.add(delta)\n}\n\n// Sub subtracts the provided delta from the metric's value.\nfunc (m *Metric) Sub(delta float64) {\n\tm.fvalue.add(-delta)\n}\n\n// Set sets the metric's value.\nfunc (m *Metric) Set(val float64) {\n\tm.fvalue.set(val)\n}\n\n// Put adds the provided value to the metric's histogram.\nfunc (m *Metric) Put(val float64) {\n\tvar idx int\n\tif len(m.bounds) == 0 || val < m.bounds[0] {\n\t\t// Skip binary search for values that fall in the first bucket\n\t\t// (often true for short latency operations).\n\t} else {\n\t\tidx = sort.SearchFloat64s(m.bounds, val)\n\t\tif idx < len(m.bounds) && val == m.bounds[idx] {\n\t\t\tidx++\n\t\t}\n\t}\n\tm.counts[idx].Add(1)\n\n\t// Microsecond latencies are often zero for very fast functions.\n\tif val != 0 {\n\t\tm.fvalue.add(val)\n\t}\n\tm.putCount.Add(1)\n}\n\n// initIdAndLabels initializes the id and labels of a metric.\n// We delay this initialization until the first time we export a\n// metric to avoid slowing down a Get() call.\nfunc (m *Metric) initIdAndLabels() {\n\tm.once.Do(func() {\n\t\tif labels := m.labelsThunk(); len(labels) > 0 {\n\t\t\tm.labels = labels\n\t\t}\n\t\tvar id [16]byte = uuid.New()\n\t\tm.id = binary.LittleEndian.Uint64(id[:8])\n\t})\n}\n\n// get returns the current value (sum of all added values for histograms).\nfunc (m *Metric) get() float64 {\n\treturn m.fvalue.get() + float64(m.ivalue.Load())\n}\n\n// Snapshot returns a snapshot of the metric. You must call Init at least once\n// before calling Snapshot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// MetricDef returns a MetricDef derived from the metric. You must call Init at\n// least once before calling Snapshot.\nfunc (m *Metric) MetricDef() *protos.MetricDef {\n\treturn &protos.MetricDef{\n\t\tId:     m.id,\n\t\tName:   m.name,\n\t\tTyp:    m.typ,\n\t\tHelp:   m.help,\n\t\tLabels: maps.Clone(m.labels),\n\t\tBounds: slices.Clone(m.bounds),\n\t}\n}\n\n// MetricValue returns a MetricValue derived from the metric.\nfunc (m *Metric) MetricValue() *protos.MetricValue {\n\tvar counts []uint64\n\tif n := len(m.counts); n > 0 {\n\t\tcounts = make([]uint64, n)\n\t\tfor i := range m.counts {\n\t\t\tcounts[i] = m.counts[i].Load()\n\t\t}\n\t}\n\treturn &protos.MetricValue{\n\t\tId:     m.id,\n\t\tValue:  m.get(),\n\t\tCounts: counts,\n\t}\n}\n\n// MetricMap is a collection of metrics with the same name and label schema\n// but with different label values. See public metric documentation for\n// an explanation of labels.\n//\n// TODO(mwhittaker): Understand the behavior of prometheus and Google Cloud\n// Metrics when we add or remove metric labels over time.\ntype MetricMap[L comparable] struct {\n\tconfig    config             // configures the metrics returned by Get\n\textractor *labelExtractor[L] // extracts labels from a value of type L\n\tmu        sync.Mutex         // guards metrics\n\tmetrics   map[L]*Metric      // cache of metrics, by label\n}\n\nfunc RegisterMap[L comparable](typ protos.MetricType, name string, help string, bounds []float64) *MetricMap[L] {\n\tif err := typecheckLabels[L](); err != nil {\n\t\tpanic(err)\n\t}\n\tif name == \"\" {\n\t\tpanic(fmt.Errorf(\"empty metric name\"))\n\t}\n\tif typ == protos.MetricType_INVALID {\n\t\tpanic(fmt.Errorf(\"metric %q: invalid metric type %v\", name, typ))\n\t}\n\tfor _, x := range bounds {\n\t\tif math.IsNaN(x) {\n\t\t\tpanic(fmt.Errorf(\"metric %q: NaN histogram bound\", name))\n\t\t}\n\t}\n\tfor i := 0; i < len(bounds)-1; i++ {\n\t\tif bounds[i] >= bounds[i+1] {\n\t\t\tpanic(fmt.Errorf(\"metric %q: non-ascending histogram bounds %v\", name, bounds))\n\t\t}\n\t}\n\n\tmetricNamesMu.Lock()\n\tdefer metricNamesMu.Unlock()\n\tif metricNames[name] {\n\t\tpanic(fmt.Errorf(\"metric %q already exists\", name))\n\t}\n\tmetricNames[name] = true\n\treturn &MetricMap[L]{\n\t\tconfig:    config{Type: typ, Name: name, Help: help, Bounds: bounds},\n\t\textractor: newLabelExtractor[L](),\n\t\tmetrics:   map[L]*Metric{},\n\t}\n}\n\n// Name returns the name of the metricMap.\nfunc (mm *MetricMap[L]) Name() string {\n\treturn mm.config.Name\n}\n\n// Get returns the metric with the provided labels, constructing it if it\n// doesn't already exist. Multiple calls to Get with the same labels will\n// return the same metric.\nfunc (mm *MetricMap[L]) Get(labels L) *Metric {\n\tmm.mu.Lock()\n\tdefer mm.mu.Unlock()\n\tif metric, ok := mm.metrics[labels]; ok {\n\t\treturn metric\n\t}\n\tconfig := mm.config\n\tconfig.Labels = func() map[string]string {\n\t\treturn mm.extractor.Extract(labels)\n\t}\n\tmetric := newMetric(config)\n\tmm.metrics[labels] = metric\n\treturn metric\n}\n\n// Snapshot returns a snapshot of all currently registered metrics. The\n// snapshot is not guaranteed to be atomic.\nfunc Snapshot() []*MetricSnapshot {\n\tmetricsMu.RLock()\n\tdefer metricsMu.RUnlock()\n\tsnapshots := make([]*MetricSnapshot, 0, len(metrics))\n\tfor _, metric := range metrics {\n\t\tmetric.initIdAndLabels()\n\t\tsnapshots = append(snapshots, metric.Snapshot())\n\t}\n\treturn snapshots\n}\n\n```\n# Function Signature: func (m *Metric) Snapshot() *MetricSnapshot {\n# Natural Language Annotations: // Snapshot returns a snapshot of the metric. You must call Init at least once\n// before calling Snapshot.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Snapshot returns a snapshot of the metric. You must call Init at least once\n// before calling Snapshot.\nfunc (m *Metric) Snapshot() *MetricSnapshot {\n\tm.initIdAndLabels()\n\tvar counts []uint64\n\tif n := len(m.counts); n > 0 {\n\t\tcounts = make([]uint64, n)\n\t\tfor i := range m.counts {\n\t\t\tcounts[i] = m.counts[i].Load()\n\t\t}\n\t}\n\n\treturn &MetricSnapshot{\n\t\tId:     m.id,\n\t\tType:   m.typ,\n\t\tName:   m.name,\n\t\tLabels: maps.Clone(m.labels),\n\t\tHelp:   m.help,\n\t\tValue:  m.get(),\n\t\tBounds: slices.Clone(m.bounds),\n\t\tCounts: counts,\n\t}\n}\n```"]}, "weaver-internal/tool/generate/types.go-sizeOfType": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/types.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"fmt\"\n\t\"go/types\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\nconst weaverPackagePath = \"github.com/ServiceWeaver/weaver\"\n\n// typeSet holds type information needed by the code generator.\ntype typeSet struct {\n\tpkg            *packages.Package\n\timported       []importPkg          // imported packages\n\timportedByPath map[string]importPkg // imported, indexed by path\n\timportedByName map[string]importPkg // imported, indexed by name\n\n\tautomarshals          *typeutil.Map // types that implement AutoMarshal\n\tautomarshalCandidates *typeutil.Map // types that declare themselves AutoMarshal\n\n\t// If checked[t] != nil, then checked[t] is the cached result of calling\n\t// check(pkg, t, string[]{}). Otherwise, if checked[t] == nil, then t has\n\t// not yet been checked for serializability. Read typeutil.Map's\n\t// documentation for why checked shouldn't be a map[types.Type]bool.\n\tchecked typeutil.Map\n\n\t// If sizes[t] != nil, then sizes[t] == sizeOfType(t).\n\tsizes typeutil.Map\n\n\t// If measurable[t] != nil, then measurable[t] == isMeasurableType(t).\n\tmeasurable typeutil.Map\n}\n\n// importPkg is a package imported by the generated code.\ntype importPkg struct {\n\tpath  string // e.g., \"github.com/ServiceWeaver/weaver\"\n\tpkg   string // e.g., \"weaver\", \"context\", \"time\"\n\talias string // e.g., foo in `import foo \"context\"`\n\tlocal bool   // are we in this package?\n}\n\n// name returns the name by which the imported package should be referenced in\n// the generated code. If the package is imported without an alias, like this:\n//\n//\timport \"context\"\n//\n// then the name is the same as the package name (e.g., \"context\"). However, if\n// a package is imported with an alias, then the name is the alias:\n//\n//\timport thisIsAnAlias \"context\"\n//\n// If the package is local, an empty string is returned.\nfunc (i importPkg) name() string {\n\tif i.local {\n\t\treturn \"\"\n\t} else if i.alias != \"\" {\n\t\treturn i.alias\n\t}\n\treturn i.pkg\n}\n\n// qualify returns the provided member of the package, qualified with the\n// package name. For example, the \"Context\" type inside the \"context\" package\n// is qualified \"context.Context\". The \"Now\" function inside the \"time\" package\n// is qualified \"time.Now\". Note that the package name is not prefixed when\n// qualifying members of the local package.\nfunc (i importPkg) qualify(member string) string {\n\tif i.local {\n\t\treturn member\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", i.name(), member)\n}\n\n// newTypeSet returns the container for types found in pkg.\nfunc newTypeSet(pkg *packages.Package, automarshals, automarshalCandidates *typeutil.Map) *typeSet {\n\treturn &typeSet{\n\t\tpkg:                   pkg,\n\t\timported:              []importPkg{},\n\t\timportedByPath:        map[string]importPkg{},\n\t\timportedByName:        map[string]importPkg{},\n\t\tautomarshals:          automarshals,\n\t\tautomarshalCandidates: automarshalCandidates,\n\t}\n}\n\n// importPackage imports a package with the provided path and package name. The\n// package is imported with an alias if there is a package name clash.\nfunc (tset *typeSet) importPackage(path, pkg string) importPkg {\n\tnewImportPkg := func(path, pkg, alias string, local bool) importPkg {\n\t\ti := importPkg{path: path, pkg: pkg, alias: alias, local: local}\n\t\ttset.imported = append(tset.imported, i)\n\t\ttset.importedByPath[i.path] = i\n\t\ttset.importedByName[i.name()] = i\n\t\treturn i\n\t}\n\n\tif imp, ok := tset.importedByPath[path]; ok {\n\t\t// This package has already been imported.\n\t\treturn imp\n\t}\n\n\tif _, ok := tset.importedByName[pkg]; !ok {\n\t\t// Import the package without an alias.\n\t\treturn newImportPkg(path, pkg, \"\", path == tset.pkg.PkgPath)\n\t}\n\n\t// Find an unused alias.\n\tvar alias string\n\tcounter := 1\n\tfor {\n\t\talias = fmt.Sprintf(\"%s%d\", pkg, counter)\n\t\tif _, ok := tset.importedByName[alias]; !ok {\n\t\t\tbreak\n\t\t}\n\t\tcounter++\n\t}\n\treturn newImportPkg(path, pkg, alias, path == tset.pkg.PkgPath)\n}\n\n// imports returns the list of packages to import in generated code.\nfunc (tset *typeSet) imports() []importPkg {\n\tsort.Slice(tset.imported, func(i, j int) bool {\n\t\treturn tset.imported[i].path < tset.imported[j].path\n\t})\n\treturn tset.imported\n}\n\n// checkSerializable checks that type t is serializable.\nfunc (tset *typeSet) checkSerializable(t types.Type) []error {\n\t// lineage can generate a human readable description of the lineage of a\n\t// checked type. As check recurses on type t, it encounters a number of\n\t// nested types. For example, if we have the following type A\n\t//\n\t//     type A struct{ x []chan int }\n\t//\n\t// then check(A) will encounter the types A, struct{ x []chan int }, []chan\n\t// int, chan int, and int. We associate each of these types with a\n\t// corresponding \"path\", a concise description of the relationship between\n\t// the root type and the nested types. For example, the type chan int has\n\t// path A.x[0].\n\t//\n\t// lineage is a stack that stores a history of these paths as check\n\t// traverses a type. For example, if we call check(A), then lineage will\n\t// look like this when the chan int is discovered:\n\t//\n\t//     []pathAndType{\n\t//         pathAndType{\"A\", A},\n\t//         pathAndType{\"A.x\", []chan int},\n\t//         pathAndType{\"A.x[0]\", chan int},\n\t//     }\n\t//\n\t// This lineage is printed in error messages as:\n\t//\n\t//     A (type A)\n\t//     A.x (type []chan int)\n\t//     A.x[0] (type chan int)\n\t//\n\t// Note that for brevity, not every encountered type is entered into the\n\t// lineage.\n\ttype pathAndType struct {\n\t\tpath string\n\t\tt    types.Type\n\t}\n\tvar lineage []pathAndType\n\n\tvar errors []error\n\taddError := func(err error) {\n\t\tvar builder strings.Builder\n\n\t\t// If the lineage is trivial, then don't show it.\n\t\tif len(lineage) > 1 {\n\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\tfor i, pn := range lineage {\n\t\t\t\tfmt.Fprintf(&builder, \"%v (type %v)\", pn.path, pn.t.String())\n\t\t\t\tif i < len(lineage)-1 {\n\t\t\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tqualifier := func(pkg *types.Package) string { return pkg.Name() }\n\t\terr = fmt.Errorf(\"%s: %w%s\", types.TypeString(t, qualifier), err, builder.String())\n\t\terrors = append(errors, err)\n\t}\n\n\t// stack contains the set of types encountered in the call stack of check.\n\t// It's used to detect recursive types.\n\t//\n\t// More specifically, the check function below is performing an implicit\n\t// depth first search of the graph of types formed by t. We record the\n\t// stack of visited types in stack and know we have a recursive type if we\n\t// ever run into a type that is already in stack.\n\t//\n\t// For example, consider the following types:\n\t//\n\t//   type A struct { b: *B }\n\t//   type B struct { a: *A }\n\t//\n\t// Calling check on A will yield a call stack that looks something like:\n\t//\n\t//   check(A)\n\t//     check(struct { b: *B })\n\t//       check(*B)\n\t//         check(B)\n\t//           check(struct { a: *A })\n\t//             check(*A)\n\t//               check(A)\n\t//\n\t// When performing the second check(A) call, stack includes A, struct { b:\n\t// *B }, *B, B, struct { a: *A }, and *A. Because we called check on A and\n\t// A is already in stack, we detect a recursive type and mark A as not\n\t// serializable.\n\tvar stack typeutil.Map\n\n\t// check recursively checks whether a type t is serializable. See lineage\n\t// above for a description of path. record is true if the current type\n\t// should be recorded in lineage. There are a few things worth noting:\n\t//\n\t//   (1) The results of calling check are memoized in tset.checked, but not\n\t//       for some trivial arguments. Some arguments like chan int are not\n\t//       memoized because they are trivial to check and because not\n\t//       memoizing can lead to a clearer error message.\n\t//\n\t//   (2) Consider the type t = struct { x chan int; y chan int }. t is not\n\t//       serializable because neither x nor y is serializable. check\n\t//       reports errors for both x and y as not serializable.\n\t//       Alternatively, check could find that x is not serializable and\n\t//       then immediately report that t is not serializable, skipping y\n\t//       completely. check doesn't do this. check will inspect a type fully\n\t//       to report the full set of errors.\n\t//\n\t// Note that the function also takes the parent type pt. This is needed in cases\n\t// whether we need to know the type of the parent type t (e.g., a named type\n\t// that is a proto is serializable iff the parent type is a pointer).\n\tvar check func(t types.Type, path string, record bool) bool\n\n\tcheck = func(t types.Type, path string, record bool) bool {\n\t\tif record {\n\t\t\tlineage = append(lineage, pathAndType{path, t})\n\t\t\tdefer func() { lineage = lineage[:len(lineage)-1] }()\n\t\t}\n\n\t\t// Return early if we've already checked this type.\n\t\tif result := tset.checked.At(t); result != nil {\n\t\t\tb := result.(bool)\n\t\t\tif b {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// We've already encountered type t and determined that it is not\n\t\t\t// serializable. We won't recurse down type t to compute the full\n\t\t\t// lineage and explanation of why t isn't serializable because we\n\t\t\t// already did that when determining t wasn't serializable in the\n\t\t\t// first place. Instead, we instruct the user to read the\n\t\t\t// previously reported error.\n\t\t\taddError(fmt.Errorf(\"not a serializable type; see above for details\"))\n\t\t\treturn false\n\t\t}\n\n\t\t// Check for recursive types.\n\t\tif stack.At(t) != nil {\n\t\t\taddError(fmt.Errorf(\"serialization of recursive types not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\tstack.Set(t, struct{}{})\n\t\tdefer func() { stack.Delete(t) }()\n\n\t\tswitch x := t.(type) {\n\t\tcase *types.Named:\n\t\t\t// No need to check if x is an unexported type from another package\n\t\t\t// since the Go compiler takes care of that.\n\n\t\t\t// Check if the type implements one of the marshaler interfaces.\n\t\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t\t// on the underlying type.\n\t\t\ts, ok := x.Underlying().(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\ttset.checked.Set(t, check(x.Underlying(), path, false))\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has not been declared to\n\t\t\t// implement the AutoMarshal interface, then it is not\n\t\t\t// serializable.\n\t\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t\t// weaver.AutoMarshal.\n\t\t\t\taddError(fmt.Errorf(\"named structs are not serializable by default. Consider using weaver.AutoMarshal.\"))\n\t\t\t\ttset.checked.Set(t, false)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has been declared to\n\t\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t\t// then we need to recurse to detect cycles.\n\t\t\tserializable := true\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\t// We store the result of calling check in b rather than\n\t\t\t\t// writing serializable = serializable && check(...) because we\n\t\t\t\t// don't want to short circuit and avoid calling check.\n\t\t\t\tb := check(f.Type(), path+\".\"+f.Name(), true)\n\t\t\t\tserializable = serializable && b\n\t\t\t}\n\t\t\ttset.checked.Set(t, serializable)\n\n\t\tcase *types.Interface:\n\t\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t\t// a way to instantiate the type.\n\t\t\taddError(fmt.Errorf(\"serialization of interfaces not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Struct:\n\t\t\taddError(fmt.Errorf(\"struct literals are not serializable\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Basic:\n\t\t\tswitch x.Kind() {\n\t\t\tcase types.Bool,\n\t\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\ttypes.Complex64, types.Complex128,\n\t\t\t\ttypes.String:\n\t\t\t\t// Supported.\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\tdefault:\n\t\t\t\tif isInvalid(t) {\n\t\t\t\t\taddError(fmt.Errorf(\"Maybe you forgot to run `go mod tidy`? Also try running `go build` to diagnose further.\"))\n\t\t\t\t} else {\n\t\t\t\t\taddError(fmt.Errorf(\"unsupported basic type\"))\n\t\t\t\t}\n\t\t\t\t// For a better error message, we don't memoize this.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\tcase *types.Array:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Slice:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Pointer:\n\t\t\ttset.checked.Set(t, check(x.Elem(), \"(*\"+path+\")\", true))\n\n\t\tcase *types.Map:\n\t\t\tkeySerializable := check(x.Key(), path+\".key\", true)\n\t\t\tvalSerializable := check(x.Elem(), path+\".value\", true)\n\t\t\ttset.checked.Set(t, keySerializable && valSerializable)\n\n\t\tdefault:\n\t\t\taddError(fmt.Errorf(\"not a serializable type\"))\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn false\n\t\t}\n\n\t\treturn tset.checked.At(t).(bool)\n\t}\n\n\tcheck(t, t.String(), true)\n\treturn errors\n}\n\n// isFixedSizeType returns whether the provided type has a fixed serialization\n// size. Here is a summary of which types are fixed sized:\n//\n//   - Every basic type (e.g., bool, int) except string is fixed sized.\n//   - The array type [N]t is fixed sized if t is fixed sized.\n//   - A struct is fixed sized if the types of its fields are fixed sized.\n//   - A named type is fixed sized if its underlying type is fixed sized.\nfunc (tset *typeSet) isFixedSizeType(t types.Type) bool {\n\treturn tset.sizeOfType(t) >= 0\n}\n\n// sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// genTypeString returns the string representation of t as to be printed\n// in the generated code, updating import definitions to account for the\n// returned type string.\n//\n// Since this call has side-effects (i.e., updating import definitions), it\n// should only be called when the returned type string is written into\n// the generated file; otherwise, the generated code may end up with spurious\n// imports.\nfunc (tset *typeSet) genTypeString(t types.Type) string {\n\t// qualifier is passed to types.TypeString(Type, Qualifier) to determine\n\t// how packages are printed when pretty printing types. For this qualifier,\n\t// types in the root package are printed without their package name, while\n\t// types outside the root package are printed with their package name. For\n\t// example, if we're in root package foo, then the type foo.Bar is printed\n\t// as Bar, while the type io.Reader is printed as io.Reader. See [1] for\n\t// more information on qualifiers and pretty printing types.\n\t//\n\t// [1]: https://github.com/golang/example/tree/master/gotypes#formatting-support\n\tvar qualifier = func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\treturn types.TypeString(t, qualifier)\n}\n\n// isInvalid returns true iff the given type is invalid.\nfunc isInvalid(t types.Type) bool {\n\treturn t.String() == \"invalid type\"\n}\n\n// implementsError returns whether the provided type is a concrete type that\n// implements error.\nfunc (tset *typeSet) implementsError(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\treturn false\n\t}\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"Error\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\tif args := sig.Params(); args.Len() != 0 {\n\t\treturn false\n\t}\n\tif results := sig.Results(); results.Len() != 1 || !isString(results.At(0).Type()) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isProto returns whether the provided type is a concrete type that implements\n// the proto.Message interface.\nfunc (tset *typeSet) isProto(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of proto.Message does \"implement\" proto.Message,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"ProtoReflect\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\trecv, args, results := sig.Recv(), sig.Params(), sig.Results()\n\tif args.Len() != 0 || results.Len() != 1 {\n\t\treturn false\n\t}\n\tif !isProtoMessage(results.At(0).Type()) {\n\t\treturn false\n\t}\n\t// Check the receiver. We avoid complicated embeddings by requiring that\n\t// the method is defined on the type itself.\n\t//\n\t// TODO(mwhittaker): Relax this requirement if it becomes annoying.\n\tif p, ok := recv.Type().(*types.Pointer); ok {\n\t\treturn types.Identical(p.Elem(), t)\n\t} else {\n\t\treturn types.Identical(recv.Type(), t)\n\t}\n}\n\nfunc isProtoMessage(t types.Type) bool {\n\tn, ok := t.(*types.Named)\n\tif !ok {\n\t\treturn false\n\t}\n\tconst protoreflect = \"google.golang.org/protobuf/reflect/protoreflect\"\n\treturn n.Obj().Pkg().Path() == protoreflect && n.Obj().Name() == \"Message\"\n}\n\n// implementsAutoMarshal returns whether the provided type is a concrete\n// type that implements the weaver.AutoMarshal interface.\nfunc (tset *typeSet) implementsAutoMarshal(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of AutoMarshal does \"implement\" the interface,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n```\n# Function Signature: func (tset *typeSet) sizeOfType(t types.Type) int {\n# Natural Language Annotations: // sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\nPlease return the generated code surrounded by ```\n", "response": ["To implement the `sizeOfType` function, we need to determine the serialization size of a given Go type. If the type has a fixed size, the function should calculate and return its size; otherwise, the function should return -1. This involves examining various cases like basic types, arrays, slices, maps, pointers, etc. Here's the implementation:\n\n```go\nfunc (tset *typeSet) sizeOfType(t types.Type) int {\n\tswitch x := t.(type) {\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Bool:\n\t\t\treturn 1\n\t\tcase types.Int, types.Uint, types.Uintptr:\n\t\t\treturn 4 // architecture-dependent, assuming 32-bit for simplicity\n\t\tcase types.Int8, types.Uint8:\n\t\t\treturn 1\n\t\tcase types.Int16, types.Uint16:\n\t\t\treturn 2\n\t\tcase types.Int32, types.Uint32:\n\t\t\treturn 4\n\t\tcase types.Int64, types.Uint64:\n\t\t\treturn 8\n\t\tcase types.Float32:\n\t\t\treturn 4\n\t\tcase types.Float64:\n\t\t\treturn 8\n\t\tcase types.Complex64:\n\t\t\treturn 8 // 4 bytes each for real and imaginary parts\n\t\tcase types.Complex128:\n\t\t\treturn 16 // 8 bytes each for real and imaginary parts\n\t\tcase types.String:\n\t\t\treturn -1 // variable length\n\t\tdefault:\n\t\t\treturn -1\n\t\t}\n\n\tcase *types.Array:\n\t\telemSize := tset.sizeOfType(x.Elem())\n\t\tif elemSize < 0 {\n\t\t\treturn -1\n\t\t}\n\t\treturn int(x.Len()) * elemSize\n\n\tcase *types.Slice:\n\t\treturn -1 // variable length\n\n\tcase *types.Struct:\n\t\ttotalSize := 0\n\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\tfieldSize := tset.sizeOfType(x.Field(i).Type())\n\t\t\tif fieldSize < 0 {\n\t\t\t\treturn -1\n\t\t\t}\n\t\t\ttotalSize += fieldSize\n\t\t}\n\t\treturn totalSize\n\n\tcase *types.Pointer:\n\t\treturn 8 // assuming 64-bit architecture for pointer size\n\n\tcase *types.Map, *types.Interface, *types.Chan:\n\t\treturn -1 // these types do not have a fixed size\n\n\tcase *types.Named:\n\t\treturn tset.sizeOfType(x.Underlying())\n\n\tdefault:\n\t\treturn -1\n\t}\n}\n```\n\n### Explanation:\n\n1. **Basic Types:** We handle fundamental types like int, float, etc., returning their sizes in bytes. Sizes like `int` are assumed based on a 32-bit architecture as a general simplification.\n\n2. **Array:** The size is determined by the length of the array multiplied by the size of its element type. If the element type is not of fixed size, the whole array is treated as not fixed size.\n\n3. **Struct:** The total size is the sum of its fields' sizes. If any field is not of fixed size, then the struct is not of fixed size.\n\n4. **Pointers:** Given an assumption of a 64-bit architecture for simplicity, we return a fixed size of 8 bytes.\n\n5. **Other Composite Types:** Types like slices, maps, channels, and interfaces inherently do not have a fixed serialization size due to their dynamic nature.\n\n6. **Named Types:** We resolve the underlying type and determine its size, ensuring that type aliases and custom named types are handled appropriately."]}, "weaver-internal/tool/generate/types.go-isMeasurable": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/types.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"fmt\"\n\t\"go/types\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\nconst weaverPackagePath = \"github.com/ServiceWeaver/weaver\"\n\n// typeSet holds type information needed by the code generator.\ntype typeSet struct {\n\tpkg            *packages.Package\n\timported       []importPkg          // imported packages\n\timportedByPath map[string]importPkg // imported, indexed by path\n\timportedByName map[string]importPkg // imported, indexed by name\n\n\tautomarshals          *typeutil.Map // types that implement AutoMarshal\n\tautomarshalCandidates *typeutil.Map // types that declare themselves AutoMarshal\n\n\t// If checked[t] != nil, then checked[t] is the cached result of calling\n\t// check(pkg, t, string[]{}). Otherwise, if checked[t] == nil, then t has\n\t// not yet been checked for serializability. Read typeutil.Map's\n\t// documentation for why checked shouldn't be a map[types.Type]bool.\n\tchecked typeutil.Map\n\n\t// If sizes[t] != nil, then sizes[t] == sizeOfType(t).\n\tsizes typeutil.Map\n\n\t// If measurable[t] != nil, then measurable[t] == isMeasurableType(t).\n\tmeasurable typeutil.Map\n}\n\n// importPkg is a package imported by the generated code.\ntype importPkg struct {\n\tpath  string // e.g., \"github.com/ServiceWeaver/weaver\"\n\tpkg   string // e.g., \"weaver\", \"context\", \"time\"\n\talias string // e.g., foo in `import foo \"context\"`\n\tlocal bool   // are we in this package?\n}\n\n// name returns the name by which the imported package should be referenced in\n// the generated code. If the package is imported without an alias, like this:\n//\n//\timport \"context\"\n//\n// then the name is the same as the package name (e.g., \"context\"). However, if\n// a package is imported with an alias, then the name is the alias:\n//\n//\timport thisIsAnAlias \"context\"\n//\n// If the package is local, an empty string is returned.\nfunc (i importPkg) name() string {\n\tif i.local {\n\t\treturn \"\"\n\t} else if i.alias != \"\" {\n\t\treturn i.alias\n\t}\n\treturn i.pkg\n}\n\n// qualify returns the provided member of the package, qualified with the\n// package name. For example, the \"Context\" type inside the \"context\" package\n// is qualified \"context.Context\". The \"Now\" function inside the \"time\" package\n// is qualified \"time.Now\". Note that the package name is not prefixed when\n// qualifying members of the local package.\nfunc (i importPkg) qualify(member string) string {\n\tif i.local {\n\t\treturn member\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", i.name(), member)\n}\n\n// newTypeSet returns the container for types found in pkg.\nfunc newTypeSet(pkg *packages.Package, automarshals, automarshalCandidates *typeutil.Map) *typeSet {\n\treturn &typeSet{\n\t\tpkg:                   pkg,\n\t\timported:              []importPkg{},\n\t\timportedByPath:        map[string]importPkg{},\n\t\timportedByName:        map[string]importPkg{},\n\t\tautomarshals:          automarshals,\n\t\tautomarshalCandidates: automarshalCandidates,\n\t}\n}\n\n// importPackage imports a package with the provided path and package name. The\n// package is imported with an alias if there is a package name clash.\nfunc (tset *typeSet) importPackage(path, pkg string) importPkg {\n\tnewImportPkg := func(path, pkg, alias string, local bool) importPkg {\n\t\ti := importPkg{path: path, pkg: pkg, alias: alias, local: local}\n\t\ttset.imported = append(tset.imported, i)\n\t\ttset.importedByPath[i.path] = i\n\t\ttset.importedByName[i.name()] = i\n\t\treturn i\n\t}\n\n\tif imp, ok := tset.importedByPath[path]; ok {\n\t\t// This package has already been imported.\n\t\treturn imp\n\t}\n\n\tif _, ok := tset.importedByName[pkg]; !ok {\n\t\t// Import the package without an alias.\n\t\treturn newImportPkg(path, pkg, \"\", path == tset.pkg.PkgPath)\n\t}\n\n\t// Find an unused alias.\n\tvar alias string\n\tcounter := 1\n\tfor {\n\t\talias = fmt.Sprintf(\"%s%d\", pkg, counter)\n\t\tif _, ok := tset.importedByName[alias]; !ok {\n\t\t\tbreak\n\t\t}\n\t\tcounter++\n\t}\n\treturn newImportPkg(path, pkg, alias, path == tset.pkg.PkgPath)\n}\n\n// imports returns the list of packages to import in generated code.\nfunc (tset *typeSet) imports() []importPkg {\n\tsort.Slice(tset.imported, func(i, j int) bool {\n\t\treturn tset.imported[i].path < tset.imported[j].path\n\t})\n\treturn tset.imported\n}\n\n// checkSerializable checks that type t is serializable.\nfunc (tset *typeSet) checkSerializable(t types.Type) []error {\n\t// lineage can generate a human readable description of the lineage of a\n\t// checked type. As check recurses on type t, it encounters a number of\n\t// nested types. For example, if we have the following type A\n\t//\n\t//     type A struct{ x []chan int }\n\t//\n\t// then check(A) will encounter the types A, struct{ x []chan int }, []chan\n\t// int, chan int, and int. We associate each of these types with a\n\t// corresponding \"path\", a concise description of the relationship between\n\t// the root type and the nested types. For example, the type chan int has\n\t// path A.x[0].\n\t//\n\t// lineage is a stack that stores a history of these paths as check\n\t// traverses a type. For example, if we call check(A), then lineage will\n\t// look like this when the chan int is discovered:\n\t//\n\t//     []pathAndType{\n\t//         pathAndType{\"A\", A},\n\t//         pathAndType{\"A.x\", []chan int},\n\t//         pathAndType{\"A.x[0]\", chan int},\n\t//     }\n\t//\n\t// This lineage is printed in error messages as:\n\t//\n\t//     A (type A)\n\t//     A.x (type []chan int)\n\t//     A.x[0] (type chan int)\n\t//\n\t// Note that for brevity, not every encountered type is entered into the\n\t// lineage.\n\ttype pathAndType struct {\n\t\tpath string\n\t\tt    types.Type\n\t}\n\tvar lineage []pathAndType\n\n\tvar errors []error\n\taddError := func(err error) {\n\t\tvar builder strings.Builder\n\n\t\t// If the lineage is trivial, then don't show it.\n\t\tif len(lineage) > 1 {\n\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\tfor i, pn := range lineage {\n\t\t\t\tfmt.Fprintf(&builder, \"%v (type %v)\", pn.path, pn.t.String())\n\t\t\t\tif i < len(lineage)-1 {\n\t\t\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tqualifier := func(pkg *types.Package) string { return pkg.Name() }\n\t\terr = fmt.Errorf(\"%s: %w%s\", types.TypeString(t, qualifier), err, builder.String())\n\t\terrors = append(errors, err)\n\t}\n\n\t// stack contains the set of types encountered in the call stack of check.\n\t// It's used to detect recursive types.\n\t//\n\t// More specifically, the check function below is performing an implicit\n\t// depth first search of the graph of types formed by t. We record the\n\t// stack of visited types in stack and know we have a recursive type if we\n\t// ever run into a type that is already in stack.\n\t//\n\t// For example, consider the following types:\n\t//\n\t//   type A struct { b: *B }\n\t//   type B struct { a: *A }\n\t//\n\t// Calling check on A will yield a call stack that looks something like:\n\t//\n\t//   check(A)\n\t//     check(struct { b: *B })\n\t//       check(*B)\n\t//         check(B)\n\t//           check(struct { a: *A })\n\t//             check(*A)\n\t//               check(A)\n\t//\n\t// When performing the second check(A) call, stack includes A, struct { b:\n\t// *B }, *B, B, struct { a: *A }, and *A. Because we called check on A and\n\t// A is already in stack, we detect a recursive type and mark A as not\n\t// serializable.\n\tvar stack typeutil.Map\n\n\t// check recursively checks whether a type t is serializable. See lineage\n\t// above for a description of path. record is true if the current type\n\t// should be recorded in lineage. There are a few things worth noting:\n\t//\n\t//   (1) The results of calling check are memoized in tset.checked, but not\n\t//       for some trivial arguments. Some arguments like chan int are not\n\t//       memoized because they are trivial to check and because not\n\t//       memoizing can lead to a clearer error message.\n\t//\n\t//   (2) Consider the type t = struct { x chan int; y chan int }. t is not\n\t//       serializable because neither x nor y is serializable. check\n\t//       reports errors for both x and y as not serializable.\n\t//       Alternatively, check could find that x is not serializable and\n\t//       then immediately report that t is not serializable, skipping y\n\t//       completely. check doesn't do this. check will inspect a type fully\n\t//       to report the full set of errors.\n\t//\n\t// Note that the function also takes the parent type pt. This is needed in cases\n\t// whether we need to know the type of the parent type t (e.g., a named type\n\t// that is a proto is serializable iff the parent type is a pointer).\n\tvar check func(t types.Type, path string, record bool) bool\n\n\tcheck = func(t types.Type, path string, record bool) bool {\n\t\tif record {\n\t\t\tlineage = append(lineage, pathAndType{path, t})\n\t\t\tdefer func() { lineage = lineage[:len(lineage)-1] }()\n\t\t}\n\n\t\t// Return early if we've already checked this type.\n\t\tif result := tset.checked.At(t); result != nil {\n\t\t\tb := result.(bool)\n\t\t\tif b {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// We've already encountered type t and determined that it is not\n\t\t\t// serializable. We won't recurse down type t to compute the full\n\t\t\t// lineage and explanation of why t isn't serializable because we\n\t\t\t// already did that when determining t wasn't serializable in the\n\t\t\t// first place. Instead, we instruct the user to read the\n\t\t\t// previously reported error.\n\t\t\taddError(fmt.Errorf(\"not a serializable type; see above for details\"))\n\t\t\treturn false\n\t\t}\n\n\t\t// Check for recursive types.\n\t\tif stack.At(t) != nil {\n\t\t\taddError(fmt.Errorf(\"serialization of recursive types not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\tstack.Set(t, struct{}{})\n\t\tdefer func() { stack.Delete(t) }()\n\n\t\tswitch x := t.(type) {\n\t\tcase *types.Named:\n\t\t\t// No need to check if x is an unexported type from another package\n\t\t\t// since the Go compiler takes care of that.\n\n\t\t\t// Check if the type implements one of the marshaler interfaces.\n\t\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t\t// on the underlying type.\n\t\t\ts, ok := x.Underlying().(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\ttset.checked.Set(t, check(x.Underlying(), path, false))\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has not been declared to\n\t\t\t// implement the AutoMarshal interface, then it is not\n\t\t\t// serializable.\n\t\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t\t// weaver.AutoMarshal.\n\t\t\t\taddError(fmt.Errorf(\"named structs are not serializable by default. Consider using weaver.AutoMarshal.\"))\n\t\t\t\ttset.checked.Set(t, false)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has been declared to\n\t\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t\t// then we need to recurse to detect cycles.\n\t\t\tserializable := true\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\t// We store the result of calling check in b rather than\n\t\t\t\t// writing serializable = serializable && check(...) because we\n\t\t\t\t// don't want to short circuit and avoid calling check.\n\t\t\t\tb := check(f.Type(), path+\".\"+f.Name(), true)\n\t\t\t\tserializable = serializable && b\n\t\t\t}\n\t\t\ttset.checked.Set(t, serializable)\n\n\t\tcase *types.Interface:\n\t\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t\t// a way to instantiate the type.\n\t\t\taddError(fmt.Errorf(\"serialization of interfaces not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Struct:\n\t\t\taddError(fmt.Errorf(\"struct literals are not serializable\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Basic:\n\t\t\tswitch x.Kind() {\n\t\t\tcase types.Bool,\n\t\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\ttypes.Complex64, types.Complex128,\n\t\t\t\ttypes.String:\n\t\t\t\t// Supported.\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\tdefault:\n\t\t\t\tif isInvalid(t) {\n\t\t\t\t\taddError(fmt.Errorf(\"Maybe you forgot to run `go mod tidy`? Also try running `go build` to diagnose further.\"))\n\t\t\t\t} else {\n\t\t\t\t\taddError(fmt.Errorf(\"unsupported basic type\"))\n\t\t\t\t}\n\t\t\t\t// For a better error message, we don't memoize this.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\tcase *types.Array:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Slice:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Pointer:\n\t\t\ttset.checked.Set(t, check(x.Elem(), \"(*\"+path+\")\", true))\n\n\t\tcase *types.Map:\n\t\t\tkeySerializable := check(x.Key(), path+\".key\", true)\n\t\t\tvalSerializable := check(x.Elem(), path+\".value\", true)\n\t\t\ttset.checked.Set(t, keySerializable && valSerializable)\n\n\t\tdefault:\n\t\t\taddError(fmt.Errorf(\"not a serializable type\"))\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn false\n\t\t}\n\n\t\treturn tset.checked.At(t).(bool)\n\t}\n\n\tcheck(t, t.String(), true)\n\treturn errors\n}\n\n// isFixedSizeType returns whether the provided type has a fixed serialization\n// size. Here is a summary of which types are fixed sized:\n//\n//   - Every basic type (e.g., bool, int) except string is fixed sized.\n//   - The array type [N]t is fixed sized if t is fixed sized.\n//   - A struct is fixed sized if the types of its fields are fixed sized.\n//   - A named type is fixed sized if its underlying type is fixed sized.\nfunc (tset *typeSet) isFixedSizeType(t types.Type) bool {\n\treturn tset.sizeOfType(t) >= 0\n}\n\n// sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// genTypeString returns the string representation of t as to be printed\n// in the generated code, updating import definitions to account for the\n// returned type string.\n//\n// Since this call has side-effects (i.e., updating import definitions), it\n// should only be called when the returned type string is written into\n// the generated file; otherwise, the generated code may end up with spurious\n// imports.\nfunc (tset *typeSet) genTypeString(t types.Type) string {\n\t// qualifier is passed to types.TypeString(Type, Qualifier) to determine\n\t// how packages are printed when pretty printing types. For this qualifier,\n\t// types in the root package are printed without their package name, while\n\t// types outside the root package are printed with their package name. For\n\t// example, if we're in root package foo, then the type foo.Bar is printed\n\t// as Bar, while the type io.Reader is printed as io.Reader. See [1] for\n\t// more information on qualifiers and pretty printing types.\n\t//\n\t// [1]: https://github.com/golang/example/tree/master/gotypes#formatting-support\n\tvar qualifier = func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\treturn types.TypeString(t, qualifier)\n}\n\n// isInvalid returns true iff the given type is invalid.\nfunc isInvalid(t types.Type) bool {\n\treturn t.String() == \"invalid type\"\n}\n\n// implementsError returns whether the provided type is a concrete type that\n// implements error.\nfunc (tset *typeSet) implementsError(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\treturn false\n\t}\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"Error\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\tif args := sig.Params(); args.Len() != 0 {\n\t\treturn false\n\t}\n\tif results := sig.Results(); results.Len() != 1 || !isString(results.At(0).Type()) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isProto returns whether the provided type is a concrete type that implements\n// the proto.Message interface.\nfunc (tset *typeSet) isProto(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of proto.Message does \"implement\" proto.Message,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"ProtoReflect\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\trecv, args, results := sig.Recv(), sig.Params(), sig.Results()\n\tif args.Len() != 0 || results.Len() != 1 {\n\t\treturn false\n\t}\n\tif !isProtoMessage(results.At(0).Type()) {\n\t\treturn false\n\t}\n\t// Check the receiver. We avoid complicated embeddings by requiring that\n\t// the method is defined on the type itself.\n\t//\n\t// TODO(mwhittaker): Relax this requirement if it becomes annoying.\n\tif p, ok := recv.Type().(*types.Pointer); ok {\n\t\treturn types.Identical(p.Elem(), t)\n\t} else {\n\t\treturn types.Identical(recv.Type(), t)\n\t}\n}\n\nfunc isProtoMessage(t types.Type) bool {\n\tn, ok := t.(*types.Named)\n\tif !ok {\n\t\treturn false\n\t}\n\tconst protoreflect = \"google.golang.org/protobuf/reflect/protoreflect\"\n\treturn n.Obj().Pkg().Path() == protoreflect && n.Obj().Name() == \"Message\"\n}\n\n// implementsAutoMarshal returns whether the provided type is a concrete\n// type that implements the weaver.AutoMarshal interface.\nfunc (tset *typeSet) implementsAutoMarshal(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of AutoMarshal does \"implement\" the interface,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n```\n# Function Signature: func (tset *typeSet) isMeasurable(t types.Type) bool {\n# Natural Language Annotations: // isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (tset *typeSet) isMeasurable(t types.Type) bool {\n\t// If the result is already cached, return it.\n\tif result := tset.measurable.At(t); result != nil {\n\t\treturn result.(bool)\n\t}\n\n\t// Helper function to qualify packages\n\tqualifier := func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\n\t// Determines if a type is local to the package or implements AutoMarshal\n\tisLocalOrAutomarshal := func(t types.Type) bool {\n\t\tnamed, ok := t.(*types.Named)\n\t\tif !ok {\n\t\t\treturn false\n\t\t}\n\t\treturn named.Obj().Pkg() == tset.pkg.Types || tset.implementsAutoMarshal(t)\n\t}\n\n\tcheck := func(t types.Type) bool {\n\t\tswitch x := t.(type) {\n\t\tcase *types.Basic:\n\t\t\t// Basic types except string are fixed size, hence measurable\n\t\t\treturn x.Kind() != types.String\n\n\t\tcase *types.Array:\n\t\t\t// Array is measurable if element type is measurable\n\t\t\treturn isLocalOrAutomarshal(x.Elem()) && tset.isMeasurable(x.Elem())\n\n\t\tcase *types.Slice:\n\t\t\t// Slice is not measurable\n\t\t\treturn false\n\n\t\tcase *types.Struct:\n\t\t\t// Struct is measurable if all its fields are measurable and it's package local\n\t\t\tif !isLocalOrAutomarshal(t) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\t\tif !tset.isMeasurable(x.Field(i).Type()) {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true\n\n\t\tcase *types.Pointer:\n\t\t\t// Pointer is measurable if it points to a measurable local type\n\t\t\treturn isLocalOrAutomarshal(x.Elem()) && tset.isMeasurable(x.Elem())\n\n\t\tcase *types.Named:\n\t\t\t// Named type is measurable if its underlying type is measurable\n\t\t\treturn check(x.Underlying())\n\n\t\tcase *types.Interface:\n\t\t\t// Interfaces are not measurable\n\t\t\treturn false\n\n\t\tdefault:\n\t\t\t// Other types are not measurable\n\t\t\treturn false\n\t\t}\n\t}\n\n\t// Determine if the type is measurable and cache the result\n\tisMeas := check(t)\n\ttset.measurable.Set(t, isMeas)\n\treturn isMeas\n}\n```"]}, "weaver-internal/tool/generate/types.go-isValidRouterType": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/types.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"fmt\"\n\t\"go/types\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\nconst weaverPackagePath = \"github.com/ServiceWeaver/weaver\"\n\n// typeSet holds type information needed by the code generator.\ntype typeSet struct {\n\tpkg            *packages.Package\n\timported       []importPkg          // imported packages\n\timportedByPath map[string]importPkg // imported, indexed by path\n\timportedByName map[string]importPkg // imported, indexed by name\n\n\tautomarshals          *typeutil.Map // types that implement AutoMarshal\n\tautomarshalCandidates *typeutil.Map // types that declare themselves AutoMarshal\n\n\t// If checked[t] != nil, then checked[t] is the cached result of calling\n\t// check(pkg, t, string[]{}). Otherwise, if checked[t] == nil, then t has\n\t// not yet been checked for serializability. Read typeutil.Map's\n\t// documentation for why checked shouldn't be a map[types.Type]bool.\n\tchecked typeutil.Map\n\n\t// If sizes[t] != nil, then sizes[t] == sizeOfType(t).\n\tsizes typeutil.Map\n\n\t// If measurable[t] != nil, then measurable[t] == isMeasurableType(t).\n\tmeasurable typeutil.Map\n}\n\n// importPkg is a package imported by the generated code.\ntype importPkg struct {\n\tpath  string // e.g., \"github.com/ServiceWeaver/weaver\"\n\tpkg   string // e.g., \"weaver\", \"context\", \"time\"\n\talias string // e.g., foo in `import foo \"context\"`\n\tlocal bool   // are we in this package?\n}\n\n// name returns the name by which the imported package should be referenced in\n// the generated code. If the package is imported without an alias, like this:\n//\n//\timport \"context\"\n//\n// then the name is the same as the package name (e.g., \"context\"). However, if\n// a package is imported with an alias, then the name is the alias:\n//\n//\timport thisIsAnAlias \"context\"\n//\n// If the package is local, an empty string is returned.\nfunc (i importPkg) name() string {\n\tif i.local {\n\t\treturn \"\"\n\t} else if i.alias != \"\" {\n\t\treturn i.alias\n\t}\n\treturn i.pkg\n}\n\n// qualify returns the provided member of the package, qualified with the\n// package name. For example, the \"Context\" type inside the \"context\" package\n// is qualified \"context.Context\". The \"Now\" function inside the \"time\" package\n// is qualified \"time.Now\". Note that the package name is not prefixed when\n// qualifying members of the local package.\nfunc (i importPkg) qualify(member string) string {\n\tif i.local {\n\t\treturn member\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", i.name(), member)\n}\n\n// newTypeSet returns the container for types found in pkg.\nfunc newTypeSet(pkg *packages.Package, automarshals, automarshalCandidates *typeutil.Map) *typeSet {\n\treturn &typeSet{\n\t\tpkg:                   pkg,\n\t\timported:              []importPkg{},\n\t\timportedByPath:        map[string]importPkg{},\n\t\timportedByName:        map[string]importPkg{},\n\t\tautomarshals:          automarshals,\n\t\tautomarshalCandidates: automarshalCandidates,\n\t}\n}\n\n// importPackage imports a package with the provided path and package name. The\n// package is imported with an alias if there is a package name clash.\nfunc (tset *typeSet) importPackage(path, pkg string) importPkg {\n\tnewImportPkg := func(path, pkg, alias string, local bool) importPkg {\n\t\ti := importPkg{path: path, pkg: pkg, alias: alias, local: local}\n\t\ttset.imported = append(tset.imported, i)\n\t\ttset.importedByPath[i.path] = i\n\t\ttset.importedByName[i.name()] = i\n\t\treturn i\n\t}\n\n\tif imp, ok := tset.importedByPath[path]; ok {\n\t\t// This package has already been imported.\n\t\treturn imp\n\t}\n\n\tif _, ok := tset.importedByName[pkg]; !ok {\n\t\t// Import the package without an alias.\n\t\treturn newImportPkg(path, pkg, \"\", path == tset.pkg.PkgPath)\n\t}\n\n\t// Find an unused alias.\n\tvar alias string\n\tcounter := 1\n\tfor {\n\t\talias = fmt.Sprintf(\"%s%d\", pkg, counter)\n\t\tif _, ok := tset.importedByName[alias]; !ok {\n\t\t\tbreak\n\t\t}\n\t\tcounter++\n\t}\n\treturn newImportPkg(path, pkg, alias, path == tset.pkg.PkgPath)\n}\n\n// imports returns the list of packages to import in generated code.\nfunc (tset *typeSet) imports() []importPkg {\n\tsort.Slice(tset.imported, func(i, j int) bool {\n\t\treturn tset.imported[i].path < tset.imported[j].path\n\t})\n\treturn tset.imported\n}\n\n// checkSerializable checks that type t is serializable.\nfunc (tset *typeSet) checkSerializable(t types.Type) []error {\n\t// lineage can generate a human readable description of the lineage of a\n\t// checked type. As check recurses on type t, it encounters a number of\n\t// nested types. For example, if we have the following type A\n\t//\n\t//     type A struct{ x []chan int }\n\t//\n\t// then check(A) will encounter the types A, struct{ x []chan int }, []chan\n\t// int, chan int, and int. We associate each of these types with a\n\t// corresponding \"path\", a concise description of the relationship between\n\t// the root type and the nested types. For example, the type chan int has\n\t// path A.x[0].\n\t//\n\t// lineage is a stack that stores a history of these paths as check\n\t// traverses a type. For example, if we call check(A), then lineage will\n\t// look like this when the chan int is discovered:\n\t//\n\t//     []pathAndType{\n\t//         pathAndType{\"A\", A},\n\t//         pathAndType{\"A.x\", []chan int},\n\t//         pathAndType{\"A.x[0]\", chan int},\n\t//     }\n\t//\n\t// This lineage is printed in error messages as:\n\t//\n\t//     A (type A)\n\t//     A.x (type []chan int)\n\t//     A.x[0] (type chan int)\n\t//\n\t// Note that for brevity, not every encountered type is entered into the\n\t// lineage.\n\ttype pathAndType struct {\n\t\tpath string\n\t\tt    types.Type\n\t}\n\tvar lineage []pathAndType\n\n\tvar errors []error\n\taddError := func(err error) {\n\t\tvar builder strings.Builder\n\n\t\t// If the lineage is trivial, then don't show it.\n\t\tif len(lineage) > 1 {\n\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\tfor i, pn := range lineage {\n\t\t\t\tfmt.Fprintf(&builder, \"%v (type %v)\", pn.path, pn.t.String())\n\t\t\t\tif i < len(lineage)-1 {\n\t\t\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tqualifier := func(pkg *types.Package) string { return pkg.Name() }\n\t\terr = fmt.Errorf(\"%s: %w%s\", types.TypeString(t, qualifier), err, builder.String())\n\t\terrors = append(errors, err)\n\t}\n\n\t// stack contains the set of types encountered in the call stack of check.\n\t// It's used to detect recursive types.\n\t//\n\t// More specifically, the check function below is performing an implicit\n\t// depth first search of the graph of types formed by t. We record the\n\t// stack of visited types in stack and know we have a recursive type if we\n\t// ever run into a type that is already in stack.\n\t//\n\t// For example, consider the following types:\n\t//\n\t//   type A struct { b: *B }\n\t//   type B struct { a: *A }\n\t//\n\t// Calling check on A will yield a call stack that looks something like:\n\t//\n\t//   check(A)\n\t//     check(struct { b: *B })\n\t//       check(*B)\n\t//         check(B)\n\t//           check(struct { a: *A })\n\t//             check(*A)\n\t//               check(A)\n\t//\n\t// When performing the second check(A) call, stack includes A, struct { b:\n\t// *B }, *B, B, struct { a: *A }, and *A. Because we called check on A and\n\t// A is already in stack, we detect a recursive type and mark A as not\n\t// serializable.\n\tvar stack typeutil.Map\n\n\t// check recursively checks whether a type t is serializable. See lineage\n\t// above for a description of path. record is true if the current type\n\t// should be recorded in lineage. There are a few things worth noting:\n\t//\n\t//   (1) The results of calling check are memoized in tset.checked, but not\n\t//       for some trivial arguments. Some arguments like chan int are not\n\t//       memoized because they are trivial to check and because not\n\t//       memoizing can lead to a clearer error message.\n\t//\n\t//   (2) Consider the type t = struct { x chan int; y chan int }. t is not\n\t//       serializable because neither x nor y is serializable. check\n\t//       reports errors for both x and y as not serializable.\n\t//       Alternatively, check could find that x is not serializable and\n\t//       then immediately report that t is not serializable, skipping y\n\t//       completely. check doesn't do this. check will inspect a type fully\n\t//       to report the full set of errors.\n\t//\n\t// Note that the function also takes the parent type pt. This is needed in cases\n\t// whether we need to know the type of the parent type t (e.g., a named type\n\t// that is a proto is serializable iff the parent type is a pointer).\n\tvar check func(t types.Type, path string, record bool) bool\n\n\tcheck = func(t types.Type, path string, record bool) bool {\n\t\tif record {\n\t\t\tlineage = append(lineage, pathAndType{path, t})\n\t\t\tdefer func() { lineage = lineage[:len(lineage)-1] }()\n\t\t}\n\n\t\t// Return early if we've already checked this type.\n\t\tif result := tset.checked.At(t); result != nil {\n\t\t\tb := result.(bool)\n\t\t\tif b {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// We've already encountered type t and determined that it is not\n\t\t\t// serializable. We won't recurse down type t to compute the full\n\t\t\t// lineage and explanation of why t isn't serializable because we\n\t\t\t// already did that when determining t wasn't serializable in the\n\t\t\t// first place. Instead, we instruct the user to read the\n\t\t\t// previously reported error.\n\t\t\taddError(fmt.Errorf(\"not a serializable type; see above for details\"))\n\t\t\treturn false\n\t\t}\n\n\t\t// Check for recursive types.\n\t\tif stack.At(t) != nil {\n\t\t\taddError(fmt.Errorf(\"serialization of recursive types not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\tstack.Set(t, struct{}{})\n\t\tdefer func() { stack.Delete(t) }()\n\n\t\tswitch x := t.(type) {\n\t\tcase *types.Named:\n\t\t\t// No need to check if x is an unexported type from another package\n\t\t\t// since the Go compiler takes care of that.\n\n\t\t\t// Check if the type implements one of the marshaler interfaces.\n\t\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t\t// on the underlying type.\n\t\t\ts, ok := x.Underlying().(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\ttset.checked.Set(t, check(x.Underlying(), path, false))\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has not been declared to\n\t\t\t// implement the AutoMarshal interface, then it is not\n\t\t\t// serializable.\n\t\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t\t// weaver.AutoMarshal.\n\t\t\t\taddError(fmt.Errorf(\"named structs are not serializable by default. Consider using weaver.AutoMarshal.\"))\n\t\t\t\ttset.checked.Set(t, false)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has been declared to\n\t\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t\t// then we need to recurse to detect cycles.\n\t\t\tserializable := true\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\t// We store the result of calling check in b rather than\n\t\t\t\t// writing serializable = serializable && check(...) because we\n\t\t\t\t// don't want to short circuit and avoid calling check.\n\t\t\t\tb := check(f.Type(), path+\".\"+f.Name(), true)\n\t\t\t\tserializable = serializable && b\n\t\t\t}\n\t\t\ttset.checked.Set(t, serializable)\n\n\t\tcase *types.Interface:\n\t\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t\t// a way to instantiate the type.\n\t\t\taddError(fmt.Errorf(\"serialization of interfaces not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Struct:\n\t\t\taddError(fmt.Errorf(\"struct literals are not serializable\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Basic:\n\t\t\tswitch x.Kind() {\n\t\t\tcase types.Bool,\n\t\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\ttypes.Complex64, types.Complex128,\n\t\t\t\ttypes.String:\n\t\t\t\t// Supported.\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\tdefault:\n\t\t\t\tif isInvalid(t) {\n\t\t\t\t\taddError(fmt.Errorf(\"Maybe you forgot to run `go mod tidy`? Also try running `go build` to diagnose further.\"))\n\t\t\t\t} else {\n\t\t\t\t\taddError(fmt.Errorf(\"unsupported basic type\"))\n\t\t\t\t}\n\t\t\t\t// For a better error message, we don't memoize this.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\tcase *types.Array:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Slice:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Pointer:\n\t\t\ttset.checked.Set(t, check(x.Elem(), \"(*\"+path+\")\", true))\n\n\t\tcase *types.Map:\n\t\t\tkeySerializable := check(x.Key(), path+\".key\", true)\n\t\t\tvalSerializable := check(x.Elem(), path+\".value\", true)\n\t\t\ttset.checked.Set(t, keySerializable && valSerializable)\n\n\t\tdefault:\n\t\t\taddError(fmt.Errorf(\"not a serializable type\"))\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn false\n\t\t}\n\n\t\treturn tset.checked.At(t).(bool)\n\t}\n\n\tcheck(t, t.String(), true)\n\treturn errors\n}\n\n// isFixedSizeType returns whether the provided type has a fixed serialization\n// size. Here is a summary of which types are fixed sized:\n//\n//   - Every basic type (e.g., bool, int) except string is fixed sized.\n//   - The array type [N]t is fixed sized if t is fixed sized.\n//   - A struct is fixed sized if the types of its fields are fixed sized.\n//   - A named type is fixed sized if its underlying type is fixed sized.\nfunc (tset *typeSet) isFixedSizeType(t types.Type) bool {\n\treturn tset.sizeOfType(t) >= 0\n}\n\n// sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// genTypeString returns the string representation of t as to be printed\n// in the generated code, updating import definitions to account for the\n// returned type string.\n//\n// Since this call has side-effects (i.e., updating import definitions), it\n// should only be called when the returned type string is written into\n// the generated file; otherwise, the generated code may end up with spurious\n// imports.\nfunc (tset *typeSet) genTypeString(t types.Type) string {\n\t// qualifier is passed to types.TypeString(Type, Qualifier) to determine\n\t// how packages are printed when pretty printing types. For this qualifier,\n\t// types in the root package are printed without their package name, while\n\t// types outside the root package are printed with their package name. For\n\t// example, if we're in root package foo, then the type foo.Bar is printed\n\t// as Bar, while the type io.Reader is printed as io.Reader. See [1] for\n\t// more information on qualifiers and pretty printing types.\n\t//\n\t// [1]: https://github.com/golang/example/tree/master/gotypes#formatting-support\n\tvar qualifier = func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\treturn types.TypeString(t, qualifier)\n}\n\n// isInvalid returns true iff the given type is invalid.\nfunc isInvalid(t types.Type) bool {\n\treturn t.String() == \"invalid type\"\n}\n\n// implementsError returns whether the provided type is a concrete type that\n// implements error.\nfunc (tset *typeSet) implementsError(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\treturn false\n\t}\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"Error\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\tif args := sig.Params(); args.Len() != 0 {\n\t\treturn false\n\t}\n\tif results := sig.Results(); results.Len() != 1 || !isString(results.At(0).Type()) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isProto returns whether the provided type is a concrete type that implements\n// the proto.Message interface.\nfunc (tset *typeSet) isProto(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of proto.Message does \"implement\" proto.Message,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"ProtoReflect\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\trecv, args, results := sig.Recv(), sig.Params(), sig.Results()\n\tif args.Len() != 0 || results.Len() != 1 {\n\t\treturn false\n\t}\n\tif !isProtoMessage(results.At(0).Type()) {\n\t\treturn false\n\t}\n\t// Check the receiver. We avoid complicated embeddings by requiring that\n\t// the method is defined on the type itself.\n\t//\n\t// TODO(mwhittaker): Relax this requirement if it becomes annoying.\n\tif p, ok := recv.Type().(*types.Pointer); ok {\n\t\treturn types.Identical(p.Elem(), t)\n\t} else {\n\t\treturn types.Identical(recv.Type(), t)\n\t}\n}\n\nfunc isProtoMessage(t types.Type) bool {\n\tn, ok := t.(*types.Named)\n\tif !ok {\n\t\treturn false\n\t}\n\tconst protoreflect = \"google.golang.org/protobuf/reflect/protoreflect\"\n\treturn n.Obj().Pkg().Path() == protoreflect && n.Obj().Name() == \"Message\"\n}\n\n// implementsAutoMarshal returns whether the provided type is a concrete\n// type that implements the weaver.AutoMarshal interface.\nfunc (tset *typeSet) implementsAutoMarshal(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of AutoMarshal does \"implement\" the interface,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n```\n# Function Signature: func isValidRouterType(t types.Type) bool {\n# Natural Language Annotations: // isValidRouterType returns whether the provided type is a valid router type.\n// A router type can be one of the following: an integer (signed or unsigned),\n// a float, or a string. Alternatively, it can be a struct that may optioanly\n// embed the weaver.AutoMarshal struct and rest of the fields must be either\n// integers, floats, or strings.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// isValidRouterType returns whether the provided type is a valid router type.\n// A router type can be one of the following: an integer (signed or unsigned),\n// a float, or a string. Alternatively, it can be a struct that may optionally\n// embed the weaver.AutoMarshal struct and rest of the fields must be either\n// integers, floats, or strings.\nfunc isValidRouterType(t types.Type) bool {\n\tswitch x := t.(type) {\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64, types.String:\n\t\t\treturn true\n\t\t}\n\tcase *types.Struct:\n\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\tf := x.Field(i)\n\t\t\tfieldType := f.Type()\n\n\t\t\t// Check if the field is embedded and implements weaver.AutoMarshal.\n\t\t\tif f.Embedded() && implementsAutoMarshal(fieldType) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// If the field is not an embedded AutoMarshal, ensure it's a valid basic type.\n\t\t\tbt, ok := fieldType.(*types.Basic)\n\t\t\tif !ok || (bt.Kind() != types.Int && bt.Kind() != types.Int8 && bt.Kind() != types.Int16 && bt.Kind() != types.Int32 &&\n\t\t\t\tbt.Kind() != types.Int64 && bt.Kind() != types.Uint && bt.Kind() != types.Uint8 && bt.Kind() != types.Uint16 &&\n\t\t\t\tbt.Kind() != types.Uint32 && bt.Kind() != types.Uint64 && bt.Kind() != types.Float32 && bt.Kind() != types.Float64 &&\n\t\t\t\tbt.Kind() != types.String) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\treturn false\n}\n```"]}, "weaver-internal/tool/generate/generator.go-sanitize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/generator.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"errors\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/format\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"go/types\"\n\t\"io\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"unicode\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/internal/tool\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\n// TODO(rgrandl): Modify the generator code to use only the types package. Right\n// now we are doing code generation relying both on the types and ast packages,\n// which can be confusing and also we might do unnecessary work.\n\nconst (\n\tgeneratedCodeFile = \"weaver_gen.go\"\n\n\tUsage = `Generate code for a Service Weaver application.\n\nUsage:\n  weaver generate [packages]\n\nDescription:\n  \"weaver generate\" generates code for the Service Weaver applications in the\n  provided packages. For example, \"weaver generate . ./foo\" will generate code\n  for the Service Weaver applications in the current directory and in the ./foo\n  directory. For every package, the generated code is placed in a weaver_gen.go\n  file in the package's directory. For example, \"weaver generate . ./foo\" will\n  create ./weaver_gen.go and ./foo/weaver_gen.go.\n\n  You specify packages for \"weaver generate\" in the same way you specify\n  packages for go build, go test, go vet, etc. See \"go help packages\" for more\n  information.\n\n  Rather than invoking \"weaver generate\" directly, you can place a line of the\n  following form in one of the .go files in the package:\n\n      //go:generate weaver generate\n\n  and then use the normal \"go generate\" command.\n\nExamples:\n  # Generate code for the package in the current directory.\n  weaver generate\n\n  # Same as \"weaver generate\".\n  weaver generate .\n\n  # Generate code for the package in the ./foo directory.\n  weaver generate ./foo\n\n  # Generate code for all packages in all subdirectories of current directory.\n  weaver generate ./...`\n)\n\n// Options controls the operation of Generate.\ntype Options struct {\n\t// If non-nil, use the specified function to report warnings.\n\tWarn func(error)\n}\n\n// Generate generates Service Weaver code for the specified packages.\n// The list of supplied packages are treated similarly to the arguments\n// passed to \"go build\" (see \"go help packages\" for details).\nfunc Generate(dir string, pkgs []string, opt Options) error {\n\tif opt.Warn == nil {\n\t\topt.Warn = func(err error) { fmt.Fprintln(os.Stderr, err) }\n\t}\n\tfset := token.NewFileSet()\n\tcfg := &packages.Config{\n\t\tMode:       packages.NeedName | packages.NeedSyntax | packages.NeedImports | packages.NeedTypes | packages.NeedTypesInfo,\n\t\tDir:        dir,\n\t\tFset:       fset,\n\t\tParseFile:  parseNonWeaverGenFile,\n\t\tBuildFlags: []string{\"--tags=ignoreWeaverGen\"},\n\t}\n\tpkgList, err := packages.Load(cfg, pkgs...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"packages.Load: %w\", err)\n\t}\n\n\tvar automarshals typeutil.Map\n\tvar errs []error\n\tfor _, pkg := range pkgList {\n\t\tg, err := newGenerator(opt, pkg, fset, &automarshals)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tif err := g.generate(); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// parseNonWeaverGenFile parses a Go file, except for weaver_gen.go files whose\n// contents are ignored since those contents may reference types that no longer\n// exist.\nfunc parseNonWeaverGenFile(fset *token.FileSet, filename string, src []byte) (*ast.File, error) {\n\tif filepath.Base(filename) == generatedCodeFile {\n\t\treturn parser.ParseFile(fset, filename, src, parser.PackageClauseOnly)\n\t}\n\treturn parser.ParseFile(fset, filename, src, parser.ParseComments|parser.DeclarationErrors)\n}\n\ntype generator struct {\n\tpkg            *packages.Package\n\ttset           *typeSet\n\tfileset        *token.FileSet\n\tcomponents     []*component\n\tsizeFuncNeeded typeutil.Map // types that need a serviceweaver_size_* function\n\tgenerated      typeutil.Map // memo cache for generateEncDecMethodsFor\n}\n\n// errorf is like fmt.Errorf but prefixes the error with the provided position.\nfunc errorf(fset *token.FileSet, pos token.Pos, format string, args ...interface{}) error {\n\t// Rewrite the position's filename relative to the current directory. This\n\t// replaces long filenames like \"/home/foo/ServiceWeaver/weaver/weaver.go\"\n\t// with much shorter filenames like \"./weaver.go\".\n\tposition := fset.Position(pos)\n\tif cwd, err := filepath.Abs(\".\"); err == nil {\n\t\tif filename, err := filepath.Rel(cwd, position.Filename); err == nil {\n\t\t\tposition.Filename = filename\n\t\t}\n\t}\n\n\tprefix := position.String()\n\tif colors.Enabled() {\n\t\t// Color the filename red when colors are enabled.\n\t\tprefix = fmt.Sprintf(\"%s%v%s\", colors.Color256(160), position, colors.Reset)\n\t}\n\treturn fmt.Errorf(\"%s: %w\", prefix, fmt.Errorf(format, args...))\n}\n\nfunc newGenerator(opt Options, pkg *packages.Package, fset *token.FileSet, automarshals *typeutil.Map) (*generator, error) {\n\t// Abort if there were any errors loading the package.\n\tvar errs []error\n\tfor _, err := range pkg.Errors {\n\t\terrs = append(errs, err)\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Search every file in the package for types that embed the\n\t// weaver.AutoMarshal struct.\n\ttset := newTypeSet(pkg, automarshals, &typeutil.Map{})\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tts, err := findAutoMarshals(pkg, file)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, t := range ts {\n\t\t\ttset.automarshalCandidates.Set(t, struct{}{})\n\t\t}\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Just because a type embeds weaver.AutoMarshal doesn't mean we can\n\t// automatically marshal it. Some types, like `struct { x chan int }`, are\n\t// just not serializable. Here, we check that every type that embeds\n\t// weaver.AutoMarshal is actually serializable.\n\tfor _, t := range tset.automarshalCandidates.Keys() {\n\t\tn := t.(*types.Named)\n\t\tif err := errors.Join(tset.checkSerializable(n)...); err != nil {\n\t\t\terrs = append(errs, errorf(fset, n.Obj().Pos(), \"type %v is not serializable\\n%w\", t, err))\n\t\t\tcontinue\n\t\t}\n\t\ttset.automarshals.Set(t, struct{}{})\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Find and process all components.\n\tcomponents := map[string]*component{}\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\n\t\tfileComponents, err := findComponents(opt, pkg, file, tset)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, c := range fileComponents {\n\t\t\t// Check for component duplicates, two components that embed the\n\t\t\t// same weaver.Implements[T].\n\t\t\t//\n\t\t\t// TODO(mwhittaker): This code relies on the fact that a component\n\t\t\t// interface and component implementation have to be in the same\n\t\t\t// package. If we lift this requirement, then this code will break.\n\t\t\tif existing, ok := components[c.fullIntfName()]; ok {\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, c.impl.Obj().Pos(),\n\t\t\t\t\t\"Duplicate implementation for component %s, other declaration: %v\",\n\t\t\t\t\tc.fullIntfName(), fset.Position(existing.impl.Obj().Pos())))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponents[c.fullIntfName()] = c\n\t\t}\n\t}\n\n\t// Find method attributes.\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tif err := findMethodAttributes(pkg, file, components); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &generator{\n\t\tpkg:        pkg,\n\t\ttset:       tset,\n\t\tfileset:    fset,\n\t\tcomponents: maps.Values(components),\n\t}, nil\n}\n\n// findComponents returns the components in the provided file. For example,\n// findComponents will find and return the following component.\n//\n//\ttype something struct {\n//\t    weaver.Implements[SomeComponentType]\n//\t    ...\n//\t}\nfunc findComponents(opt Options, pkg *packages.Package, f *ast.File, tset *typeSet) ([]*component, error) {\n\tvar components []*component\n\tvar errs []error\n\tfor _, d := range f.Decls {\n\t\tgendecl, ok := d.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tts, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponent, err := extractComponent(opt, pkg, f, tset, ts)\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif component != nil {\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\treturn components, errors.Join(errs...)\n}\n\nfunc findMethodAttributes(pkg *packages.Package, f *ast.File, components map[string]*component) error {\n\t// Look for declarations of the form:\n\t//\tvar _ weaver.NotRetriable = Component.Method\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.VAR {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tvalspec, ok := spec.(*ast.ValueSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttypeAndValue, ok := pkg.TypesInfo.Types[valspec.Type]\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt := typeAndValue.Type\n\t\t\tif !isWeaverNotRetriable(t) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, val := range valspec.Values {\n\t\t\t\t// We allow non-blank vars for uniformity.\n\t\t\t\tcomp, method, ok := findComponentMethod(pkg, components, val)\n\t\t\t\tif !ok {\n\t\t\t\t\terrs = append(errs, errorf(pkg.Fset, valspec.Pos(), \"weaver.NonRetriable should only be assigned a value that identifies a method of a component implemented by this package\"))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif comp.noretry == nil {\n\t\t\t\t\tcomp.noretry = map[string]struct{}{}\n\t\t\t\t}\n\t\t\t\tcomp.noretry[method] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// findComponentMethod returns the component and method if val is an expression of\n// the form C.M where C is a component listed in components and C has a method named M.\nfunc findComponentMethod(pkg *packages.Package, components map[string]*component, val ast.Expr) (*component, string, bool) {\n\tsel, ok := val.(*ast.SelectorExpr)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctypeval, ok := pkg.TypesInfo.Types[sel.X]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctype, ok := ctypeval.Type.(*types.Named)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tcname := fullName(ctype)\n\tc, ok := components[cname]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tmethod := sel.Sel.Name\n\tfor _, m := range c.methods() {\n\t\tif m.Name() == method {\n\t\t\treturn c, method, true\n\t\t}\n\t}\n\treturn nil, \"\", false\n}\n\n// findAutoMarshals returns the types in the provided file which embed the\n// weaver.AutoMarshal struct.\nfunc findAutoMarshals(pkg *packages.Package, f *ast.File) ([]*types.Named, error) {\n\tvar automarshals []*types.Named\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\t// This is not a type declaration.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Recall that a type declaration can have multiple type specs. We have\n\t\t// to iterate over all of them. For example:\n\t\t//\n\t\t//     type (\n\t\t//         a struct{} // Spec 1\n\t\t//         b struct{} // Spec 2\n\t\t//     )\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\ttypespec, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"type declaration has non-TypeSpec spec: %v\", spec))\n\t\t\t}\n\n\t\t\t// Extract the type's name.\n\t\t\tdef, ok := pkg.TypesInfo.Defs[typespec.Name]\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", typespec.Name))\n\t\t\t}\n\t\t\tn, ok := def.Type().(*types.Named)\n\t\t\tif !ok {\n\t\t\t\t// For type aliases like `type Int = int`, Int has type int and\n\t\t\t\t// not type Named. We ignore these.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check if the type of the expression is struct.\n\t\t\tt, ok := pkg.TypesInfo.Types[typespec.Type].Type.(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check for an embedded weaver.AutoMarshal field.\n\t\t\tautomarshal := false\n\t\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\t\tf := t.Field(i)\n\t\t\t\tif f.Embedded() && isWeaverAutoMarshal(f.Type()) {\n\t\t\t\t\tautomarshal = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !automarshal {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Ignore generic types. Generic types don't play well with\n\t\t\t// embedded AutoMarshals. For example, consider the following type\n\t\t\t// declaration:\n\t\t\t//\n\t\t\t//     type Register[A any] struct {\n\t\t\t//         weaver.AutoMarshal\n\t\t\t//         a A\n\t\t\t//     }\n\t\t\t//\n\t\t\t// Is Register[A] serializable? It depends on A. Plus, we cannot\n\t\t\t// really generate WeaverMarshal and WeaverUnmarshal methods for\n\t\t\t// specific instantiations of Register[A]. Because of these\n\t\t\t// complications, we ignore generic types.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Handle generics somehow?\n\t\t\tif n.TypeParams() != nil { // generics have non-nil TypeParams()\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\t\t\"generic struct %v cannot embed weaver.AutoMarshal. See serviceweaver.dev/docs.html#serializable-types for more information.\",\n\t\t\t\t\tformatType(pkg, n)))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tautomarshals = append(automarshals, n)\n\t\t}\n\t}\n\treturn automarshals, errors.Join(errs...)\n}\n\n// extractComponent attempts to extract a component from the provided TypeSpec.\n// It returns a nil component if the TypeSpec doesn't define a component.\nfunc extractComponent(opt Options, pkg *packages.Package, file *ast.File, tset *typeSet, spec *ast.TypeSpec) (*component, error) {\n\t// Check that the type spec is of the form `type t struct {...}`.\n\ts, ok := spec.Type.(*ast.StructType)\n\tif !ok {\n\t\t// This type declaration does not involve a struct. For example, it\n\t\t// might look like `type t int`. These non-struct type declarations\n\t\t// cannot be components.\n\t\treturn nil, nil\n\t}\n\tdef, ok := pkg.TypesInfo.Defs[spec.Name]\n\tif !ok {\n\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", spec.Name))\n\t}\n\timpl, ok := def.Type().(*types.Named)\n\tif !ok {\n\t\t// For type aliases like `type t = struct{}`, t has type *types.Struct\n\t\t// and not type *types.Named. We ignore these.\n\t\treturn nil, nil\n\t}\n\n\t// Find any weaver.Implements[T] or weaver.WithRouter[T] embedded fields.\n\tvar intf *types.Named   // The component interface type\n\tvar router *types.Named // Router type (if any)\n\tvar isMain bool         // Is intf weaver.Main?\n\tvar refs []*types.Named // T for which weaver.Ref[T] exists in struct\n\tvar listeners []string  // Names of all listener fields declared in struct\n\tfor _, f := range s.Fields.List {\n\t\ttypeAndValue, ok := pkg.TypesInfo.Types[f.Type]\n\t\tif !ok {\n\t\t\tpanic(errorf(pkg.Fset, f.Pos(), \"type %v not found\", f.Type))\n\t\t}\n\t\tt := typeAndValue.Type\n\n\t\tif isWeaverRef(t) {\n\t\t\t// The field f has type weaver.Ref[T].\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tif isWeaverMain(arg) {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"components cannot contain a reference to weaver.Main\")\n\t\t\t}\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Ref argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\trefs = append(refs, named)\n\t\t} else if isWeaverListener(t) {\n\t\t\tlis, err := getListenerNamesFromStructField(pkg, f)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlisteners = append(listeners, lis...)\n\t\t}\n\n\t\tif len(f.Names) != 0 {\n\t\t\t// Ignore unembedded fields.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Warn the user about unembedded\n\t\t\t// weaver.Implements, weaver.WithConfig, or weaver.WithRouter?\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch {\n\t\t// The field f is an embedded weaver.Implements[T].\n\t\tcase isWeaverImplements(t):\n\t\t\t// Check that T is a named interface type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tisMain = isWeaverMain(arg)\n\t\t\tif !isMain && named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is a type outside the current package. A component interface and implementation must be in the same package. If you can't move them into the same package, you can add `type %s %v` to the implementation's package and embed `weaver.Implements[%s]` instead of `weaver.Implements[%s]`.\",\n\t\t\t\t\tformatType(pkg, named), named.Obj().Name(), formatType(pkg, named), named.Obj().Name(), formatType(pkg, named))\n\t\t\t}\n\t\t\tif _, ok := named.Underlying().(*types.Interface); !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not an interface.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\tintf = named\n\n\t\t// The field f is an embedded weaver.WithRouter[T].\n\t\tcase isWeaverWithRouter(t):\n\t\t\t// Check that T is a named type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tif named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is a type outside the current package.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\trouter = named\n\t\t}\n\t}\n\n\tif intf == nil {\n\t\t// TODO(mwhittaker): Warn the user if they embed weaver.WithRouter or\n\t\t// weaver.WithConfig but don't embed weaver.Implements.\n\t\treturn nil, nil\n\t}\n\n\t// Check that that the component implementation implements the component\n\t// interface.\n\tif !types.Implements(types.NewPointer(impl), intf.Underlying().(*types.Interface)) {\n\t\treturn nil, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\"type %s embeds weaver\n```\n# Function Signature: func sanitize(t types.Type) string {\n# Natural Language Annotations: // sanitize generates a (somewhat pretty printed) name for the provided type\n// that is a valid go identifier [1]. sanitize also produces unique names. That\n// is, if u != t, then sanitize(u) != sanitize(t).\n//\n// Some examples:\n//\n//   - map[int]string -> map_int_string_589aebd1\n//   - map[int][]X    -> map_int_slice_X_ac498abc\n//   - []int          -> slice_int_1048ebf9\n//   - [][]string     -> slice_slice_string_13efa8aa\n//   - [20]int        -> array_20_int_00ae9a0a\n//   - *int           -> ptr_int_916711b2\n//\n// [1]: https://go.dev/ref/spec#Identifiers\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc sanitize(t types.Type) string {\n\tvar buf bytes.Buffer\n\tsanitizeHelper(&buf, t)\n\n\t// Generate a unique suffix. We use a hash of the string representation\n\t// of the type, which includes package paths and other information\n\t// that affects type identity. This ensures that different types with\n\t// the same string representation in Go source have different names.\n\th := sha256.New()\n\tio.WriteString(h, t.String())\n\tfmt.Fprintf(&buf, \"_%x\", h.Sum(nil)[:4]) // Shorten the hash for brevity\n\n\treturn buf.String()\n}\n\nfunc sanitizeHelper(buf *bytes.Buffer, t types.Type) {\n\tswitch u := t.(type) {\n\tcase *types.Basic:\n\t\tbuf.WriteString(u.String())\n\n\tcase *types.Array:\n\t\tfmt.Fprintf(buf, \"array_%d_\", u.Len())\n\t\tsanitizeHelper(buf, u.Elem())\n\n\tcase *types.Slice:\n\t\tbuf.WriteString(\"slice_\")\n\t\tsanitizeHelper(buf, u.Elem())\n\n\tcase *types.Pointer:\n\t\tbuf.WriteString(\"ptr_\")\n\t\tsanitizeHelper(buf, u.Elem())\n\n\tcase *types.Map:\n\t\tbuf.WriteString(\"map_\")\n\t\tsanitizeHelper(buf, u.Key())\n\t\tbuf.WriteByte('_')\n\t\tsanitizeHelper(buf, u.Elem())\n\n\tcase *types.Struct:\n\t\tbuf.WriteString(\"struct_\")\n\t\tfor i := 0; i < u.NumFields(); i++ {\n\t\t\tif i > 0 {\n\t\t\t\tbuf.WriteByte('_')\n\t\t\t}\n\t\t\tsanitizeHelper(buf, u.Field(i).Type())\n\t\t}\n\n\tcase *types.Signature:\n\t\tbuf.WriteString(\"func_\")\n\t\t// Process parameters and results to uniquely identify the function signature.\n\t\tprocessTuple(buf, u.Params())\n\t\tbuf.WriteByte('_')\n\t\tprocessTuple(buf, u.Results())\n\n\tcase *types.Named:\n\t\tif u.Obj() != nil && u.Obj().Pkg() != nil {\n\t\t\tbuf.WriteString(u.Obj().Pkg().Path())\n\t\t\tbuf.WriteByte('.')\n\t\t}\n\t\tbuf.WriteString(u.Obj().Name())\n\n\tdefault:\n\t\tbuf.WriteString(u.String())\n\t}\n}\n\nfunc processTuple(buf *bytes.Buffer, tup *types.Tuple) {\n\tfor i := 0; i < tup.Len(); i++ {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte('_')\n\t\t}\n\t\tsanitizeHelper(buf, tup.At(i).Type())\n\t}\n}\n```\n\nThis function generates a name for a Go type that is a valid identifier, including a unique suffix to differentiate between types that otherwise have similar string representations. The helper function `sanitizeHelper` works recursively on the components of complex types. Additionally, it uses the SHA-256 hash algorithm to generate a distinct suffix ensuring uniqueness."]}, "weaver-internal/tool/generate/generator.go-uniqueName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/generator.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"errors\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/format\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"go/types\"\n\t\"io\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"unicode\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/internal/tool\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\n// TODO(rgrandl): Modify the generator code to use only the types package. Right\n// now we are doing code generation relying both on the types and ast packages,\n// which can be confusing and also we might do unnecessary work.\n\nconst (\n\tgeneratedCodeFile = \"weaver_gen.go\"\n\n\tUsage = `Generate code for a Service Weaver application.\n\nUsage:\n  weaver generate [packages]\n\nDescription:\n  \"weaver generate\" generates code for the Service Weaver applications in the\n  provided packages. For example, \"weaver generate . ./foo\" will generate code\n  for the Service Weaver applications in the current directory and in the ./foo\n  directory. For every package, the generated code is placed in a weaver_gen.go\n  file in the package's directory. For example, \"weaver generate . ./foo\" will\n  create ./weaver_gen.go and ./foo/weaver_gen.go.\n\n  You specify packages for \"weaver generate\" in the same way you specify\n  packages for go build, go test, go vet, etc. See \"go help packages\" for more\n  information.\n\n  Rather than invoking \"weaver generate\" directly, you can place a line of the\n  following form in one of the .go files in the package:\n\n      //go:generate weaver generate\n\n  and then use the normal \"go generate\" command.\n\nExamples:\n  # Generate code for the package in the current directory.\n  weaver generate\n\n  # Same as \"weaver generate\".\n  weaver generate .\n\n  # Generate code for the package in the ./foo directory.\n  weaver generate ./foo\n\n  # Generate code for all packages in all subdirectories of current directory.\n  weaver generate ./...`\n)\n\n// Options controls the operation of Generate.\ntype Options struct {\n\t// If non-nil, use the specified function to report warnings.\n\tWarn func(error)\n}\n\n// Generate generates Service Weaver code for the specified packages.\n// The list of supplied packages are treated similarly to the arguments\n// passed to \"go build\" (see \"go help packages\" for details).\nfunc Generate(dir string, pkgs []string, opt Options) error {\n\tif opt.Warn == nil {\n\t\topt.Warn = func(err error) { fmt.Fprintln(os.Stderr, err) }\n\t}\n\tfset := token.NewFileSet()\n\tcfg := &packages.Config{\n\t\tMode:       packages.NeedName | packages.NeedSyntax | packages.NeedImports | packages.NeedTypes | packages.NeedTypesInfo,\n\t\tDir:        dir,\n\t\tFset:       fset,\n\t\tParseFile:  parseNonWeaverGenFile,\n\t\tBuildFlags: []string{\"--tags=ignoreWeaverGen\"},\n\t}\n\tpkgList, err := packages.Load(cfg, pkgs...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"packages.Load: %w\", err)\n\t}\n\n\tvar automarshals typeutil.Map\n\tvar errs []error\n\tfor _, pkg := range pkgList {\n\t\tg, err := newGenerator(opt, pkg, fset, &automarshals)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tif err := g.generate(); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// parseNonWeaverGenFile parses a Go file, except for weaver_gen.go files whose\n// contents are ignored since those contents may reference types that no longer\n// exist.\nfunc parseNonWeaverGenFile(fset *token.FileSet, filename string, src []byte) (*ast.File, error) {\n\tif filepath.Base(filename) == generatedCodeFile {\n\t\treturn parser.ParseFile(fset, filename, src, parser.PackageClauseOnly)\n\t}\n\treturn parser.ParseFile(fset, filename, src, parser.ParseComments|parser.DeclarationErrors)\n}\n\ntype generator struct {\n\tpkg            *packages.Package\n\ttset           *typeSet\n\tfileset        *token.FileSet\n\tcomponents     []*component\n\tsizeFuncNeeded typeutil.Map // types that need a serviceweaver_size_* function\n\tgenerated      typeutil.Map // memo cache for generateEncDecMethodsFor\n}\n\n// errorf is like fmt.Errorf but prefixes the error with the provided position.\nfunc errorf(fset *token.FileSet, pos token.Pos, format string, args ...interface{}) error {\n\t// Rewrite the position's filename relative to the current directory. This\n\t// replaces long filenames like \"/home/foo/ServiceWeaver/weaver/weaver.go\"\n\t// with much shorter filenames like \"./weaver.go\".\n\tposition := fset.Position(pos)\n\tif cwd, err := filepath.Abs(\".\"); err == nil {\n\t\tif filename, err := filepath.Rel(cwd, position.Filename); err == nil {\n\t\t\tposition.Filename = filename\n\t\t}\n\t}\n\n\tprefix := position.String()\n\tif colors.Enabled() {\n\t\t// Color the filename red when colors are enabled.\n\t\tprefix = fmt.Sprintf(\"%s%v%s\", colors.Color256(160), position, colors.Reset)\n\t}\n\treturn fmt.Errorf(\"%s: %w\", prefix, fmt.Errorf(format, args...))\n}\n\nfunc newGenerator(opt Options, pkg *packages.Package, fset *token.FileSet, automarshals *typeutil.Map) (*generator, error) {\n\t// Abort if there were any errors loading the package.\n\tvar errs []error\n\tfor _, err := range pkg.Errors {\n\t\terrs = append(errs, err)\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Search every file in the package for types that embed the\n\t// weaver.AutoMarshal struct.\n\ttset := newTypeSet(pkg, automarshals, &typeutil.Map{})\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tts, err := findAutoMarshals(pkg, file)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, t := range ts {\n\t\t\ttset.automarshalCandidates.Set(t, struct{}{})\n\t\t}\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Just because a type embeds weaver.AutoMarshal doesn't mean we can\n\t// automatically marshal it. Some types, like `struct { x chan int }`, are\n\t// just not serializable. Here, we check that every type that embeds\n\t// weaver.AutoMarshal is actually serializable.\n\tfor _, t := range tset.automarshalCandidates.Keys() {\n\t\tn := t.(*types.Named)\n\t\tif err := errors.Join(tset.checkSerializable(n)...); err != nil {\n\t\t\terrs = append(errs, errorf(fset, n.Obj().Pos(), \"type %v is not serializable\\n%w\", t, err))\n\t\t\tcontinue\n\t\t}\n\t\ttset.automarshals.Set(t, struct{}{})\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Find and process all components.\n\tcomponents := map[string]*component{}\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\n\t\tfileComponents, err := findComponents(opt, pkg, file, tset)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, c := range fileComponents {\n\t\t\t// Check for component duplicates, two components that embed the\n\t\t\t// same weaver.Implements[T].\n\t\t\t//\n\t\t\t// TODO(mwhittaker): This code relies on the fact that a component\n\t\t\t// interface and component implementation have to be in the same\n\t\t\t// package. If we lift this requirement, then this code will break.\n\t\t\tif existing, ok := components[c.fullIntfName()]; ok {\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, c.impl.Obj().Pos(),\n\t\t\t\t\t\"Duplicate implementation for component %s, other declaration: %v\",\n\t\t\t\t\tc.fullIntfName(), fset.Position(existing.impl.Obj().Pos())))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponents[c.fullIntfName()] = c\n\t\t}\n\t}\n\n\t// Find method attributes.\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tif err := findMethodAttributes(pkg, file, components); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &generator{\n\t\tpkg:        pkg,\n\t\ttset:       tset,\n\t\tfileset:    fset,\n\t\tcomponents: maps.Values(components),\n\t}, nil\n}\n\n// findComponents returns the components in the provided file. For example,\n// findComponents will find and return the following component.\n//\n//\ttype something struct {\n//\t    weaver.Implements[SomeComponentType]\n//\t    ...\n//\t}\nfunc findComponents(opt Options, pkg *packages.Package, f *ast.File, tset *typeSet) ([]*component, error) {\n\tvar components []*component\n\tvar errs []error\n\tfor _, d := range f.Decls {\n\t\tgendecl, ok := d.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tts, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponent, err := extractComponent(opt, pkg, f, tset, ts)\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif component != nil {\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\treturn components, errors.Join(errs...)\n}\n\nfunc findMethodAttributes(pkg *packages.Package, f *ast.File, components map[string]*component) error {\n\t// Look for declarations of the form:\n\t//\tvar _ weaver.NotRetriable = Component.Method\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.VAR {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tvalspec, ok := spec.(*ast.ValueSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttypeAndValue, ok := pkg.TypesInfo.Types[valspec.Type]\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt := typeAndValue.Type\n\t\t\tif !isWeaverNotRetriable(t) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, val := range valspec.Values {\n\t\t\t\t// We allow non-blank vars for uniformity.\n\t\t\t\tcomp, method, ok := findComponentMethod(pkg, components, val)\n\t\t\t\tif !ok {\n\t\t\t\t\terrs = append(errs, errorf(pkg.Fset, valspec.Pos(), \"weaver.NonRetriable should only be assigned a value that identifies a method of a component implemented by this package\"))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif comp.noretry == nil {\n\t\t\t\t\tcomp.noretry = map[string]struct{}{}\n\t\t\t\t}\n\t\t\t\tcomp.noretry[method] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// findComponentMethod returns the component and method if val is an expression of\n// the form C.M where C is a component listed in components and C has a method named M.\nfunc findComponentMethod(pkg *packages.Package, components map[string]*component, val ast.Expr) (*component, string, bool) {\n\tsel, ok := val.(*ast.SelectorExpr)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctypeval, ok := pkg.TypesInfo.Types[sel.X]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctype, ok := ctypeval.Type.(*types.Named)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tcname := fullName(ctype)\n\tc, ok := components[cname]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tmethod := sel.Sel.Name\n\tfor _, m := range c.methods() {\n\t\tif m.Name() == method {\n\t\t\treturn c, method, true\n\t\t}\n\t}\n\treturn nil, \"\", false\n}\n\n// findAutoMarshals returns the types in the provided file which embed the\n// weaver.AutoMarshal struct.\nfunc findAutoMarshals(pkg *packages.Package, f *ast.File) ([]*types.Named, error) {\n\tvar automarshals []*types.Named\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\t// This is not a type declaration.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Recall that a type declaration can have multiple type specs. We have\n\t\t// to iterate over all of them. For example:\n\t\t//\n\t\t//     type (\n\t\t//         a struct{} // Spec 1\n\t\t//         b struct{} // Spec 2\n\t\t//     )\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\ttypespec, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"type declaration has non-TypeSpec spec: %v\", spec))\n\t\t\t}\n\n\t\t\t// Extract the type's name.\n\t\t\tdef, ok := pkg.TypesInfo.Defs[typespec.Name]\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", typespec.Name))\n\t\t\t}\n\t\t\tn, ok := def.Type().(*types.Named)\n\t\t\tif !ok {\n\t\t\t\t// For type aliases like `type Int = int`, Int has type int and\n\t\t\t\t// not type Named. We ignore these.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check if the type of the expression is struct.\n\t\t\tt, ok := pkg.TypesInfo.Types[typespec.Type].Type.(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check for an embedded weaver.AutoMarshal field.\n\t\t\tautomarshal := false\n\t\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\t\tf := t.Field(i)\n\t\t\t\tif f.Embedded() && isWeaverAutoMarshal(f.Type()) {\n\t\t\t\t\tautomarshal = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !automarshal {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Ignore generic types. Generic types don't play well with\n\t\t\t// embedded AutoMarshals. For example, consider the following type\n\t\t\t// declaration:\n\t\t\t//\n\t\t\t//     type Register[A any] struct {\n\t\t\t//         weaver.AutoMarshal\n\t\t\t//         a A\n\t\t\t//     }\n\t\t\t//\n\t\t\t// Is Register[A] serializable? It depends on A. Plus, we cannot\n\t\t\t// really generate WeaverMarshal and WeaverUnmarshal methods for\n\t\t\t// specific instantiations of Register[A]. Because of these\n\t\t\t// complications, we ignore generic types.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Handle generics somehow?\n\t\t\tif n.TypeParams() != nil { // generics have non-nil TypeParams()\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\t\t\"generic struct %v cannot embed weaver.AutoMarshal. See serviceweaver.dev/docs.html#serializable-types for more information.\",\n\t\t\t\t\tformatType(pkg, n)))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tautomarshals = append(automarshals, n)\n\t\t}\n\t}\n\treturn automarshals, errors.Join(errs...)\n}\n\n// extractComponent attempts to extract a component from the provided TypeSpec.\n// It returns a nil component if the TypeSpec doesn't define a component.\nfunc extractComponent(opt Options, pkg *packages.Package, file *ast.File, tset *typeSet, spec *ast.TypeSpec) (*component, error) {\n\t// Check that the type spec is of the form `type t struct {...}`.\n\ts, ok := spec.Type.(*ast.StructType)\n\tif !ok {\n\t\t// This type declaration does not involve a struct. For example, it\n\t\t// might look like `type t int`. These non-struct type declarations\n\t\t// cannot be components.\n\t\treturn nil, nil\n\t}\n\tdef, ok := pkg.TypesInfo.Defs[spec.Name]\n\tif !ok {\n\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", spec.Name))\n\t}\n\timpl, ok := def.Type().(*types.Named)\n\tif !ok {\n\t\t// For type aliases like `type t = struct{}`, t has type *types.Struct\n\t\t// and not type *types.Named. We ignore these.\n\t\treturn nil, nil\n\t}\n\n\t// Find any weaver.Implements[T] or weaver.WithRouter[T] embedded fields.\n\tvar intf *types.Named   // The component interface type\n\tvar router *types.Named // Router type (if any)\n\tvar isMain bool         // Is intf weaver.Main?\n\tvar refs []*types.Named // T for which weaver.Ref[T] exists in struct\n\tvar listeners []string  // Names of all listener fields declared in struct\n\tfor _, f := range s.Fields.List {\n\t\ttypeAndValue, ok := pkg.TypesInfo.Types[f.Type]\n\t\tif !ok {\n\t\t\tpanic(errorf(pkg.Fset, f.Pos(), \"type %v not found\", f.Type))\n\t\t}\n\t\tt := typeAndValue.Type\n\n\t\tif isWeaverRef(t) {\n\t\t\t// The field f has type weaver.Ref[T].\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tif isWeaverMain(arg) {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"components cannot contain a reference to weaver.Main\")\n\t\t\t}\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Ref argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\trefs = append(refs, named)\n\t\t} else if isWeaverListener(t) {\n\t\t\tlis, err := getListenerNamesFromStructField(pkg, f)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlisteners = append(listeners, lis...)\n\t\t}\n\n\t\tif len(f.Names) != 0 {\n\t\t\t// Ignore unembedded fields.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Warn the user about unembedded\n\t\t\t// weaver.Implements, weaver.WithConfig, or weaver.WithRouter?\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch {\n\t\t// The field f is an embedded weaver.Implements[T].\n\t\tcase isWeaverImplements(t):\n\t\t\t// Check that T is a named interface type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tisMain = isWeaverMain(arg)\n\t\t\tif !isMain && named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is a type outside the current package. A component interface and implementation must be in the same package. If you can't move them into the same package, you can add `type %s %v` to the implementation's package and embed `weaver.Implements[%s]` instead of `weaver.Implements[%s]`.\",\n\t\t\t\t\tformatType(pkg, named), named.Obj().Name(), formatType(pkg, named), named.Obj().Name(), formatType(pkg, named))\n\t\t\t}\n\t\t\tif _, ok := named.Underlying().(*types.Interface); !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not an interface.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\tintf = named\n\n\t\t// The field f is an embedded weaver.WithRouter[T].\n\t\tcase isWeaverWithRouter(t):\n\t\t\t// Check that T is a named type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tif named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is a type outside the current package.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\trouter = named\n\t\t}\n\t}\n\n\tif intf == nil {\n\t\t// TODO(mwhittaker): Warn the user if they embed weaver.WithRouter or\n\t\t// weaver.WithConfig but don't embed weaver.Implements.\n\t\treturn nil, nil\n\t}\n\n\t// Check that that the component implementation implements the component\n\t// interface.\n\tif !types.Implements(types.NewPointer(impl), intf.Underlying().(*types.Interface)) {\n\t\treturn nil, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\"type %s embeds weaver\n```\n# Function Signature: func uniqueName(t types.Type) string {\n# Natural Language Annotations: // uniqueName returns a unique pretty printed representation of the provided\n// type (e.g., \"int\", \"map[int]bool\"). The key property is that if u != t, then\n// uniqueName(u) != uniqueName(t).\n//\n// Note that types.TypeString returns a pretty printed representation of a\n// string, but it is not guaranteed to be unique. For example, if have `type\n// int bool`, then TypeString returns \"int\" for both the named type int and the\n// primitive type int.\nPlease return the generated code surrounded by ```\n", "response": ["To implement the `uniqueName` function in Go with the specified annotations, we need to ensure that it returns a string representing the type which is unique across different types. We cannot rely on `types.TypeString()` for uniqueness due to its behavior with named types and aliases.\n\nHere's one way to implement it by considering the underlying primitive types and qualifier settings:\n\n```go\nfunc uniqueName(t types.Type) string {\n    var typeName string\n    switch u := t.(type) {\n    case *types.Basic:\n        typeName = u.Name()\n    case *types.Named:\n        if u.Obj().Pkg() != nil {\n            typeName = u.Obj().Pkg().Path() + \".\" + u.Obj().Name()\n        } else {\n            typeName = u.Obj().Name()\n        }\n    case *types.Pointer:\n        typeName = \"*\" + uniqueName(u.Elem())\n    case *types.Array:\n        typeName = fmt.Sprintf(\"[%d]%s\", u.Len(), uniqueName(u.Elem()))\n    case *types.Slice:\n        typeName = \"[]\" + uniqueName(u.Elem())\n    case *types.Map:\n        typeName = fmt.Sprintf(\"map[%s]%s\", uniqueName(u.Key()), uniqueName(u.Elem()))\n    case *types.Chan:\n        var dir string\n        switch u.Dir() {\n        case types.SendRecv:\n            dir = \"chan \"\n        case types.SendOnly:\n            dir = \"chan<- \"\n        case types.RecvOnly:\n            dir = \"<-chan \"\n        }\n        typeName = dir + uniqueName(u.Elem())\n    case *types.Struct:\n        var fields []string\n        for i := 0; i < u.NumFields(); i++ {\n            fields = append(fields, u.Field(i).Name()+\":\"+uniqueName(u.Field(i).Type()))\n        }\n        typeName = \"struct{\" + strings.Join(fields, \";\") + \"}\"\n    case *types.Interface:\n        var methods []string\n        for i := 0; i < u.NumMethods(); i++ {\n            methods = append(methods, u.Method(i).Name())\n        }\n        typeName = \"interface{\" + strings.Join(methods, \";\") + \"}\"\n    case *types.Signature:\n        params := []string{}\n        for i := 0; i < u.Params().Len(); i++ {\n            params = append(params, uniqueName(u.Params().At(i).Type()))\n        }\n        results := []string{}\n        for i := 0; i < u.Results().Len(); i++ {\n            results = append(results, uniqueName(u.Results().At(i).Type()))\n        }\n        typeName = \"func(\" + strings.Join(params, \",\") + \")\"\n        if u.Results().Len() > 0 {\n            typeName += \" (\" + strings.Join(results, \",\") + \")\"\n        }\n    default:\n        typeName = \"<unknown>\"\n    }\n    return typeName\n}\n```\n\nThis `uniqueName` function is designed to recursively process complex types like pointers, slices, structs, etc., to ensure that their string representation is unique and captures enough context about the type. This should satisfy the requirement that different types have different `uniqueName` outputs."]}, "weaver-internal/tool/certs/certs.go-GenerateSignedCert": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/certs/certs.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage certs\n\nimport (\n\t\"bytes\"\n\t\"crypto\"\n\t\"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/x509\"\n\t\"crypto/x509/pkix\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"time\"\n)\n\n// GenerateCACert generates a self-signed CA certificate and a corresponding\n// private key.\n//\n// The returned certificate has a one-year validity and is attributed to a fake\n// authority. As such, it should only ever be used on a temporary basis and for\n// in-process certificate signing.\nfunc GenerateCACert() (*x509.Certificate, crypto.PrivateKey, error) {\n\treturn generateLeafCert(true /*isCA*/, \"ca\")\n}\n\n// GenerateSignedCert generates a certificate for the given DNS names, signed\n// by the given Certificate Authority, and a corresponding private key.\n//\n// The returned certificate has a one-year validity and should only ever\n// be used on a temporary basis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc generateLeafCert(isCA bool, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 4096)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tserialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)\n\tserialNumber, err := rand.Int(rand.Reader, serialNumberLimit)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tkeyUsage := x509.KeyUsageDigitalSignature | x509.KeyUsageKeyEncipherment\n\tif isCA {\n\t\tkeyUsage |= x509.KeyUsageCertSign\n\t}\n\ttemplate := x509.Certificate{\n\t\tSerialNumber: serialNumber,\n\t\tSubject:      pkix.Name{Organization: []string{\"ACME Co.\"}},\n\t\tNotBefore:    time.Now(),\n\t\tNotAfter:     time.Now().Add(365 * 24 * time.Hour),\n\t\tKeyUsage:     keyUsage,\n\t\tExtKeyUsage: []x509.ExtKeyUsage{\n\t\t\tx509.ExtKeyUsageClientAuth,\n\t\t\tx509.ExtKeyUsageServerAuth,\n\t\t},\n\t\tBasicConstraintsValid: true,\n\t\tIsCA:                  isCA,\n\t\tDNSNames:              names,\n\t}\n\tcertDER, err := x509.CreateCertificate(rand.Reader, &template, &template, &priv.PublicKey, priv)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn cert, priv, nil\n}\n\n// PEMEncode returns the PEM-encoded blocks for the given certificate and\n// private key.\nfunc PEMEncode(cert *x509.Certificate, key crypto.PrivateKey) ([]byte, []byte, error) {\n\tkeyDER, err := x509.MarshalPKCS8PrivateKey(key)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tvar certOut bytes.Buffer\n\tif err := pem.Encode(&certOut, &pem.Block{\n\t\tType:  \"CERTIFICATE\",\n\t\tBytes: cert.Raw,\n\t}); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tvar keyOut bytes.Buffer\n\tif err := pem.Encode(&keyOut, &pem.Block{\n\t\tType:  \"PRIVATE KEY\",\n\t\tBytes: keyDER,\n\t}); err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn certOut.Bytes(), keyOut.Bytes(), nil\n}\n\n// VerifySignedCert verifies the given signed certificate using the given\n// root CA, returning the DNS names stored in the leaf certificate.\nfunc VerifySignedCert(certDER []byte, ca *x509.Certificate) ([]string, error) {\n\tcaCertPool := x509.NewCertPool()\n\tcaCertPool.AddCert(ca)\n\topts := x509.VerifyOptions{\n\t\tRoots:       caCertPool,\n\t\tCurrentTime: time.Now(),\n\t}\n\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, errors.New(\"bad certificate\")\n\t}\n\tverifiedCerts, err := cert.Verify(opts)\n\tif err != nil {\n\t\treturn nil, errors.New(\"couldn't verify certificate chain\")\n\t}\n\tif len(verifiedCerts) != 1 {\n\t\treturn nil, fmt.Errorf(\"expected a single verified chain, got %d\", len(verifiedCerts))\n\t}\n\tverifiedCert := verifiedCerts[0]\n\tif len(verifiedCert) < 1 { // should never happen\n\t\treturn nil, fmt.Errorf(\"empty verified chain\")\n\t}\n\treturn verifiedCert[0].DNSNames, nil\n}\n\n```\n# Function Signature: func GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n# Natural Language Annotations: // GenerateSignedCert generates a certificate for the given DNS names, signed\n// by the given Certificate Authority, and a corresponding private key.\n//\n// The returned certificate has a one-year validity and should only ever\n// be used on a temporary basis.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// GenerateSignedCert generates a certificate for the given DNS names, signed\n// by the given Certificate Authority, and a corresponding private key.\n//\n// The returned certificate has a one-year validity and should only ever\n// be used on a temporary basis.\nfunc GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n\tleafCert, leafKey, err := generateLeafCert(false, names...)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tcertDER, err := x509.CreateCertificate(rand.Reader, leafCert, ca, &leafKey.(rsa.PrivateKey).PublicKey, caKey)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\t\n\tsignedCert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn signedCert, leafKey, nil\n}\n```"]}, "weaver-internal/weaver/load.go-updateAssignment": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/load.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/DataDog/hyperloglog\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/lightstep/varopt\"\n)\n\nfunc approxEqual(a, b float64) bool {\n\tconst float64EqualityThreshold = 1e-9\n\treturn math.Abs(a-b) <= float64EqualityThreshold\n}\n\n// TODO(mwhittaker): Right now, load collection is slow. It grabs a mutex\n// every time the load needs to be updated. Make this faster.\n\n// loadCollector collects load for a Service Weaver component. As an example, imagine we\n// have a load collector lc for a Service Weaver component that owns slices [0, 10) and\n// [100, 200). We add the following load over the course of a second.\n//\n//   - lc.Add(0, 1)\n//   - lc.Add(1, 1)\n//   - lc.Add(2, 1)\n//   - lc.Add(3, 1)\n//   - lc.Add(100, 1)\n//   - lc.Add(101, 1)\n//\n// The load collector will report a load of 4 requests per second on the slice\n// [0, 10) and a load of 2 requests per second on the slice [100, 200).\ntype loadCollector struct {\n\tcomponent string           // Service Weaver component\n\taddr      string           // dialable address found in assignments\n\tnow       func() time.Time // time.Now usually, but injected fake in tests\n\n\tmu         sync.Mutex               // guards the following fields\n\tassignment *protos.Assignment       // latest assignment\n\tindex      index                    // index on assignment\n\tstart      time.Time                // start of load collection\n\tslices     map[uint64]*sliceSummary // keyed by start of slice\n}\n\n// sliceSummary contains a summary of the observed keys and load of a slice for\n// a replica.\ntype sliceSummary struct {\n\tslice  slice                    // the slice\n\tload   float64                  // total load\n\tcount  *hyperloglog.HyperLogLog // counts distinct elements\n\tsample *varopt.Varopt[uint64]   // reservoir sample of keys\n}\n\n// newLoadCollector returns a new load collector. Note that load is collected\n// with respect to an assignment, so load won't be collected until\n// UpdateAssignment is called.\nfunc newLoadCollector(component string, addr string) *loadCollector {\n\treturn &loadCollector{\n\t\tcomponent: component,\n\t\taddr:      addr,\n\t\tnow:       func() time.Time { return time.Now() },\n\t\tstart:     time.Now(),\n\t\tslices:    map[uint64]*sliceSummary{},\n\t}\n}\n\n// add adds load for the provided key.\nfunc (lc *loadCollector) add(key uint64, v float64) error {\n\tif v != 1.0 {\n\t\tpanic(\"load != 1.0 not yet implemented\")\n\t}\n\n\t// Find the corresponding slice.\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\tslice, found := lc.index.find(key)\n\tif !found {\n\t\t// TODO(mwhittaker): It is currently possible to receive a request for\n\t\t// a key that is not in our current assignment. For example, a\n\t\t// different weavelet may have an older or newer version of the current\n\t\t// assignment and send us keys not in our current assignment. In the\n\t\t// future, we may want to catch these requests and discard them. For\n\t\t// now, we execute them.\n\t\treturn nil\n\t}\n\tif !slice.replicaSet[lc.addr] {\n\t\treturn nil\n\t}\n\n\tsummary, found := lc.slices[slice.start]\n\tif !found {\n\t\tvar err error\n\t\tsummary, err = newSliceSummary(slice)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.slices[slice.start] = summary\n\t}\n\n\t// Update the load.\n\tsummary.load += v\n\n\t// Update the count. Note that we compute a hash of our key before passing\n\t// it to the hyperloglog, even though the key is itself a hash. The reason\n\t// is that this slice represents only a small sliver of the total hash\n\t// space. To operate correctly, a hyperloglog assumes values are drawn\n\t// uniformly from the space of all uint32s, so if we feed the hyperloglog\n\t// values only from this slice, the count will be inaccurate.\n\t//\n\t// TODO(mwhittaker): Compute the hash outside of the lock?\n\t// TODO(mwhittaker): Use a different sketch?\n\t// TODO(mwhittaker): If the slice is small (< 1024), we can count the\n\t// number of distinct elements exactly. Don't use a hyperloglog here.\n\t// TODO(mwhittaker): Start with an exact count and only switch to a\n\t// hyperloglog if the number of unique elements gets too big?\n\tsummary.count.Add(hyperloglog.Murmur64(key))\n\n\t// Update the sample. Note that Add takes in a key and a weight, but we are\n\t// recording unweighted samples, so we use a constant weight of 1.0 for\n\t// every key.\n\tif _, err := summary.sample.Add(key, 1.0); err != nil {\n\t\treturn fmt.Errorf(\"cannot sample %d: %v\", key, err)\n\t}\n\treturn nil\n}\n\n// updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\n\n\n\n\n\n\n\n\n\n\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// reset resets the load collector. If you want to collect load over 5\n// minute windows, for example, call Reset every five minutes.\nfunc (lc *loadCollector) reset() {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n\n// newSliceSummary returns a new sliceSummary for the provided slice with\n// initially 0 load.\nfunc newSliceSummary(slice slice) (*sliceSummary, error) {\n\t// Initialize the hyperloglog. A hyperloglog with n registers uses roughly\n\t// n bytes of memory. We choose n=1024 so that every hyperloglog takes\n\t// about a kilobyte of memory. Given that a weavelet should manage a\n\t// moderate number of slices and components, the total memory usage of all\n\t// hyperloglogs should be relatively small. New's documentation also\n\t// suggests that n be a power of 2.\n\tcount, err := hyperloglog.New(1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the reservoir sample. A reservoir sample of size n stores at\n\t// most n keys, or roughly 8n bytes. As with the hyperloglogs, this should\n\t// lead to a modest memory usage.\n\t//\n\t// TODO(mwhittaker): Compute the expected errors in our estimates based on\n\t// the size of the sample.\n\t// TODO(mwhittaker): When we switch to range sharding, keys might be large\n\t// and 1000 keys might be too big.\n\tr := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tsample := varopt.New[uint64](1000, r)\n\n\treturn &sliceSummary{slice: slice, count: count, sample: sample}, nil\n}\n\n// splits splits the slice into subslices with roughly even load.\nfunc (s *sliceSummary) splits(delta time.Duration) []*protos.LoadReport_SubsliceLoad {\n\t// Splits divides the slice into subslices of roughly even load. In the\n\t// normal case, Splits splits a slice into 20 subslices, each representing\n\t// 5% of the total load. If the number of samples is small, however, fewer\n\t// splits are used. Moreover, if adjacent splits are formed from a single\n\t// hot key, they are combined.\n\n\t// Materialize and sort the sample.\n\tk := s.sample.Size()\n\txs := make([]uint64, k)\n\tfor i := 0; i < k; i++ {\n\t\tx, _ := s.sample.Get(i)\n\t\txs[i] = x\n\t}\n\tsort.Slice(xs, func(i, j int) bool { return xs[i] < xs[j] })\n\n\t// Determine the number of splits. More splits is better, but if we don't\n\t// have many points in our sample, then using a large number of splits will\n\t// lead to inaccurate estimates.\n\tvar n int\n\tswitch {\n\tcase k < 10:\n\t\tn = 1 // 100%\n\tcase k < 50:\n\t\tn = 2 // 50%\n\tcase k < 100:\n\t\tn = 4 // 25%\n\tcase k < 250:\n\t\tn = 5 // 20%\n\tcase k < 500:\n\t\tn = 10 // 10%\n\tdefault:\n\t\tn = 20 // 5%\n\t}\n\n\t// Adjust the first subslice so that it starts at our slice boundary.\n\ttotalLoad := s.load / delta.Seconds()\n\tsplits := subslices(totalLoad, xs, n)\n\tsplits[0].Start = s.slice.start\n\n\t// Double check that the split loads sum to the total load.\n\tvar sum float64\n\tfor _, split := range splits {\n\t\tsum += split.Load\n\t}\n\tif !approxEqual(sum, totalLoad) {\n\t\tpanic(fmt.Sprintf(\"bad sum of split loads: got %f, want %f\", sum, totalLoad))\n\t}\n\n\treturn splits\n}\n\n// subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// percentiles returns n equally spaced percentiles of the provided sorted set\n// of points. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}\n// and n = 4, percentiles will return []uint64{10, 30, 50, 70} where\n//\n//   - 10 is the 0th percentile,\n//   - 30 is the 25th percentile,\n//   - 50 is the 50th percentile,\n//   - 70 is the 75th percentile,\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n\n// index is a read-only search index of a protos.Assignment, optimized to\n// find the slice that contains a key.\ntype index []slice\n\n// slice is the segment [start, end) of the key space, along with a set of\n// assigned replicas.\ntype slice struct {\n\tstart      uint64          // start of slice, inclusive\n\tend        uint64          // end of slice, exclusive\n\treplicas   []string        // replicas assigned to this slice\n\treplicaSet map[string]bool // replicas assigned to this slice\n}\n\n// newIndex returns a new index of the provided assignment.\nfunc newIndex(proto *protos.Assignment) index {\n\tn := len(proto.Slices)\n\tslices := make([]slice, n)\n\tfor i := 0; i < n; i++ {\n\t\t// Gather the set of replicas.\n\t\treplicas := proto.Slices[i].Replicas\n\t\treplicaSet := make(map[string]bool, len(replicas))\n\t\tfor _, replica := range replicas {\n\t\t\treplicaSet[replica] = true\n\t\t}\n\n\t\t// Compute the end of the slice.\n\t\tvar end uint64 = math.MaxUint64\n\t\tif i < n-1 {\n\t\t\tend = proto.Slices[i+1].Start\n\t\t}\n\n\t\t// Form the slice.\n\t\tslices[i] = slice{\n\t\t\tstart:      proto.Slices[i].Start,\n\t\t\tend:        end,\n\t\t\treplicas:   replicas,\n\t\t\treplicaSet: replicaSet,\n\t\t}\n\t}\n\treturn slices\n}\n\n// find finds the slice that contains the given key in O(log n) time where n is\n// the number of slices in the assignment.\nfunc (ind index) find(key uint64) (slice, bool) {\n\ti := sort.Search(len(ind), func(i int) bool {\n\t\treturn key < ind[i].end\n\t})\n\tif i == len(ind) {\n\t\treturn slice{}, false\n\t}\n\treturn ind[i], true\n}\n\n```\n# Function Signature: func (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n# Natural Language Annotations: // updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\n\t// Update the assignment\n\tlc.assignment = assignment\n\n\t// Update the index with the new assignment\n\tlc.index = newIndex(assignment)\n\n\t// Clear the accumulated load\n\tlc.reset()\n}\n```"]}, "weaver-internal/weaver/load.go-report": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/load.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/DataDog/hyperloglog\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/lightstep/varopt\"\n)\n\nfunc approxEqual(a, b float64) bool {\n\tconst float64EqualityThreshold = 1e-9\n\treturn math.Abs(a-b) <= float64EqualityThreshold\n}\n\n// TODO(mwhittaker): Right now, load collection is slow. It grabs a mutex\n// every time the load needs to be updated. Make this faster.\n\n// loadCollector collects load for a Service Weaver component. As an example, imagine we\n// have a load collector lc for a Service Weaver component that owns slices [0, 10) and\n// [100, 200). We add the following load over the course of a second.\n//\n//   - lc.Add(0, 1)\n//   - lc.Add(1, 1)\n//   - lc.Add(2, 1)\n//   - lc.Add(3, 1)\n//   - lc.Add(100, 1)\n//   - lc.Add(101, 1)\n//\n// The load collector will report a load of 4 requests per second on the slice\n// [0, 10) and a load of 2 requests per second on the slice [100, 200).\ntype loadCollector struct {\n\tcomponent string           // Service Weaver component\n\taddr      string           // dialable address found in assignments\n\tnow       func() time.Time // time.Now usually, but injected fake in tests\n\n\tmu         sync.Mutex               // guards the following fields\n\tassignment *protos.Assignment       // latest assignment\n\tindex      index                    // index on assignment\n\tstart      time.Time                // start of load collection\n\tslices     map[uint64]*sliceSummary // keyed by start of slice\n}\n\n// sliceSummary contains a summary of the observed keys and load of a slice for\n// a replica.\ntype sliceSummary struct {\n\tslice  slice                    // the slice\n\tload   float64                  // total load\n\tcount  *hyperloglog.HyperLogLog // counts distinct elements\n\tsample *varopt.Varopt[uint64]   // reservoir sample of keys\n}\n\n// newLoadCollector returns a new load collector. Note that load is collected\n// with respect to an assignment, so load won't be collected until\n// UpdateAssignment is called.\nfunc newLoadCollector(component string, addr string) *loadCollector {\n\treturn &loadCollector{\n\t\tcomponent: component,\n\t\taddr:      addr,\n\t\tnow:       func() time.Time { return time.Now() },\n\t\tstart:     time.Now(),\n\t\tslices:    map[uint64]*sliceSummary{},\n\t}\n}\n\n// add adds load for the provided key.\nfunc (lc *loadCollector) add(key uint64, v float64) error {\n\tif v != 1.0 {\n\t\tpanic(\"load != 1.0 not yet implemented\")\n\t}\n\n\t// Find the corresponding slice.\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\tslice, found := lc.index.find(key)\n\tif !found {\n\t\t// TODO(mwhittaker): It is currently possible to receive a request for\n\t\t// a key that is not in our current assignment. For example, a\n\t\t// different weavelet may have an older or newer version of the current\n\t\t// assignment and send us keys not in our current assignment. In the\n\t\t// future, we may want to catch these requests and discard them. For\n\t\t// now, we execute them.\n\t\treturn nil\n\t}\n\tif !slice.replicaSet[lc.addr] {\n\t\treturn nil\n\t}\n\n\tsummary, found := lc.slices[slice.start]\n\tif !found {\n\t\tvar err error\n\t\tsummary, err = newSliceSummary(slice)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.slices[slice.start] = summary\n\t}\n\n\t// Update the load.\n\tsummary.load += v\n\n\t// Update the count. Note that we compute a hash of our key before passing\n\t// it to the hyperloglog, even though the key is itself a hash. The reason\n\t// is that this slice represents only a small sliver of the total hash\n\t// space. To operate correctly, a hyperloglog assumes values are drawn\n\t// uniformly from the space of all uint32s, so if we feed the hyperloglog\n\t// values only from this slice, the count will be inaccurate.\n\t//\n\t// TODO(mwhittaker): Compute the hash outside of the lock?\n\t// TODO(mwhittaker): Use a different sketch?\n\t// TODO(mwhittaker): If the slice is small (< 1024), we can count the\n\t// number of distinct elements exactly. Don't use a hyperloglog here.\n\t// TODO(mwhittaker): Start with an exact count and only switch to a\n\t// hyperloglog if the number of unique elements gets too big?\n\tsummary.count.Add(hyperloglog.Murmur64(key))\n\n\t// Update the sample. Note that Add takes in a key and a weight, but we are\n\t// recording unweighted samples, so we use a constant weight of 1.0 for\n\t// every key.\n\tif _, err := summary.sample.Add(key, 1.0); err != nil {\n\t\treturn fmt.Errorf(\"cannot sample %d: %v\", key, err)\n\t}\n\treturn nil\n}\n\n// updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\n\n\n\n\n\n\n\n\n\n\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// reset resets the load collector. If you want to collect load over 5\n// minute windows, for example, call Reset every five minutes.\nfunc (lc *loadCollector) reset() {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n\n// newSliceSummary returns a new sliceSummary for the provided slice with\n// initially 0 load.\nfunc newSliceSummary(slice slice) (*sliceSummary, error) {\n\t// Initialize the hyperloglog. A hyperloglog with n registers uses roughly\n\t// n bytes of memory. We choose n=1024 so that every hyperloglog takes\n\t// about a kilobyte of memory. Given that a weavelet should manage a\n\t// moderate number of slices and components, the total memory usage of all\n\t// hyperloglogs should be relatively small. New's documentation also\n\t// suggests that n be a power of 2.\n\tcount, err := hyperloglog.New(1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the reservoir sample. A reservoir sample of size n stores at\n\t// most n keys, or roughly 8n bytes. As with the hyperloglogs, this should\n\t// lead to a modest memory usage.\n\t//\n\t// TODO(mwhittaker): Compute the expected errors in our estimates based on\n\t// the size of the sample.\n\t// TODO(mwhittaker): When we switch to range sharding, keys might be large\n\t// and 1000 keys might be too big.\n\tr := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tsample := varopt.New[uint64](1000, r)\n\n\treturn &sliceSummary{slice: slice, count: count, sample: sample}, nil\n}\n\n// splits splits the slice into subslices with roughly even load.\nfunc (s *sliceSummary) splits(delta time.Duration) []*protos.LoadReport_SubsliceLoad {\n\t// Splits divides the slice into subslices of roughly even load. In the\n\t// normal case, Splits splits a slice into 20 subslices, each representing\n\t// 5% of the total load. If the number of samples is small, however, fewer\n\t// splits are used. Moreover, if adjacent splits are formed from a single\n\t// hot key, they are combined.\n\n\t// Materialize and sort the sample.\n\tk := s.sample.Size()\n\txs := make([]uint64, k)\n\tfor i := 0; i < k; i++ {\n\t\tx, _ := s.sample.Get(i)\n\t\txs[i] = x\n\t}\n\tsort.Slice(xs, func(i, j int) bool { return xs[i] < xs[j] })\n\n\t// Determine the number of splits. More splits is better, but if we don't\n\t// have many points in our sample, then using a large number of splits will\n\t// lead to inaccurate estimates.\n\tvar n int\n\tswitch {\n\tcase k < 10:\n\t\tn = 1 // 100%\n\tcase k < 50:\n\t\tn = 2 // 50%\n\tcase k < 100:\n\t\tn = 4 // 25%\n\tcase k < 250:\n\t\tn = 5 // 20%\n\tcase k < 500:\n\t\tn = 10 // 10%\n\tdefault:\n\t\tn = 20 // 5%\n\t}\n\n\t// Adjust the first subslice so that it starts at our slice boundary.\n\ttotalLoad := s.load / delta.Seconds()\n\tsplits := subslices(totalLoad, xs, n)\n\tsplits[0].Start = s.slice.start\n\n\t// Double check that the split loads sum to the total load.\n\tvar sum float64\n\tfor _, split := range splits {\n\t\tsum += split.Load\n\t}\n\tif !approxEqual(sum, totalLoad) {\n\t\tpanic(fmt.Sprintf(\"bad sum of split loads: got %f, want %f\", sum, totalLoad))\n\t}\n\n\treturn splits\n}\n\n// subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// percentiles returns n equally spaced percentiles of the provided sorted set\n// of points. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}\n// and n = 4, percentiles will return []uint64{10, 30, 50, 70} where\n//\n//   - 10 is the 0th percentile,\n//   - 30 is the 25th percentile,\n//   - 50 is the 50th percentile,\n//   - 70 is the 75th percentile,\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n\n// index is a read-only search index of a protos.Assignment, optimized to\n// find the slice that contains a key.\ntype index []slice\n\n// slice is the segment [start, end) of the key space, along with a set of\n// assigned replicas.\ntype slice struct {\n\tstart      uint64          // start of slice, inclusive\n\tend        uint64          // end of slice, exclusive\n\treplicas   []string        // replicas assigned to this slice\n\treplicaSet map[string]bool // replicas assigned to this slice\n}\n\n// newIndex returns a new index of the provided assignment.\nfunc newIndex(proto *protos.Assignment) index {\n\tn := len(proto.Slices)\n\tslices := make([]slice, n)\n\tfor i := 0; i < n; i++ {\n\t\t// Gather the set of replicas.\n\t\treplicas := proto.Slices[i].Replicas\n\t\treplicaSet := make(map[string]bool, len(replicas))\n\t\tfor _, replica := range replicas {\n\t\t\treplicaSet[replica] = true\n\t\t}\n\n\t\t// Compute the end of the slice.\n\t\tvar end uint64 = math.MaxUint64\n\t\tif i < n-1 {\n\t\t\tend = proto.Slices[i+1].Start\n\t\t}\n\n\t\t// Form the slice.\n\t\tslices[i] = slice{\n\t\t\tstart:      proto.Slices[i].Start,\n\t\t\tend:        end,\n\t\t\treplicas:   replicas,\n\t\t\treplicaSet: replicaSet,\n\t\t}\n\t}\n\treturn slices\n}\n\n// find finds the slice that contains the given key in O(log n) time where n is\n// the number of slices in the assignment.\nfunc (ind index) find(key uint64) (slice, bool) {\n\ti := sort.Search(len(ind), func(i int) bool {\n\t\treturn key < ind[i].end\n\t})\n\tif i == len(ind) {\n\t\treturn slice{}, false\n\t}\n\treturn ind[i], true\n}\n\n```\n# Function Signature: func (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n# Natural Language Annotations: // report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then report returns nil.\nfunc (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\n\tif lc.assignment == nil || len(lc.slices) == 0 {\n\t\t// No assignment or no collected load, return nil.\n\t\treturn nil\n\t}\n\n\t// Calculate the duration over which the load has been collected.\n\tduration := lc.now().Sub(lc.start)\n\n\t// Prepare the slices load report.\n\tsliceLoads := make([]*protos.LoadReport_SliceLoad, 0, len(lc.slices))\n\tfor _, summary := range lc.slices {\n\t\t// Calculate the subslice loads for this slice.\n\t\tsubsliceLoads := summary.splits(duration)\n\n\t\t// Create the slice load report.\n\t\tsliceLoad := &protos.LoadReport_SliceLoad{\n\t\t\tStart:     summary.slice.start,\n\t\t\tEnd:       summary.slice.end,\n\t\t\tLoad:      summary.load / duration.Seconds(),\n\t\t\tSubSlices: subsliceLoads,\n\t\t}\n\t\tsliceLoads = append(sliceLoads, sliceLoad)\n\t}\n\n\t// Create and return the component load report.\n\tcomponentLoad := &protos.LoadReport_ComponentLoad{\n\t\tComponent: lc.component,\n\t\tSlices:    sliceLoads,\n\t}\n\treturn componentLoad\n}\n```"]}, "weaver-internal/weaver/load.go-subslices": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/load.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/DataDog/hyperloglog\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/lightstep/varopt\"\n)\n\nfunc approxEqual(a, b float64) bool {\n\tconst float64EqualityThreshold = 1e-9\n\treturn math.Abs(a-b) <= float64EqualityThreshold\n}\n\n// TODO(mwhittaker): Right now, load collection is slow. It grabs a mutex\n// every time the load needs to be updated. Make this faster.\n\n// loadCollector collects load for a Service Weaver component. As an example, imagine we\n// have a load collector lc for a Service Weaver component that owns slices [0, 10) and\n// [100, 200). We add the following load over the course of a second.\n//\n//   - lc.Add(0, 1)\n//   - lc.Add(1, 1)\n//   - lc.Add(2, 1)\n//   - lc.Add(3, 1)\n//   - lc.Add(100, 1)\n//   - lc.Add(101, 1)\n//\n// The load collector will report a load of 4 requests per second on the slice\n// [0, 10) and a load of 2 requests per second on the slice [100, 200).\ntype loadCollector struct {\n\tcomponent string           // Service Weaver component\n\taddr      string           // dialable address found in assignments\n\tnow       func() time.Time // time.Now usually, but injected fake in tests\n\n\tmu         sync.Mutex               // guards the following fields\n\tassignment *protos.Assignment       // latest assignment\n\tindex      index                    // index on assignment\n\tstart      time.Time                // start of load collection\n\tslices     map[uint64]*sliceSummary // keyed by start of slice\n}\n\n// sliceSummary contains a summary of the observed keys and load of a slice for\n// a replica.\ntype sliceSummary struct {\n\tslice  slice                    // the slice\n\tload   float64                  // total load\n\tcount  *hyperloglog.HyperLogLog // counts distinct elements\n\tsample *varopt.Varopt[uint64]   // reservoir sample of keys\n}\n\n// newLoadCollector returns a new load collector. Note that load is collected\n// with respect to an assignment, so load won't be collected until\n// UpdateAssignment is called.\nfunc newLoadCollector(component string, addr string) *loadCollector {\n\treturn &loadCollector{\n\t\tcomponent: component,\n\t\taddr:      addr,\n\t\tnow:       func() time.Time { return time.Now() },\n\t\tstart:     time.Now(),\n\t\tslices:    map[uint64]*sliceSummary{},\n\t}\n}\n\n// add adds load for the provided key.\nfunc (lc *loadCollector) add(key uint64, v float64) error {\n\tif v != 1.0 {\n\t\tpanic(\"load != 1.0 not yet implemented\")\n\t}\n\n\t// Find the corresponding slice.\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\tslice, found := lc.index.find(key)\n\tif !found {\n\t\t// TODO(mwhittaker): It is currently possible to receive a request for\n\t\t// a key that is not in our current assignment. For example, a\n\t\t// different weavelet may have an older or newer version of the current\n\t\t// assignment and send us keys not in our current assignment. In the\n\t\t// future, we may want to catch these requests and discard them. For\n\t\t// now, we execute them.\n\t\treturn nil\n\t}\n\tif !slice.replicaSet[lc.addr] {\n\t\treturn nil\n\t}\n\n\tsummary, found := lc.slices[slice.start]\n\tif !found {\n\t\tvar err error\n\t\tsummary, err = newSliceSummary(slice)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.slices[slice.start] = summary\n\t}\n\n\t// Update the load.\n\tsummary.load += v\n\n\t// Update the count. Note that we compute a hash of our key before passing\n\t// it to the hyperloglog, even though the key is itself a hash. The reason\n\t// is that this slice represents only a small sliver of the total hash\n\t// space. To operate correctly, a hyperloglog assumes values are drawn\n\t// uniformly from the space of all uint32s, so if we feed the hyperloglog\n\t// values only from this slice, the count will be inaccurate.\n\t//\n\t// TODO(mwhittaker): Compute the hash outside of the lock?\n\t// TODO(mwhittaker): Use a different sketch?\n\t// TODO(mwhittaker): If the slice is small (< 1024), we can count the\n\t// number of distinct elements exactly. Don't use a hyperloglog here.\n\t// TODO(mwhittaker): Start with an exact count and only switch to a\n\t// hyperloglog if the number of unique elements gets too big?\n\tsummary.count.Add(hyperloglog.Murmur64(key))\n\n\t// Update the sample. Note that Add takes in a key and a weight, but we are\n\t// recording unweighted samples, so we use a constant weight of 1.0 for\n\t// every key.\n\tif _, err := summary.sample.Add(key, 1.0); err != nil {\n\t\treturn fmt.Errorf(\"cannot sample %d: %v\", key, err)\n\t}\n\treturn nil\n}\n\n// updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\n\n\n\n\n\n\n\n\n\n\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// reset resets the load collector. If you want to collect load over 5\n// minute windows, for example, call Reset every five minutes.\nfunc (lc *loadCollector) reset() {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n\n// newSliceSummary returns a new sliceSummary for the provided slice with\n// initially 0 load.\nfunc newSliceSummary(slice slice) (*sliceSummary, error) {\n\t// Initialize the hyperloglog. A hyperloglog with n registers uses roughly\n\t// n bytes of memory. We choose n=1024 so that every hyperloglog takes\n\t// about a kilobyte of memory. Given that a weavelet should manage a\n\t// moderate number of slices and components, the total memory usage of all\n\t// hyperloglogs should be relatively small. New's documentation also\n\t// suggests that n be a power of 2.\n\tcount, err := hyperloglog.New(1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the reservoir sample. A reservoir sample of size n stores at\n\t// most n keys, or roughly 8n bytes. As with the hyperloglogs, this should\n\t// lead to a modest memory usage.\n\t//\n\t// TODO(mwhittaker): Compute the expected errors in our estimates based on\n\t// the size of the sample.\n\t// TODO(mwhittaker): When we switch to range sharding, keys might be large\n\t// and 1000 keys might be too big.\n\tr := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tsample := varopt.New[uint64](1000, r)\n\n\treturn &sliceSummary{slice: slice, count: count, sample: sample}, nil\n}\n\n// splits splits the slice into subslices with roughly even load.\nfunc (s *sliceSummary) splits(delta time.Duration) []*protos.LoadReport_SubsliceLoad {\n\t// Splits divides the slice into subslices of roughly even load. In the\n\t// normal case, Splits splits a slice into 20 subslices, each representing\n\t// 5% of the total load. If the number of samples is small, however, fewer\n\t// splits are used. Moreover, if adjacent splits are formed from a single\n\t// hot key, they are combined.\n\n\t// Materialize and sort the sample.\n\tk := s.sample.Size()\n\txs := make([]uint64, k)\n\tfor i := 0; i < k; i++ {\n\t\tx, _ := s.sample.Get(i)\n\t\txs[i] = x\n\t}\n\tsort.Slice(xs, func(i, j int) bool { return xs[i] < xs[j] })\n\n\t// Determine the number of splits. More splits is better, but if we don't\n\t// have many points in our sample, then using a large number of splits will\n\t// lead to inaccurate estimates.\n\tvar n int\n\tswitch {\n\tcase k < 10:\n\t\tn = 1 // 100%\n\tcase k < 50:\n\t\tn = 2 // 50%\n\tcase k < 100:\n\t\tn = 4 // 25%\n\tcase k < 250:\n\t\tn = 5 // 20%\n\tcase k < 500:\n\t\tn = 10 // 10%\n\tdefault:\n\t\tn = 20 // 5%\n\t}\n\n\t// Adjust the first subslice so that it starts at our slice boundary.\n\ttotalLoad := s.load / delta.Seconds()\n\tsplits := subslices(totalLoad, xs, n)\n\tsplits[0].Start = s.slice.start\n\n\t// Double check that the split loads sum to the total load.\n\tvar sum float64\n\tfor _, split := range splits {\n\t\tsum += split.Load\n\t}\n\tif !approxEqual(sum, totalLoad) {\n\t\tpanic(fmt.Sprintf(\"bad sum of split loads: got %f, want %f\", sum, totalLoad))\n\t}\n\n\treturn splits\n}\n\n// subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// percentiles returns n equally spaced percentiles of the provided sorted set\n// of points. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}\n// and n = 4, percentiles will return []uint64{10, 30, 50, 70} where\n//\n//   - 10 is the 0th percentile,\n//   - 30 is the 25th percentile,\n//   - 50 is the 50th percentile,\n//   - 70 is the 75th percentile,\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n\n// index is a read-only search index of a protos.Assignment, optimized to\n// find the slice that contains a key.\ntype index []slice\n\n// slice is the segment [start, end) of the key space, along with a set of\n// assigned replicas.\ntype slice struct {\n\tstart      uint64          // start of slice, inclusive\n\tend        uint64          // end of slice, exclusive\n\treplicas   []string        // replicas assigned to this slice\n\treplicaSet map[string]bool // replicas assigned to this slice\n}\n\n// newIndex returns a new index of the provided assignment.\nfunc newIndex(proto *protos.Assignment) index {\n\tn := len(proto.Slices)\n\tslices := make([]slice, n)\n\tfor i := 0; i < n; i++ {\n\t\t// Gather the set of replicas.\n\t\treplicas := proto.Slices[i].Replicas\n\t\treplicaSet := make(map[string]bool, len(replicas))\n\t\tfor _, replica := range replicas {\n\t\t\treplicaSet[replica] = true\n\t\t}\n\n\t\t// Compute the end of the slice.\n\t\tvar end uint64 = math.MaxUint64\n\t\tif i < n-1 {\n\t\t\tend = proto.Slices[i+1].Start\n\t\t}\n\n\t\t// Form the slice.\n\t\tslices[i] = slice{\n\t\t\tstart:      proto.Slices[i].Start,\n\t\t\tend:        end,\n\t\t\treplicas:   replicas,\n\t\t\treplicaSet: replicaSet,\n\t\t}\n\t}\n\treturn slices\n}\n\n// find finds the slice that contains the given key in O(log n) time where n is\n// the number of slices in the assignment.\nfunc (ind index) find(key uint64) (slice, bool) {\n\ti := sort.Search(len(ind), func(i int) bool {\n\t\treturn key < ind[i].end\n\t})\n\tif i == len(ind) {\n\t\treturn slice{}, false\n\t}\n\treturn ind[i], true\n}\n\n```\n# Function Signature: func subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n# Natural Language Annotations: // subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n\tif len(xs) == 0 || n <= 0 {\n\t\treturn nil\n\t}\n\n\t// Calculate the load per subslice\n\tloadPerSubslice := load / float64(n)\n\n\t// Determine the percentiles for the n slices\n\tsplitPoints := percentiles(xs, n)\n\n\t// Create the subslices\n\tsubslices := make([]*protos.LoadReport_SubsliceLoad, n)\n\n\tfor i := 0; i < n; i++ {\n\t\tstart := splitPoints[i]\n\n\t\t// Determine the end of the current subslice\n\t\tvar end uint64\n\t\tif i == n-1 {\n\t\t\tend = math.MaxUint64 // Use infinity for the last subslice\n\t\t} else {\n\t\t\tend = splitPoints[i+1]\n\t\t}\n\n\t\tsubslices[i] = &protos.LoadReport_SubsliceLoad{\n\t\t\tStart: start,\n\t\t\tLoad:  loadPerSubslice,\n\t\t}\n\t}\n\n\treturn subslices\n}\n```"]}, "weaver-internal/weaver/routing.go-update": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/routing.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"context\"\n\t\"crypto/tls\"\n\t\"math/rand\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/cond\"\n\t\"github.com/ServiceWeaver/weaver/internal/net/call\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/google/uuid\"\n)\n\n// routingBalancer balances requests according to a routing assignment.\ntype routingBalancer struct {\n\tbalancer  call.Balancer // balancer to use for non-routed calls\n\ttlsConfig *tls.Config   // tls config to use; may be nil.\n\n\tmu         sync.RWMutex\n\tassignment *protos.Assignment\n\tindex      index\n\n\t// Map from address to connection. We currently allow just one\n\t// connection per address.\n\t// Guarded by mu.\n\tconns map[string]call.ReplicaConnection\n}\n\n// newRoutingBalancer returns a new routingBalancer.\nfunc newRoutingBalancer(tlsConfig *tls.Config) *routingBalancer {\n\treturn &routingBalancer{\n\t\tbalancer:  call.RoundRobin(),\n\t\ttlsConfig: tlsConfig,\n\t\tconns:     map[string]call.ReplicaConnection{},\n\t}\n}\n\n// Add adds c to the set of connections we are balancing across.\nfunc (rb *routingBalancer) Add(c call.ReplicaConnection) {\n\trb.balancer.Add(c)\n\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\trb.conns[c.Address()] = c\n}\n\n// Remove removes c from the set of connections we are balancing across.\nfunc (rb *routingBalancer) Remove(c call.ReplicaConnection) {\n\trb.balancer.Remove(c)\n\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\tdelete(rb.conns, c.Address())\n}\n\n// update updates the balancer with the provided assignment\n\n\n\n\n\n\n\n\n\n\n\n\n// Pick implements the call.Balancer interface.\nfunc (rb *routingBalancer) Pick(opts call.CallOptions) (call.ReplicaConnection, bool) {\n\tif opts.ShardKey == 0 {\n\t\t// If the method we're calling is not sharded (which is guaranteed to\n\t\t// be true for nonsharded components), then the shard key is 0.\n\t\treturn rb.balancer.Pick(opts)\n\t}\n\n\t// Grab the current assignment. It's possible that the current assignment\n\t// changes between when we release the lock and when we pick an endpoint,\n\t// but using a slightly stale assignment is okay.\n\trb.mu.RLock()\n\tassignment := rb.assignment\n\tindex := rb.index\n\trb.mu.RUnlock()\n\n\tif assignment == nil {\n\t\t// There is no assignment. This is possible if we haven't received an\n\t\t// assignment from the assigner yet.\n\t\treturn rb.balancer.Pick(opts)\n\t}\n\n\tslice, ok := index.find(opts.ShardKey)\n\tif !ok {\n\t\t// TODO(mwhittaker): Shouldn't this be impossible. Understand better\n\t\t// when this happens.\n\t\treturn rb.balancer.Pick(opts)\n\t}\n\n\t// Search for an available ReplicConnection starting at a random offset.\n\t// TODO(sanjay):Precompute the set of available ReplicaConnections per slice.\n\toffset := rand.Intn(len(slice.replicas))\n\trb.mu.RLock()\n\tdefer rb.mu.RUnlock()\n\tfor i, n := 0, len(slice.replicas); i < n; i++ {\n\t\toffset++\n\t\tif offset == n {\n\t\t\toffset = 0\n\t\t}\n\t\tif c, ok := rb.conns[slice.replicas[offset]]; ok {\n\t\t\treturn c, true\n\t\t}\n\t}\n\treturn nil, false\n}\n\n// routingResolver is a dummy resolver that returns whatever endpoints are\n// passed to the update method.\ntype routingResolver struct {\n\tm         sync.Mutex      // guards all of the following fields\n\tchanged   cond.Cond       // fires when endpoints changes\n\tversion   *call.Version   // the current version of endpoints\n\tendpoints []call.Endpoint // the endpoints returned by Resolve\n}\n\n// newRoutingResolver returns a new routingResolver.\nfunc newRoutingResolver() *routingResolver {\n\tr := &routingResolver{\n\t\tversion: &call.Version{Opaque: call.Missing.Opaque},\n\t}\n\tr.changed.L = &r.m\n\treturn r\n}\n\n// IsConstant implements the call.Resolver interface.\nfunc (rr *routingResolver) IsConstant() bool { return false }\n\n// update updates the resolver with the provided endpoints.\nfunc (rr *routingResolver) update(endpoints []call.Endpoint) {\n\trr.m.Lock()\n\tdefer rr.m.Unlock()\n\trr.version = &call.Version{Opaque: uuid.New().String()}\n\trr.endpoints = endpoints\n\trr.changed.Broadcast()\n}\n\n// Resolve implements the call.Resolver interface.\nfunc (rr *routingResolver) Resolve(ctx context.Context, version *call.Version) ([]call.Endpoint, *call.Version, error) {\n\trr.m.Lock()\n\tdefer rr.m.Unlock()\n\n\tif version == nil {\n\t\treturn rr.endpoints, rr.version, nil\n\t}\n\n\tfor *version == *rr.version {\n\t\tif err := rr.changed.Wait(ctx); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\treturn rr.endpoints, rr.version, nil\n}\n\n```\n# Function Signature: func (rb *routingBalancer) update(assignment *protos.Assignment) {\n# Natural Language Annotations: // update updates the balancer with the provided assignment\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (rb *routingBalancer) update(assignment *protos.Assignment) {\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\n\t// Update the current assignment.\n\trb.assignment = assignment\n\n\t// Update the index with the new assignment.\n\trb.index = newIndex(assignment) // Assuming newIndex is a function that creates an index from the assignment.\n\n\t// Iterate over all replicas in the assignment and ensure a connection for each.\n\tfor _, route := range assignment.Routes {\n\t\tfor _, replica := range route.Replicas {\n\t\t\tif _, ok := rb.conns[replica.Address]; !ok {\n\t\t\t\t// If we don't have a connection for the replica, create and add one.\n\t\t\t\tconn := call.NewReplicaConnection(replica, rb.tlsConfig) // Assuming a constructor function like this.\n\t\t\t\trb.balancer.Add(conn)\n\t\t\t\trb.conns[replica.Address] = conn\n\t\t\t}\n\t\t}\n\t}\n\n\t// Remove connections that are no longer part of the new assignment.\n\tfor address, conn := range rb.conns {\n\t\tif !assignment.ContainsReplica(address) { // Assuming ContainsReplica checks if any route contains the given address.\n\t\t\trb.balancer.Remove(conn)\n\t\t\tdelete(rb.conns, address)\n\t\t}\n\t}\n}\n```"]}, "weaver-internal/status/registry.go-NewRegistry": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/status/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package status contains code for implementing status related commands like\n// \"weaver multi status\" and \"weaver single dashboard\".\npackage status\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"syscall\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n)\n\n// A Registry is a persistent collection of Service Weaver application metadata.\n//\n// Tools like \"weaver multi status\" and \"weaver multi dashboard\" use the registry\n// to know which applications are running and to fetch the status of the\n// running applications.\ntype Registry struct {\n\t// A Registry stores registrations as files in a directory. Every\n\t// registration r is stored in a JSON file called\n\t// {r.DeploymentId}.registration.json.\n\t//\n\t// TODO(mwhittaker): Store as protos instead of JSON?\n\tdir string\n\n\t// newClient returns a new status client that curls the provided address.\n\t// It is a field of Registry to enable dependency injection in\n\t// registry_test.go.\n\tnewClient func(string) Server\n}\n\n// A Registration contains basic metadata about a Service Weaver application.\ntype Registration struct {\n\tDeploymentId string // deployment id (e.g, \"eba18295\")\n\tApp          string // app name (e.g., \"todo\")\n\tAddr         string // status server (e.g., \"localhost:12345\")\n}\n\n// Rolodex returns a pretty-printed rolodex displaying the registration.\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 app        : collatz                              \u2502\n//\t\u2502 deployment : fdeeb059-825b-4606-9e99-e22e63e10552 \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfunc (r Registration) Rolodex() string {\n\t// Declare the contents.\n\ttype kv struct {\n\t\tkey string\n\t\tval colors.Text\n\t}\n\tprefix, suffix := formatId(r.DeploymentId)\n\tkvs := []kv{\n\t\t{\"app        :\", colors.Text{colors.Atom{S: r.App}}},\n\t\t{\"deployment :\", colors.Text{prefix, suffix}},\n\t}\n\n\tlength := func(t colors.Text) int {\n\t\tvar n int\n\t\tfor _, a := range t {\n\t\t\tn += len(a.S)\n\t\t}\n\t\treturn n\n\t}\n\n\t// Calculate widths.\n\tvalWidth := 0\n\tfor _, kv := range kvs {\n\t\tif length(kv.val) > valWidth {\n\t\t\tvalWidth = length(kv.val)\n\t\t}\n\t}\n\twidth := valWidth + len(kvs[0].key) + 5\n\n\t// Pretty print.\n\tvar b strings.Builder\n\tfmt.Fprintf(&b, \"\u256d%s\u256e\\n\", strings.Repeat(\"\u2500\", width-2))\n\tfor _, kv := range kvs {\n\t\ts := kv.val.String()\n\t\tfmt.Fprintf(&b, \"\u2502 %s %-*s \u2502\\n\", kv.key, valWidth+len(s)-length(kv.val), s)\n\t}\n\tfmt.Fprintf(&b, \"\u2570%s\u256f\\n\", strings.Repeat(\"\u2500\", width-2))\n\treturn b.String()\n}\n\n// NewRegistry returns a registry that persists data to the provided directory.\n\n\n\n\n\n\n\n\n\n\n\n\n// Register adds a registration to the registry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a registration from the registry.\nfunc (r *Registry) Unregister(_ context.Context, deploymentId string) error {\n\tfilename := fmt.Sprintf(\"%s.registration.json\", deploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tif err := os.Remove(filename); err != nil {\n\t\treturn fmt.Errorf(\"registry: remove %q: %w\", filename, err)\n\t}\n\treturn nil\n}\n\n// Get returns the Registration for the provided deployment. If the deployment\n// doesn't exist or is not active, a non-nil error is returned.\nfunc (r *Registry) Get(ctx context.Context, deploymentId string) (Registration, error) {\n\t// TODO(mwhittaker): r.list() reads and parses every registration file.\n\t// This is inefficient, as we could instead stop reading and parsing as\n\t// soon as we find the corresponding registration file. Even more\n\t// efficient, we could match the deploymentId to the filenames instead of\n\t// reading and parsing the files. Since the number of registrations is\n\t// small, and the size of every registration file is small, I don't think\n\t// these optimizations are urgently needed.\n\tregs, err := r.list()\n\tif err != nil {\n\t\treturn Registration{}, err\n\t}\n\tfor _, reg := range regs {\n\t\tif reg.DeploymentId != deploymentId {\n\t\t\tcontinue\n\t\t}\n\t\tif r.dead(ctx, reg) {\n\t\t\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n\t\t}\n\t\treturn reg, nil\n\t}\n\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n}\n\n// List returns all active Registrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// list returns all registrations, dead or alive.\nfunc (r *Registry) list() ([]Registration, error) {\n\tentries, err := os.ReadDir(r.dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"registry: read dir %q: %w\", r.dir, err)\n\t}\n\n\tvar regs []Registration\n\tfor _, entry := range entries {\n\t\tif !strings.HasSuffix(entry.Name(), \".registration.json\") {\n\t\t\t// Ignore non-registration files in the registry directory.\n\t\t\tcontinue\n\t\t}\n\t\tfilename := filepath.Join(r.dir, entry.Name())\n\t\tbytes, err := os.ReadFile(filename)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: read file %q: %w\", filename, err)\n\t\t}\n\t\tvar reg Registration\n\t\tif err := json.Unmarshal(bytes, &reg); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: decode file %q: %w\", filename, err)\n\t\t}\n\t\tregs = append(regs, reg)\n\t}\n\treturn regs, nil\n}\n\n// dead returns whether the provided registration is associated with a\n// deployment that is definitely dead.\nfunc (r *Registry) dead(ctx context.Context, reg Registration) bool {\n\tstatus, err := r.newClient(reg.Addr).Status(ctx)\n\tswitch {\n\tcase errors.Is(err, syscall.ECONNREFUSED):\n\t\t// There is no status server for this deployment, so we consider\n\t\t// the deployment dead.\n\t\treturn true\n\tcase errors.Is(err, syscall.Errno(10061)):\n\t\t// The syscall.ECONNREFUSED doesn't work on Windows. Windows will\n\t\t// return WSAECONNREFUSED(syscall.Errno = 10061) when the connection is\n\t\t// refused.\n\t\treturn true\n\tcase err != nil:\n\t\t// Something went wrong. The deployment may be dead, but we're not 100%\n\t\t// sure, so we return false.\n\t\treturn false\n\tcase status.DeploymentId != reg.DeploymentId:\n\t\t// The status server for this deployment is dead and has been\n\t\t// superseded by a newer status server.\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n```\n# Function Signature: func NewRegistry(_ context.Context, dir string) (*Registry, error) {\n# Natural Language Annotations: // NewRegistry returns a registry that persists data to the provided directory.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// NewRegistry returns a registry that persists data to the provided directory.\nfunc NewRegistry(_ context.Context, dir string) (*Registry, error) {\n    // Ensure the directory exists, create it if it does not\n    if err := os.MkdirAll(dir, os.ModePerm); err != nil {\n        return nil, fmt.Errorf(\"unable to create directory %q: %w\", dir, err)\n    }\n\n    // Create a new Registry instance with the directory set\n    registry := &Registry{\n        dir: dir,\n        // Provide a default implementation for newClient\n        newClient: func(addr string) Server {\n            return &DefaultServer{address: addr}\n        },\n    }\n\n    return registry, nil\n}\n\n// DefaultServer is a simple implementation of the Server interface.\ntype DefaultServer struct {\n    address string\n}\n\n// Status is a stub method to implement the Server interface.\nfunc (d *DefaultServer) Status(ctx context.Context) (StatusResponse, error) {\n    // Implementation details would go here\n    // This is a stub and should be replaced with the actual logic\n    return StatusResponse{}, nil\n}\n\n// Server is an interface representing a status server.\ntype Server interface {\n    Status(ctx context.Context) (StatusResponse, error)\n}\n\n// StatusResponse is a struct representing the server's status response.\ntype StatusResponse struct {\n    DeploymentId string // Example field, adjust as necessary\n}\n```"]}, "weaver-internal/routing/routing.go-FormatAssignment": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/routing/routing.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package routing includes utilities for routing and assignments. See\n// https://serviceweaver.dev/docs.html#routing for more information on routing.\npackage routing\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// FormatAssignment pretty formats the provided assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// nextPowerOfTwo returns the least power of 2 that is greater or equal to x.\nfunc nextPowerOfTwo(x int) int {\n\tswitch {\n\tcase x == 0:\n\t\treturn 1\n\tcase x&(x-1) == 0:\n\t\t// x is already power of 2.\n\t\treturn x\n\tdefault:\n\t\treturn int(math.Pow(2, math.Ceil(math.Log2(float64(x)))))\n\t}\n}\n\n```\n# Function Signature: func FormatAssignment(a *protos.Assignment) string {\n# Natural Language Annotations: // FormatAssignment pretty formats the provided assignment.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FormatAssignment(a *protos.Assignment) string {\n\tif a == nil || len(a.Slices) == 0 {\n\t\treturn \"No assignment available\"\n\t}\n\n\tvar b strings.Builder\n\tb.WriteString(fmt.Sprintf(\"Assignment (Version: %d):\\n\", a.Version))\n\n\tfor i, slice := range a.Slices {\n\t\tb.WriteString(fmt.Sprintf(\"  Slice %d:\\n\", i+1))\n\t\tif len(slice.Replicas) == 0 {\n\t\t\tb.WriteString(\"    No replicas\\n\")\n\t\t} else {\n\t\t\tfor _, replica := range slice.Replicas {\n\t\t\t\tb.WriteString(fmt.Sprintf(\"    %s\\n\", replica))\n\t\t\t}\n\t\t}\n\t}\n\n\treturn b.String()\n}\n```"]}, "weaver-internal/routing/routing.go-EqualSlices": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/routing/routing.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package routing includes utilities for routing and assignments. See\n// https://serviceweaver.dev/docs.html#routing for more information on routing.\npackage routing\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// FormatAssignment pretty formats the provided assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// nextPowerOfTwo returns the least power of 2 that is greater or equal to x.\nfunc nextPowerOfTwo(x int) int {\n\tswitch {\n\tcase x == 0:\n\t\treturn 1\n\tcase x&(x-1) == 0:\n\t\t// x is already power of 2.\n\t\treturn x\n\tdefault:\n\t\treturn int(math.Pow(2, math.Ceil(math.Log2(float64(x)))))\n\t}\n}\n\n```\n# Function Signature: func EqualSlices(replicas []string) *protos.Assignment {\n# Natural Language Annotations: // EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc EqualSlices(replicas []string) *protos.Assignment {\n\tif len(replicas) == 0 {\n\t\treturn &protos.Assignment{\n\t\t\tVersion: 0,\n\t\t\tSlices:  []*protos.Assignment_Slice{},\n\t\t}\n\t}\n\n\tnumReplicas := len(replicas)\n\tnumSlices := nextPowerOfTwo(numReplicas)\n\n\t// Initialize slices\n\tslices := make([]*protos.Assignment_Slice, numSlices)\n\tfor i := range slices {\n\t\tslices[i] = &protos.Assignment_Slice{}\n\t}\n\n\t// Assign replicas to slices in a round robin fashion\n\tfor i, replica := range replicas {\n\t\tindex := i % numSlices\n\t\tslices[index].Replicas = append(slices[index].Replicas, replica)\n\t}\n\n\treturn &protos.Assignment{\n\t\tVersion: 0,\n\t\tSlices:  slices,\n\t}\n}\n```"]}, "weaver-internal/queue/queue.go-Pop": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/queue/queue.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage queue\n\nimport (\n\t\"context\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/cond\"\n)\n\n// Queue is a thread-safe queue.\n//\n// Unlike a Go channel, Queue doesn't have any constraints on how many\n// elements can be in the queue.\ntype Queue[T any] struct {\n\tmu    sync.Mutex\n\telems []T\n\twait  *cond.Cond\n}\n\n// Push places elem at the back of the queue.\nfunc (q *Queue[T]) Push(elem T) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.init()\n\tq.elems = append(q.elems, elem)\n\tq.wait.Signal()\n}\n\n// Pop removes the element from the front of the queue and returns it.\n// It blocks if the queue is empty.\n// It returns an error if the passed-in context is canceled.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the queue.\n//\n// REQUIRES: q.mu is held\nfunc (q *Queue[T]) init() {\n\tif q.wait == nil {\n\t\tq.wait = cond.NewCond(&q.mu)\n\t}\n}\n\n```\n# Function Signature: func (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n# Natural Language Annotations: // Pop removes the element from the front of the queue and returns it.\n// It blocks if the queue is empty.\n// It returns an error if the passed-in context is canceled.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.init()\n\t\n\tfor len(q.elems) == 0 {\n\t\t// Wait for an element to be pushed or for context cancellation\n\t\tdone := make(chan struct{})\n\t\tgo func() {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tq.wait.Broadcast() // Wake up other goroutines waiting on this condition\n\t\t\t\tclose(done)\n\t\t\t}\n\t\t}()\n\t\t\n\t\tq.wait.Wait()\n\t\t\n\t\tif ctx.Err() != nil {\n\t\t\terr = ctx.Err()\n\t\t\treturn\n\t\t}\n\n\t\tselect {\n\t\tcase <-done:\n\t\t\terr = ctx.Err()\n\t\t\treturn\n\t\tdefault:\n\t\t\t// Do nothing, continue to check the queue\n\t\t}\n\t}\n\n\telem = q.elems[0]\n\tq.elems = q.elems[1:]\n\treturn\n}\n```"]}, "weaver-internal/pipe/pipe.go-MakePipePair": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/pipe/pipe.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package pipe extends os.exec, making it easier to create pipes to subcommands.\npackage pipe\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"os/exec\"\n)\n\n// Cmd is drop-in replacement for exec.Cmd, extended with the Pipe method.\ntype Cmd struct {\n\t*exec.Cmd\n\tcloseAfterStart []io.Closer // closed after Start()\n\tcloseAfterWait  []io.Closer // closed after Wait()\n}\n\n// CommandContext returns a new Cmd. See exec.CommandContext for details.\nfunc CommandContext(ctx context.Context, name string, arg ...string) *Cmd {\n\treturn &Cmd{Cmd: exec.CommandContext(ctx, name, arg...)}\n}\n\n// PipePair holds a pair of pipes that can be used for bi-directional\n// communication with a child process.\ntype PipePair struct {\n\tParentReader io.ReadCloser  // Reader from which parent can read\n\tParentWriter io.WriteCloser // Writer to which parent can write\n\tChildReader  uintptr        // Descriptor from which child can read\n\tChildWriter  uintptr        // Descriptor to which child can write\n}\n\n// MakePipePair makes a pair of pipes that can be used for bi-directional\n// communication with the child process.\n//\n// Cmd.ExtraFiles should not be modified directly if MakePipePair is called.\n//\n// Wait will close ParentWriter automatically after seeing the command exit. A\n// caller need only close ParentWriter to force the pipe to close sooner. For\n// example, if the command being run will not exit until standard input is\n// closed, the caller must close ParentWriter.\n//\n// Wait will close ParentReader automatically after seeing the command exit, so\n// most callers need not close ParentReader themselves. It is thus incorrect to\n// call Wait before all reads from ParentReader have completed. For the same\n// reason, it is incorrect to use Run when using MakePipePair. See the\n// exec.Cmd.StdoutPipe example [1] for idiomatic usage.\n//\n// [1]: https://pkg.go.dev/os/exec#example-Cmd.StdoutPipe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c *Cmd) registerPipe(local, remote *os.File) uintptr {\n\tc.closeAfterStart = append(c.closeAfterStart, remote)\n\tc.closeAfterWait = append(c.closeAfterWait, local)\n\treturn addInheritedFile(c.Cmd, remote)\n}\n\n// Start is identical to exec.Command.Start.\nfunc (c *Cmd) Start() error {\n\tif err := c.Cmd.Start(); err != nil {\n\t\treturn err\n\t}\n\tcloseAll(&c.closeAfterStart)\n\treturn nil\n}\n\n// Wait is identical to exec.Command.Wait.\nfunc (c *Cmd) Wait() error {\n\tif err := c.Cmd.Wait(); err != nil {\n\t\treturn err\n\t}\n\tcloseAll(&c.closeAfterWait)\n\treturn nil\n}\n\n// Cleanup cleans up any unused resources.\nfunc (c *Cmd) Cleanup() {\n\tcloseAll(&c.closeAfterStart)\n\tcloseAll(&c.closeAfterWait)\n}\n\nfunc closeAll(files *[]io.Closer) {\n\tfor _, f := range *files {\n\t\tf.Close()\n\t}\n\t*files = nil\n}\n\n```\n# Function Signature: func (c *Cmd) MakePipePair() (PipePair, error) {\n# Natural Language Annotations: // MakePipePair makes a pair of pipes that can be used for bi-directional\n// communication with the child process.\n//\n// Cmd.ExtraFiles should not be modified directly if MakePipePair is called.\n//\n// Wait will close ParentWriter automatically after seeing the command exit. A\n// caller need only close ParentWriter to force the pipe to close sooner. For\n// example, if the command being run will not exit until standard input is\n// closed, the caller must close ParentWriter.\n//\n// Wait will close ParentReader automatically after seeing the command exit, so\n// most callers need not close ParentReader themselves. It is thus incorrect to\n// call Wait before all reads from ParentReader have completed. For the same\n// reason, it is incorrect to use Run when using MakePipePair. See the\n// exec.Cmd.StdoutPipe example [1] for idiomatic usage.\n//\n// [1]: https://pkg.go.dev/os/exec#example-Cmd.StdoutPipe\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *Cmd) MakePipePair() (PipePair, error) {\n\t// Create the parent side reader and child side writer.\n\tparentReader, childWriter, err := os.Pipe()\n\tif err != nil {\n\t\treturn PipePair{}, fmt.Errorf(\"failed to create stdout pipe: %v\", err)\n\t}\n\n\t// Create the child side reader and parent side writer.\n\tchildReader, parentWriter, err := os.Pipe()\n\tif err != nil {\n\t\tparentReader.Close()\n\t\tchildWriter.Close()\n\t\treturn PipePair{}, fmt.Errorf(\"failed to create stdin pipe: %v\", err)\n\t}\n\n\t// Register the pipes with the Cmd, so they are closed at the right time.\n\tc.registerPipe(parentReader, childWriter)\n\tc.registerPipe(childReader, parentWriter)\n\n\t// Return the PipePair for communication.\n\treturn PipePair{\n\t\tParentReader: parentReader,\n\t\tParentWriter: parentWriter,\n\t\tChildReader:  childReader.Fd(),\n\t\tChildWriter:  childWriter.Fd(),\n\t}, nil\n}\n```"]}, "weaver-internal/register/writeonce.go-Write": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/register/writeonce.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package register implements a write-once register.\npackage register\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// WriteOnce is a concurrent write-once register.\ntype WriteOnce[T any] struct {\n\tmu      sync.Mutex\n\tc       sync.Cond\n\twritten bool\n\tval     T\n}\n\n// Write writes to the register, or panics if the register was already written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWrite tries to write to the register and returns if the write succeeds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read returns the value of the register, blocking until it is written.\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the register. We have an init method rather than a\n// WriteOnce constructor so that the zero value of WriteOnce is valid.\n//\n// REQUIRES: w.mu is held.\nfunc (w *WriteOnce[T]) init() {\n\tif w.c.L == nil {\n\t\tw.c.L = &w.mu\n\t}\n}\n\n```\n# Function Signature: func (w *WriteOnce[T]) Write(val T) {\n# Natural Language Annotations: // Write writes to the register, or panics if the register was already written.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (w *WriteOnce[T]) Write(val T) {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    w.init() // Ensure the condition variable is initialized\n\n    if w.written {\n        panic(\"register has already been written to\")\n    }\n    w.val = val\n    w.written = true\n    w.c.Broadcast() // Notify any goroutines waiting for a value to be written\n}\n```"]}, "weaver-internal/register/writeonce.go-TryWrite": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/register/writeonce.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package register implements a write-once register.\npackage register\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// WriteOnce is a concurrent write-once register.\ntype WriteOnce[T any] struct {\n\tmu      sync.Mutex\n\tc       sync.Cond\n\twritten bool\n\tval     T\n}\n\n// Write writes to the register, or panics if the register was already written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWrite tries to write to the register and returns if the write succeeds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read returns the value of the register, blocking until it is written.\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the register. We have an init method rather than a\n// WriteOnce constructor so that the zero value of WriteOnce is valid.\n//\n// REQUIRES: w.mu is held.\nfunc (w *WriteOnce[T]) init() {\n\tif w.c.L == nil {\n\t\tw.c.L = &w.mu\n\t}\n}\n\n```\n# Function Signature: func (w *WriteOnce[T]) TryWrite(val T) bool {\n# Natural Language Annotations: // TryWrite tries to write to the register and returns if the write succeeds.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// TryWrite tries to write to the register and returns if the write succeeds.\nfunc (w *WriteOnce[T]) TryWrite(val T) bool {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    \n    if w.written {\n        return false\n    }\n    \n    w.val = val\n    w.written = true\n    w.c.Broadcast() // Notify any readers waiting for the value.\n    return true\n}\n```"]}, "weaver-internal/register/writeonce.go-Read": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/register/writeonce.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package register implements a write-once register.\npackage register\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// WriteOnce is a concurrent write-once register.\ntype WriteOnce[T any] struct {\n\tmu      sync.Mutex\n\tc       sync.Cond\n\twritten bool\n\tval     T\n}\n\n// Write writes to the register, or panics if the register was already written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWrite tries to write to the register and returns if the write succeeds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read returns the value of the register, blocking until it is written.\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the register. We have an init method rather than a\n// WriteOnce constructor so that the zero value of WriteOnce is valid.\n//\n// REQUIRES: w.mu is held.\nfunc (w *WriteOnce[T]) init() {\n\tif w.c.L == nil {\n\t\tw.c.L = &w.mu\n\t}\n}\n\n```\n# Function Signature: func (w *WriteOnce[T]) Read() T {\n# Natural Language Annotations: // Read returns the value of the register, blocking until it is written.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Read returns the value of the register, blocking until it is written.\nfunc (w *WriteOnce[T]) Read() T {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\t\n\tw.init()\n\n\tfor !w.written {\n\t\tw.c.Wait()\n\t}\n\t\n\treturn w.val\n}\n```"]}, "weaver-internal/versioned/versioned.go-RLock": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/versioned/versioned.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage versioned\n\nimport (\n\t\"sync\"\n\n\t\"github.com/google/uuid\"\n)\n\n// Versioned[T] is a linearizable register storing a value of type T.\n// Each update to the value changes its unique (but not necessarily ordered)\n// version. Spurious version changes are possible, i.e., the version may change\n// even if the value hasn't.\n//\n// Like a sync.Mutex, Versioned should not be copied.\ntype Versioned[T any] struct {\n\tmu      sync.RWMutex\n\tchanged sync.Cond\n\tVal     T\n\tversion string\n}\n\nfunc Version[T any](val T) *Versioned[T] {\n\tv := &Versioned[T]{Val: val, version: uuid.New().String()}\n\tv.changed.L = &v.mu\n\treturn v\n}\n\n// Lock acquires the write lock.\nfunc (v *Versioned[T]) Lock() {\n\tv.mu.Lock()\n}\n\n// Unlock releases the write lock.\nfunc (v *Versioned[T]) Unlock() {\n\tv.version = uuid.New().String()\n\tv.changed.Broadcast()\n\tv.mu.Unlock()\n}\n\n// RLock waits until the current version is different than the passed-in\n// version, and then acquires the read lock and returns the new\n// version.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// RUnlock releases the read lock.\nfunc (v *Versioned[T]) RUnlock() {\n\tv.mu.RUnlock()\n}\n\n```\n# Function Signature: func (v *Versioned[T]) RLock(version string) string {\n# Natural Language Annotations: // RLock waits until the current version is different than the passed-in\n// version, and then acquires the read lock and returns the new\n// version.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (v *Versioned[T]) RLock(version string) string {\n\tv.mu.RLock()\n\tfor v.version == version {\n\t\tv.mu.RUnlock() // release lock temporarily to wait to avoid deadlocks\n\t\tv.mu.Lock()\n\t\tv.changed.Wait()\n\t\tv.mu.Unlock()\n\t\tv.mu.RLock() // reacquire read lock before checking version again\n\t}\n\treturn v.version\n}\n```"]}, "weaver-internal/net/call/stub.go-makeStubMethods": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/stub.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// stub holds information about a client stub to the remote component.\ntype stub struct {\n\tconn          Connection   // connection to talk to the remote component\n\tmethods       []stubMethod // per method info\n\ttracer        trace.Tracer // component tracer\n\tinjectRetries int          // Number of artificial retries per retriable call\n}\n\ntype stubMethod struct {\n\tkey   MethodKey // key for remote component method\n\tretry bool      // Whether or not the method should be retred\n}\n\nvar _ codegen.Stub = &stub{}\n\n// NewStub creates a client-side stub of the type matching reg. Calls on the stub are sent on\n// conn to the component with the specified name.\nfunc NewStub(name string, reg *codegen.Registration, conn Connection, tracer trace.Tracer, injectRetries int) codegen.Stub {\n\treturn &stub{\n\t\tconn:          conn,\n\t\tmethods:       makeStubMethods(name, reg),\n\t\ttracer:        tracer,\n\t\tinjectRetries: injectRetries,\n\t}\n}\n\n// Tracer implements the codegen.Stub interface.\nfunc (s *stub) Tracer() trace.Tracer {\n\treturn s.tracer\n}\n\n// Run implements the codegen.Stub interface.\nfunc (s *stub) Run(ctx context.Context, method int, args []byte, shardKey uint64) (result []byte, err error) {\n\tm := s.methods[method]\n\topts := CallOptions{\n\t\tRetry:    m.retry,\n\t\tShardKey: shardKey,\n\t}\n\tn := 1\n\tif m.retry {\n\t\tn += s.injectRetries\n\t}\n\tfor i := 0; i < n; i++ {\n\t\tresult, err = s.conn.Call(ctx, m.key, args, opts)\n\t\t// No backoff since these retries are fake ones injected for testing.\n\t}\n\treturn\n}\n\n// makeStubMethods returns a slice of stub methods for the component methods of reg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func makeStubMethods(fullName string, reg *codegen.Registration) []stubMethod {\n# Natural Language Annotations: // makeStubMethods returns a slice of stub methods for the component methods of reg.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc makeStubMethods(fullName string, reg *codegen.Registration) []stubMethod {\n\t// Assuming reg.ComponentMethods is a slice or similar structure that holds the methods \n\t// associated with the registration of the component.\n\tvar methods []stubMethod\n\n\tfor _, method := range reg.ComponentMethods {\n\t\t// Assuming method has fields MethodKey and Retry which are used to create a stubMethod.\n\t\tstubMethod := stubMethod{\n\t\t\tkey:   method.MethodKey,\n\t\t\tretry: method.Retry,\n\t\t}\n\t\tmethods = append(methods, stubMethod)\n\t}\n\n\treturn methods\n}\n```"]}, "weaver-internal/net/call/call.go-Serve": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/call.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package call implements an RPC mechanism.\npackage call\n\n// # Overview\n//\n// RPCs are conveyed across a bidirectional connection. A connection carries\n// a sequence of messages in each direction. A message has the following\n// information:\n//\trequest-id\t-- A number that identifies a particular RPC\n//\tmessage-type\t-- E.g., request or response\n//\tlength\t\t-- How many payload bytes follow\n//\tpayload\t\t-- length bytes of payload\n// The payload format varies depending on the message-type.\n// See msg.go for details.\n//\n// # Server operation\n//\n// The server listens for connections (typically on a TCP socket). For\n// each accepted connection, it starts a readRequests() goroutine that\n// reads messages from that connection. When readRequests() gets a\n// request message, it starts a runHandler() goroutine. runHandler()\n// looks up the registered handler for the message, runs it, and sends\n// the response back over the connection.\n//\n// # Client operation\n//\n// For each newly discovered server, the client starts a manage() goroutine\n// that connects to server, and then reads messages from the connection. If the\n// network connection breaks, manage() reconnects (after a retry delay).\n//\n// When the client wants to send an RPC, it selects one of its server\n// connections to use, creates a call object, assigns it a new request-id, and\n// registers the object in a map in the connection. It then sends a request\n// message over the connection and waits for the call object to be marked as\n// done.\n//\n// When the response arrives, it is picked up by readAndProcessMessage().\n// readAndProcessMessage() finds the call object corresponding to the\n// request-id in the response, and marks the call object as done which\n// wakes up goroutine that initiated the RPC.\n//\n// If a client is constructed with a non-constant resolver, the client also\n// spawns a watchResolver goroutine that repeatedly calls Resolve on the\n// resolver to get notified of updates to the set of endpoints. When the\n// endpoints are updated, existing connections are retained, and stale\n// connections are transitioned to a \"draining\" state.\n//\n// New RPCs are never issued over draining connections, but the pending\n// requests on a draining connection are allowed to finish. As soon as a\n// draining connection has no active calls, the connection closes itself. If\n// the resolver later returns a new set of endpoints that includes a draining\n// connection that hasn't closed itself, the draining connection is turned\n// back into a normal connection.\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"go.opentelemetry.io/otel/codes\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// Connection allows a client to send RPCs.\ntype Connection interface {\n\t// Call makes an RPC over a Connection.\n\tCall(context.Context, MethodKey, []byte, CallOptions) ([]byte, error)\n\n\t// Close closes a connection. Pending invocations of Call are cancelled and\n\t// return an error. All future invocations of Call fail and return an error\n\t// immediately. Close can be called more than once.\n\tClose()\n}\n\n// Listener allows the server to accept RPCs.\ntype Listener interface {\n\tAccept() (net.Conn, *HandlerMap, error)\n\tClose() error\n\tAddr() net.Addr\n}\n\n// reconnectingConnection is the concrete client-side Connection implementation.\n// It automatically reconnects to the servers on first call or the first call\n// after a shutdown.\ntype reconnectingConnection struct {\n\topts ClientOptions\n\n\t// mu guards the following fields and some of the fields in the\n\t// clientConnections inside connections and draining.\n\tmu     sync.Mutex\n\tconns  map[string]*clientConnection\n\tclosed bool\n\n\tresolver       Resolver\n\tcancelResolver func()         // cancels the watchResolver goroutine\n\tresolverDone   sync.WaitGroup // used to wait for watchResolver to finish\n}\n\n// connState is the state of a clientConnection (connection to a particular\n// server replica). missing is a special state used for unknown servers. A\n// typical sequence of transitions is:\n//\n//\tmissing -> disconnected -> checking -> idle <-> active -> draining -> missing\n//\n// The events that can cause state transition are:\n//\n// - register: server has shown up in resolver results\n// - unregister: server has dropped from resolver results\n// - connected: a connection has been successfully made\n// - checked: connection has been successfully checked\n// - callstart: call starts on connection\n// - lastdone: last active call on connection has ended\n// - fail: some protocol error is detected on the connection\n// - close: reconnectingConnection is being closed\n//\n// Each event has a corresponding clientConnection method below. See\n// those methods for the corresponding state transitions.\ntype connState int8\n\nconst (\n\tmissing      connState = iota\n\tdisconnected           // cannot be used for calls\n\tchecking               // checking new network connection\n\tidle                   // can be used for calls, no calls in-flight\n\tactive                 // can be used for calls, some calls in-flight\n\tdraining               // some calls in-flight, no new calls should be added\n)\n\nvar connStateNames = []string{\n\t\"missing\",\n\t\"disconnected\",\n\t\"checking\",\n\t\"idle\",\n\t\"active\",\n\t\"draining\",\n}\n\nfunc (s connState) String() string { return connStateNames[s] }\n\n// clientConnection manages one network connection on the client-side.\ntype clientConnection struct {\n\t// Immutable after construction.\n\trc       *reconnectingConnection // owner\n\tcanceler func()                  // Used to cancel goroutine handling connection\n\tlogger   *slog.Logger\n\tendpoint Endpoint\n\n\twlock sync.Mutex // Guards writes to c\n\n\t// Guarded by rc.mu\n\tstate          connState        // current connection state\n\tloggedShutdown bool             // Have we logged a shutdown error?\n\tinBalancer     bool             // Is c registered with the balancer?\n\tc              net.Conn         // Active network connection, or nil\n\tcbuf           *bufio.Reader    // Buffered reader wrapped around c\n\tversion        version          // Version number to use for connection\n\tcalls          map[uint64]*call // In-progress calls\n\tlastID         uint64           // Last assigned request ID for a call\n}\n\nvar _ ReplicaConnection = &clientConnection{}\n\n// call holds the state for an active call at the client.\ntype call struct {\n\tid         uint64\n\tdoneSignal chan struct{}\n\n\t// Fields below are accessed across goroutines, but their access is\n\t// synchronized via doneSignal, i.e., it is never concurrent.\n\terr      error\n\tresponse []byte\n\n\t// Is the call done?\n\t// This field is accessed across goroutines using atomics.\n\tdone uint32 // is the call done?\n\n}\n\n// serverConnection manages one network connection on the server-side.\ntype serverConnection struct {\n\topts        ServerOptions\n\tc           net.Conn\n\tcbuf        *bufio.Reader // Buffered reader wrapped around c\n\twlock       sync.Mutex    // Guards writes to c\n\tmu          sync.Mutex\n\tclosed      bool              // has c been closed?\n\tversion     version           // Version number to use for connection\n\tcancelFuncs map[uint64]func() // Cancellation functions for in-progress calls\n}\n\n// serverState tracks all live server-side connections so we can clean things up when canceled.\ntype serverState struct {\n\topts  ServerOptions\n\tmu    sync.Mutex\n\tconns map[*serverConnection]struct{} // Live connections\n}\n\n// Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// onceCloseListener wraps a Listener, protecting it from multiple Close calls.\ntype onceCloseListener struct {\n\tListener\n\tcloser func() error // Must be result of sync.OnceValue\n}\n\nfunc (oc *onceCloseListener) Close() error {\n\treturn oc.closer()\n}\n\n// ServeOn serves client requests received over an already established\n// network connection with a client. This can be useful in tests or\n// when using custom networking transports.\nfunc ServeOn(ctx context.Context, conn net.Conn, hmap *HandlerMap, opts ServerOptions) {\n\tss := &serverState{opts: opts.withDefaults()}\n\tss.serveConnection(ctx, conn, hmap)\n}\n\nfunc (ss *serverState) serveConnection(ctx context.Context, conn net.Conn, hmap *HandlerMap) {\n\tc := &serverConnection{\n\t\topts:        ss.opts,\n\t\tc:           conn,\n\t\tcbuf:        bufio.NewReader(conn),\n\t\tversion:     initialVersion, // Updated when we hear from client\n\t\tcancelFuncs: map[uint64]func(){},\n\t}\n\tss.register(c)\n\n\tgo c.readRequests(ctx, hmap, func() { ss.unregister(c) })\n}\n\nfunc (ss *serverState) stop() {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tfor c := range ss.conns {\n\t\tc.c.Close() // Should stop readRequests in its tracks\n\t}\n}\n\nfunc (ss *serverState) register(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tif ss.conns == nil {\n\t\tss.conns = map[*serverConnection]struct{}{}\n\t}\n\tss.conns[c] = struct{}{}\n}\n\nfunc (ss *serverState) unregister(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tdelete(ss.conns, c)\n}\n\n// Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes a connection.\nfunc (rc *reconnectingConnection) Close() {\n\tcloseWithLock := func() {\n\t\trc.mu.Lock()\n\t\tdefer rc.mu.Unlock()\n\t\tif rc.closed {\n\t\t\treturn\n\t\t}\n\t\trc.closed = true\n\t\tfor _, c := range rc.conns {\n\t\t\tc.close()\n\t\t}\n\t}\n\tcloseWithLock()\n\n\t// Cancel the watchResolver goroutine and wait for it to terminate. If the\n\t// watchResolver has already been terminated, then this code is a no-op.\n\t// Note that if we hold the lock while waiting for watchResolver to\n\t// terminate, we may deadlock.\n\trc.cancelResolver()\n\trc.resolverDone.Wait()\n}\n\n// Call makes an RPC over connection c, retrying it on network errors if retries are allowed.\nfunc (rc *reconnectingConnection) Call(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tif !opts.Retry {\n\t\treturn rc.callOnce(ctx, h, arg, opts)\n\t}\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tresponse, err := rc.callOnce(ctx, h, arg, opts)\n\t\tif errors.Is(err, Unreachable) || errors.Is(err, CommunicationError) {\n\t\t\tcontinue\n\t\t}\n\t\treturn response, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (rc *reconnectingConnection) callOnce(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tvar micros int64\n\tdeadline, haveDeadline := ctx.Deadline()\n\tif haveDeadline {\n\t\t// Send the deadline in the header. We use the relative time instead\n\t\t// of absolute in case there is significant clock skew. This does mean\n\t\t// that we will not count transmission delay against the deadline.\n\t\tmicros = time.Until(deadline).Microseconds()\n\t\tif micros <= 0 {\n\t\t\t// Fail immediately without attempting to send a zero or negative\n\t\t\t// deadline to the server which will be misinterpreted.\n\t\t\t<-ctx.Done()\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t}\n\n\t// Encode the header.\n\thdr := encodeHeader(ctx, h, micros)\n\n\t// Note that we send the header and the payload as follows:\n\t// [header_length][encoded_header][payload]\n\tvar hdrLen [hdrLenLen]byte\n\tbinary.LittleEndian.PutUint32(hdrLen[:], uint32(len(hdr)))\n\thdrSlice := append(hdrLen[:], hdr...)\n\n\trpc := &call{}\n\trpc.doneSignal = make(chan struct{})\n\n\t// TODO: Arrange to obey deadline in any reconnection done inside startCall.\n\tconn, nc, err := rc.startCall(ctx, rpc, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := writeMessage(nc, &conn.wlock, requestMessage, rpc.id, hdrSlice, arg, rc.opts.WriteFlattenLimit); err != nil {\n\t\tconn.shutdown(\"client send request\", err)\n\t\tconn.endCall(rpc)\n\t\treturn nil, fmt.Errorf(\"%w: %s\", CommunicationError, err)\n\t}\n\n\tif rc.opts.OptimisticSpinDuration > 0 {\n\t\t// Optimistically spin, waiting for the results.\n\t\tfor start := time.Now(); time.Since(start) < rc.opts.OptimisticSpinDuration; {\n\t\t\tif atomic.LoadUint32(&rpc.done) > 0 {\n\t\t\t\treturn rpc.response, rpc.err\n\t\t\t}\n\t\t}\n\t}\n\n\tif cdone := ctx.Done(); cdone != nil {\n\t\tselect {\n\t\tcase <-rpc.doneSignal:\n\t\t\t// Regular return\n\t\tcase <-cdone:\n\t\t\t// Canceled or deadline expired.\n\t\t\tconn.endCall(rpc)\n\n\t\t\tif !haveDeadline || time.Now().Before(deadline) {\n\t\t\t\t// Early cancellation. Tell server about it.\n\t\t\t\tif err := writeMessage(nc, &conn.wlock, cancelMessage, rpc.id, nil, nil, rc.opts.WriteFlattenLimit); err != nil {\n\t\t\t\t\tconn.shutdown(\"client send cancel\", err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t} else {\n\t\t<-rpc.doneSignal\n\t}\n\treturn rpc.response, rpc.err\n}\n\n// watchResolver watches for updates to the set of endpoints. When a new set of\n// updates is available, watchResolver passes it to updateEndpoints.\n// REQUIRES: version != nil.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) watchResolver(ctx context.Context, version *Version) {\n\tdefer rc.resolverDone.Done()\n\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tendpoints, newVersion, err := rc.resolver.Resolve(ctx, version)\n\t\tif err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif newVersion == nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", errors.New(\"non-constant resolver returned a nil version\"))\n\t\t\tcontinue\n\t\t}\n\t\tif *version == *newVersion {\n\t\t\t// Resolver wishes to be called again after an appropriate delay.\n\t\t\tcontinue\n\t\t}\n\t\tif err := rc.updateEndpoints(ctx, endpoints); err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t}\n\t\tversion = newVersion\n\t\tr.Reset()\n\t}\n}\n\n// updateEndpoints updates the set of endpoints. Existing connections are\n// retained, and stale connections are closed.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) updateEndpoints(ctx context.Context, endpoints []Endpoint) error {\n\trc.mu.Lock()\n\tdefer rc.mu.Unlock()\n\n\tif rc.closed {\n\t\treturn fmt.Errorf(\"updateEndpoints on closed Connection\")\n\t}\n\n\t// Make new endpoints.\n\tkeep := make(map[string]struct{}, len(endpoints))\n\tfor _, endpoint := range endpoints {\n\t\taddr := endpoint.Address()\n\t\tkeep[addr] = struct{}{}\n\t\tif _, ok := rc.conns[addr]; !ok {\n\t\t\t// New endpoint, create connection and manage it.\n\t\t\tctx, cancel := context.WithCancel(ctx)\n\t\t\tc := &clientConnection{\n\t\t\t\trc:       rc,\n\t\t\t\tcanceler: cancel,\n\t\t\t\tlogger:   rc.opts.Logger,\n\t\t\t\tendpoint: endpoint,\n\t\t\t\tcalls:    map[uint64]*call{},\n\t\t\t\tlastID:   0,\n\t\t\t}\n\t\t\trc.conns[addr] = c\n\t\t\tc.register()\n\t\t\tgo c.manage(ctx)\n\t\t}\n\t}\n\n\t// Drop old endpoints.\n\tfor addr, c := range rc.conns {\n\t\tif _, ok := keep[addr]; ok {\n\t\t\t// Still live, so keep it.\n\t\t\tcontinue\n\t\t}\n\t\tc.unregister()\n\t}\n\n\treturn nil\n}\n\n// startCall registers a new in-progress call.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) startCall(ctx context.Context, rpc *call, opts CallOptions) (*clientConnection, net.Conn, error) {\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trc.mu.Lock()\n\t\tif rc.closed {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"Call on closed Connection\")\n\t\t}\n\n\t\treplica, ok := rc.opts.Balancer.Pick(opts)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\n\t\tc, ok := replica.(*clientConnection)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"internal error: wrong connection type %#v returned by load balancer\", replica)\n\t\t}\n\n\t\tc.lastID++\n\t\trpc.id = c.lastID\n\t\tc.calls[rpc.id] = rpc\n\t\tc.callstart()\n\t\tnc := c.c\n\t\trc.mu.Unlock()\n\n\t\treturn c, nc, nil\n\t}\n\n\treturn nil, nil, ctx.Err()\n}\n\nfunc (c *clientConnection) Address() string {\n\treturn c.endpoint.Address()\n}\n\n// State transition actions: all of these are called with rc.mu held.\n\nfunc (c *clientConnection) register() {\n\tswitch c.state {\n\tcase missing:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\t// We were attempting to get rid of the old connection, but it\n\t\t// seems like the server-side problem was transient, so we\n\t\t// resurrect the draining connection into a non-draining state.\n\t\t//\n\t\t// New state is active instead of idle since state==draining\n\t\t// implies there is at least one call in-flight.\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) unregister() {\n\tswitch c.state {\n\tcase disconnected, checking, idle:\n\t\tc.setState(missing)\n\tcase active:\n\t\tc.setState(draining)\n\t}\n}\n\nfunc (c *clientConnection) connected() {\n\tswitch c.state {\n\tcase disconnected:\n\t\tc.setState(checking)\n\t}\n}\n\nfunc (c *clientConnection) checked() {\n\tswitch c.state {\n\tcase checking:\n\t\tc.setState(idle)\n\t}\n}\n\nfunc (c *clientConnection) callstart() {\n\tswitch c.state {\n\tcase idle:\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) lastdone() {\n\tswitch c.state {\n\tcase active:\n\t\tc.setState(idle)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) fail(details string, err error) {\n\tif !c.loggedShutdown {\n\t\tc.loggedShutdown = true\n\t\tlogError(c.logger, details, err)\n\t}\n\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: %s: %s\", CommunicationError, details, err))\n\n\tswitch c.state {\n\tcase checking, idle, active:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) close() {\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: connection closed\", CommunicationError))\n\n\tc.setState(missing)\n}\n\n// checkInvariants verifies clientConnection invariants.\nfunc (c *clientConnection) checkInvariants() {\n\ts := c.state\n\n\t// connection in reconnectingConnection.conns iff state not in {missing}\n\tif _, ok := c.rc.conns[c.endpoint.Address()]; ok != (s != missing) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong connection table presence %v\", s, ok))\n\t}\n\n\t// has net.Conn iff state in {checking, idle, active, draining}\n\tif (c.c != nil) != (s == checking || s == idle || s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong net.Conn %v\", s, c.c))\n\t}\n\n\t// connection is in the balancer iff state in {idle, active}\n\tif c.inBalancer != (s == idle || s == active) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong balancer presence %v\", s, c.inBalancer))\n\t}\n\n\t// len(calls) > 0 iff state in {active, draining}\n\tif (len(c.calls) != 0) != (s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t}\n}\n\n// setState transitions to state s and updates any related state.\nfunc (c *clientConnection) setState(s connState) {\n\t// idle<-> active transitions may happen a lot, so short-circuit them\n\t// by avoiding logging and full invariant maintenance.\n\tif c.state == active && s == idle {\n\t\tc.state = idle\n\t\tif len(c.calls) != 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t} else if c.state == idle && s == active {\n\t\tc.state = active\n\t\tif len(c.calls) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t}\n\n\tc.logger.Info(\"connection\", \"addr\", c.endpoint.Address(), \"from\", c.state, \"to\", s)\n\tc.state = s\n\n\t// Fix membership in rc.conns.\n\tif s == missing {\n\t\tdelete(c.rc.conns, c.endpoint.Address())\n\t\tif c.canceler != nil {\n\t\t\tc.canceler() // Forces retry loop to end early\n\t\t\tc.canceler = nil\n\t\t}\n\t} // else: caller is responsible for adding c to rc.conns\n\n\t// Fix net.Conn presence.\n\tif s == missing || s == disconnected {\n\t\tif c.c != nil {\n\t\t\tc.c.Close()\n\t\t\tc.c = nil\n\t\t\tc.cbuf = nil\n\t\t}\n\t} // else: caller is responsible for setting c.c and c.cbuf\n\n\t// Fix balancer membership.\n\tif s == idle || s == active {\n\t\tif !c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Add(c)\n\t\t\tc.inBalancer = true\n\t\t}\n\t} else {\n\t\tif c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Remove(c)\n\t\t\tc.inBalancer = false\n\t\t}\n\t}\n\n\t// Fix in-flight calls.\n\tif s == active || s == draining {\n\t\t// Keep calls live\n\t} else {\n\t\tc.endCalls(fmt.Errorf(\"%w:\n```\n# Function Signature: func Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n# Natural Language Annotations: // Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\nfunc Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n\tss := &serverState{opts: opts.withDefaults()}\n\n\t// Wrap the listener to ensure it can only be closed once\n\tonceListener := &onceCloseListener{\n\t\tListener: l,\n\t\tcloser: func() error {\n\t\t\treturn l.Close()\n\t\t},\n\t}\n\n\tdefer onceListener.Close() // Ensure the listener is closed when Serve exits\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tss.stop()\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t\tconn, hmap, err := l.Accept()\n\t\t\tif err != nil {\n\t\t\t\tss.stop()\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// Serve the connection\n\t\t\tgo ss.serveConnection(ctx, conn, hmap)\n\t\t}\n\t}\n}\n```"]}, "weaver-internal/net/call/call.go-Connect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/call.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package call implements an RPC mechanism.\npackage call\n\n// # Overview\n//\n// RPCs are conveyed across a bidirectional connection. A connection carries\n// a sequence of messages in each direction. A message has the following\n// information:\n//\trequest-id\t-- A number that identifies a particular RPC\n//\tmessage-type\t-- E.g., request or response\n//\tlength\t\t-- How many payload bytes follow\n//\tpayload\t\t-- length bytes of payload\n// The payload format varies depending on the message-type.\n// See msg.go for details.\n//\n// # Server operation\n//\n// The server listens for connections (typically on a TCP socket). For\n// each accepted connection, it starts a readRequests() goroutine that\n// reads messages from that connection. When readRequests() gets a\n// request message, it starts a runHandler() goroutine. runHandler()\n// looks up the registered handler for the message, runs it, and sends\n// the response back over the connection.\n//\n// # Client operation\n//\n// For each newly discovered server, the client starts a manage() goroutine\n// that connects to server, and then reads messages from the connection. If the\n// network connection breaks, manage() reconnects (after a retry delay).\n//\n// When the client wants to send an RPC, it selects one of its server\n// connections to use, creates a call object, assigns it a new request-id, and\n// registers the object in a map in the connection. It then sends a request\n// message over the connection and waits for the call object to be marked as\n// done.\n//\n// When the response arrives, it is picked up by readAndProcessMessage().\n// readAndProcessMessage() finds the call object corresponding to the\n// request-id in the response, and marks the call object as done which\n// wakes up goroutine that initiated the RPC.\n//\n// If a client is constructed with a non-constant resolver, the client also\n// spawns a watchResolver goroutine that repeatedly calls Resolve on the\n// resolver to get notified of updates to the set of endpoints. When the\n// endpoints are updated, existing connections are retained, and stale\n// connections are transitioned to a \"draining\" state.\n//\n// New RPCs are never issued over draining connections, but the pending\n// requests on a draining connection are allowed to finish. As soon as a\n// draining connection has no active calls, the connection closes itself. If\n// the resolver later returns a new set of endpoints that includes a draining\n// connection that hasn't closed itself, the draining connection is turned\n// back into a normal connection.\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"go.opentelemetry.io/otel/codes\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// Connection allows a client to send RPCs.\ntype Connection interface {\n\t// Call makes an RPC over a Connection.\n\tCall(context.Context, MethodKey, []byte, CallOptions) ([]byte, error)\n\n\t// Close closes a connection. Pending invocations of Call are cancelled and\n\t// return an error. All future invocations of Call fail and return an error\n\t// immediately. Close can be called more than once.\n\tClose()\n}\n\n// Listener allows the server to accept RPCs.\ntype Listener interface {\n\tAccept() (net.Conn, *HandlerMap, error)\n\tClose() error\n\tAddr() net.Addr\n}\n\n// reconnectingConnection is the concrete client-side Connection implementation.\n// It automatically reconnects to the servers on first call or the first call\n// after a shutdown.\ntype reconnectingConnection struct {\n\topts ClientOptions\n\n\t// mu guards the following fields and some of the fields in the\n\t// clientConnections inside connections and draining.\n\tmu     sync.Mutex\n\tconns  map[string]*clientConnection\n\tclosed bool\n\n\tresolver       Resolver\n\tcancelResolver func()         // cancels the watchResolver goroutine\n\tresolverDone   sync.WaitGroup // used to wait for watchResolver to finish\n}\n\n// connState is the state of a clientConnection (connection to a particular\n// server replica). missing is a special state used for unknown servers. A\n// typical sequence of transitions is:\n//\n//\tmissing -> disconnected -> checking -> idle <-> active -> draining -> missing\n//\n// The events that can cause state transition are:\n//\n// - register: server has shown up in resolver results\n// - unregister: server has dropped from resolver results\n// - connected: a connection has been successfully made\n// - checked: connection has been successfully checked\n// - callstart: call starts on connection\n// - lastdone: last active call on connection has ended\n// - fail: some protocol error is detected on the connection\n// - close: reconnectingConnection is being closed\n//\n// Each event has a corresponding clientConnection method below. See\n// those methods for the corresponding state transitions.\ntype connState int8\n\nconst (\n\tmissing      connState = iota\n\tdisconnected           // cannot be used for calls\n\tchecking               // checking new network connection\n\tidle                   // can be used for calls, no calls in-flight\n\tactive                 // can be used for calls, some calls in-flight\n\tdraining               // some calls in-flight, no new calls should be added\n)\n\nvar connStateNames = []string{\n\t\"missing\",\n\t\"disconnected\",\n\t\"checking\",\n\t\"idle\",\n\t\"active\",\n\t\"draining\",\n}\n\nfunc (s connState) String() string { return connStateNames[s] }\n\n// clientConnection manages one network connection on the client-side.\ntype clientConnection struct {\n\t// Immutable after construction.\n\trc       *reconnectingConnection // owner\n\tcanceler func()                  // Used to cancel goroutine handling connection\n\tlogger   *slog.Logger\n\tendpoint Endpoint\n\n\twlock sync.Mutex // Guards writes to c\n\n\t// Guarded by rc.mu\n\tstate          connState        // current connection state\n\tloggedShutdown bool             // Have we logged a shutdown error?\n\tinBalancer     bool             // Is c registered with the balancer?\n\tc              net.Conn         // Active network connection, or nil\n\tcbuf           *bufio.Reader    // Buffered reader wrapped around c\n\tversion        version          // Version number to use for connection\n\tcalls          map[uint64]*call // In-progress calls\n\tlastID         uint64           // Last assigned request ID for a call\n}\n\nvar _ ReplicaConnection = &clientConnection{}\n\n// call holds the state for an active call at the client.\ntype call struct {\n\tid         uint64\n\tdoneSignal chan struct{}\n\n\t// Fields below are accessed across goroutines, but their access is\n\t// synchronized via doneSignal, i.e., it is never concurrent.\n\terr      error\n\tresponse []byte\n\n\t// Is the call done?\n\t// This field is accessed across goroutines using atomics.\n\tdone uint32 // is the call done?\n\n}\n\n// serverConnection manages one network connection on the server-side.\ntype serverConnection struct {\n\topts        ServerOptions\n\tc           net.Conn\n\tcbuf        *bufio.Reader // Buffered reader wrapped around c\n\twlock       sync.Mutex    // Guards writes to c\n\tmu          sync.Mutex\n\tclosed      bool              // has c been closed?\n\tversion     version           // Version number to use for connection\n\tcancelFuncs map[uint64]func() // Cancellation functions for in-progress calls\n}\n\n// serverState tracks all live server-side connections so we can clean things up when canceled.\ntype serverState struct {\n\topts  ServerOptions\n\tmu    sync.Mutex\n\tconns map[*serverConnection]struct{} // Live connections\n}\n\n// Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// onceCloseListener wraps a Listener, protecting it from multiple Close calls.\ntype onceCloseListener struct {\n\tListener\n\tcloser func() error // Must be result of sync.OnceValue\n}\n\nfunc (oc *onceCloseListener) Close() error {\n\treturn oc.closer()\n}\n\n// ServeOn serves client requests received over an already established\n// network connection with a client. This can be useful in tests or\n// when using custom networking transports.\nfunc ServeOn(ctx context.Context, conn net.Conn, hmap *HandlerMap, opts ServerOptions) {\n\tss := &serverState{opts: opts.withDefaults()}\n\tss.serveConnection(ctx, conn, hmap)\n}\n\nfunc (ss *serverState) serveConnection(ctx context.Context, conn net.Conn, hmap *HandlerMap) {\n\tc := &serverConnection{\n\t\topts:        ss.opts,\n\t\tc:           conn,\n\t\tcbuf:        bufio.NewReader(conn),\n\t\tversion:     initialVersion, // Updated when we hear from client\n\t\tcancelFuncs: map[uint64]func(){},\n\t}\n\tss.register(c)\n\n\tgo c.readRequests(ctx, hmap, func() { ss.unregister(c) })\n}\n\nfunc (ss *serverState) stop() {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tfor c := range ss.conns {\n\t\tc.c.Close() // Should stop readRequests in its tracks\n\t}\n}\n\nfunc (ss *serverState) register(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tif ss.conns == nil {\n\t\tss.conns = map[*serverConnection]struct{}{}\n\t}\n\tss.conns[c] = struct{}{}\n}\n\nfunc (ss *serverState) unregister(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tdelete(ss.conns, c)\n}\n\n// Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes a connection.\nfunc (rc *reconnectingConnection) Close() {\n\tcloseWithLock := func() {\n\t\trc.mu.Lock()\n\t\tdefer rc.mu.Unlock()\n\t\tif rc.closed {\n\t\t\treturn\n\t\t}\n\t\trc.closed = true\n\t\tfor _, c := range rc.conns {\n\t\t\tc.close()\n\t\t}\n\t}\n\tcloseWithLock()\n\n\t// Cancel the watchResolver goroutine and wait for it to terminate. If the\n\t// watchResolver has already been terminated, then this code is a no-op.\n\t// Note that if we hold the lock while waiting for watchResolver to\n\t// terminate, we may deadlock.\n\trc.cancelResolver()\n\trc.resolverDone.Wait()\n}\n\n// Call makes an RPC over connection c, retrying it on network errors if retries are allowed.\nfunc (rc *reconnectingConnection) Call(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tif !opts.Retry {\n\t\treturn rc.callOnce(ctx, h, arg, opts)\n\t}\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tresponse, err := rc.callOnce(ctx, h, arg, opts)\n\t\tif errors.Is(err, Unreachable) || errors.Is(err, CommunicationError) {\n\t\t\tcontinue\n\t\t}\n\t\treturn response, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (rc *reconnectingConnection) callOnce(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tvar micros int64\n\tdeadline, haveDeadline := ctx.Deadline()\n\tif haveDeadline {\n\t\t// Send the deadline in the header. We use the relative time instead\n\t\t// of absolute in case there is significant clock skew. This does mean\n\t\t// that we will not count transmission delay against the deadline.\n\t\tmicros = time.Until(deadline).Microseconds()\n\t\tif micros <= 0 {\n\t\t\t// Fail immediately without attempting to send a zero or negative\n\t\t\t// deadline to the server which will be misinterpreted.\n\t\t\t<-ctx.Done()\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t}\n\n\t// Encode the header.\n\thdr := encodeHeader(ctx, h, micros)\n\n\t// Note that we send the header and the payload as follows:\n\t// [header_length][encoded_header][payload]\n\tvar hdrLen [hdrLenLen]byte\n\tbinary.LittleEndian.PutUint32(hdrLen[:], uint32(len(hdr)))\n\thdrSlice := append(hdrLen[:], hdr...)\n\n\trpc := &call{}\n\trpc.doneSignal = make(chan struct{})\n\n\t// TODO: Arrange to obey deadline in any reconnection done inside startCall.\n\tconn, nc, err := rc.startCall(ctx, rpc, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := writeMessage(nc, &conn.wlock, requestMessage, rpc.id, hdrSlice, arg, rc.opts.WriteFlattenLimit); err != nil {\n\t\tconn.shutdown(\"client send request\", err)\n\t\tconn.endCall(rpc)\n\t\treturn nil, fmt.Errorf(\"%w: %s\", CommunicationError, err)\n\t}\n\n\tif rc.opts.OptimisticSpinDuration > 0 {\n\t\t// Optimistically spin, waiting for the results.\n\t\tfor start := time.Now(); time.Since(start) < rc.opts.OptimisticSpinDuration; {\n\t\t\tif atomic.LoadUint32(&rpc.done) > 0 {\n\t\t\t\treturn rpc.response, rpc.err\n\t\t\t}\n\t\t}\n\t}\n\n\tif cdone := ctx.Done(); cdone != nil {\n\t\tselect {\n\t\tcase <-rpc.doneSignal:\n\t\t\t// Regular return\n\t\tcase <-cdone:\n\t\t\t// Canceled or deadline expired.\n\t\t\tconn.endCall(rpc)\n\n\t\t\tif !haveDeadline || time.Now().Before(deadline) {\n\t\t\t\t// Early cancellation. Tell server about it.\n\t\t\t\tif err := writeMessage(nc, &conn.wlock, cancelMessage, rpc.id, nil, nil, rc.opts.WriteFlattenLimit); err != nil {\n\t\t\t\t\tconn.shutdown(\"client send cancel\", err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t} else {\n\t\t<-rpc.doneSignal\n\t}\n\treturn rpc.response, rpc.err\n}\n\n// watchResolver watches for updates to the set of endpoints. When a new set of\n// updates is available, watchResolver passes it to updateEndpoints.\n// REQUIRES: version != nil.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) watchResolver(ctx context.Context, version *Version) {\n\tdefer rc.resolverDone.Done()\n\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tendpoints, newVersion, err := rc.resolver.Resolve(ctx, version)\n\t\tif err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif newVersion == nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", errors.New(\"non-constant resolver returned a nil version\"))\n\t\t\tcontinue\n\t\t}\n\t\tif *version == *newVersion {\n\t\t\t// Resolver wishes to be called again after an appropriate delay.\n\t\t\tcontinue\n\t\t}\n\t\tif err := rc.updateEndpoints(ctx, endpoints); err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t}\n\t\tversion = newVersion\n\t\tr.Reset()\n\t}\n}\n\n// updateEndpoints updates the set of endpoints. Existing connections are\n// retained, and stale connections are closed.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) updateEndpoints(ctx context.Context, endpoints []Endpoint) error {\n\trc.mu.Lock()\n\tdefer rc.mu.Unlock()\n\n\tif rc.closed {\n\t\treturn fmt.Errorf(\"updateEndpoints on closed Connection\")\n\t}\n\n\t// Make new endpoints.\n\tkeep := make(map[string]struct{}, len(endpoints))\n\tfor _, endpoint := range endpoints {\n\t\taddr := endpoint.Address()\n\t\tkeep[addr] = struct{}{}\n\t\tif _, ok := rc.conns[addr]; !ok {\n\t\t\t// New endpoint, create connection and manage it.\n\t\t\tctx, cancel := context.WithCancel(ctx)\n\t\t\tc := &clientConnection{\n\t\t\t\trc:       rc,\n\t\t\t\tcanceler: cancel,\n\t\t\t\tlogger:   rc.opts.Logger,\n\t\t\t\tendpoint: endpoint,\n\t\t\t\tcalls:    map[uint64]*call{},\n\t\t\t\tlastID:   0,\n\t\t\t}\n\t\t\trc.conns[addr] = c\n\t\t\tc.register()\n\t\t\tgo c.manage(ctx)\n\t\t}\n\t}\n\n\t// Drop old endpoints.\n\tfor addr, c := range rc.conns {\n\t\tif _, ok := keep[addr]; ok {\n\t\t\t// Still live, so keep it.\n\t\t\tcontinue\n\t\t}\n\t\tc.unregister()\n\t}\n\n\treturn nil\n}\n\n// startCall registers a new in-progress call.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) startCall(ctx context.Context, rpc *call, opts CallOptions) (*clientConnection, net.Conn, error) {\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trc.mu.Lock()\n\t\tif rc.closed {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"Call on closed Connection\")\n\t\t}\n\n\t\treplica, ok := rc.opts.Balancer.Pick(opts)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\n\t\tc, ok := replica.(*clientConnection)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"internal error: wrong connection type %#v returned by load balancer\", replica)\n\t\t}\n\n\t\tc.lastID++\n\t\trpc.id = c.lastID\n\t\tc.calls[rpc.id] = rpc\n\t\tc.callstart()\n\t\tnc := c.c\n\t\trc.mu.Unlock()\n\n\t\treturn c, nc, nil\n\t}\n\n\treturn nil, nil, ctx.Err()\n}\n\nfunc (c *clientConnection) Address() string {\n\treturn c.endpoint.Address()\n}\n\n// State transition actions: all of these are called with rc.mu held.\n\nfunc (c *clientConnection) register() {\n\tswitch c.state {\n\tcase missing:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\t// We were attempting to get rid of the old connection, but it\n\t\t// seems like the server-side problem was transient, so we\n\t\t// resurrect the draining connection into a non-draining state.\n\t\t//\n\t\t// New state is active instead of idle since state==draining\n\t\t// implies there is at least one call in-flight.\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) unregister() {\n\tswitch c.state {\n\tcase disconnected, checking, idle:\n\t\tc.setState(missing)\n\tcase active:\n\t\tc.setState(draining)\n\t}\n}\n\nfunc (c *clientConnection) connected() {\n\tswitch c.state {\n\tcase disconnected:\n\t\tc.setState(checking)\n\t}\n}\n\nfunc (c *clientConnection) checked() {\n\tswitch c.state {\n\tcase checking:\n\t\tc.setState(idle)\n\t}\n}\n\nfunc (c *clientConnection) callstart() {\n\tswitch c.state {\n\tcase idle:\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) lastdone() {\n\tswitch c.state {\n\tcase active:\n\t\tc.setState(idle)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) fail(details string, err error) {\n\tif !c.loggedShutdown {\n\t\tc.loggedShutdown = true\n\t\tlogError(c.logger, details, err)\n\t}\n\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: %s: %s\", CommunicationError, details, err))\n\n\tswitch c.state {\n\tcase checking, idle, active:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) close() {\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: connection closed\", CommunicationError))\n\n\tc.setState(missing)\n}\n\n// checkInvariants verifies clientConnection invariants.\nfunc (c *clientConnection) checkInvariants() {\n\ts := c.state\n\n\t// connection in reconnectingConnection.conns iff state not in {missing}\n\tif _, ok := c.rc.conns[c.endpoint.Address()]; ok != (s != missing) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong connection table presence %v\", s, ok))\n\t}\n\n\t// has net.Conn iff state in {checking, idle, active, draining}\n\tif (c.c != nil) != (s == checking || s == idle || s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong net.Conn %v\", s, c.c))\n\t}\n\n\t// connection is in the balancer iff state in {idle, active}\n\tif c.inBalancer != (s == idle || s == active) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong balancer presence %v\", s, c.inBalancer))\n\t}\n\n\t// len(calls) > 0 iff state in {active, draining}\n\tif (len(c.calls) != 0) != (s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t}\n}\n\n// setState transitions to state s and updates any related state.\nfunc (c *clientConnection) setState(s connState) {\n\t// idle<-> active transitions may happen a lot, so short-circuit them\n\t// by avoiding logging and full invariant maintenance.\n\tif c.state == active && s == idle {\n\t\tc.state = idle\n\t\tif len(c.calls) != 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t} else if c.state == idle && s == active {\n\t\tc.state = active\n\t\tif len(c.calls) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t}\n\n\tc.logger.Info(\"connection\", \"addr\", c.endpoint.Address(), \"from\", c.state, \"to\", s)\n\tc.state = s\n\n\t// Fix membership in rc.conns.\n\tif s == missing {\n\t\tdelete(c.rc.conns, c.endpoint.Address())\n\t\tif c.canceler != nil {\n\t\t\tc.canceler() // Forces retry loop to end early\n\t\t\tc.canceler = nil\n\t\t}\n\t} // else: caller is responsible for adding c to rc.conns\n\n\t// Fix net.Conn presence.\n\tif s == missing || s == disconnected {\n\t\tif c.c != nil {\n\t\t\tc.c.Close()\n\t\t\tc.c = nil\n\t\t\tc.cbuf = nil\n\t\t}\n\t} // else: caller is responsible for setting c.c and c.cbuf\n\n\t// Fix balancer membership.\n\tif s == idle || s == active {\n\t\tif !c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Add(c)\n\t\t\tc.inBalancer = true\n\t\t}\n\t} else {\n\t\tif c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Remove(c)\n\t\t\tc.inBalancer = false\n\t\t}\n\t}\n\n\t// Fix in-flight calls.\n\tif s == active || s == draining {\n\t\t// Keep calls live\n\t} else {\n\t\tc.endCalls(fmt.Errorf(\"%w:\n```\n# Function Signature: func Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n# Natural Language Annotations: // Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\nfunc Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n    rc := &reconnectingConnection{\n        opts:    opts,\n        conns:   make(map[string]*clientConnection),\n        resolver: resolver,\n    }\n\n    // Initialize the resolver cancel function\n    ctx, cancel := context.WithCancel(ctx)\n    rc.cancelResolver = cancel\n\n    // Start the resolver watcher\n    rc.resolverDone.Add(1)\n    go rc.watchResolver(ctx, &Version{})\n\n    return rc, nil\n}\n```"]}, "weaver-internal/net/call/msg.go-writeMessage": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/msg.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"sync\"\n)\n\n// messageType identifies a type of message sent across the wire.\ntype messageType uint8\n\nconst (\n\tversionMessage messageType = iota\n\trequestMessage\n\tresponseMessage\n\tresponseError\n\tcancelMessage\n\t// Other types to add?\n\t// - chunked request/response messages?\n\t// - health check\n\t// - server status info\n)\n\n// version holds the protocol version number.\ntype version uint32\n\nconst (\n\tinitialVersion version = iota\n)\n\nconst currentVersion = initialVersion\n\nconst hdrLenLen = uint32(4) // size of the header length included in each message\n\n// # Message formats\n//\n// All messages have the following format:\n//    id        [8]byte       -- identifier used to track the message\n//    type      [1]byte       -- messageType\n//    length    [7]byte       -- length of the remainder of the message\n//    payload   [length]byte  -- message-type-specific data\n//\n// The format of payload depends on the message type.\n//\n// versionMessage: this is the first message sent on a connection by both sides.\n//    version  [4]byte\n//\n// requestMessage:\n//    headerLen         [4]byte         -- length of the encoded header\n//    header            [headerLen]byte -- encoded header information\n//    payload                           -- call argument serialization\n//\n// The header is encoded using Service Weaver's encoding format for a type that\n// looks like:\n//\n// struct header {\n//   MethodKey       [16]byte\n//   Deadline        int64\n//   TraceContext    [25]byte\n//   MetadataContext map[string]string\n// }\n//\n// responseMessage:\n//    payload holds call result serialization\n//\n// responseError:\n//    payload holds error serialization\n//\n// cancelMessage:\n//    payload is empty\n\n// writeMessage formats and sends a message over w.\n//\n// The message payload is formed by concatenating extraHdr and payload.\n// (Allowing two arguments to form the payload avoids unnecessary allocation\n// and copying when we want to prepend some data to application supplied data).\n//\n// The write is guarded by wlock, which must not be locked when passed in.\n\n\n\n\n\n\n\n\n\n// writeChunked writes the header, extra header, and the payload into w using\n// three different w.Write() calls.\nfunc writeChunked(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte) error {\n\t// We use an iovec with up to three entries.\n\tvar vec [3][]byte\n\n\tnh, np := len(extraHdr), len(payload)\n\tvar hdr [16]byte\n\tbinary.LittleEndian.PutUint64(hdr[0:], id)\n\tbinary.LittleEndian.PutUint64(hdr[8:], uint64(mt)|(uint64(nh+np)<<8))\n\n\tvec[0] = hdr[:]\n\tvec[1] = extraHdr\n\tvec[2] = payload\n\tbuf := net.Buffers(vec[:])\n\n\t// buf.WriteTo is not guaranteed to write the entire contents of buf\n\t// atomically, so we guard the write with a lock to prevent writes from\n\t// interleaving.\n\twlock.Lock()\n\tdefer wlock.Unlock()\n\tn, err := buf.WriteTo(w)\n\tif err == nil && n != 16+int64(nh)+int64(np) {\n\t\terr = fmt.Errorf(\"partial write\")\n\t}\n\treturn err\n}\n\n// writeFlat concatenates the header, extra header, and the payload into\n// a single flat byte slice, and writes it into w using a single w.Write() call.\nfunc writeFlat(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte) error {\n\tnh, np := len(extraHdr), len(payload)\n\tdata := make([]byte, 16+nh+np)\n\tbinary.LittleEndian.PutUint64(data[0:], id)\n\tval := uint64(mt) | (uint64(nh+np) << 8)\n\tbinary.LittleEndian.PutUint64(data[8:], val)\n\tcopy(data[16:], extraHdr)\n\tcopy(data[16+nh:], payload)\n\n\t// Write while holding the lock, since we don't know if the underlying\n\t// io.Write is atomic.\n\t// TODO(mwhittaker): For those io.Writers that are atomic, we can avoid\n\t// locking in some cases.\n\twlock.Lock()\n\tdefer wlock.Unlock()\n\tn, err := w.Write(data)\n\tif err == nil && n != len(data) {\n\t\terr = fmt.Errorf(\"partial write\")\n\t}\n\treturn err\n}\n\n// readMessage reads, parses, and returns the next message from r.\nfunc readMessage(r io.Reader) (messageType, uint64, []byte, error) {\n\t// Read the header.\n\tconst headerSize = 16\n\tvar hdr [headerSize]byte\n\tif _, err := io.ReadFull(r, hdr[:]); err != nil {\n\t\treturn 0, 0, nil, err\n\t}\n\n\t// Extract header contents (see writeMessage for header format).\n\tid := binary.LittleEndian.Uint64(hdr[0:])\n\tw2 := binary.LittleEndian.Uint64(hdr[8:])\n\tmt := messageType(w2 & 0xff)\n\tdataLen := w2 >> 8\n\tconst maxSize = 100 << 20\n\tif dataLen > maxSize {\n\t\treturn 0, 0, nil, fmt.Errorf(\"overly large message length %d\", dataLen)\n\t}\n\n\t// Read the payload.\n\tmsg := make([]byte, int(dataLen))\n\tif _, err := io.ReadFull(r, msg); err != nil {\n\t\treturn 0, 0, nil, err\n\t}\n\treturn mt, id, msg, nil\n}\n\n// writeVersion sends my version number to the peer.\nfunc writeVersion(w io.Writer, wlock *sync.Mutex) error {\n\tvar msg [4]byte\n\tbinary.LittleEndian.PutUint32(msg[:], uint32(currentVersion))\n\treturn writeFlat(w, wlock, versionMessage, 0, nil, msg[:])\n}\n\n// getVersion extracts the version number sent by the peer and picks the\n// appropriate version number to use for communicating with the peer.\nfunc getVersion(id uint64, msg []byte) (version, error) {\n\tif id != 0 {\n\t\treturn 0, fmt.Errorf(\"invalid ID %d in handshake\", id)\n\t}\n\t// Allow messages longer than needed so that future updates can send more info.\n\tif len(msg) < 4 {\n\t\treturn 0, fmt.Errorf(\"bad version message length %d, must be >= 4\", len(msg))\n\t}\n\tv := binary.LittleEndian.Uint32(msg)\n\n\t// We use the minimum of the peer and my version numbers.\n\tif v < uint32(currentVersion) {\n\t\treturn version(v), nil\n\t}\n\treturn currentVersion, nil\n}\n\n```\n# Function Signature: func writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n# Natural Language Annotations: // # Message formats\n//\n// All messages have the following format:\n//    id        [8]byte       -- identifier used to track the message\n//    type      [1]byte       -- messageType\n//    length    [7]byte       -- length of the remainder of the message\n//    payload   [length]byte  -- message-type-specific data\n//\n// The format of payload depends on the message type.\n//\n// versionMessage: this is the first message sent on a connection by both sides.\n//    version  [4]byte\n//\n// requestMessage:\n//    headerLen         [4]byte         -- length of the encoded header\n//    header            [headerLen]byte -- encoded header information\n//    payload                           -- call argument serialization\n//\n// The header is encoded using Service Weaver's encoding format for a type that\n// looks like:\n//\n// struct header {\n//   MethodKey       [16]byte\n//   Deadline        int64\n//   TraceContext    [25]byte\n//   MetadataContext map[string]string\n// }\n//\n// responseMessage:\n//    payload holds call result serialization\n//\n// responseError:\n//    payload holds error serialization\n//\n// cancelMessage:\n//    payload is empty\n// writeMessage formats and sends a message over w.\n//\n// The message payload is formed by concatenating extraHdr and payload.\n// (Allowing two arguments to form the payload avoids unnecessary allocation\n// and copying when we want to prepend some data to application supplied data).\n//\n// The write is guarded by wlock, which must not be locked when passed in.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n    nh := len(extraHdr)\n    np := len(payload)\n    totalLen := nh + np\n\n    // Check whether to use flat or chunked writing\n    if totalLen <= flattenLimit {\n        // Use flat write if total length is within the flatten limit\n        return writeFlat(w, wlock, mt, id, extraHdr, payload)\n    } else {\n        // Use chunked write for larger payloads\n        return writeChunked(w, wlock, mt, id, extraHdr, payload)\n    }\n}\n```\n\nThis function `writeMessage` takes into account the `flattenLimit` to decide whether to use `writeFlat` for smaller messages or `writeChunked` for larger messages. The write operation is synchronized using the `wlock` mutex to ensure thread safety."]}, "weaver-internal/net/call/trace.go-writeTraceContext": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/trace.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// readTraceContext returns a span context with tracing information stored in dec.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func writeTraceContext(ctx context.Context, enc *codegen.Encoder) {\n# Natural Language Annotations: // writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc writeTraceContext(ctx context.Context, enc *codegen.Encoder) {\n\t// Retrieve the span context from the context\n\tspanContext := trace.SpanFromContext(ctx).SpanContext()\n\n\t// Check if the span context is valid\n\tif !spanContext.IsValid() {\n\t\treturn\n\t}\n\n\t// Serialize the trace ID and span ID into the encoder\n\tenc.Write(spanContext.TraceID())\n\tenc.Write(spanContext.SpanID())\n\t\n\t// Serialize the trace flags if needed\n\tenc.Write(byte(spanContext.TraceFlags()))\n\t\n\t// Add any additional fields needed for full context serialization if required\n}\n```"]}, "weaver-internal/net/call/trace.go-readTraceContext": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/trace.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// readTraceContext returns a span context with tracing information stored in dec.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func readTraceContext(dec *codegen.Decoder) *trace.SpanContext {\n# Natural Language Annotations: // readTraceContext returns a span context with tracing information stored in dec.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc readTraceContext(dec *codegen.Decoder) *trace.SpanContext {\n\t// Create a variable to hold the span context.\n\tvar spanContext trace.SpanContext\n\t\n\t// Attempt to decode the span context from the decoder.\n\tif err := dec.Decode(&spanContext); err != nil {\n\t\t// If there's an error decoding, return an empty span context.\n\t\treturn &trace.SpanContext{}\n\t}\n\n\t// Return the span context decoded from the decoder.\n\treturn &spanContext\n}\n```"]}, "ollama-types/model/name.go-ParseNameBare": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func ParseNameBare(s string) Name {\n# Natural Language Annotations: // ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\nPlease return the generated code surrounded by ```\n", "response": ["To implement `ParseNameBare`, we need to parse a string `s` according to the different model name formats as defined in the comments. We'll break the string into its components and construct a `Name` struct. The function will not apply any defaults or merges, as stated in the annotation.\n\nHere's how it can be implemented:\n\n```go\nfunc ParseNameBare(s string) Name {\n\tvar name Name\n\n\t// First, separate out the digest if present using '@'\n\tif modelAndDigest, rawDigest, hasDigest := cutLast(s, \"@\"); hasDigest {\n\t\tname.RawDigest = rawDigest\n\t\ts = modelAndDigest\n\t}\n\n\t// Then, separate out the tag if present using ':'\n\tif modelAndTag, tag, hasTag := cutLast(s, \":\"); hasTag {\n\t\tname.Tag = tag\n\t\ts = modelAndTag\n\t}\n\n\t// Then, split on '/' to separate remaining parts.\n\tparts := strings.Split(s, \"/\")\n\tswitch len(parts) {\n\tcase 3: // Expected to have {host}/{namespace}/{model}\n\t\tname.Host = parts[0]\n\t\tname.Namespace = parts[1]\n\t\tname.Model = parts[2]\n\tcase 2: // Either {namespace}/{model} or {host}/{model}\n\t\tif isValidPart(kindNamespace, parts[0]) {\n\t\t\tname.Namespace = parts[0]\n\t\t\tname.Model = parts[1]\n\t\t} else {\n\t\t\tname.Host = parts[0]\n\t\t\tname.Model = parts[1]\n\t\t}\n\tcase 1: // Only {model}\n\t\tname.Model = parts[0]\n\t}\n\n\treturn name\n}\n```\n\nIn this implementation, we:\n\n1. Check and extract the `RawDigest` from `s` using the `@` separator.\n2. Check and extract the `Tag` from `s` using the `:` separator.\n3. Split the remaining `s` by `/` to separate `Host`, `Namespace`, and `Model`.\n4. Assign these parts into corresponding fields of the `Name` struct based on number of parts available.\n5. Return the `Name` struct with parsed values. \n\nThis approach maintains flexibility given the absence of defaults injection and uses conditionals to map input strings into the structured `Name`."]}, "ollama-types/model/name.go-ParseNameFromFilepath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func ParseNameFromFilepath(s string) (n Name) {\n# Natural Language Annotations: // ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseNameFromFilepath(s string) (n Name) {\n\t// Split the input filepath using the system's file separator\n\tparts := strings.Split(filepath.Clean(s), string(filepath.Separator))\n\n\t// Check if the length of the split parts is 4, which is expected for the form:\n\t// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\tif len(parts) != 4 {\n\t\t// Return an unqualified name with missing parts if the structure is incorrect\n\t\treturn Unqualified(Name{Host: MissingPart, Namespace: MissingPart, Model: MissingPart, Tag: MissingPart})\n\t}\n\n\t// Assign the parts to the respective Name fields\n\tn.Host = parts[0]\n\tn.Namespace = parts[1]\n\tn.Model = parts[2]\n\tn.Tag = parts[3]\n\n\t// Return the constructed name\n\treturn n\n}\n```"]}, "ollama-types/model/name.go-DisplayShortest": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func (n Name) DisplayShortest() string {\n# Natural Language Annotations: // DisplayShort returns a short string version of the name.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc (n Name) DisplayShortest() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteString(\"/\")\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteString(\"/\")\n\t}\n\tif n.Model != \"\" {\n\t\tb.WriteString(n.Model)\n\t}\n\tif n.Tag != \"\" {\n\t\tb.WriteString(\":\")\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteString(\"@\")\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n```"]}, "ollama-types/model/name.go-Filepath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func (n Name) Filepath() string {\n# Natural Language Annotations: // Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (n Name) Filepath() string {\n\tif !n.IsFullyQualified() {\n\t\tpanic(\"Name is not fully qualified\")\n\t}\n\n\t// Use filepath.Join to ensure the path uses the correct system separator\n\treturn filepath.Clean(filepath.Join(n.Host, n.Namespace, n.Model, n.Tag))\n}\n```"]}, "ollama-server/prompt.go-Prompt": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/server/prompt.go:\n```\npackage server\n\nimport (\n\t\"fmt\"\n\t\"log/slog\"\n\t\"strings\"\n\n\t\"text/template/parse\"\n\n\t\"github.com/ollama/ollama/api\"\n\t\"github.com/ollama/ollama/template\"\n)\n\n// isResponseNode checks if the node contains .Response\nfunc isResponseNode(node *parse.ActionNode) bool {\n\tfor _, cmd := range node.Pipe.Cmds {\n\t\tfor _, arg := range cmd.Args {\n\t\t\tif fieldNode, ok := arg.(*parse.FieldNode); ok && len(fieldNode.Ident) > 0 {\n\t\t\t\tif fieldNode.Ident[0] == \"Response\" {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// formatTemplateForResponse formats the template AST to:\n// 1. remove all nodes after the first .Response (if generate=true)\n// 2. add a .Response node to the end if it doesn't exist\n// TODO(jmorganca): this should recursively cut the template before the first .Response\nfunc formatTemplateForResponse(tmpl *template.Template, generate bool) {\n\tvar found bool\n\tfor i, node := range tmpl.Tree.Root.Nodes {\n\t\tif actionNode, ok := node.(*parse.ActionNode); ok {\n\t\t\tif isResponseNode(actionNode) {\n\t\t\t\tfound = true\n\t\t\t\tif generate {\n\t\t\t\t\ttmpl.Tree.Root.Nodes = tmpl.Tree.Root.Nodes[:i+1]\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif !found {\n\t\t// add the response node if it doesn't exist\n\t\tresponseFieldNode := &parse.FieldNode{NodeType: parse.NodeField, Ident: []string{\"Response\"}}\n\t\tresponsePipeNode := &parse.PipeNode{NodeType: parse.NodePipe, Cmds: []*parse.CommandNode{{NodeType: parse.NodeCommand, Args: []parse.Node{responseFieldNode}}}}\n\t\tresponseActionNode := &parse.ActionNode{NodeType: parse.NodeAction, Pipe: responsePipeNode}\n\t\ttmpl.Tree.Root.Nodes = append(tmpl.Tree.Root.Nodes, responseActionNode)\n\t}\n}\n\n// Prompt renders a prompt from a template. If generate is set to true,\n// the response and parts of the template following it are not rendered\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc countTokens(tmpl *template.Template, system string, prompt string, response string, encode func(string) ([]int, error)) (int, error) {\n\trendered, err := Prompt(tmpl, system, prompt, response, false)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\ttokens, err := encode(rendered)\n\tif err != nil {\n\t\tslog.Error(\"failed to encode prompt\", \"err\", err)\n\t\treturn 0, err\n\t}\n\n\treturn len(tokens), err\n}\n\n// ChatPrompt builds up a prompt from a series of messages, truncating based on context window size\nfunc ChatPrompt(tmpl *template.Template, messages []api.Message, window int, encode func(string) ([]int, error)) (string, error) {\n\ttype prompt struct {\n\t\tSystem   string\n\t\tPrompt   string\n\t\tResponse string\n\n\t\timages []int\n\t\ttokens int\n\t}\n\n\tvar p prompt\n\n\t// iterate through messages to build up {system,user,response} prompts\n\tvar imgId int\n\tvar prompts []prompt\n\tfor _, msg := range messages {\n\t\tswitch strings.ToLower(msg.Role) {\n\t\tcase \"system\":\n\t\t\tif p.System != \"\" || p.Prompt != \"\" || p.Response != \"\" {\n\t\t\t\tprompts = append(prompts, p)\n\t\t\t\tp = prompt{}\n\t\t\t}\n\n\t\t\tp.System = msg.Content\n\t\tcase \"user\":\n\t\t\tif p.Prompt != \"\" || p.Response != \"\" {\n\t\t\t\tprompts = append(prompts, p)\n\t\t\t\tp = prompt{}\n\t\t\t}\n\n\t\t\tvar sb strings.Builder\n\t\t\tfor range msg.Images {\n\t\t\t\tfmt.Fprintf(&sb, \"[img-%d] \", imgId)\n\t\t\t\tp.images = append(p.images, imgId)\n\t\t\t\timgId += 1\n\t\t\t}\n\n\t\t\tsb.WriteString(msg.Content)\n\t\t\tp.Prompt = sb.String()\n\t\tcase \"assistant\":\n\t\t\tif p.Response != \"\" {\n\t\t\t\tprompts = append(prompts, p)\n\t\t\t\tp = prompt{}\n\t\t\t}\n\n\t\t\tp.Response = msg.Content\n\t\tdefault:\n\t\t\treturn \"\", fmt.Errorf(\"invalid role: %s, role must be one of [system, user, assistant]\", msg.Role)\n\t\t}\n\t}\n\n\t// add final prompt\n\tif p.System != \"\" || p.Prompt != \"\" || p.Response != \"\" {\n\t\tprompts = append(prompts, p)\n\t}\n\n\t// calculate token lengths for each prompt, estimating 768 tokens per images\n\tfor i, p := range prompts {\n\t\ttokens, err := countTokens(tmpl, p.System, p.Prompt, p.Response, encode)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tprompts[i].tokens = tokens + len(prompts[i].images)*768\n\t}\n\n\t// truncate images and prompts starting from the beginning of the list\n\t// until either one prompt remains or the total tokens fits the context window\n\t// TODO (jmorganca): this doesn't account for the context window room required for the response\n\tfor {\n\t\tvar required int\n\t\tfor _, p := range prompts {\n\t\t\trequired += p.tokens\n\t\t}\n\n\t\trequired += 1 // for bos token\n\n\t\tif required <= window {\n\t\t\tslog.Debug(\"prompt now fits in context window\", \"required\", required, \"window\", window)\n\t\t\tbreak\n\t\t}\n\n\t\tprompt := &prompts[0]\n\n\t\tif len(prompt.images) > 1 {\n\t\t\timg := prompt.images[0]\n\t\t\tslog.Debug(\"prompt longer than context window, removing image\", \"id\", img, \"required\", required, \"window\", window)\n\t\t\tprompt.images = prompt.images[1:]\n\t\t\tprompt.Prompt = strings.Replace(prompt.Prompt, fmt.Sprintf(\" [img-%d]\", img), \"\", 1)\n\t\t\tprompt.tokens -= 768\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(prompts) > 1 {\n\t\t\tslog.Debug(\"required tokens longer than context window, removing first prompt\", \"prompt\", prompts[0].tokens, \"required\", required, \"window\", window)\n\t\t\tsystem := prompt.System\n\t\t\tprompts = prompts[1:]\n\n\t\t\tif system != \"\" && prompts[0].System == \"\" {\n\t\t\t\tprompts[0].System = system\n\n\t\t\t\ttokens, err := countTokens(tmpl, prompts[0].System, prompts[0].Prompt, prompts[0].Response, encode)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn \"\", err\n\t\t\t\t}\n\n\t\t\t\tprompts[0].tokens = tokens + len(prompts[0].images)*768\n\t\t\t}\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// stop truncating if there's only one prompt left\n\t\tbreak\n\t}\n\n\tvar sb strings.Builder\n\tfor i, p := range prompts {\n\t\t// last prompt should leave the response unrendered (for completion)\n\t\trendered, err := Prompt(tmpl, p.System, p.Prompt, p.Response, i == len(prompts)-1)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tsb.WriteString(rendered)\n\t}\n\n\treturn sb.String(), nil\n}\n\n```\n# Function Signature: func Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) {\n# Natural Language Annotations: // Prompt renders a prompt from a template. If generate is set to true,\n// the response and parts of the template following it are not rendered\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Prompt renders a prompt from a template. If generate is set to true,\n// the response and parts of the template following it are not rendered.\nfunc Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) {\n\t// Apply transformation based on whether to generate a completion or not\n\tformatTemplateForResponse(tmpl, generate)\n\n\t// Create a data map for template execution\n\tdata := map[string]interface{}{\n\t\t\"System\":   system,\n\t\t\"Prompt\":   prompt,\n\t\t\"Response\": response,\n\t}\n\n\t// Buffer to hold the rendered template result\n\tvar sb strings.Builder\n\n\t// Execute the template with the provided data\n\terr := tmpl.Execute(&sb, data)\n\tif err != nil {\n\t\tslog.Error(\"failed to render template\", \"err\", err)\n\t\treturn \"\", err\n\t}\n\n\t// Return the rendered string and any errors encountered\n\treturn sb.String(), nil\n}\n```"]}, "ollama-server/sched.go-unload": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/server/sched.go:\n```\npackage server\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/ollama/ollama/api\"\n\t\"github.com/ollama/ollama/envconfig\"\n\t\"github.com/ollama/ollama/format\"\n\t\"github.com/ollama/ollama/gpu\"\n\t\"github.com/ollama/ollama/llm\"\n)\n\ntype LlmRequest struct {\n\tctx             context.Context //nolint:containedctx\n\tmodel           *Model\n\topts            api.Options\n\torigNumCtx      int // Track the initial ctx request\n\tsessionDuration *api.Duration\n\tsuccessCh       chan *runnerRef\n\terrCh           chan error\n\tschedAttempts   uint\n}\n\ntype Scheduler struct {\n\tpendingReqCh  chan *LlmRequest\n\tfinishedReqCh chan *LlmRequest\n\texpiredCh     chan *runnerRef\n\tunloadedCh    chan interface{}\n\n\tloaded   map[string]*runnerRef\n\tloadedMu sync.Mutex\n\n\tloadFn       func(req *LlmRequest, ggml *llm.GGML, gpus gpu.GpuInfoList, numParallel int)\n\tnewServerFn  func(gpus gpu.GpuInfoList, model string, ggml *llm.GGML, adapters []string, projectors []string, opts api.Options, numParallel int) (llm.LlamaServer, error)\n\tgetGpuFn     func() gpu.GpuInfoList\n\tgetCpuFn     func() gpu.GpuInfoList\n\treschedDelay time.Duration\n}\n\n// Default automatic value for number of models we allow per GPU\n// Model will still need to fit in VRAM, but loading many small models\n// on a large GPU can cause stalling\nvar defaultModelsPerGPU = 3\n\n// Default automatic value for parallel setting\n// Model will still need to fit in VRAM.  If this setting wont fit\n// we'll back off down to 1 to try to get it to fit\nvar defaultParallel = 4\n\nvar ErrMaxQueue = fmt.Errorf(\"server busy, please try again.  maximum pending requests exceeded\")\n\nfunc InitScheduler(ctx context.Context) *Scheduler {\n\tsched := &Scheduler{\n\t\tpendingReqCh:  make(chan *LlmRequest, envconfig.MaxQueuedRequests),\n\t\tfinishedReqCh: make(chan *LlmRequest, envconfig.MaxQueuedRequests),\n\t\texpiredCh:     make(chan *runnerRef, envconfig.MaxQueuedRequests),\n\t\tunloadedCh:    make(chan interface{}, envconfig.MaxQueuedRequests),\n\t\tloaded:        make(map[string]*runnerRef),\n\t\tnewServerFn:   llm.NewLlamaServer,\n\t\tgetGpuFn:      gpu.GetGPUInfo,\n\t\tgetCpuFn:      gpu.GetCPUInfo,\n\t\treschedDelay:  250 * time.Millisecond,\n\t}\n\tsched.loadFn = sched.load\n\treturn sched\n}\n\n// context must be canceled to decrement ref count and release the runner\nfunc (s *Scheduler) GetRunner(c context.Context, model *Model, opts api.Options, sessionDuration *api.Duration) (chan *runnerRef, chan error) {\n\tif opts.NumCtx < 4 {\n\t\topts.NumCtx = 4\n\t}\n\n\treq := &LlmRequest{\n\t\tctx:             c,\n\t\tmodel:           model,\n\t\topts:            opts,\n\t\tsessionDuration: sessionDuration,\n\t\tsuccessCh:       make(chan *runnerRef),\n\t\terrCh:           make(chan error, 1),\n\t}\n\n\tselect {\n\tcase s.pendingReqCh <- req:\n\tdefault:\n\t\treq.errCh <- ErrMaxQueue\n\t}\n\treturn req.successCh, req.errCh\n}\n\n// Returns immediately, spawns go routines for the scheduler which will shutdown when ctx is done\nfunc (s *Scheduler) Run(ctx context.Context) {\n\tslog.Debug(\"starting llm scheduler\")\n\tgo func() {\n\t\ts.processPending(ctx)\n\t}()\n\n\tgo func() {\n\t\ts.processCompleted(ctx)\n\t}()\n}\n\nfunc (s *Scheduler) processPending(ctx context.Context) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tslog.Debug(\"shutting down scheduler pending loop\")\n\t\t\treturn\n\t\tcase pending := <-s.pendingReqCh:\n\t\t\t// Block other requests until we get this pending request running\n\t\t\tpending.schedAttempts++\n\t\t\tif pending.origNumCtx == 0 {\n\t\t\t\tpending.origNumCtx = pending.opts.NumCtx\n\t\t\t}\n\n\t\t\tif pending.ctx.Err() != nil {\n\t\t\t\tslog.Debug(\"pending request cancelled or timed out, skipping scheduling\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnumParallel := envconfig.NumParallel\n\t\t\t// TODO (jmorganca): multimodal models don't support parallel yet\n\t\t\t// see https://github.com/ollama/ollama/issues/4165\n\t\t\tif len(pending.model.ProjectorPaths) > 0 && numParallel != 1 {\n\t\t\t\tnumParallel = 1\n\t\t\t\tslog.Warn(\"multimodal models don't support parallel requests yet\")\n\t\t\t}\n\t\t\t// Keep NumCtx and numParallel in sync\n\t\t\tif numParallel > 1 {\n\t\t\t\tpending.opts.NumCtx = pending.origNumCtx * numParallel\n\t\t\t}\n\n\t\t\tfor {\n\t\t\t\tvar runnerToExpire *runnerRef\n\t\t\t\ts.loadedMu.Lock()\n\t\t\t\trunner := s.loaded[pending.model.ModelPath]\n\t\t\t\tloadedCount := len(s.loaded)\n\t\t\t\ts.loadedMu.Unlock()\n\t\t\t\tif runner != nil {\n\t\t\t\t\tif runner.needsReload(ctx, pending) {\n\t\t\t\t\t\trunnerToExpire = runner\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// Runner is usable, return it\n\t\t\t\t\t\tpending.useLoadedRunner(runner, s.finishedReqCh)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t} else if envconfig.MaxRunners > 0 && loadedCount >= envconfig.MaxRunners {\n\t\t\t\t\tslog.Debug(\"max runners achieved, unloading one to make room\", \"runner_count\", loadedCount)\n\t\t\t\t\trunnerToExpire = s.findRunnerToUnload()\n\t\t\t\t} else {\n\t\t\t\t\t// Either no models are loaded or below envconfig.MaxRunners\n\t\t\t\t\t// Get a refreshed GPU list\n\t\t\t\t\tvar gpus gpu.GpuInfoList\n\t\t\t\t\tif pending.opts.NumGPU == 0 {\n\t\t\t\t\t\tgpus = s.getCpuFn()\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgpus = s.getGpuFn()\n\t\t\t\t\t}\n\n\t\t\t\t\tif envconfig.MaxRunners <= 0 {\n\t\t\t\t\t\t// No user specified MaxRunners, so figure out what automatic setting to use\n\t\t\t\t\t\t// If all GPUs have reliable free memory reporting, defaultModelsPerGPU * the number of GPUs\n\t\t\t\t\t\t// if any GPU has unreliable free memory reporting, 1x the number of GPUs\n\t\t\t\t\t\tallReliable := true\n\t\t\t\t\t\tfor _, gpu := range gpus {\n\t\t\t\t\t\t\tif gpu.UnreliableFreeMemory {\n\t\t\t\t\t\t\t\tallReliable = false\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif allReliable {\n\t\t\t\t\t\t\tenvconfig.MaxRunners = defaultModelsPerGPU * len(gpus)\n\t\t\t\t\t\t\tslog.Debug(\"updating default concurrency\", \"OLLAMA_MAX_LOADED_MODELS\", envconfig.MaxRunners, \"gpu_count\", len(gpus))\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tslog.Info(\"one or more GPUs detected that are unable to accurately report free memory - disabling default concurrency\")\n\t\t\t\t\t\t\tenvconfig.MaxRunners = len(gpus)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// Load model for fitting\n\t\t\t\t\tggml, err := llm.LoadModel(pending.model.ModelPath, 0)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tpending.errCh <- err\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\n\t\t\t\t\t// Evaluate if the model will fit in the available system memory, or if we should unload a model first\n\t\t\t\t\tif len(gpus) == 1 && gpus[0].Library == \"cpu\" {\n\t\t\t\t\t\t// simplifying assumption of defaultParallel when in CPU mode\n\t\t\t\t\t\tif numParallel <= 0 {\n\t\t\t\t\t\t\tnumParallel = defaultParallel\n\t\t\t\t\t\t\tpending.opts.NumCtx = pending.origNumCtx * numParallel\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif loadedCount == 0 {\n\t\t\t\t\t\t\tslog.Debug(\"cpu mode with first model, loading\")\n\t\t\t\t\t\t\ts.loadFn(pending, ggml, gpus, numParallel)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\trunnerToExpire = s.maybeFindCPURunnerToUnload(pending, ggml, gpus)\n\t\t\t\t\t\tif runnerToExpire == nil {\n\t\t\t\t\t\t\tslog.Debug(\"cpu mode with available system memory or first model, loading\")\n\t\t\t\t\t\t\ts.loadFn(pending, ggml, gpus, numParallel)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// else we need to expire a runner\n\t\t\t\t\t} else if loadedCount == 0 {\n\t\t\t\t\t\t// No models loaded. Load the model but prefer the best fit.\n\t\t\t\t\t\tslog.Debug(\"loading first model\", \"model\", pending.model.ModelPath)\n\t\t\t\t\t\tg := pickBestFitGPUs(pending, ggml, gpus, &numParallel)\n\t\t\t\t\t\tif g != nil {\n\t\t\t\t\t\t\tgpus = g\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.loadFn(pending, ggml, gpus, numParallel)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\n\t\t\t\t\tif runnerToExpire == nil {\n\t\t\t\t\t\t// More than one loaded model, so we have to see if the\n\t\t\t\t\t\t// new one fits\n\t\t\t\t\t\t//\n\t\t\t\t\t\t// We want to avoid loading on any GPUs that have other\n\t\t\t\t\t\t// models still loading on them to avoid potential races\n\t\t\t\t\t\t// with VRAM consumption ramping up during load\n\t\t\t\t\t\tavailGpus := s.filterGPUsWithoutLoadingModels(gpus)\n\n\t\t\t\t\t\t// Update free memory from currently loaded models\n\t\t\t\t\t\ts.updateFreeSpace(availGpus)\n\t\t\t\t\t\tfitGpus := pickBestFitGPUs(pending, ggml, availGpus, &numParallel)\n\t\t\t\t\t\tif fitGpus != nil {\n\t\t\t\t\t\t\tslog.Debug(\"new model fits with existing models, loading\")\n\t\t\t\t\t\t\ts.loadFn(pending, ggml, fitGpus, numParallel)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// We couldn't find a set of GPUs to fully load the new\n\t\t\t\t\t\t// model. If no other models are loading (both GPU lists\n\t\t\t\t\t\t// are the same) then we need to unload another model to\n\t\t\t\t\t\t// make room\n\t\t\t\t\t\tif len(availGpus) < len(gpus) {\n\t\t\t\t\t\t\t// There are other requests pending, and this one\n\t\t\t\t\t\t\t// needs more time, so put it on the back of the\n\t\t\t\t\t\t\t// queue so that we might satisfy other pending\n\t\t\t\t\t\t\t// requests that aren't blocked\n\t\t\t\t\t\t\tgo func() {\n\t\t\t\t\t\t\t\t// Process in a go routine to avoid deadlocking\n\t\t\t\t\t\t\t\t// the scheduler if our queue is full\n\t\t\t\t\t\t\t\tslog.Debug(\"delaying scheduling while other models finish loading\", \"attempts\", pending.schedAttempts, \"model\", pending.model.ModelPath)\n\t\t\t\t\t\t\t\ttime.Sleep(s.reschedDelay)\n\t\t\t\t\t\t\t\ts.pendingReqCh <- pending\n\t\t\t\t\t\t\t}()\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\trunnerToExpire = s.findRunnerToUnload()\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif runnerToExpire == nil {\n\t\t\t\t\t// Shouildn't happen\n\t\t\t\t\tslog.Error(\"runner to expire was nil!\")\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// Trigger an expiration to unload once it's done\n\t\t\t\trunnerToExpire.refMu.Lock()\n\t\t\t\tslog.Debug(\"resetting model to expire immediately to make room\", \"modelPath\", runnerToExpire.modelPath, \"refCount\", runnerToExpire.refCount)\n\t\t\t\tif runnerToExpire.expireTimer != nil {\n\t\t\t\t\trunnerToExpire.expireTimer.Stop()\n\t\t\t\t\trunnerToExpire.expireTimer = nil\n\t\t\t\t}\n\t\t\t\trunnerToExpire.sessionDuration = 0\n\t\t\t\tif runnerToExpire.refCount <= 0 {\n\t\t\t\t\ts.expiredCh <- runnerToExpire\n\t\t\t\t}\n\t\t\t\trunnerToExpire.refMu.Unlock()\n\t\t\t\t// Wait for the unload to happen\n\t\t\t\t// Note: at this point we're queueing up all incoming requests, even if they were for\n\t\t\t\t// a different model that's loaded and not scheduled to be removed.\n\t\t\t\tslog.Debug(\"waiting for pending requests to complete and unload to occur\", \"modelPath\", runnerToExpire.modelPath)\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tslog.Debug(\"shutting down scheduler pending loop\")\n\t\t\t\t\treturn\n\t\t\t\tcase <-s.unloadedCh:\n\t\t\t\t\tslog.Debug(\"unload completed\", \"modelPath\", runnerToExpire.modelPath)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\tcase <-s.unloadedCh:\n\t\t\t// An unload request when there are no pending request can be ignored\n\t\t\tslog.Debug(\"ignoring unload event with no pending requests\")\n\t\t}\n\t}\n}\n\nfunc (s *Scheduler) processCompleted(ctx context.Context) {\n\t// Process completed requests, expired timers, and unloading models\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tslog.Debug(\"shutting down scheduler completed loop\")\n\t\t\treturn\n\t\tcase finished := <-s.finishedReqCh:\n\t\t\ts.loadedMu.Lock()\n\t\t\trunner := s.loaded[finished.model.ModelPath]\n\t\t\ts.loadedMu.Unlock()\n\t\t\tif runner == nil {\n\t\t\t\tslog.Error(\"finished request signal received after model unloaded\", \"modelPath\", finished.model.ModelPath)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trunner.refMu.Lock()\n\t\t\trunner.refCount--\n\t\t\tif runner.refCount <= 0 {\n\t\t\t\tif runner.sessionDuration <= 0 {\n\t\t\t\t\tslog.Debug(\"runner with zero duration has gone idle, expiring to unload\", \"modelPath\", runner.modelPath)\n\t\t\t\t\tif runner.expireTimer != nil {\n\t\t\t\t\t\trunner.expireTimer.Stop()\n\t\t\t\t\t\trunner.expireTimer = nil\n\t\t\t\t\t}\n\t\t\t\t\ts.expiredCh <- runner\n\t\t\t\t} else if runner.expireTimer == nil {\n\t\t\t\t\tslog.Debug(\"runner with non-zero duration has gone idle, adding timer\", \"modelPath\", runner.modelPath, \"duration\", runner.sessionDuration)\n\t\t\t\t\trunner.expireTimer = time.AfterFunc(runner.sessionDuration, func() {\n\t\t\t\t\t\tslog.Debug(\"timer expired, expiring to unload\", \"modelPath\", runner.modelPath)\n\t\t\t\t\t\trunner.refMu.Lock()\n\t\t\t\t\t\tdefer runner.refMu.Unlock()\n\t\t\t\t\t\tif runner.expireTimer != nil {\n\t\t\t\t\t\t\trunner.expireTimer.Stop()\n\t\t\t\t\t\t\trunner.expireTimer = nil\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.expiredCh <- runner\n\t\t\t\t\t})\n\t\t\t\t\trunner.expiresAt = time.Now().Add(runner.sessionDuration)\n\t\t\t\t} else {\n\t\t\t\t\tslog.Debug(\"runner with non-zero duration has gone idle, resetting timer\", \"modelPath\", runner.modelPath, \"duration\", runner.sessionDuration)\n\t\t\t\t\trunner.expireTimer.Reset(runner.sessionDuration)\n\t\t\t\t\trunner.expiresAt = time.Now().Add(runner.sessionDuration)\n\t\t\t\t}\n\t\t\t}\n\t\t\tslog.Debug(\"after processing request finished event\", \"modelPath\", runner.modelPath, \"refCount\", runner.refCount)\n\t\t\trunner.refMu.Unlock()\n\t\tcase runner := <-s.expiredCh:\n\t\t\tslog.Debug(\"runner expired event received\", \"modelPath\", runner.modelPath)\n\t\t\trunner.refMu.Lock()\n\t\t\tif runner.refCount > 0 {\n\t\t\t\t// Shouldn't happen, but safeguard to ensure no leaked runners\n\t\t\t\tslog.Debug(\"expired event with positive ref count, retrying\", \"modelPath\", runner.modelPath, \"refCount\", runner.refCount)\n\t\t\t\tgo func(runner *runnerRef) {\n\t\t\t\t\t// We can't unload yet, but want to as soon as the current request completes\n\t\t\t\t\t// So queue up another expired event\n\t\t\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\t\t\ts.expiredCh <- runner\n\t\t\t\t}(runner)\n\t\t\t\trunner.refMu.Unlock()\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\ts.loadedMu.Lock()\n\t\t\tslog.Debug(\"got lock to unload\", \"modelPath\", runner.modelPath)\n\t\t\tfinished := runner.waitForVRAMRecovery()\n\t\t\trunner.unload()\n\t\t\tdelete(s.loaded, runner.modelPath)\n\t\t\ts.loadedMu.Unlock()\n\t\t\tslog.Debug(\"runner released\", \"modelPath\", runner.modelPath)\n\t\t\trunner.refMu.Unlock()\n\n\t\t\t<-finished\n\t\t\tslog.Debug(\"sending an unloaded event\", \"modelPath\", runner.modelPath)\n\t\t\ts.unloadedCh <- struct{}{}\n\t\t}\n\t}\n}\n\n// Complete the pending request and send the runner back to the requester\n// Wires up a finished event after the request context is completed\n// Updates session duration, and resets expiration timer\nfunc (pending *LlmRequest) useLoadedRunner(runner *runnerRef, finished chan *LlmRequest) {\n\trunner.refMu.Lock()\n\tdefer runner.refMu.Unlock()\n\trunner.refCount++\n\tif runner.expireTimer != nil {\n\t\trunner.expireTimer.Stop()\n\t\trunner.expireTimer = nil\n\t}\n\tif pending.sessionDuration != nil {\n\t\trunner.sessionDuration = pending.sessionDuration.Duration\n\t}\n\tpending.successCh <- runner\n\tgo func() {\n\t\t<-pending.ctx.Done()\n\t\tslog.Debug(\"context for request finished\")\n\t\tfinished <- pending\n\t}()\n}\n\nfunc (s *Scheduler) load(req *LlmRequest, ggml *llm.GGML, gpus gpu.GpuInfoList, numParallel int) {\n\tif numParallel < 1 {\n\t\tnumParallel = 1\n\t}\n\tsessionDuration := envconfig.KeepAlive\n\tif req.sessionDuration != nil {\n\t\tsessionDuration = req.sessionDuration.Duration\n\t}\n\tllama, err := s.newServerFn(gpus, req.model.ModelPath, ggml, req.model.AdapterPaths, req.model.ProjectorPaths, req.opts, numParallel)\n\tif err != nil {\n\t\t// some older models are not compatible with newer versions of llama.cpp\n\t\t// show a generalized compatibility error until there is a better way to\n\t\t// check for model compatibility\n\t\tif errors.Is(llm.ErrUnsupportedFormat, err) || strings.Contains(err.Error(), \"failed to load model\") {\n\t\t\terr = fmt.Errorf(\"%v: this model may be incompatible with your version of Ollama. If you previously pulled this model, try updating it by running `ollama pull %s`\", err, req.model.ShortName)\n\t\t}\n\t\tslog.Info(\"NewLlamaServer failed\", \"model\", req.model.ModelPath, \"error\", err)\n\t\treq.errCh <- err\n\t\treturn\n\t}\n\trunner := &runnerRef{\n\t\tmodel:           req.model,\n\t\tmodelPath:       req.model.ModelPath,\n\t\tllama:           llama,\n\t\tOptions:         &req.opts,\n\t\tsessionDuration: sessionDuration,\n\t\tgpus:            gpus,\n\t\testimatedVRAM:   llama.EstimatedVRAM(),\n\t\testimatedTotal:  llama.EstimatedTotal(),\n\t\tloading:         true,\n\t\trefCount:        1,\n\t}\n\trunner.numParallel = numParallel\n\trunner.refMu.Lock()\n\n\ts.loadedMu.Lock()\n\ts.loaded[req.model.ModelPath] = runner\n\tslog.Info(\"loaded runners\", \"count\", len(s.loaded))\n\ts.loadedMu.Unlock()\n\n\tgo func() {\n\t\tdefer runner.refMu.Unlock()\n\t\tif err = llama.WaitUntilRunning(req.ctx); err != nil {\n\t\t\tslog.Error(\"error loading llama server\", \"error\", err)\n\t\t\trunner.refCount--\n\t\t\treq.errCh <- err\n\t\t\tslog.Debug(\"triggering expiration for failed load\", \"model\", runner.modelPath)\n\t\t\ts.expiredCh <- runner\n\t\t\treturn\n\t\t}\n\t\tslog.Debug(\"finished setting up runner\", \"model\", req.model.ModelPath)\n\t\trunner.loading = false\n\t\tgo func() {\n\t\t\t<-req.ctx.Done()\n\t\t\tslog.Debug(\"context for request finished\")\n\t\t\ts.finishedReqCh <- req\n\t\t}()\n\t\treq.successCh <- runner\n\t}()\n}\n\nfunc (s *Scheduler) updateFreeSpace(allGpus gpu.GpuInfoList) {\n\ttype predKey struct {\n\t\tLibrary string\n\t\tID      string\n\t}\n\tpredMap := map[predKey]uint64{} // Sum up the total predicted usage per GPU for all runners\n\ts.loadedMu.Lock()\n\tfor _, r := range s.loaded {\n\t\tr.refMu.Lock()\n\t\tif r.llama != nil {\n\t\t\tfor _, gpu := range allGpus {\n\t\t\t\tpredMap[predKey{gpu.Library, gpu.ID}] += r.llama.EstimatedVRAMByGPU(gpu.ID)\n\t\t\t}\n\t\t} else {\n\t\t\tslog.Warn(\"unexpected nil runner reference, memory prediction may be incorrect\")\n\t\t}\n\t\tr.refMu.Unlock()\n\t}\n\ts.loadedMu.Unlock()\n\n\t// Now that we've summed up all the GPU usage predictions across all the loaded runners, update the gpu list\n\tfor i := range allGpus {\n\t\tif p, ok := predMap[predKey{allGpus[i].Library, allGpus[i].ID}]; ok {\n\t\t\tslog.Debug(\"gpu reported\", \"gpu\", allGpus[i].ID, \"library\", allGpus[i].Library, \"available\", format.HumanBytes2(allGpus[i].FreeMemory))\n\t\t\tif p > allGpus[i].TotalMemory {\n\t\t\t\t// Shouldn't happen\n\t\t\t\tslog.Warn(\"predicted usage exceeds VRAM\", \"gpu\", allGpus[i].ID, \"totalMemory\", allGpus[i].TotalMemory, \"predicted\", p)\n\t\t\t\tallGpus[i].FreeMemory = 0\n\t\t\t} else if (allGpus[i].TotalMemory - p) < allGpus[i].FreeMemory { // predicted free is smaller than reported free, use it\n\t\t\t\t// TODO maybe we should just always trust our numbers, since cuda's free memory reporting is laggy\n\t\t\t\t// and we might unload models we didn't actually need to.  The risk is if some other GPU intensive app is loaded\n\t\t\t\t// after we start our first runner, then we'll never acount for that, so picking the smallest free value seems prudent.\n\t\t\t\tallGpus[i].FreeMemory = allGpus[i].TotalMemory - p\n\t\t\t}\n\t\t\tslog.Info(\"updated VRAM based on existing loaded models\", \"gpu\", allGpus[i].ID, \"library\", allGpus[i].Library, \"total\", format.HumanBytes2(allGpus[i].TotalMemory), \"available\", format.HumanBytes2(allGpus[i].FreeMemory))\n\t\t}\n\t}\n}\n\n// While models are loading the VRAM consumption numbers will be indeterminate, so we have\n// to avoid scheduling another model on the same GPU(s) that haven't stabilized.\n// This routine returns the set of GPUs that do not have an active loading model.\n// If all GPUs have loading models, an empty list will be returned (not a single CPU entry)\nfunc (s *Scheduler) filterGPUsWithoutLoadingModels(allGpus gpu.GpuInfoList) gpu.GpuInfoList {\n\tret := append(gpu.GpuInfoList{}, allGpus...)\n\ts.loadedMu.Lock()\n\tdefer s.loadedMu.Unlock()\n\tfor _, runner := range s.loaded {\n\t\tif runner.loading {\n\t\t\tslog.Debug(\"overlapping loads detected\", \"gpus\", runner.gpus, \"model\", runner.modelPath)\n\t\t\tfor _, busyGPU := range runner.gpus {\n\t\t\t\tfor i := range ret {\n\t\t\t\t\tif ret[i].ID == busyGPU.ID {\n\t\t\t\t\t\tret = append(ret[:i], ret[i+1:]...)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn ret\n}\n\n// TODO consolidate sched_types.go\ntype runnerRef struct {\n\trefMu sync.Mutex\n\t// refCond   sync.Cond // Signaled on transition from 1 -> 0\n```\n# Function Signature: func (runner *runnerRef) unload() {\n# Natural Language Annotations: // The refMu must already be held when calling unload\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (runner *runnerRef) unload() {\n\t// Assuming refMu is already locked by the caller of this function as per the comments\n\n\t// Unload the model from memory by releasing resources held by llama\n\tif runner.llama != nil {\n\t\trunner.llama.Unload()\n\t\trunner.llama = nil\n\t}\n\n\t// Reset other state variables\n\trunner.sessionDuration = 0\n\trunner.numParallel = 0\n\trunner.estimatedVRAM = 0\n\trunner.estimatedTotal = 0\n\trunner.loading = false\n\n\t// If there was an expiration timer set, stop and nil it\n\tif runner.expireTimer != nil {\n\t\trunner.expireTimer.Stop()\n\t\trunner.expireTimer = nil\n\t}\n}\n```"]}, "lo-retry.go-NewDebounce": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/retry.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\ntype debounce struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\ttimer     *time.Timer\n\tdone      bool\n\tcallbacks []func()\n}\n\nfunc (d *debounce) reset() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.done {\n\t\treturn\n\t}\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t}\n\n\td.timer = time.AfterFunc(d.after, func() {\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i]()\n\t\t}\n\t})\n}\n\nfunc (d *debounce) cancel() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t\td.timer = nil\n\t}\n\n\td.done = true\n}\n\n// NewDebounce creates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/mz32VMK2nqe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype debounceByItem struct {\n\tmu    *sync.Mutex\n\ttimer *time.Timer\n\tcount int\n}\n\ntype debounceBy[T comparable] struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\titems     map[T]*debounceByItem\n\tcallbacks []func(key T, count int)\n}\n\nfunc (d *debounceBy[T]) reset(key T) {\n\td.mu.Lock()\n\tif _, ok := d.items[key]; !ok {\n\t\td.items[key] = &debounceByItem{\n\t\t\tmu:    new(sync.Mutex),\n\t\t\ttimer: nil,\n\t\t}\n\t}\n\n\titem := d.items[key]\n\n\td.mu.Unlock()\n\n\titem.mu.Lock()\n\tdefer item.mu.Unlock()\n\n\titem.count++\n\n\tif item.timer != nil {\n\t\titem.timer.Stop()\n\t}\n\n\titem.timer = time.AfterFunc(d.after, func() {\n\t\titem.mu.Lock()\n\t\tcount := item.count\n\t\titem.count = 0\n\t\titem.mu.Unlock()\n\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i](key, count)\n\t\t}\n\n\t})\n}\n\nfunc (d *debounceBy[T]) cancel(key T) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif item, ok := d.items[key]; ok {\n\t\titem.mu.Lock()\n\n\t\tif item.timer != nil {\n\t\t\titem.timer.Stop()\n\t\t\titem.timer = nil\n\t\t}\n\n\t\titem.mu.Unlock()\n\n\t\tdelete(d.items, key)\n\t}\n}\n\n// NewDebounceBy creates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/d3Vpt6pxhY8\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Attempt invokes a function N times until it returns valid output. Returning either the caught error or nil. When first argument is less than `1`, the function runs until a successful response is returned.\n// Play: https://go.dev/play/p/3ggJZ2ZKcMj\nfunc Attempt(maxIteration int, f func(index int) error) (int, error) {\n\tvar err error\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr = f(i)\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n\n\n// AttemptWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil.\n// When first argument is less than `1`, the function runs until a successful\n// response is returned.\n// Play: https://go.dev/play/p/tVs6CygC7m1\nfunc AttemptWithDelay(maxIteration int, delay time.Duration, f func(index int, duration time.Duration) error) (int, time.Duration, error) {\n\tvar err error\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr = f(i, time.Since(start))\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\n// AttemptWhile invokes a function N times until it returns valid output.\n// Returning either the caught error or nil, and along with a bool value to identify\n// whether it needs invoke function continuously. It will terminate the invoke\n// immediately if second bool value is returned with falsy value. When first\n// argument is less than `1`, the function runs until a successful response is\n// returned.\nfunc AttemptWhile(maxIteration int, f func(int) (error, bool)) (int, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr, shouldContinueInvoke = f(i)\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n// AttemptWhileWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil, and along\n// with a bool value to identify whether it needs to invoke function continuously.\n// It will terminate the invoke immediately if second bool value is returned with falsy\n// value. When first argument is less than `1`, the function runs until a successful\n// response is returned.\nfunc AttemptWhileWithDelay(maxIteration int, delay time.Duration, f func(int, time.Duration) (error, bool)) (int, time.Duration, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr, shouldContinueInvoke = f(i, time.Since(start))\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, time.Since(start), err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\ntype transactionStep[T any] struct {\n\texec       func(T) (T, error)\n\tonRollback func(T) T\n}\n\n// NewTransaction instantiate a new transaction.\nfunc NewTransaction[T any]() *Transaction[T] {\n\treturn &Transaction[T]{\n\t\tsteps: []transactionStep[T]{},\n\t}\n}\n\n// Transaction implements a Saga pattern\ntype Transaction[T any] struct {\n\tsteps []transactionStep[T]\n}\n\n// Then adds a step to the chain of callbacks. It returns the same Transaction.\nfunc (t *Transaction[T]) Then(exec func(T) (T, error), onRollback func(T) T) *Transaction[T] {\n\tt.steps = append(t.steps, transactionStep[T]{\n\t\texec:       exec,\n\t\tonRollback: onRollback,\n\t})\n\n\treturn t\n}\n\n// Process runs the Transaction steps and rollbacks in case of errors.\nfunc (t *Transaction[T]) Process(state T) (T, error) {\n\tvar i int\n\tvar err error\n\n\tfor i < len(t.steps) {\n\t\tstate, err = t.steps[i].exec(state)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\ti++\n\t}\n\n\tif err == nil {\n\t\treturn state, nil\n\t}\n\n\tfor i > 0 {\n\t\ti--\n\t\tstate = t.steps[i].onRollback(state)\n\t}\n\n\treturn state, err\n}\n\n// throttle ?\n\n```\n# Function Signature: func NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n# Natural Language Annotations: // NewDebounce creates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/mz32VMK2nqe\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n\td := &debounce{\n\t\tafter:     duration,\n\t\tmu:        &sync.Mutex{},\n\t\tcallbacks: f,\n\t}\n\n\tresetFunc := func() {\n\t\td.reset()\n\t}\n\n\tcancelFunc := func() {\n\t\td.cancel()\n\t}\n\n\treturn resetFunc, cancelFunc\n}\n```"]}, "lo-retry.go-NewDebounceBy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/retry.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\ntype debounce struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\ttimer     *time.Timer\n\tdone      bool\n\tcallbacks []func()\n}\n\nfunc (d *debounce) reset() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.done {\n\t\treturn\n\t}\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t}\n\n\td.timer = time.AfterFunc(d.after, func() {\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i]()\n\t\t}\n\t})\n}\n\nfunc (d *debounce) cancel() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t\td.timer = nil\n\t}\n\n\td.done = true\n}\n\n// NewDebounce creates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/mz32VMK2nqe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype debounceByItem struct {\n\tmu    *sync.Mutex\n\ttimer *time.Timer\n\tcount int\n}\n\ntype debounceBy[T comparable] struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\titems     map[T]*debounceByItem\n\tcallbacks []func(key T, count int)\n}\n\nfunc (d *debounceBy[T]) reset(key T) {\n\td.mu.Lock()\n\tif _, ok := d.items[key]; !ok {\n\t\td.items[key] = &debounceByItem{\n\t\t\tmu:    new(sync.Mutex),\n\t\t\ttimer: nil,\n\t\t}\n\t}\n\n\titem := d.items[key]\n\n\td.mu.Unlock()\n\n\titem.mu.Lock()\n\tdefer item.mu.Unlock()\n\n\titem.count++\n\n\tif item.timer != nil {\n\t\titem.timer.Stop()\n\t}\n\n\titem.timer = time.AfterFunc(d.after, func() {\n\t\titem.mu.Lock()\n\t\tcount := item.count\n\t\titem.count = 0\n\t\titem.mu.Unlock()\n\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i](key, count)\n\t\t}\n\n\t})\n}\n\nfunc (d *debounceBy[T]) cancel(key T) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif item, ok := d.items[key]; ok {\n\t\titem.mu.Lock()\n\n\t\tif item.timer != nil {\n\t\t\titem.timer.Stop()\n\t\t\titem.timer = nil\n\t\t}\n\n\t\titem.mu.Unlock()\n\n\t\tdelete(d.items, key)\n\t}\n}\n\n// NewDebounceBy creates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/d3Vpt6pxhY8\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Attempt invokes a function N times until it returns valid output. Returning either the caught error or nil. When first argument is less than `1`, the function runs until a successful response is returned.\n// Play: https://go.dev/play/p/3ggJZ2ZKcMj\nfunc Attempt(maxIteration int, f func(index int) error) (int, error) {\n\tvar err error\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr = f(i)\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n\n\n// AttemptWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil.\n// When first argument is less than `1`, the function runs until a successful\n// response is returned.\n// Play: https://go.dev/play/p/tVs6CygC7m1\nfunc AttemptWithDelay(maxIteration int, delay time.Duration, f func(index int, duration time.Duration) error) (int, time.Duration, error) {\n\tvar err error\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr = f(i, time.Since(start))\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\n// AttemptWhile invokes a function N times until it returns valid output.\n// Returning either the caught error or nil, and along with a bool value to identify\n// whether it needs invoke function continuously. It will terminate the invoke\n// immediately if second bool value is returned with falsy value. When first\n// argument is less than `1`, the function runs until a successful response is\n// returned.\nfunc AttemptWhile(maxIteration int, f func(int) (error, bool)) (int, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr, shouldContinueInvoke = f(i)\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n// AttemptWhileWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil, and along\n// with a bool value to identify whether it needs to invoke function continuously.\n// It will terminate the invoke immediately if second bool value is returned with falsy\n// value. When first argument is less than `1`, the function runs until a successful\n// response is returned.\nfunc AttemptWhileWithDelay(maxIteration int, delay time.Duration, f func(int, time.Duration) (error, bool)) (int, time.Duration, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr, shouldContinueInvoke = f(i, time.Since(start))\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, time.Since(start), err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\ntype transactionStep[T any] struct {\n\texec       func(T) (T, error)\n\tonRollback func(T) T\n}\n\n// NewTransaction instantiate a new transaction.\nfunc NewTransaction[T any]() *Transaction[T] {\n\treturn &Transaction[T]{\n\t\tsteps: []transactionStep[T]{},\n\t}\n}\n\n// Transaction implements a Saga pattern\ntype Transaction[T any] struct {\n\tsteps []transactionStep[T]\n}\n\n// Then adds a step to the chain of callbacks. It returns the same Transaction.\nfunc (t *Transaction[T]) Then(exec func(T) (T, error), onRollback func(T) T) *Transaction[T] {\n\tt.steps = append(t.steps, transactionStep[T]{\n\t\texec:       exec,\n\t\tonRollback: onRollback,\n\t})\n\n\treturn t\n}\n\n// Process runs the Transaction steps and rollbacks in case of errors.\nfunc (t *Transaction[T]) Process(state T) (T, error) {\n\tvar i int\n\tvar err error\n\n\tfor i < len(t.steps) {\n\t\tstate, err = t.steps[i].exec(state)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\ti++\n\t}\n\n\tif err == nil {\n\t\treturn state, nil\n\t}\n\n\tfor i > 0 {\n\t\ti--\n\t\tstate = t.steps[i].onRollback(state)\n\t}\n\n\treturn state, err\n}\n\n// throttle ?\n\n```\n# Function Signature: func NewDebounceBy[T comparable](duration time.Duration, f ...func(key T, count int)) (func(key T), func(key T)) {\n# Natural Language Annotations: // NewDebounceBy creates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/d3Vpt6pxhY8\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewDebounceBy[T comparable](duration time.Duration, f ...func(key T, count int)) (func(key T), func(key T)) {\n\td := &debounceBy[T]{\n\t\tafter:     duration,\n\t\tmu:        &sync.Mutex{},\n\t\titems:     make(map[T]*debounceByItem),\n\t\tcallbacks: f,\n\t}\n\n\tresetFunc := func(key T) {\n\t\td.reset(key)\n\t}\n\n\tcancelFunc := func(key T) {\n\t\td.cancel(key)\n\t}\n\n\treturn resetFunc, cancelFunc\n}\n```"]}, "lo-errors.go-Try6": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/errors.go:\n```\npackage lo\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n)\n\n// Validate is a helper that creates an error when a condition is not met.\n// Play: https://go.dev/play/p/vPyh51XpCBt\nfunc Validate(ok bool, format string, args ...any) error {\n\tif !ok {\n\t\treturn fmt.Errorf(fmt.Sprintf(format, args...))\n\t}\n\treturn nil\n}\n\nfunc messageFromMsgAndArgs(msgAndArgs ...any) string {\n\tif len(msgAndArgs) == 1 {\n\t\tif msgAsStr, ok := msgAndArgs[0].(string); ok {\n\t\t\treturn msgAsStr\n\t\t}\n\t\treturn fmt.Sprintf(\"%+v\", msgAndArgs[0])\n\t}\n\tif len(msgAndArgs) > 1 {\n\t\treturn fmt.Sprintf(msgAndArgs[0].(string), msgAndArgs[1:]...)\n\t}\n\treturn \"\"\n}\n\n// must panics if err is error or false.\nfunc must(err any, messageArgs ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\n\tswitch e := err.(type) {\n\tcase bool:\n\t\tif !e {\n\t\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\t\tif message == \"\" {\n\t\t\t\tmessage = \"not ok\"\n\t\t\t}\n\n\t\t\tpanic(message)\n\t\t}\n\n\tcase error:\n\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\tif message != \"\" {\n\t\t\tpanic(message + \": \" + e.Error())\n\t\t} else {\n\t\t\tpanic(e.Error())\n\t\t}\n\n\tdefault:\n\t\tpanic(\"must: invalid err type '\" + reflect.TypeOf(err).Name() + \"', should either be a bool or an error\")\n\t}\n}\n\n// Must is a helper that wraps a call to a function returning a value and an error\n// and panics if err is error or false.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must[T any](val T, err any, messageArgs ...any) T {\n\tmust(err, messageArgs...)\n\treturn val\n}\n\n// Must0 has the same behavior as Must, but callback returns no variable.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must0(err any, messageArgs ...any) {\n\tmust(err, messageArgs...)\n}\n\n// Must1 is an alias to Must\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must1[T any](val T, err any, messageArgs ...any) T {\n\treturn Must(val, err, messageArgs...)\n}\n\n// Must2 has the same behavior as Must, but callback returns 2 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must2[T1, T2 any](val1 T1, val2 T2, err any, messageArgs ...any) (T1, T2) {\n\tmust(err, messageArgs...)\n\treturn val1, val2\n}\n\n// Must3 has the same behavior as Must, but callback returns 3 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must3[T1, T2, T3 any](val1 T1, val2 T2, val3 T3, err any, messageArgs ...any) (T1, T2, T3) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3\n}\n\n// Must4 has the same behavior as Must, but callback returns 4 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must4[T1, T2, T3, T4 any](val1 T1, val2 T2, val3 T3, val4 T4, err any, messageArgs ...any) (T1, T2, T3, T4) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4\n}\n\n// Must5 has the same behavior as Must, but callback returns 5 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must5[T1, T2, T3, T4, T5 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, err any, messageArgs ...any) (T1, T2, T3, T4, T5) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5\n}\n\n// Must6 has the same behavior as Must, but callback returns 6 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must6[T1, T2, T3, T4, T5, T6 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, val6 T6, err any, messageArgs ...any) (T1, T2, T3, T4, T5, T6) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5, val6\n}\n\n// Try calls the function and return false in case of error.\nfunc Try(callback func() error) (ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t}\n\n\treturn\n}\n\n// Try0 has the same behavior as Try, but callback returns no variable.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try0(callback func()) bool {\n\treturn Try(func() error {\n\t\tcallback()\n\t\treturn nil\n\t})\n}\n\n// Try1 is an alias to Try.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try1(callback func() error) bool {\n\treturn Try(callback)\n}\n\n// Try2 has the same behavior as Try, but callback returns 2 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try2[T any](callback func() (T, error)) bool {\n\treturn Try(func() error {\n\t\t_, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try3 has the same behavior as Try, but callback returns 3 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try3[T, R any](callback func() (T, R, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try4 has the same behavior as Try, but callback returns 4 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try4[T, R, S any](callback func() (T, R, S, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try5 has the same behavior as Try, but callback returns 5 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try5[T, R, S, Q any](callback func() (T, R, S, Q, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\n\n\n\n\n\n\n\n// TryOr has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\treturn TryOr1(callback, fallbackA)\n}\n\n// TryOr1 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr1[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, ok\n}\n\n// TryOr2 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr2[A, B any](callback func() (A, B, error), fallbackA A, fallbackB B) (A, B, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, ok\n}\n\n// TryOr3 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr3[A, B, C any](callback func() (A, B, C, error), fallbackA A, fallbackB B, fallbackC C) (A, B, C, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, ok\n}\n\n// TryOr4 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr4[A, B, C, D any](callback func() (A, B, C, D, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D) (A, B, C, D, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, ok\n}\n\n// TryOr5 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr5[A, B, C, D, E any](callback func() (A, B, C, D, E, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E) (A, B, C, D, E, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, ok\n}\n\n// TryOr6 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWithErrorValue has the same behavior as Try, but also returns value passed to panic.\n// Play: https://go.dev/play/p/Kc7afQIT2Fs\nfunc TryWithErrorValue(callback func() error) (errorValue any, ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t\terrorValue = r\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t\terrorValue = err\n\t}\n\n\treturn\n}\n\n// TryCatch has the same behavior as Try, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/PnOON-EqBiU\nfunc TryCatch(callback func() error, catch func()) {\n\tif !Try(callback) {\n\t\tcatch()\n\t}\n}\n\n// TryCatchWithErrorValue has the same behavior as TryWithErrorValue, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/8Pc9gwX_GZO\nfunc TryCatchWithErrorValue(callback func() error, catch func(any)) {\n\tif err, ok := TryWithErrorValue(callback); !ok {\n\t\tcatch(err)\n\t}\n}\n\n// ErrorsAs is a shortcut for errors.As(err, &&T).\n// Play: https://go.dev/play/p/8wk5rH8UfrE\nfunc ErrorsAs[T error](err error) (T, bool) {\n\tvar t T\n\tok := errors.As(err, &t)\n\treturn t, ok\n}\n\n```\n# Function Signature: func Try6[T, R, S, Q, U any](callback func() (T, R, S, Q, U, error)) bool {\n# Natural Language Annotations: // Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Try6[T, R, S, Q, U any](callback func() (T, R, S, Q, U, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, _, _, err := callback()\n\t\treturn err\n\t})\n}\n```"]}, "lo-errors.go-TryOr6": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/errors.go:\n```\npackage lo\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n)\n\n// Validate is a helper that creates an error when a condition is not met.\n// Play: https://go.dev/play/p/vPyh51XpCBt\nfunc Validate(ok bool, format string, args ...any) error {\n\tif !ok {\n\t\treturn fmt.Errorf(fmt.Sprintf(format, args...))\n\t}\n\treturn nil\n}\n\nfunc messageFromMsgAndArgs(msgAndArgs ...any) string {\n\tif len(msgAndArgs) == 1 {\n\t\tif msgAsStr, ok := msgAndArgs[0].(string); ok {\n\t\t\treturn msgAsStr\n\t\t}\n\t\treturn fmt.Sprintf(\"%+v\", msgAndArgs[0])\n\t}\n\tif len(msgAndArgs) > 1 {\n\t\treturn fmt.Sprintf(msgAndArgs[0].(string), msgAndArgs[1:]...)\n\t}\n\treturn \"\"\n}\n\n// must panics if err is error or false.\nfunc must(err any, messageArgs ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\n\tswitch e := err.(type) {\n\tcase bool:\n\t\tif !e {\n\t\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\t\tif message == \"\" {\n\t\t\t\tmessage = \"not ok\"\n\t\t\t}\n\n\t\t\tpanic(message)\n\t\t}\n\n\tcase error:\n\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\tif message != \"\" {\n\t\t\tpanic(message + \": \" + e.Error())\n\t\t} else {\n\t\t\tpanic(e.Error())\n\t\t}\n\n\tdefault:\n\t\tpanic(\"must: invalid err type '\" + reflect.TypeOf(err).Name() + \"', should either be a bool or an error\")\n\t}\n}\n\n// Must is a helper that wraps a call to a function returning a value and an error\n// and panics if err is error or false.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must[T any](val T, err any, messageArgs ...any) T {\n\tmust(err, messageArgs...)\n\treturn val\n}\n\n// Must0 has the same behavior as Must, but callback returns no variable.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must0(err any, messageArgs ...any) {\n\tmust(err, messageArgs...)\n}\n\n// Must1 is an alias to Must\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must1[T any](val T, err any, messageArgs ...any) T {\n\treturn Must(val, err, messageArgs...)\n}\n\n// Must2 has the same behavior as Must, but callback returns 2 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must2[T1, T2 any](val1 T1, val2 T2, err any, messageArgs ...any) (T1, T2) {\n\tmust(err, messageArgs...)\n\treturn val1, val2\n}\n\n// Must3 has the same behavior as Must, but callback returns 3 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must3[T1, T2, T3 any](val1 T1, val2 T2, val3 T3, err any, messageArgs ...any) (T1, T2, T3) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3\n}\n\n// Must4 has the same behavior as Must, but callback returns 4 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must4[T1, T2, T3, T4 any](val1 T1, val2 T2, val3 T3, val4 T4, err any, messageArgs ...any) (T1, T2, T3, T4) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4\n}\n\n// Must5 has the same behavior as Must, but callback returns 5 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must5[T1, T2, T3, T4, T5 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, err any, messageArgs ...any) (T1, T2, T3, T4, T5) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5\n}\n\n// Must6 has the same behavior as Must, but callback returns 6 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must6[T1, T2, T3, T4, T5, T6 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, val6 T6, err any, messageArgs ...any) (T1, T2, T3, T4, T5, T6) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5, val6\n}\n\n// Try calls the function and return false in case of error.\nfunc Try(callback func() error) (ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t}\n\n\treturn\n}\n\n// Try0 has the same behavior as Try, but callback returns no variable.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try0(callback func()) bool {\n\treturn Try(func() error {\n\t\tcallback()\n\t\treturn nil\n\t})\n}\n\n// Try1 is an alias to Try.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try1(callback func() error) bool {\n\treturn Try(callback)\n}\n\n// Try2 has the same behavior as Try, but callback returns 2 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try2[T any](callback func() (T, error)) bool {\n\treturn Try(func() error {\n\t\t_, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try3 has the same behavior as Try, but callback returns 3 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try3[T, R any](callback func() (T, R, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try4 has the same behavior as Try, but callback returns 4 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try4[T, R, S any](callback func() (T, R, S, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try5 has the same behavior as Try, but callback returns 5 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try5[T, R, S, Q any](callback func() (T, R, S, Q, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\n\n\n\n\n\n\n\n// TryOr has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\treturn TryOr1(callback, fallbackA)\n}\n\n// TryOr1 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr1[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, ok\n}\n\n// TryOr2 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr2[A, B any](callback func() (A, B, error), fallbackA A, fallbackB B) (A, B, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, ok\n}\n\n// TryOr3 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr3[A, B, C any](callback func() (A, B, C, error), fallbackA A, fallbackB B, fallbackC C) (A, B, C, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, ok\n}\n\n// TryOr4 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr4[A, B, C, D any](callback func() (A, B, C, D, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D) (A, B, C, D, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, ok\n}\n\n// TryOr5 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr5[A, B, C, D, E any](callback func() (A, B, C, D, E, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E) (A, B, C, D, E, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, ok\n}\n\n// TryOr6 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWithErrorValue has the same behavior as Try, but also returns value passed to panic.\n// Play: https://go.dev/play/p/Kc7afQIT2Fs\nfunc TryWithErrorValue(callback func() error) (errorValue any, ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t\terrorValue = r\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t\terrorValue = err\n\t}\n\n\treturn\n}\n\n// TryCatch has the same behavior as Try, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/PnOON-EqBiU\nfunc TryCatch(callback func() error, catch func()) {\n\tif !Try(callback) {\n\t\tcatch()\n\t}\n}\n\n// TryCatchWithErrorValue has the same behavior as TryWithErrorValue, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/8Pc9gwX_GZO\nfunc TryCatchWithErrorValue(callback func() error, catch func(any)) {\n\tif err, ok := TryWithErrorValue(callback); !ok {\n\t\tcatch(err)\n\t}\n}\n\n// ErrorsAs is a shortcut for errors.As(err, &&T).\n// Play: https://go.dev/play/p/8wk5rH8UfrE\nfunc ErrorsAs[T error](err error) (T, bool) {\n\tvar t T\n\tok := errors.As(err, &t)\n\treturn t, ok\n}\n\n```\n# Function Signature: func TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n# Natural Language Annotations: // TryOr6 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, f, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tfallbackF = f\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, fallbackF, ok\n}\n```"]}, "lo-string.go-Substring": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/string.go:\n```\npackage lo\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n\n\t\"github.com/samber/lo/internal/rand\"\n\n\t\"golang.org/x/text/cases\"\n\t\"golang.org/x/text/language\"\n)\n\nvar (\n\tLowerCaseLettersCharset = []rune(\"abcdefghijklmnopqrstuvwxyz\")\n\tUpperCaseLettersCharset = []rune(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n\tLettersCharset          = append(LowerCaseLettersCharset, UpperCaseLettersCharset...)\n\tNumbersCharset          = []rune(\"0123456789\")\n\tAlphanumericCharset     = append(LettersCharset, NumbersCharset...)\n\tSpecialCharset          = []rune(\"!@#$%^&*()_+-=[]{}|;':\\\",./<>?\")\n\tAllCharset              = append(AlphanumericCharset, SpecialCharset...)\n\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitWordReg = regexp.MustCompile(`([a-z])([A-Z0-9])|([a-zA-Z])([0-9])|([0-9])([a-zA-Z])|([A-Z])([A-Z])([a-z])`)\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitNumberLetterReg = regexp.MustCompile(`([0-9])([a-zA-Z])`)\n)\n\n// RandomString return a random string.\n// Play: https://go.dev/play/p/rRseOQVVum4\nfunc RandomString(size int, charset []rune) string {\n\tif size <= 0 {\n\t\tpanic(\"lo.RandomString: Size parameter must be greater than 0\")\n\t}\n\tif len(charset) <= 0 {\n\t\tpanic(\"lo.RandomString: Charset parameter must not be empty\")\n\t}\n\n\tb := make([]rune, size)\n\tpossibleCharactersCount := len(charset)\n\tfor i := range b {\n\t\tb[i] = charset[rand.IntN(possibleCharactersCount)]\n\t}\n\treturn string(b)\n}\n\n// Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ChunkString returns an array of strings split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/__FLTuJVz54\nfunc ChunkString[T ~string](str T, size int) []T {\n\tif size <= 0 {\n\t\tpanic(\"lo.ChunkString: Size parameter must be greater than 0\")\n\t}\n\n\tif len(str) == 0 {\n\t\treturn []T{\"\"}\n\t}\n\n\tif size >= len(str) {\n\t\treturn []T{str}\n\t}\n\n\tvar chunks []T = make([]T, 0, ((len(str)-1)/size)+1)\n\tcurrentLen := 0\n\tcurrentStart := 0\n\tfor i := range str {\n\t\tif currentLen == size {\n\t\t\tchunks = append(chunks, str[currentStart:i])\n\t\t\tcurrentLen = 0\n\t\t\tcurrentStart = i\n\t\t}\n\t\tcurrentLen++\n\t}\n\tchunks = append(chunks, str[currentStart:])\n\treturn chunks\n}\n\n// RuneLength is an alias to utf8.RuneCountInString which returns the number of runes in string.\n// Play: https://go.dev/play/p/tuhgW_lWY8l\nfunc RuneLength(str string) int {\n\treturn utf8.RuneCountInString(str)\n}\n\n// PascalCase converts string to pascal case.\n\n\n\n\n\n\n\n\n// CamelCase converts string to camel case.\nfunc CamelCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titem = strings.ToLower(item)\n\t\tif i > 0 {\n\t\t\titem = Capitalize(item)\n\t\t}\n\t\titems[i] = item\n\t}\n\treturn strings.Join(items, \"\")\n}\n\n// KebabCase converts string to kebab case.\nfunc KebabCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"-\")\n}\n\n// SnakeCase converts string to snake case.\nfunc SnakeCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"_\")\n}\n\n// Words splits string into an array of its words.\nfunc Words(str string) []string {\n\tstr = splitWordReg.ReplaceAllString(str, `$1$3$5$7 $2$4$6$8$9`)\n\t// example: Int8Value => Int 8Value => Int 8 Value\n\tstr = splitNumberLetterReg.ReplaceAllString(str, \"$1 $2\")\n\tvar result strings.Builder\n\tfor _, r := range str {\n\t\tif unicode.IsLetter(r) || unicode.IsDigit(r) {\n\t\t\tresult.WriteRune(r)\n\t\t} else {\n\t\t\tresult.WriteRune(' ')\n\t\t}\n\t}\n\treturn strings.Fields(result.String())\n}\n\n// Capitalize converts the first character of string to upper case and the remaining to lower case.\nfunc Capitalize(str string) string {\n\treturn cases.Title(language.English).String(str)\n}\n\n// Elipse truncates a string to a specified length and appends an ellipsis if truncated.\nfunc Elipse(str string, length int) string {\n\tif len(str) > length {\n\t\tif len(str) < 3 || length < 3 {\n\t\t\treturn \"...\"\n\t\t}\n\t\treturn str[0:length-3] + \"...\"\n\t}\n\n\treturn str\n}\n\n```\n# Function Signature: func Substring[T ~string](str T, offset int, length uint) T {\n# Natural Language Annotations: // Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\nPlease return the generated code surrounded by ```\n", "response": ["The requested function `Substring` should return a part of a given string starting from a specified offset and spanning a specified length. Here's how you can implement it based on the function signature and annotations provided:\n\n```go\nfunc Substring[T ~string](str T, offset int, length uint) T {\n\tif offset < 0 || int(length) < 0 || offset >= len(str) {\n\t\tpanic(\"lo.Substring: Offset and length must be within the bounds of the string\")\n\t}\n\n\t// Calculate the end index, ensuring it doesn't exceed the string length\n\tend := offset + int(length)\n\tif end > len(str) {\n\t\tend = len(str)\n\t}\n\n\treturn str[offset:end]\n}\n```\n\nThis function checks if the `offset` and `length` are within the bounds of the input string. It uses slicing to return the substring. If the provided `offset` is out of bounds, it raises a panic with a descriptive message. The end index is adjusted to ensure it doesn't exceed the string length."]}, "lo-string.go-PascalCase": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/string.go:\n```\npackage lo\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n\n\t\"github.com/samber/lo/internal/rand\"\n\n\t\"golang.org/x/text/cases\"\n\t\"golang.org/x/text/language\"\n)\n\nvar (\n\tLowerCaseLettersCharset = []rune(\"abcdefghijklmnopqrstuvwxyz\")\n\tUpperCaseLettersCharset = []rune(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n\tLettersCharset          = append(LowerCaseLettersCharset, UpperCaseLettersCharset...)\n\tNumbersCharset          = []rune(\"0123456789\")\n\tAlphanumericCharset     = append(LettersCharset, NumbersCharset...)\n\tSpecialCharset          = []rune(\"!@#$%^&*()_+-=[]{}|;':\\\",./<>?\")\n\tAllCharset              = append(AlphanumericCharset, SpecialCharset...)\n\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitWordReg = regexp.MustCompile(`([a-z])([A-Z0-9])|([a-zA-Z])([0-9])|([0-9])([a-zA-Z])|([A-Z])([A-Z])([a-z])`)\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitNumberLetterReg = regexp.MustCompile(`([0-9])([a-zA-Z])`)\n)\n\n// RandomString return a random string.\n// Play: https://go.dev/play/p/rRseOQVVum4\nfunc RandomString(size int, charset []rune) string {\n\tif size <= 0 {\n\t\tpanic(\"lo.RandomString: Size parameter must be greater than 0\")\n\t}\n\tif len(charset) <= 0 {\n\t\tpanic(\"lo.RandomString: Charset parameter must not be empty\")\n\t}\n\n\tb := make([]rune, size)\n\tpossibleCharactersCount := len(charset)\n\tfor i := range b {\n\t\tb[i] = charset[rand.IntN(possibleCharactersCount)]\n\t}\n\treturn string(b)\n}\n\n// Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ChunkString returns an array of strings split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/__FLTuJVz54\nfunc ChunkString[T ~string](str T, size int) []T {\n\tif size <= 0 {\n\t\tpanic(\"lo.ChunkString: Size parameter must be greater than 0\")\n\t}\n\n\tif len(str) == 0 {\n\t\treturn []T{\"\"}\n\t}\n\n\tif size >= len(str) {\n\t\treturn []T{str}\n\t}\n\n\tvar chunks []T = make([]T, 0, ((len(str)-1)/size)+1)\n\tcurrentLen := 0\n\tcurrentStart := 0\n\tfor i := range str {\n\t\tif currentLen == size {\n\t\t\tchunks = append(chunks, str[currentStart:i])\n\t\t\tcurrentLen = 0\n\t\t\tcurrentStart = i\n\t\t}\n\t\tcurrentLen++\n\t}\n\tchunks = append(chunks, str[currentStart:])\n\treturn chunks\n}\n\n// RuneLength is an alias to utf8.RuneCountInString which returns the number of runes in string.\n// Play: https://go.dev/play/p/tuhgW_lWY8l\nfunc RuneLength(str string) int {\n\treturn utf8.RuneCountInString(str)\n}\n\n// PascalCase converts string to pascal case.\n\n\n\n\n\n\n\n\n// CamelCase converts string to camel case.\nfunc CamelCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titem = strings.ToLower(item)\n\t\tif i > 0 {\n\t\t\titem = Capitalize(item)\n\t\t}\n\t\titems[i] = item\n\t}\n\treturn strings.Join(items, \"\")\n}\n\n// KebabCase converts string to kebab case.\nfunc KebabCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"-\")\n}\n\n// SnakeCase converts string to snake case.\nfunc SnakeCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"_\")\n}\n\n// Words splits string into an array of its words.\nfunc Words(str string) []string {\n\tstr = splitWordReg.ReplaceAllString(str, `$1$3$5$7 $2$4$6$8$9`)\n\t// example: Int8Value => Int 8Value => Int 8 Value\n\tstr = splitNumberLetterReg.ReplaceAllString(str, \"$1 $2\")\n\tvar result strings.Builder\n\tfor _, r := range str {\n\t\tif unicode.IsLetter(r) || unicode.IsDigit(r) {\n\t\t\tresult.WriteRune(r)\n\t\t} else {\n\t\t\tresult.WriteRune(' ')\n\t\t}\n\t}\n\treturn strings.Fields(result.String())\n}\n\n// Capitalize converts the first character of string to upper case and the remaining to lower case.\nfunc Capitalize(str string) string {\n\treturn cases.Title(language.English).String(str)\n}\n\n// Elipse truncates a string to a specified length and appends an ellipsis if truncated.\nfunc Elipse(str string, length int) string {\n\tif len(str) > length {\n\t\tif len(str) < 3 || length < 3 {\n\t\t\treturn \"...\"\n\t\t}\n\t\treturn str[0:length-3] + \"...\"\n\t}\n\n\treturn str\n}\n\n```\n# Function Signature: func PascalCase(str string) string {\n# Natural Language Annotations: // PascalCase converts string to pascal case.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc PascalCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titems[i] = Capitalize(strings.ToLower(item))\n\t}\n\treturn strings.Join(items, \"\")\n}\n```"]}, "lo-map.go-PickByValues": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/map.go:\n```\npackage lo\n\n// Keys creates an array of the map keys.\n// Play: https://go.dev/play/p/Uu11fHASqrU\nfunc Keys[K comparable, V any](in map[K]V) []K {\n\tresult := make([]K, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, k)\n\t}\n\n\treturn result\n}\n\n// HasKey returns whether the given key exists.\n// Play: https://go.dev/play/p/aVwubIvECqS\nfunc HasKey[K comparable, V any](in map[K]V, key K) bool {\n\t_, ok := in[key]\n\treturn ok\n}\n\n// Values creates an array of the map values.\n// Play: https://go.dev/play/p/nnRTQkzQfF6\nfunc Values[K comparable, V any](in map[K]V) []V {\n\tresult := make([]V, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, in[k])\n\t}\n\n\treturn result\n}\n\n// ValueOr returns the value of the given key or the fallback value if the key is not present.\n// Play: https://go.dev/play/p/bAq9mHErB4V\nfunc ValueOr[K comparable, V any](in map[K]V, key K, fallback V) V {\n\tif v, ok := in[key]; ok {\n\t\treturn v\n\t}\n\treturn fallback\n}\n\n// PickBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/kdg8GR_QMmf\nfunc PickBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/R1imbuci9qU\nfunc PickByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor i := range keys {\n\t\tif v, ok := in[keys[i]]; ok {\n\t\t\tr[keys[i]] = v\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\n\n\n\n\n\n\n\n\n\n\n// OmitBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/EtBsR43bdsd\nfunc OmitBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif !predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// OmitByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/t1QjCrs-ysk\nfunc OmitByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range keys {\n\t\tdelete(r, keys[i])\n\t}\n\treturn r\n}\n\n// OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\n\n\n\n\n\n\n\n\n\n\n// Entries transforms a map into array of key/value pairs.\n// Play:\nfunc Entries[K comparable, V any](in map[K]V) []Entry[K, V] {\n\tentries := make([]Entry[K, V], 0, len(in))\n\n\tfor k := range in {\n\t\tentries = append(entries, Entry[K, V]{\n\t\t\tKey:   k,\n\t\t\tValue: in[k],\n\t\t})\n\t}\n\n\treturn entries\n}\n\n// ToPairs transforms a map into array of key/value pairs.\n// Alias of Entries().\n// Play: https://go.dev/play/p/3Dhgx46gawJ\nfunc ToPairs[K comparable, V any](in map[K]V) []Entry[K, V] {\n\treturn Entries(in)\n}\n\n// FromEntries transforms an array of key/value pairs into a map.\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromEntries[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\tout := make(map[K]V, len(entries))\n\n\tfor i := range entries {\n\t\tout[entries[i].Key] = entries[i].Value\n\t}\n\n\treturn out\n}\n\n// FromPairs transforms an array of key/value pairs into a map.\n// Alias of FromEntries().\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromPairs[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\treturn FromEntries(entries)\n}\n\n// Invert creates a map composed of the inverted keys and values. If map\n// contains duplicate values, subsequent values overwrite property assignments\n// of previous values.\n// Play: https://go.dev/play/p/rFQ4rak6iA1\nfunc Invert[K comparable, V comparable](in map[K]V) map[V]K {\n\tout := make(map[V]K, len(in))\n\n\tfor k := range in {\n\t\tout[in[k]] = k\n\t}\n\n\treturn out\n}\n\n// Assign merges multiple maps from left to right.\n// Play: https://go.dev/play/p/VhwfJOyxf5o\nfunc Assign[K comparable, V any, Map ~map[K]V](maps ...Map) Map {\n\tcount := 0\n\tfor i := range maps {\n\t\tcount += len(maps[i])\n\t}\n\n\tout := make(Map, count)\n\tfor i := range maps {\n\t\tfor k := range maps[i] {\n\t\t\tout[k] = maps[i][k]\n\t\t}\n\t}\n\n\treturn out\n}\n\n// MapKeys manipulates a map keys and transforms it to a map of another type.\n// Play: https://go.dev/play/p/9_4WPIqOetJ\nfunc MapKeys[K comparable, V any, R comparable](in map[K]V, iteratee func(value V, key K) R) map[R]V {\n\tresult := make(map[R]V, len(in))\n\n\tfor k := range in {\n\t\tresult[iteratee(in[k], k)] = in[k]\n\t}\n\n\treturn result\n}\n\n// MapValues manipulates a map values and transforms it to a map of another type.\n// Play: https://go.dev/play/p/T_8xAfvcf0W\nfunc MapValues[K comparable, V any, R any](in map[K]V, iteratee func(value V, key K) R) map[K]R {\n\tresult := make(map[K]R, len(in))\n\n\tfor k := range in {\n\t\tresult[k] = iteratee(in[k], k)\n\t}\n\n\treturn result\n}\n\n// MapEntries manipulates a map entries and transforms it to a map of another type.\n// Play: https://go.dev/play/p/VuvNQzxKimT\nfunc MapEntries[K1 comparable, V1 any, K2 comparable, V2 any](in map[K1]V1, iteratee func(key K1, value V1) (K2, V2)) map[K2]V2 {\n\tresult := make(map[K2]V2, len(in))\n\n\tfor k1 := range in {\n\t\tk2, v2 := iteratee(k1, in[k1])\n\t\tresult[k2] = v2\n\t}\n\n\treturn result\n}\n\n// MapToSlice transforms a map into a slice based on specific iteratee\n// Play: https://go.dev/play/p/ZuiCZpDt6LD\nfunc MapToSlice[K comparable, V any, R any](in map[K]V, iteratee func(key K, value V) R) []R {\n\tresult := make([]R, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, iteratee(k, in[k]))\n\t}\n\n\treturn result\n}\n\n```\n# Function Signature: func PickByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n# Natural Language Annotations: // PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\nfunc PickByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tvalueSet := make(map[V]struct{}, len(values))\n\n\tfor _, v := range values {\n\t\tvalueSet[v] = struct{}{}\n\t}\n\n\tfor k, v := range in {\n\t\tif _, exists := valueSet[v]; exists {\n\t\t\tr[k] = v\n\t\t}\n\t}\n\n\treturn r\n}\n```"]}, "lo-map.go-OmitByValues": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/map.go:\n```\npackage lo\n\n// Keys creates an array of the map keys.\n// Play: https://go.dev/play/p/Uu11fHASqrU\nfunc Keys[K comparable, V any](in map[K]V) []K {\n\tresult := make([]K, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, k)\n\t}\n\n\treturn result\n}\n\n// HasKey returns whether the given key exists.\n// Play: https://go.dev/play/p/aVwubIvECqS\nfunc HasKey[K comparable, V any](in map[K]V, key K) bool {\n\t_, ok := in[key]\n\treturn ok\n}\n\n// Values creates an array of the map values.\n// Play: https://go.dev/play/p/nnRTQkzQfF6\nfunc Values[K comparable, V any](in map[K]V) []V {\n\tresult := make([]V, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, in[k])\n\t}\n\n\treturn result\n}\n\n// ValueOr returns the value of the given key or the fallback value if the key is not present.\n// Play: https://go.dev/play/p/bAq9mHErB4V\nfunc ValueOr[K comparable, V any](in map[K]V, key K, fallback V) V {\n\tif v, ok := in[key]; ok {\n\t\treturn v\n\t}\n\treturn fallback\n}\n\n// PickBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/kdg8GR_QMmf\nfunc PickBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/R1imbuci9qU\nfunc PickByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor i := range keys {\n\t\tif v, ok := in[keys[i]]; ok {\n\t\t\tr[keys[i]] = v\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\n\n\n\n\n\n\n\n\n\n\n// OmitBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/EtBsR43bdsd\nfunc OmitBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif !predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// OmitByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/t1QjCrs-ysk\nfunc OmitByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range keys {\n\t\tdelete(r, keys[i])\n\t}\n\treturn r\n}\n\n// OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\n\n\n\n\n\n\n\n\n\n\n// Entries transforms a map into array of key/value pairs.\n// Play:\nfunc Entries[K comparable, V any](in map[K]V) []Entry[K, V] {\n\tentries := make([]Entry[K, V], 0, len(in))\n\n\tfor k := range in {\n\t\tentries = append(entries, Entry[K, V]{\n\t\t\tKey:   k,\n\t\t\tValue: in[k],\n\t\t})\n\t}\n\n\treturn entries\n}\n\n// ToPairs transforms a map into array of key/value pairs.\n// Alias of Entries().\n// Play: https://go.dev/play/p/3Dhgx46gawJ\nfunc ToPairs[K comparable, V any](in map[K]V) []Entry[K, V] {\n\treturn Entries(in)\n}\n\n// FromEntries transforms an array of key/value pairs into a map.\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromEntries[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\tout := make(map[K]V, len(entries))\n\n\tfor i := range entries {\n\t\tout[entries[i].Key] = entries[i].Value\n\t}\n\n\treturn out\n}\n\n// FromPairs transforms an array of key/value pairs into a map.\n// Alias of FromEntries().\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromPairs[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\treturn FromEntries(entries)\n}\n\n// Invert creates a map composed of the inverted keys and values. If map\n// contains duplicate values, subsequent values overwrite property assignments\n// of previous values.\n// Play: https://go.dev/play/p/rFQ4rak6iA1\nfunc Invert[K comparable, V comparable](in map[K]V) map[V]K {\n\tout := make(map[V]K, len(in))\n\n\tfor k := range in {\n\t\tout[in[k]] = k\n\t}\n\n\treturn out\n}\n\n// Assign merges multiple maps from left to right.\n// Play: https://go.dev/play/p/VhwfJOyxf5o\nfunc Assign[K comparable, V any, Map ~map[K]V](maps ...Map) Map {\n\tcount := 0\n\tfor i := range maps {\n\t\tcount += len(maps[i])\n\t}\n\n\tout := make(Map, count)\n\tfor i := range maps {\n\t\tfor k := range maps[i] {\n\t\t\tout[k] = maps[i][k]\n\t\t}\n\t}\n\n\treturn out\n}\n\n// MapKeys manipulates a map keys and transforms it to a map of another type.\n// Play: https://go.dev/play/p/9_4WPIqOetJ\nfunc MapKeys[K comparable, V any, R comparable](in map[K]V, iteratee func(value V, key K) R) map[R]V {\n\tresult := make(map[R]V, len(in))\n\n\tfor k := range in {\n\t\tresult[iteratee(in[k], k)] = in[k]\n\t}\n\n\treturn result\n}\n\n// MapValues manipulates a map values and transforms it to a map of another type.\n// Play: https://go.dev/play/p/T_8xAfvcf0W\nfunc MapValues[K comparable, V any, R any](in map[K]V, iteratee func(value V, key K) R) map[K]R {\n\tresult := make(map[K]R, len(in))\n\n\tfor k := range in {\n\t\tresult[k] = iteratee(in[k], k)\n\t}\n\n\treturn result\n}\n\n// MapEntries manipulates a map entries and transforms it to a map of another type.\n// Play: https://go.dev/play/p/VuvNQzxKimT\nfunc MapEntries[K1 comparable, V1 any, K2 comparable, V2 any](in map[K1]V1, iteratee func(key K1, value V1) (K2, V2)) map[K2]V2 {\n\tresult := make(map[K2]V2, len(in))\n\n\tfor k1 := range in {\n\t\tk2, v2 := iteratee(k1, in[k1])\n\t\tresult[k2] = v2\n\t}\n\n\treturn result\n}\n\n// MapToSlice transforms a map into a slice based on specific iteratee\n// Play: https://go.dev/play/p/ZuiCZpDt6LD\nfunc MapToSlice[K comparable, V any, R any](in map[K]V, iteratee func(key K, value V) R) []R {\n\tresult := make([]R, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, iteratee(k, in[k]))\n\t}\n\n\treturn result\n}\n\n```\n# Function Signature: func OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n# Natural Language Annotations: // OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\nfunc OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\n\tvalueSet := make(map[V]struct{}, len(values))\n\tfor _, v := range values {\n\t\tvalueSet[v] = struct{}{}\n\t}\n\n\tfor k, v := range in {\n\t\tif _, found := valueSet[v]; !found {\n\t\t\tr[k] = v\n\t\t}\n\t}\n\n\treturn r\n}\n```"]}, "lo-tuples.go-Zip9": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/tuples.go:\n```\npackage lo\n\n// T2 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T2[A, B any](a A, b B) Tuple2[A, B] {\n\treturn Tuple2[A, B]{A: a, B: b}\n}\n\n// T3 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T3[A, B, C any](a A, b B, c C) Tuple3[A, B, C] {\n\treturn Tuple3[A, B, C]{A: a, B: b, C: c}\n}\n\n// T4 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T4[A, B, C, D any](a A, b B, c C, d D) Tuple4[A, B, C, D] {\n\treturn Tuple4[A, B, C, D]{A: a, B: b, C: c, D: d}\n}\n\n// T5 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T5[A, B, C, D, E any](a A, b B, c C, d D, e E) Tuple5[A, B, C, D, E] {\n\treturn Tuple5[A, B, C, D, E]{A: a, B: b, C: c, D: d, E: e}\n}\n\n// T6 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T6[A, B, C, D, E, F any](a A, b B, c C, d D, e E, f F) Tuple6[A, B, C, D, E, F] {\n\treturn Tuple6[A, B, C, D, E, F]{A: a, B: b, C: c, D: d, E: e, F: f}\n}\n\n// T7 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T7[A, B, C, D, E, F, G any](a A, b B, c C, d D, e E, f F, g G) Tuple7[A, B, C, D, E, F, G] {\n\treturn Tuple7[A, B, C, D, E, F, G]{A: a, B: b, C: c, D: d, E: e, F: f, G: g}\n}\n\n// T8 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T8[A, B, C, D, E, F, G, H any](a A, b B, c C, d D, e E, f F, g G, h H) Tuple8[A, B, C, D, E, F, G, H] {\n\treturn Tuple8[A, B, C, D, E, F, G, H]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h}\n}\n\n// T9 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T9[A, B, C, D, E, F, G, H, I any](a A, b B, c C, d D, e E, f F, g G, h H, i I) Tuple9[A, B, C, D, E, F, G, H, I] {\n\treturn Tuple9[A, B, C, D, E, F, G, H, I]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h, I: i}\n}\n\n// Unpack2 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack2[A, B any](tuple Tuple2[A, B]) (A, B) {\n\treturn tuple.A, tuple.B\n}\n\n// Unpack3 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack3[A, B, C any](tuple Tuple3[A, B, C]) (A, B, C) {\n\treturn tuple.A, tuple.B, tuple.C\n}\n\n// Unpack4 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack4[A, B, C, D any](tuple Tuple4[A, B, C, D]) (A, B, C, D) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D\n}\n\n// Unpack5 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack5[A, B, C, D, E any](tuple Tuple5[A, B, C, D, E]) (A, B, C, D, E) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E\n}\n\n// Unpack6 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack6[A, B, C, D, E, F any](tuple Tuple6[A, B, C, D, E, F]) (A, B, C, D, E, F) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F\n}\n\n// Unpack7 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack7[A, B, C, D, E, F, G any](tuple Tuple7[A, B, C, D, E, F, G]) (A, B, C, D, E, F, G) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G\n}\n\n// Unpack8 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack8[A, B, C, D, E, F, G, H any](tuple Tuple8[A, B, C, D, E, F, G, H]) (A, B, C, D, E, F, G, H) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H\n}\n\n// Unpack9 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack9[A, B, C, D, E, F, G, H, I any](tuple Tuple9[A, B, C, D, E, F, G, H, I]) (A, B, C, D, E, F, G, H, I) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H, tuple.I\n}\n\n// Zip2 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip2[A, B any](a []A, b []B) []Tuple2[A, B] {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Tuple2[A, B], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, Tuple2[A, B]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip3 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip3[A, B, C any](a []A, b []B, c []C) []Tuple3[A, B, C] {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Tuple3[A, B, C], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, Tuple3[A, B, C]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip4 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip4[A, B, C, D any](a []A, b []B, c []C, d []D) []Tuple4[A, B, C, D] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Tuple4[A, B, C, D], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, Tuple4[A, B, C, D]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip5 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip5[A, B, C, D, E any](a []A, b []B, c []C, d []D, e []E) []Tuple5[A, B, C, D, E] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Tuple5[A, B, C, D, E], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, Tuple5[A, B, C, D, E]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip6 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip6[A, B, C, D, E, F any](a []A, b []B, c []C, d []D, e []E, f []F) []Tuple6[A, B, C, D, E, F] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Tuple6[A, B, C, D, E, F], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, Tuple6[A, B, C, D, E, F]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip7 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip7[A, B, C, D, E, F, G any](a []A, b []B, c []C, d []D, e []E, f []F, g []G) []Tuple7[A, B, C, D, E, F, G] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g)})\n\n\tresult := make([]Tuple7[A, B, C, D, E, F, G], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\n\t\tresult = append(result, Tuple7[A, B, C, D, E, F, G]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip8 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip8[A, B, C, D, E, F, G, H any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H) []Tuple8[A, B, C, D, E, F, G, H] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h)})\n\n\tresult := make([]Tuple8[A, B, C, D, E, F, G, H], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\n\t\tresult = append(result, Tuple8[A, B, C, D, E, F, G, H]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ZipBy2 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy2[A any, B any, Out any](a []A, b []B, iteratee func(a A, b B) Out) []Out {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, iteratee(_a, _b))\n\t}\n\n\treturn result\n}\n\n// ZipBy3 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy3[A any, B any, C any, Out any](a []A, b []B, c []C, iteratee func(a A, b B, c C) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c))\n\t}\n\n\treturn result\n}\n\n// ZipBy4 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy4[A any, B any, C any, D any, Out any](a []A, b []B, c []C, d []D, iteratee func(a A, b B, c C, d D) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d))\n\t}\n\n\treturn result\n}\n\n// ZipBy5 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy5[A any, B any, C any, D any, E any, Out any](a []A, b []B, c []C, d []D, e []E, iteratee func(a A, b B, c C, d D, e E) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e))\n\t}\n\n\treturn result\n}\n\n// ZipBy6 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy6[A any, B any, C any, D any, E any, F any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, iteratee func(a A, b B, c C, d D, e E, f F) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f))\n\t}\n\n\treturn result\n}\n\n// ZipBy7 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy7[A any, B any, C any\n```\n# Function Signature: func Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n# Natural Language Annotations: // Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Tuple9[A, B, C, D, E, F, G, H, I], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, Tuple9[A, B, C, D, E, F, G, H, I]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t\tI: _i,\n\t\t})\n\t}\n\n\treturn result\n}\n```"]}, "lo-tuples.go-ZipBy9": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/tuples.go:\n```\npackage lo\n\n// T2 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T2[A, B any](a A, b B) Tuple2[A, B] {\n\treturn Tuple2[A, B]{A: a, B: b}\n}\n\n// T3 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T3[A, B, C any](a A, b B, c C) Tuple3[A, B, C] {\n\treturn Tuple3[A, B, C]{A: a, B: b, C: c}\n}\n\n// T4 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T4[A, B, C, D any](a A, b B, c C, d D) Tuple4[A, B, C, D] {\n\treturn Tuple4[A, B, C, D]{A: a, B: b, C: c, D: d}\n}\n\n// T5 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T5[A, B, C, D, E any](a A, b B, c C, d D, e E) Tuple5[A, B, C, D, E] {\n\treturn Tuple5[A, B, C, D, E]{A: a, B: b, C: c, D: d, E: e}\n}\n\n// T6 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T6[A, B, C, D, E, F any](a A, b B, c C, d D, e E, f F) Tuple6[A, B, C, D, E, F] {\n\treturn Tuple6[A, B, C, D, E, F]{A: a, B: b, C: c, D: d, E: e, F: f}\n}\n\n// T7 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T7[A, B, C, D, E, F, G any](a A, b B, c C, d D, e E, f F, g G) Tuple7[A, B, C, D, E, F, G] {\n\treturn Tuple7[A, B, C, D, E, F, G]{A: a, B: b, C: c, D: d, E: e, F: f, G: g}\n}\n\n// T8 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T8[A, B, C, D, E, F, G, H any](a A, b B, c C, d D, e E, f F, g G, h H) Tuple8[A, B, C, D, E, F, G, H] {\n\treturn Tuple8[A, B, C, D, E, F, G, H]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h}\n}\n\n// T9 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T9[A, B, C, D, E, F, G, H, I any](a A, b B, c C, d D, e E, f F, g G, h H, i I) Tuple9[A, B, C, D, E, F, G, H, I] {\n\treturn Tuple9[A, B, C, D, E, F, G, H, I]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h, I: i}\n}\n\n// Unpack2 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack2[A, B any](tuple Tuple2[A, B]) (A, B) {\n\treturn tuple.A, tuple.B\n}\n\n// Unpack3 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack3[A, B, C any](tuple Tuple3[A, B, C]) (A, B, C) {\n\treturn tuple.A, tuple.B, tuple.C\n}\n\n// Unpack4 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack4[A, B, C, D any](tuple Tuple4[A, B, C, D]) (A, B, C, D) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D\n}\n\n// Unpack5 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack5[A, B, C, D, E any](tuple Tuple5[A, B, C, D, E]) (A, B, C, D, E) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E\n}\n\n// Unpack6 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack6[A, B, C, D, E, F any](tuple Tuple6[A, B, C, D, E, F]) (A, B, C, D, E, F) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F\n}\n\n// Unpack7 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack7[A, B, C, D, E, F, G any](tuple Tuple7[A, B, C, D, E, F, G]) (A, B, C, D, E, F, G) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G\n}\n\n// Unpack8 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack8[A, B, C, D, E, F, G, H any](tuple Tuple8[A, B, C, D, E, F, G, H]) (A, B, C, D, E, F, G, H) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H\n}\n\n// Unpack9 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack9[A, B, C, D, E, F, G, H, I any](tuple Tuple9[A, B, C, D, E, F, G, H, I]) (A, B, C, D, E, F, G, H, I) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H, tuple.I\n}\n\n// Zip2 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip2[A, B any](a []A, b []B) []Tuple2[A, B] {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Tuple2[A, B], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, Tuple2[A, B]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip3 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip3[A, B, C any](a []A, b []B, c []C) []Tuple3[A, B, C] {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Tuple3[A, B, C], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, Tuple3[A, B, C]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip4 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip4[A, B, C, D any](a []A, b []B, c []C, d []D) []Tuple4[A, B, C, D] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Tuple4[A, B, C, D], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, Tuple4[A, B, C, D]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip5 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip5[A, B, C, D, E any](a []A, b []B, c []C, d []D, e []E) []Tuple5[A, B, C, D, E] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Tuple5[A, B, C, D, E], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, Tuple5[A, B, C, D, E]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip6 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip6[A, B, C, D, E, F any](a []A, b []B, c []C, d []D, e []E, f []F) []Tuple6[A, B, C, D, E, F] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Tuple6[A, B, C, D, E, F], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, Tuple6[A, B, C, D, E, F]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip7 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip7[A, B, C, D, E, F, G any](a []A, b []B, c []C, d []D, e []E, f []F, g []G) []Tuple7[A, B, C, D, E, F, G] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g)})\n\n\tresult := make([]Tuple7[A, B, C, D, E, F, G], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\n\t\tresult = append(result, Tuple7[A, B, C, D, E, F, G]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip8 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip8[A, B, C, D, E, F, G, H any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H) []Tuple8[A, B, C, D, E, F, G, H] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h)})\n\n\tresult := make([]Tuple8[A, B, C, D, E, F, G, H], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\n\t\tresult = append(result, Tuple8[A, B, C, D, E, F, G, H]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ZipBy2 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy2[A any, B any, Out any](a []A, b []B, iteratee func(a A, b B) Out) []Out {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, iteratee(_a, _b))\n\t}\n\n\treturn result\n}\n\n// ZipBy3 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy3[A any, B any, C any, Out any](a []A, b []B, c []C, iteratee func(a A, b B, c C) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c))\n\t}\n\n\treturn result\n}\n\n// ZipBy4 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy4[A any, B any, C any, D any, Out any](a []A, b []B, c []C, d []D, iteratee func(a A, b B, c C, d D) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d))\n\t}\n\n\treturn result\n}\n\n// ZipBy5 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy5[A any, B any, C any, D any, E any, Out any](a []A, b []B, c []C, d []D, e []E, iteratee func(a A, b B, c C, d D, e E) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e))\n\t}\n\n\treturn result\n}\n\n// ZipBy6 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy6[A any, B any, C any, D any, E any, F any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, iteratee func(a A, b B, c C, d D, e E, f F) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f))\n\t}\n\n\treturn result\n}\n\n// ZipBy7 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy7[A any, B any, C any\n```\n# Function Signature: func ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n# Natural Language Annotations: // ZipBy9 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ZipBy9 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f, _g, _h, _i))\n\t}\n\n\treturn result\n}\n```"]}, "lo-channel.go-ChannelDispatcher": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n# Natural Language Annotations: // ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n\t// Create child channels\n\tchildren := createChannels[T](count, channelBufferCap)\n\treadOnlyChildren := channelsToReadOnly(children)\n\n\tgo func() {\n\t\tvar index uint64\n\t\tfor msg := range stream {\n\t\t\t// Use the supplied strategy to select the channel to send the message to\n\t\t\ttarget := strategy(msg, index, readOnlyChildren)\n\t\t\tchildren[target] <- msg\n\t\t\tindex++\n\t\t}\n\t\t// Propagate close event to all child channels\n\t\tcloseChannels(children)\n\t}()\n\n\treturn readOnlyChildren\n}\n```"]}, "lo-channel.go-DispatchingStrategyRoundRobin": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n# Natural Language Annotations: // DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n\tstart := int(index % uint64(len(channels)))\n\tfor i := 0; i < len(channels); i++ {\n\t\tidx := (start + i) % len(channels)\n\t\tif channelIsNotFull(channels[idx]) {\n\t\t\treturn idx\n\t\t}\n\t}\n\n\t// If all channels are full, return the starting index\n\treturn start\n}\n```"]}, "lo-channel.go-FanOut": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n# Natural Language Annotations: // FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n\t// Create buffered downstream channels.\n\tdownstreams := createChannels[T](count, channelsBufferCap)\n\troDownstreams := channelsToReadOnly(downstreams)\n\n\t// Start a goroutine to broadcast messages from the upstream channel to all downstream channels.\n\tgo func() {\n\t\tdefer closeChannels(downstreams)\n\n\t\tfor msg := range upstream {\n\t\t\tfor {\n\t\t\t\tpaused := false\n\n\t\t\t\t// Try to send the message to all downstream channels.\n\t\t\t\tfor _, ch := range downstreams {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase ch <- msg:\n\t\t\t\t\t\t// Message successfully sent to the channel.\n\t\t\t\t\tdefault:\n\t\t\t\t\t\t// Channel is full, broadcast is paused.\n\t\t\t\t\t\tpaused = true\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// If the broadcast was not paused and all messages were sent, proceed to next message.\n\t\t\t\tif !paused {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\t// Pause as some channel was full, so we should wait before trying again.\n\t\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn roDownstreams\n}\n```"]}, "lo-slice.go-Shuffle": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n# Natural Language Annotations: // Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\nfunc Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\t\n\t// Implementing Fisher-Yates shuffle\n\tn := len(result)\n\tfor i := n - 1; i > 0; i-- {\n\t\tj := rand.Intn(i + 1) // random index from 0 to i\n\t\tresult[i], result[j] = result[j], result[i] // swap\n\t}\n\t\n\treturn result\n}\n```"]}, "lo-slice.go-Fill": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func Fill[T Clonable[T]](collection []T, initial T) []T {\n# Natural Language Annotations: // Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Fill[T Clonable[T]](collection []T, initial T) []T {\n\tfor i := range collection {\n\t\tcollection[i] = initial\n\t}\n\treturn collection\n}\n```"]}, "lo-slice.go-Repeat": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func Repeat[T Clonable[T]](count int, initial T) []T {\n# Natural Language Annotations: // Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage lo\n\n// Repeat builds a slice with N copies of initial value.\nfunc Repeat[T Clonable[T]](count int, initial T) []T {\n\tresult := make([]T, count)\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = initial.Clone()\n\t}\n\treturn result\n}\n\n```"]}, "lo-slice.go-DropByIndex": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func DropByIndex[T any](collection []T, indexes ...int) []T {\n# Natural Language Annotations: // DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\nfunc DropByIndex[T any](collection []T, indexes ...int) []T {\n\t// Create a map to track which indexes should be dropped for quick lookup\n\tdropMap := make(map[int]struct{})\n\tlength := len(collection)\n\tfor _, idx := range indexes {\n\t\t// Adjust negative indices to be relative to the slice\n\t\tif idx < 0 {\n\t\t\tidx += length\n\t\t}\n\t\t// Only add valid indices within the slice range to dropMap\n\t\tif idx >= 0 && idx < length {\n\t\t\tdropMap[idx] = struct{}{}\n\t\t}\n\t}\n\n\t// Create a result slice with a length equal to the original for efficiency\n\t// as we can't append in-place when conditionally excluding elements\n\tresult := make([]T, 0, length)\n\tfor i, item := range collection {\n\t\t// Only add elements that are not in dropMap\n\t\tif _, shouldDrop := dropMap[i]; !shouldDrop {\n\t\t\tresult = append(result, item)\n\t\t}\n\t}\n\n\treturn result\n}\n```"]}, "lo-intersect.go-Some": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/intersect.go:\n```\npackage lo\n\n// Contains returns true if an element is present in a collection.\nfunc Contains[T comparable](collection []T, element T) bool {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// ContainsBy returns true if predicate function return true.\nfunc ContainsBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// Every returns true if all elements of a subset are contained into a collection or if the subset is empty.\nfunc Every[T comparable](collection []T, subset []T) bool {\n\tfor i := range subset {\n\t\tif !Contains(collection, subset[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// EveryBy returns true if the predicate returns true for all of the elements in the collection or if the collection is empty.\nfunc EveryBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif !predicate(collection[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Some returns true if at least 1 element of a subset is contained into a collection.\n// If the subset is empty Some returns false.\n\n\n\n\n\n\n\n\n\n\n// SomeBy returns true if the predicate returns true for any of the elements in the collection.\n// If the collection is empty SomeBy returns false.\nfunc SomeBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// None returns true if no element of a subset are contained into a collection or if the subset is empty.\nfunc None[T comparable](collection []T, subset []T) bool {\n\tfor i := range subset {\n\t\tif Contains(collection, subset[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// NoneBy returns true if the predicate returns true for none of the elements in the collection or if the collection is empty.\nfunc NoneBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Intersect returns the intersection between two collections.\nfunc Intersect[T comparable, Slice ~[]T](list1 Slice, list2 Slice) Slice {\n\tresult := Slice{}\n\tseen := map[T]struct{}{}\n\n\tfor i := range list1 {\n\t\tseen[list1[i]] = struct{}{}\n\t}\n\n\tfor i := range list2 {\n\t\tif _, ok := seen[list2[i]]; ok {\n\t\t\tresult = append(result, list2[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Difference returns the difference between two collections.\n// The first value is the collection of element absent of list2.\n// The second value is the collection of element absent of list1.\nfunc Difference[T comparable, Slice ~[]T](list1 Slice, list2 Slice) (Slice, Slice) {\n\tleft := Slice{}\n\tright := Slice{}\n\n\tseenLeft := map[T]struct{}{}\n\tseenRight := map[T]struct{}{}\n\n\tfor i := range list1 {\n\t\tseenLeft[list1[i]] = struct{}{}\n\t}\n\n\tfor i := range list2 {\n\t\tseenRight[list2[i]] = struct{}{}\n\t}\n\n\tfor i := range list1 {\n\t\tif _, ok := seenRight[list1[i]]; !ok {\n\t\t\tleft = append(left, list1[i])\n\t\t}\n\t}\n\n\tfor i := range list2 {\n\t\tif _, ok := seenLeft[list2[i]]; !ok {\n\t\t\tright = append(right, list2[i])\n\t\t}\n\t}\n\n\treturn left, right\n}\n\n// Union returns all distinct elements from given collections.\n// result returns will not change the order of elements relatively.\nfunc Union[T comparable, Slice ~[]T](lists ...Slice) Slice {\n\tvar capLen int\n\n\tfor _, list := range lists {\n\t\tcapLen += len(list)\n\t}\n\n\tresult := make(Slice, 0, capLen)\n\tseen := make(map[T]struct{}, capLen)\n\n\tfor i := range lists {\n\t\tfor j := range lists[i] {\n\t\t\tif _, ok := seen[lists[i][j]]; !ok {\n\t\t\t\tseen[lists[i][j]] = struct{}{}\n\t\t\t\tresult = append(result, lists[i][j])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Without returns slice excluding all given values.\nfunc Without[T comparable, Slice ~[]T](collection Slice, exclude ...T) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tfor i := range collection {\n\t\tif !Contains(exclude, collection[i]) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\treturn result\n}\n\n// WithoutEmpty returns slice excluding empty values.\n//\n// Deprecated: Use lo.Compact instead.\nfunc WithoutEmpty[T comparable, Slice ~[]T](collection Slice) Slice {\n\treturn Compact(collection)\n}\n\n```\n# Function Signature: func Some[T comparable](collection []T, subset []T) bool {\n# Natural Language Annotations: // Some returns true if at least 1 element of a subset is contained into a collection.\n// If the subset is empty Some returns false.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Some returns true if at least 1 element of a subset is contained into a collection.\n// If the subset is empty Some returns false.\nfunc Some[T comparable](collection []T, subset []T) bool {\n\tif len(subset) == 0 {\n\t\treturn false\n\t}\n\tfor i := range subset {\n\t\tif Contains(collection, subset[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```"]}, "lo-find.go-FindKeyBy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n# Natural Language Annotations: // FindKeyBy returns the key of the first element predicate returns truthy for.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n\tfor key, value := range object {\n\t\tif predicate(key, value) {\n\t\t\treturn key, true\n\t\t}\n\t}\n\n\tvar zeroKey K\n\treturn zeroKey, false\n}\n```"]}, "lo-find.go-FirstOr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func FirstOr[T any](collection []T, fallback T) T {\n# Natural Language Annotations: // FirstOr returns the first element of a collection or the fallback value if empty.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// FirstOr returns the first element of a collection or the fallback value if empty.\nfunc FirstOr[T any](collection []T, fallback T) T {\n\tif len(collection) == 0 {\n\t\treturn fallback\n\t}\n\treturn collection[0]\n}\n```"]}, "lo-find.go-LastOr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func LastOr[T any](collection []T, fallback T) T {\n# Natural Language Annotations: // LastOr returns the last element of a collection or the fallback value if empty.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc LastOr[T any](collection []T, fallback T) T {\n\tif len(collection) == 0 {\n\t\treturn fallback\n\t}\n\n\treturn collection[len(collection)-1]\n}\n```"]}, "lo-math.go-Range": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Range(elementNum int) []int {\n# Natural Language Annotations: // Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Range creates an array of numbers (positive and/or negative) with given length.\nfunc Range(elementNum int) []int {\n    result := make([]int, elementNum)\n    for i := 0; i < elementNum; i++ {\n        result[i] = i\n    }\n    return result\n}\n```"]}, "lo-math.go-RangeFrom": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n# Natural Language Annotations: // RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// RangeFrom creates an array of numbers from start with specified length.\nfunc RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n\tresult := make([]T, elementNum)\n\tfor i := 0; i < elementNum; i++ {\n\t\tresult[i] = start + T(i)\n\t}\n\treturn result\n}\n```"]}, "lo-math.go-Mean": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Mean[T constraints.Float | constraints.Integer](collection []T) T {\n# Natural Language Annotations: // Mean calculates the mean of a collection of numbers.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Mean[T constraints.Float | constraints.Integer](collection []T) T {\n\tif len(collection) == 0 {\n\t\tvar zero T\n\t\treturn zero\n\t}\n\treturn Sum(collection) / T(len(collection))\n}\n```"]}, "lo-math.go-MeanBy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n# Natural Language Annotations: // MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\nfunc MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tcount := len(collection)\n\tif count == 0 {\n\t\treturn sum // Return 0 if the collection is empty to avoid division by zero.\n\t}\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum / R(count) // Divide the total sum by the count of items to get the mean.\n}\n```"]}, "conc-waitgroup.go-WaitAndRecover": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/waitgroup.go:\n```\npackage conc\n\nimport (\n\t\"sync\"\n\n\t\"github.com/sourcegraph/conc/panics\"\n)\n\n// NewWaitGroup creates a new WaitGroup.\nfunc NewWaitGroup() *WaitGroup {\n\treturn &WaitGroup{}\n}\n\n// WaitGroup is the primary building block for scoped concurrency.\n// Goroutines can be spawned in the WaitGroup with the Go method,\n// and calling Wait() will ensure that each of those goroutines exits\n// before continuing. Any panics in a child goroutine will be caught\n// and propagated to the caller of Wait().\n//\n// The zero value of WaitGroup is usable, just like sync.WaitGroup.\n// Also like sync.WaitGroup, it must not be copied after first use.\ntype WaitGroup struct {\n\twg sync.WaitGroup\n\tpc panics.Catcher\n}\n\n// Go spawns a new goroutine in the WaitGroup.\nfunc (h *WaitGroup) Go(f func()) {\n\th.wg.Add(1)\n\tgo func() {\n\t\tdefer h.wg.Done()\n\t\th.pc.Try(f)\n\t}()\n}\n\n// Wait will block until all goroutines spawned with Go exit and will\n// propagate any panics spawned in a child goroutine.\nfunc (h *WaitGroup) Wait() {\n\th.wg.Wait()\n\n\t// Propagate a panic if we caught one from a child goroutine.\n\th.pc.Repanic()\n}\n\n// WaitAndRecover will block until all goroutines spawned with Go exit and\n// will return a *panics.Recovered if one of the child goroutines panics.\nfunc (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n\th.wg.Wait()\n\n\t// Return a recovered panic if we caught one from a child goroutine.\n\treturn h.pc.Recovered()\n}\n\n```\n# Function Signature: func (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n# Natural Language Annotations: // WaitAndRecover will block until all goroutines spawned with Go exit and\n// will return a *panics.Recovered if one of the child goroutines panics.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n\th.wg.Wait()\n\n\t// Return a recovered panic if we caught one from a child goroutine.\n\treturn h.pc.Recovered()\n}\n```"]}, "conc-stream/stream.go-Wait": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/stream/stream.go:\n```\n// Package stream provides a concurrent, ordered stream implementation.\npackage stream\n\nimport (\n\t\"sync\"\n\n\t\"github.com/sourcegraph/conc\"\n\t\"github.com/sourcegraph/conc/panics\"\n\t\"github.com/sourcegraph/conc/pool\"\n)\n\n// New creates a new Stream with default settings.\nfunc New() *Stream {\n\treturn &Stream{\n\t\tpool: *pool.New(),\n\t}\n}\n\n// Stream is used to execute a stream of tasks concurrently while maintaining\n// the order of the results.\n//\n// To use a stream, you submit some number of `Task`s, each of which\n// return a callback. Each task will be executed concurrently in the stream's\n// associated Pool, and the callbacks will be executed sequentially in the\n// order the tasks were submitted.\n//\n// Once all your tasks have been submitted, Wait() must be called to clean up\n// running goroutines and propagate any panics.\n//\n// In the case of panic during execution of a task or a callback, all other\n// tasks and callbacks will still execute. The panic will be propagated to the\n// caller when Wait() is called.\n//\n// A Stream is efficient, but not zero cost. It should not be used for very\n// short tasks. Startup and teardown adds an overhead of a couple of\n// microseconds, and the overhead for each task is roughly 500ns. It should be\n// good enough for any task that requires a network call.\ntype Stream struct {\n\tpool             pool.Pool\n\tcallbackerHandle conc.WaitGroup\n\tqueue            chan callbackCh\n\n\tinitOnce sync.Once\n}\n\n// Task is a task that is submitted to the stream. Submitted tasks will\n// be executed concurrently. It returns a callback that will be called after\n// the task has completed.\ntype Task func() Callback\n\n// Callback is a function that is returned by a Task. Callbacks are\n// called in the same order that tasks are submitted.\ntype Callback func()\n\n// Go schedules a task to be run in the stream's pool. All submitted tasks\n// will be executed concurrently in worker goroutines. Then, the callbacks\n// returned by the tasks will be executed in the order that the tasks were\n// submitted. All callbacks will be executed by the same goroutine, so no\n// synchronization is necessary between callbacks. If all goroutines in the\n// stream's pool are busy, a call to Go() will block until the task can be\n// started.\nfunc (s *Stream) Go(f Task) {\n\ts.init()\n\n\t// Get a channel from the cache.\n\tch := getCh()\n\n\t// Queue the channel for the callbacker.\n\ts.queue <- ch\n\n\t// Submit the task for execution.\n\ts.pool.Go(func() {\n\t\tdefer func() {\n\t\t\t// In the case of a panic from f, we don't want the callbacker to\n\t\t\t// starve waiting for a callback from this channel, so give it an\n\t\t\t// empty callback.\n\t\t\tif r := recover(); r != nil {\n\t\t\t\tch <- func() {}\n\t\t\t\tpanic(r)\n\t\t\t}\n\t\t}()\n\n\t\t// Run the task, sending its callback down this task's channel.\n\t\tcallback := f()\n\t\tch <- callback\n\t})\n}\n\n// Wait signals to the stream that all tasks have been submitted. Wait will\n// not return until all tasks and callbacks have been run.\nfunc (s *Stream) Wait() {\n\ts.init()\n\n\t// Defer the callbacker cleanup so that it occurs even in the case\n\t// that one of the tasks panics and is propagated up by s.pool.Wait().\n\tdefer func() {\n\t\tclose(s.queue)\n\t\ts.callbackerHandle.Wait()\n\t}()\n\n\t// Wait for all the workers to exit.\n\ts.pool.Wait()\n}\n\nfunc (s *Stream) WithMaxGoroutines(n int) *Stream {\n\ts.pool.WithMaxGoroutines(n)\n\treturn s\n}\n\nfunc (s *Stream) init() {\n\ts.initOnce.Do(func() {\n\t\ts.queue = make(chan callbackCh, s.pool.MaxGoroutines()+1)\n\n\t\t// Start the callbacker.\n\t\ts.callbackerHandle.Go(s.callbacker)\n\t})\n}\n\n// callbacker is responsible for calling the returned callbacks in the order\n// they were submitted. There is only a single instance of callbacker running.\nfunc (s *Stream) callbacker() {\n\tvar panicCatcher panics.Catcher\n\tdefer panicCatcher.Repanic()\n\n\t// For every scheduled task, read that tasks channel from the queue.\n\tfor callbackCh := range s.queue {\n\t\t// Wait for the task to complete and get its callback from the channel.\n\t\tcallback := <-callbackCh\n\n\t\t// Execute the callback (with panic protection).\n\t\tif callback != nil {\n\t\t\tpanicCatcher.Try(callback)\n\t\t}\n\n\t\t// Return the channel to the pool of unused channels.\n\t\tputCh(callbackCh)\n\t}\n}\n\ntype callbackCh chan func()\n\nvar callbackChPool = sync.Pool{\n\tNew: func() any {\n\t\treturn make(callbackCh, 1)\n\t},\n}\n\nfunc getCh() callbackCh {\n\treturn callbackChPool.Get().(callbackCh)\n}\n\nfunc putCh(ch callbackCh) {\n\tcallbackChPool.Put(ch)\n}\n\n```\n# Function Signature: func (s *Stream) Wait() {\n# Natural Language Annotations: // Wait signals to the stream that all tasks have been submitted. Wait will\n// not return until all tasks and callbacks have been run.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *Stream) Wait() {\n\ts.init()\n\n\t// Defer the callbacker cleanup so that it occurs even in the case\n\t// that one of the tasks panics and is propagated up by s.pool.Wait().\n\tdefer func() {\n\t\tclose(s.queue)\n\t\ts.callbackerHandle.Wait()\n\t}()\n\n\t// Wait for all the workers to exit.\n\ts.pool.Wait()\n}\n```"]}, "OpenGFW-analyzer/udp/internal/quic/packet_protector.go-UnProtect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/OpenGFW/analyzer/udp/internal/quic/packet_protector.go:\n```\npackage quic\n\nimport (\n\t\"crypto\"\n\t\"crypto/aes\"\n\t\"crypto/cipher\"\n\t\"crypto/sha256\"\n\t\"crypto/tls\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\n\t\"golang.org/x/crypto/chacha20\"\n\t\"golang.org/x/crypto/chacha20poly1305\"\n\t\"golang.org/x/crypto/cryptobyte\"\n\t\"golang.org/x/crypto/hkdf\"\n)\n\n// NewProtectionKey creates a new ProtectionKey.\nfunc NewProtectionKey(suite uint16, secret []byte, v uint32) (*ProtectionKey, error) {\n\treturn newProtectionKey(suite, secret, v)\n}\n\n// NewInitialProtectionKey is like NewProtectionKey, but the returned protection key\n// is used for encrypt/decrypt Initial Packet only.\n//\n// See: https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-initial-secrets\nfunc NewInitialProtectionKey(secret []byte, v uint32) (*ProtectionKey, error) {\n\treturn NewProtectionKey(tls.TLS_AES_128_GCM_SHA256, secret, v)\n}\n\n// NewPacketProtector creates a new PacketProtector.\nfunc NewPacketProtector(key *ProtectionKey) *PacketProtector {\n\treturn &PacketProtector{key: key}\n}\n\n// PacketProtector is used for protecting a QUIC packet.\n//\n// See: https://www.rfc-editor.org/rfc/rfc9001.html#name-packet-protection\ntype PacketProtector struct {\n\tkey *ProtectionKey\n}\n\n// UnProtect decrypts a QUIC packet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ProtectionKey is the key used to protect a QUIC packet.\ntype ProtectionKey struct {\n\taead             cipher.AEAD\n\theaderProtection func(sample []byte) (mask []byte)\n\tiv               []byte\n}\n\n// https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-aead-usage\n//\n// \"The 62 bits of the reconstructed QUIC packet number in network byte order are\n// left-padded with zeros to the size of the IV. The exclusive OR of the padded\n// packet number and the IV forms the AEAD nonce.\"\nfunc (pk *ProtectionKey) nonce(pn int64) []byte {\n\tnonce := make([]byte, len(pk.iv))\n\tbinary.BigEndian.PutUint64(nonce[len(nonce)-8:], uint64(pn))\n\tfor i := range pk.iv {\n\t\tnonce[i] ^= pk.iv[i]\n\t}\n\treturn nonce\n}\n\nfunc newProtectionKey(suite uint16, secret []byte, v uint32) (*ProtectionKey, error) {\n\tswitch suite {\n\tcase tls.TLS_AES_128_GCM_SHA256:\n\t\tkey := hkdfExpandLabel(crypto.SHA256.New, secret, keyLabel(v), nil, 16)\n\t\tc, err := aes.NewCipher(key)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\taead, err := cipher.NewGCM(c)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tiv := hkdfExpandLabel(crypto.SHA256.New, secret, ivLabel(v), nil, aead.NonceSize())\n\t\thpKey := hkdfExpandLabel(crypto.SHA256.New, secret, headerProtectionLabel(v), nil, 16)\n\t\thp, err := aes.NewCipher(hpKey)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tk := &ProtectionKey{}\n\t\tk.aead = aead\n\t\t// https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-aes-based-header-protection\n\t\tk.headerProtection = func(sample []byte) []byte {\n\t\t\tmask := make([]byte, hp.BlockSize())\n\t\t\thp.Encrypt(mask, sample)\n\t\t\treturn mask\n\t\t}\n\t\tk.iv = iv\n\t\treturn k, nil\n\tcase tls.TLS_CHACHA20_POLY1305_SHA256:\n\t\tkey := hkdfExpandLabel(crypto.SHA256.New, secret, keyLabel(v), nil, chacha20poly1305.KeySize)\n\t\taead, err := chacha20poly1305.New(key)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tiv := hkdfExpandLabel(crypto.SHA256.New, secret, ivLabel(v), nil, aead.NonceSize())\n\t\thpKey := hkdfExpandLabel(sha256.New, secret, headerProtectionLabel(v), nil, chacha20.KeySize)\n\t\tk := &ProtectionKey{}\n\t\tk.aead = aead\n\t\t// https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-chacha20-based-header-prote\n\t\tk.headerProtection = func(sample []byte) []byte {\n\t\t\tnonce := sample[4:16]\n\t\t\tc, err := chacha20.NewUnauthenticatedCipher(hpKey, nonce)\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t\tc.SetCounter(binary.LittleEndian.Uint32(sample[:4]))\n\t\t\tmask := make([]byte, 5)\n\t\t\tc.XORKeyStream(mask, mask)\n\t\t\treturn mask\n\t\t}\n\t\tk.iv = iv\n\t\treturn k, nil\n\t}\n\treturn nil, errors.New(\"not supported cipher suite\")\n}\n\n// decodePacketNumber decode the packet number after header protection removed.\n//\n// See: https://datatracker.ietf.org/doc/html/draft-ietf-quic-transport-32#section-appendix.a\nfunc decodePacketNumber(largest, truncated int64, nbits uint8) int64 {\n\texpected := largest + 1\n\twin := int64(1 << (nbits * 8))\n\thwin := win / 2\n\tmask := win - 1\n\tcandidate := (expected &^ mask) | truncated\n\tswitch {\n\tcase candidate <= expected-hwin && candidate < (1<<62)-win:\n\t\treturn candidate + win\n\tcase candidate > expected+hwin && candidate >= win:\n\t\treturn candidate - win\n\t}\n\treturn candidate\n}\n\n// Copied from crypto/tls/key_schedule.go.\nfunc hkdfExpandLabel(hash func() hash.Hash, secret []byte, label string, context []byte, length int) []byte {\n\tvar hkdfLabel cryptobyte.Builder\n\thkdfLabel.AddUint16(uint16(length))\n\thkdfLabel.AddUint8LengthPrefixed(func(b *cryptobyte.Builder) {\n\t\tb.AddBytes([]byte(\"tls13 \"))\n\t\tb.AddBytes([]byte(label))\n\t})\n\thkdfLabel.AddUint8LengthPrefixed(func(b *cryptobyte.Builder) {\n\t\tb.AddBytes(context)\n\t})\n\tout := make([]byte, length)\n\tn, err := hkdf.Expand(hash, secret, hkdfLabel.BytesOrPanic()).Read(out)\n\tif err != nil || n != length {\n\t\tpanic(\"quic: HKDF-Expand-Label invocation failed unexpectedly\")\n\t}\n\treturn out\n}\n\n```\n# Function Signature: func (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n# Natural Language Annotations: // UnProtect decrypts a QUIC packet.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n    if len(packet) < int(pnOffset) {\n        return nil, errors.New(\"packet too short\")\n    }\n\n    // Extract the sample needed for header protection\n    sampleOffset := pnOffset + 4 // Sample starts after the Packet Number\n    if len(packet) < int(sampleOffset)+16 { // Sample size for AES is typically 16 bytes\n        return nil, errors.New(\"packet too short for header protection sample\")\n    }\n    sample := packet[sampleOffset : sampleOffset+16]\n\n    // Apply header protection\n    mask := pp.key.headerProtection(sample)\n    packetNumberBytes := packet[pnOffset:pnOffset+4] // Read the Packet Number field (up to 4 bytes)\n    for i := range packetNumberBytes {\n        packetNumberBytes[i] ^= mask[i]\n    }\n\n    truncatedPacketNumber := int64(binary.BigEndian.Uint32(packetNumberBytes))\n    packetNumber := decodePacketNumber(pnMax, truncatedPacketNumber, uint8(len(packetNumberBytes)))\n\n    // Decrypt the packet payload\n    nonce := pp.key.nonce(packetNumber)\n    plainText, err := pp.key.aead.Open(nil, nonce, packet[pnOffset+4:], packet[:pnOffset]) // Skip packet number bytes\n    if err != nil {\n        return nil, err\n    }\n\n    // The decrypted payload is the plaintext of the packet\n    return plainText, nil\n}\n```"]}, "hatchet-pkg/encryption/local.go-NewLocalEncryption": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/hatchet/pkg/encryption/local.go:\n```\npackage encryption\n\nimport (\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\n\t\"github.com/tink-crypto/tink-go/aead\"\n\t\"github.com/tink-crypto/tink-go/insecurecleartextkeyset\"\n\t\"github.com/tink-crypto/tink-go/jwt\"\n\t\"github.com/tink-crypto/tink-go/keyset\"\n\t\"github.com/tink-crypto/tink-go/tink\"\n)\n\ntype localEncryptionService struct {\n\tkey                *aead.KMSEnvelopeAEAD\n\tprivateEc256Handle *keyset.Handle\n\tpublicEc256Handle  *keyset.Handle\n}\n\n// NewLocalEncryption creates a new local encryption service. keysetBytes is the raw keyset in\n// base64-encoded JSON format. This can be generated by calling hatchet-admin keyset create-local.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc GenerateLocalKeys() (masterKey []byte, privateEc256 []byte, publicEc256 []byte, err error) {\n\tmasterKey, masterHandle, err := generateLocalMasterKey()\n\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\ta, err := aead.New(masterHandle)\n\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tprivateEc256, publicEc256, err = generateJWTKeysets(a)\n\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\treturn masterKey, privateEc256, publicEc256, nil\n}\n\nfunc generateLocalMasterKey() ([]byte, *keyset.Handle, error) {\n\taeadTemplate := aead.AES256GCMKeyTemplate()\n\n\taes256GcmHandle, err := keyset.NewHandle(aeadTemplate)\n\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to create new keyset handle with AES256GCM template: %w\", err)\n\t}\n\n\tbytes, err := insecureBytesFromHandle(aes256GcmHandle)\n\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to get bytes from handle: %w\", err)\n\t}\n\n\treturn bytes, aes256GcmHandle, nil\n}\n\n// generateJWTKeysets creates the keysets for JWT signing and verification encrypted with the\n// masterKey. The masterKey can be from a remote KMS service or a local keyset.\nfunc generateJWTKeysets(masterKey tink.AEAD) (privateEc256 []byte, publicEc256 []byte, err error) {\n\tprivateHandle, err := keyset.NewHandle(jwt.ES256Template())\n\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to create new keyset handle with ES256 template: %w\", err)\n\t\treturn\n\t}\n\n\tprivateEc256, err = bytesFromHandle(privateHandle, masterKey)\n\n\tif err != nil {\n\t\treturn\n\t}\n\n\tpublicHandle, err := privateHandle.Public()\n\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to get public keyset: %w\", err)\n\t\treturn\n\t}\n\n\tpublicEc256, err = bytesFromHandle(publicHandle, masterKey)\n\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn\n}\n\n// bytesFromHandle returns the encrypted keyset in base64-encoded JSON format, encrypted with the\n// masterKey\nfunc bytesFromHandle(kh *keyset.Handle, masterKey tink.AEAD) ([]byte, error) {\n\tbuf := new(bytes.Buffer)\n\twriter := keyset.NewJSONWriter(buf)\n\terr := kh.Write(writer, masterKey)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to write keyset: %w\", err)\n\t}\n\n\t// base64-encode bytes\n\tkeysetBytes := make([]byte, base64.RawStdEncoding.EncodedLen(len(buf.Bytes())))\n\tbase64.RawStdEncoding.Encode(keysetBytes, buf.Bytes())\n\n\treturn keysetBytes, nil\n}\n\n// insecureBytesFromHandle returns the raw (unencrypted) keyset in base64-encoded JSON format.\nfunc insecureBytesFromHandle(kh *keyset.Handle) ([]byte, error) {\n\tbuf := new(bytes.Buffer)\n\twriter := keyset.NewJSONWriter(buf)\n\terr := insecurecleartextkeyset.Write(kh, writer)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to write keyset: %w\", err)\n\t}\n\n\t// base64-encode bytes\n\tkeysetBytes := make([]byte, base64.RawStdEncoding.EncodedLen(len(buf.Bytes())))\n\tbase64.RawStdEncoding.Encode(keysetBytes, buf.Bytes())\n\n\treturn keysetBytes, nil\n}\n\nfunc handleFromBytes(keysetBytes []byte, masterKey tink.AEAD) (*keyset.Handle, error) {\n\t// base64-decode bytes\n\tkeysetJsonBytes := make([]byte, base64.RawStdEncoding.DecodedLen(len(keysetBytes)))\n\t_, err := base64.RawStdEncoding.Decode(keysetJsonBytes, keysetBytes)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode keyset bytes: %w\", err)\n\t}\n\n\t// read keyset\n\thandle, err := keyset.Read(keyset.NewJSONReader(bytes.NewReader(keysetJsonBytes)), masterKey)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read keyset: %w\", err)\n\t}\n\n\treturn handle, nil\n}\n\nfunc insecureHandleFromBytes(keysetBytes []byte) (*keyset.Handle, error) {\n\t// base64-decode bytes\n\tkeysetJsonBytes := make([]byte, base64.RawStdEncoding.DecodedLen(len(keysetBytes)))\n\t_, err := base64.RawStdEncoding.Decode(keysetJsonBytes, keysetBytes)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode keyset bytes: %w\", err)\n\t}\n\n\t// read keyset\n\thandle, err := insecurecleartextkeyset.Read(keyset.NewJSONReader(bytes.NewReader(keysetJsonBytes)))\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read keyset: %w\", err)\n\t}\n\n\treturn handle, nil\n}\n\nfunc (svc *localEncryptionService) Encrypt(plaintext []byte, dataId string) ([]byte, error) {\n\treturn encrypt(svc.key, plaintext, dataId)\n}\n\nfunc (svc *localEncryptionService) Decrypt(ciphertext []byte, dataId string) ([]byte, error) {\n\treturn decrypt(svc.key, ciphertext, dataId)\n}\n\nfunc (svc *localEncryptionService) EncryptString(data string, dataId string) (string, error) {\n\tb, err := encrypt(svc.key, []byte(data), dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.StdEncoding.EncodeToString(b), nil\n}\n\nfunc (svc *localEncryptionService) DecryptString(data string, dataId string) (string, error) {\n\tplain, err := base64.StdEncoding.DecodeString(data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tb, err := decrypt(svc.key, plain, dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(b), nil\n}\n\nfunc (svc *localEncryptionService) GetPrivateJWTHandle() *keyset.Handle {\n\treturn svc.privateEc256Handle\n}\n\nfunc (svc *localEncryptionService) GetPublicJWTHandle() *keyset.Handle {\n\treturn svc.publicEc256Handle\n}\n\n```\n# Function Signature: func NewLocalEncryption(masterKey []byte, privateEc256 []byte, publicEc256 []byte) (*localEncryptionService, error) {\n# Natural Language Annotations: // NewLocalEncryption creates a new local encryption service. keysetBytes is the raw keyset in\n// base64-encoded JSON format. This can be generated by calling hatchet-admin keyset create-local.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewLocalEncryption(masterKey []byte, privateEc256 []byte, publicEc256 []byte) (*localEncryptionService, error) {\n\t// Decode the master key to get the AEAD handle\n\tmasterKeyAEAD, err := handleFromBytes(masterKey, nil)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create master key handle: %w\", err)\n\t}\n\n\tkey, err := aead.New(masterKeyAEAD)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create AEAD from master key: %w\", err)\n\t}\n\n\t// Decode the private EC256 keyset handle\n\tprivateKey, err := handleFromBytes(privateEc256, key)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode private EC256 handle: %w\", err)\n\t}\n\n\t// Decode the public EC256 keyset handle\n\tpublicKey, err := handleFromBytes(publicEc256, key)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode public EC256 handle: %w\", err)\n\t}\n\n\t// Return a new localEncryptionService instance\n\treturn &localEncryptionService{\n\t\tkey:                key.(*aead.KMSEnvelopeAEAD),\n\t\tprivateEc256Handle: privateKey,\n\t\tpublicEc256Handle:  publicKey,\n\t}, nil\n}\n```"]}, "hatchet-pkg/encryption/cloudkms.go-NewCloudKMSEncryption": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/hatchet/pkg/encryption/cloudkms.go:\n```\npackage encryption\n\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\n\t\"github.com/tink-crypto/tink-go-gcpkms/integration/gcpkms\"\n\t\"github.com/tink-crypto/tink-go/aead\"\n\t\"github.com/tink-crypto/tink-go/core/registry\"\n\t\"github.com/tink-crypto/tink-go/keyset\"\n\t\"google.golang.org/api/option\"\n)\n\ntype cloudkmsEncryptionService struct {\n\tkey                *aead.KMSEnvelopeAEAD\n\tprivateEc256Handle *keyset.Handle\n\tpublicEc256Handle  *keyset.Handle\n}\n\n// NewCloudKMSEncryption creates a GCP CloudKMS-backed encryption service.\n\n\n\n\n\n\n\n\n\n\nfunc GenerateJWTKeysetsFromCloudKMS(keyUri string, credentialsJSON []byte) (privateEc256 []byte, publicEc256 []byte, err error) {\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn generateJWTKeysetsWithClient(keyUri, client)\n}\n\nfunc generateJWTKeysetsWithClient(keyUri string, client registry.KMSClient) (privateEc256 []byte, publicEc256 []byte, err error) {\n\tregistry.RegisterKMSClient(client)\n\n\tremote, err := client.GetAEAD(keyUri)\n\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn generateJWTKeysets(remote)\n}\n\nfunc newWithClient(client registry.KMSClient, keyUri string, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tregistry.RegisterKMSClient(client)\n\n\tdek := aead.AES128CTRHMACSHA256KeyTemplate()\n\ttemplate, err := aead.CreateKMSEnvelopeAEADKeyTemplate(keyUri, dek)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// get the remote KEK from the client\n\tremote, err := client.GetAEAD(keyUri)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tenvelope := aead.NewKMSEnvelopeAEAD2(template, remote)\n\n\tif envelope == nil {\n\t\treturn nil, fmt.Errorf(\"failed to create envelope\")\n\t}\n\n\tprivateEc256Handle, err := handleFromBytes(privateEc256, remote)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpublicEc256Handle, err := handleFromBytes(publicEc256, remote)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &cloudkmsEncryptionService{\n\t\tkey:                envelope,\n\t\tprivateEc256Handle: privateEc256Handle,\n\t\tpublicEc256Handle:  publicEc256Handle,\n\t}, nil\n}\n\nfunc (svc *cloudkmsEncryptionService) Encrypt(plaintext []byte, dataId string) ([]byte, error) {\n\treturn encrypt(svc.key, plaintext, dataId)\n}\n\nfunc (svc *cloudkmsEncryptionService) Decrypt(ciphertext []byte, dataId string) ([]byte, error) {\n\treturn decrypt(svc.key, ciphertext, dataId)\n}\n\nfunc (svc *cloudkmsEncryptionService) EncryptString(plaintext string, dataId string) (string, error) {\n\tb, err := encrypt(svc.key, []byte(plaintext), dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.StdEncoding.EncodeToString(b), nil\n}\n\nfunc (svc *cloudkmsEncryptionService) DecryptString(ciphertext string, dataId string) (string, error) {\n\tdecoded, err := base64.StdEncoding.DecodeString(ciphertext)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tb, err := decrypt(svc.key, decoded, dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(b), nil\n}\n\nfunc (svc *cloudkmsEncryptionService) GetPrivateJWTHandle() *keyset.Handle {\n\treturn svc.privateEc256Handle\n}\n\nfunc (svc *cloudkmsEncryptionService) GetPublicJWTHandle() *keyset.Handle {\n\treturn svc.publicEc256Handle\n}\n\n```\n# Function Signature: func NewCloudKMSEncryption(keyUri string, credentialsJSON, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n# Natural Language Annotations: // NewCloudKMSEncryption creates a GCP CloudKMS-backed encryption service.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewCloudKMSEncryption(keyUri string, credentialsJSON, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newWithClient(client, keyUri, privateEc256, publicEc256)\n}\n```"]}, "hatchet-internal/datautils/render.go-RenderTemplateFields": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/hatchet/internal/datautils/render.go:\n```\npackage datautils\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"text/template\"\n)\n\n// RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n# Natural Language Annotations: // RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage datautils\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"text/template\"\n\t\"reflect\"\n)\n\n// RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\nfunc RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n\toutput := make(map[string]interface{})\n\tfor k, v := range input {\n\t\tswitch vType := v.(type) {\n\t\tcase string:\n\t\t\t// Render string fields using the template\n\t\t\ttmpl, err := template.New(\"template\").Parse(vType)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"failed to parse template: %w\", err)\n\t\t\t}\n\t\t\tvar tplBytes bytes.Buffer\n\t\t\tif err := tmpl.Execute(&tplBytes, data); err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"failed to execute template: %w\", err)\n\t\t\t}\n\t\t\toutput[k] = tplBytes.String()\n\t\tcase map[string]interface{}:\n\t\t\t// Handle the nested map recursively\n\t\t\trenderedMap, err := RenderTemplateFields(data, vType)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\toutput[k] = renderedMap\n\t\tdefault:\n\t\t\t// For other types, just copy the value as it is\n\t\t\toutput[k] = v\n\t\t}\n\t}\n\treturn output, nil\n}\n```"]}, "mox-ratelimit/ratelimit.go-Reset": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/ratelimit/ratelimit.go:\n```\n// Package ratelimit provides a simple window-based rate limiter.\npackage ratelimit\n\nimport (\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Limiter is a simple rate limiter with one or more fixed windows, e.g. the\n// last minute/hour/day/week, working on three classes/subnets of an IP.\ntype Limiter struct {\n\tsync.Mutex\n\tWindowLimits []WindowLimit\n\tipmasked     [3][16]byte\n}\n\n// WindowLimit holds counters for one window, with limits for each IP class/subnet.\ntype WindowLimit struct {\n\tWindow time.Duration\n\tLimits [3]int64 // For \"ipmasked1\" through \"ipmasked3\".\n\tTime   uint32   // Time/Window.\n\tCounts map[struct {\n\t\tIndex    uint8\n\t\tIPMasked [16]byte\n\t}]int64\n}\n\n// Add attempts to consume \"n\" items from the rate limiter. If the total for this\n// key and this interval would exceed limit, \"n\" is not counted and false is\n// returned. If now represents a different time interval, all counts are reset.\nfunc (l *Limiter) Add(ip net.IP, tm time.Time, n int64) bool {\n\treturn l.checkAdd(true, ip, tm, n)\n}\n\n// CanAdd returns if n could be added to the limiter.\nfunc (l *Limiter) CanAdd(ip net.IP, tm time.Time, n int64) bool {\n\treturn l.checkAdd(false, ip, tm, n)\n}\n\nfunc (l *Limiter) checkAdd(add bool, ip net.IP, tm time.Time, n int64) bool {\n\tl.Lock()\n\tdefer l.Unlock()\n\n\t// First check.\n\tfor i, pl := range l.WindowLimits {\n\t\tt := uint32(tm.UnixNano() / int64(pl.Window))\n\n\t\tif t > pl.Time || pl.Counts == nil {\n\t\t\tl.WindowLimits[i].Time = t\n\t\t\tpl.Counts = map[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}]int64{} // Used below.\n\t\t\tl.WindowLimits[i].Counts = pl.Counts\n\t\t}\n\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tif i == 0 {\n\t\t\t\tl.ipmasked[j] = l.maskIP(j, ip)\n\t\t\t}\n\n\t\t\tv := pl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}]\n\t\t\tif v+n > pl.Limits[j] {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\tif !add {\n\t\treturn true\n\t}\n\t// Finally record.\n\tfor _, pl := range l.WindowLimits {\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tpl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}] += n\n\t\t}\n\t}\n\treturn true\n}\n\n// Reset sets the counter to 0 for key and ip, and subtracts from the ipmasked counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (l *Limiter) maskIP(i int, ip net.IP) [16]byte {\n\tisv4 := ip.To4() != nil\n\n\tvar ipmasked net.IP\n\tif isv4 {\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tipmasked = ip\n\t\tcase 1:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(26, 32))\n\t\tcase 2:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(21, 32))\n\t\tdefault:\n\t\t\tpanic(\"missing case for maskip ipv4\")\n\t\t}\n\t} else {\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(64, 128))\n\t\tcase 1:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(48, 128))\n\t\tcase 2:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(32, 128))\n\t\tdefault:\n\t\t\tpanic(\"missing case for masking ipv6\")\n\t\t}\n\t}\n\treturn *(*[16]byte)(ipmasked.To16())\n}\n\n```\n# Function Signature: func (l *Limiter) Reset(ip net.IP, tm time.Time) {\n# Natural Language Annotations: // Reset sets the counter to 0 for key and ip, and subtracts from the ipmasked counts.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (l *Limiter) Reset(ip net.IP, tm time.Time) {\n    l.Lock()\n    defer l.Unlock()\n\n    for i, pl := range l.WindowLimits {\n        t := uint32(tm.UnixNano() / int64(pl.Window))\n\n        if t < pl.Time || pl.Counts == nil {\n            // No counts to reset for previous intervals.\n            continue\n        }\n\n        // Loop through the IP masks.\n        for j := 0; j < 3; j++ {\n            if i == 0 {\n                l.ipmasked[j] = l.maskIP(j, ip)\n            }\n\n            key := struct {\n                Index    uint8\n                IPMasked [16]byte\n            }{uint8(j), l.ipmasked[j]}\n\n            // Reset the count for this key.\n            if count, exists := pl.Counts[key]; exists {\n                delete(pl.Counts, key) // Remove the key, setting its count to 0.\n\n                // If we've removed the key, ensure to subtract from any other masked counts.\n                for k := j + 1; k < 3; k++ {\n                    keyHigherMask := struct {\n                        Index    uint8\n                        IPMasked [16]byte\n                    }{uint8(k), l.ipmasked[k]}\n                    pl.Counts[keyHigherMask] -= count\n                }\n            }\n        }\n    }\n}\n```"]}, "mox-dns/dns.go-ParseDomainLax": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dns/dns.go:\n```\n// Package dns helps parse internationalized domain names (IDNA), canonicalize\n// names and provides a strict and metrics-keeping logging DNS resolver.\npackage dns\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"golang.org/x/net/idna\"\n\n\t\"github.com/mjl-/adns\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar (\n\terrTrailingDot = errors.New(\"dns name has trailing dot\")\n\terrUnderscore  = errors.New(\"domain name with underscore\")\n\terrIDNA        = errors.New(\"idna\")\n)\n\n// Domain is a domain name, with one or more labels, with at least an ASCII\n// representation, and for IDNA non-ASCII domains a unicode representation.\n// The ASCII string must be used for DNS lookups. The strings do not have a\n// trailing dot. When using with StrictResolver, add the trailing dot.\ntype Domain struct {\n\t// A non-unicode domain, e.g. with A-labels (xn--...) or NR-LDH (non-reserved\n\t// letters/digits/hyphens) labels. Always in lower case. No trailing dot.\n\tASCII string\n\n\t// Name as U-labels, in Unicode NFC. Empty if this is an ASCII-only domain. No\n\t// trailing dot.\n\tUnicode string\n}\n\n// Name returns the unicode name if set, otherwise the ASCII name.\nfunc (d Domain) Name() string {\n\tif d.Unicode != \"\" {\n\t\treturn d.Unicode\n\t}\n\treturn d.ASCII\n}\n\n// XName is like Name, but only returns a unicode name when utf8 is true.\nfunc (d Domain) XName(utf8 bool) string {\n\tif utf8 && d.Unicode != \"\" {\n\t\treturn d.Unicode\n\t}\n\treturn d.ASCII\n}\n\n// ASCIIExtra returns the ASCII version of the domain name if smtputf8 is true and\n// this is a unicode domain name. Otherwise it returns an empty string.\n//\n// This function is used to add the punycode name in a comment to SMTP message\n// headers, e.g. Received and Authentication-Results.\nfunc (d Domain) ASCIIExtra(smtputf8 bool) string {\n\tif smtputf8 && d.Unicode != \"\" {\n\t\treturn d.ASCII\n\t}\n\treturn \"\"\n}\n\n// Strings returns a human-readable string.\n// For IDNA names, the string contains both the unicode and ASCII name.\nfunc (d Domain) String() string {\n\treturn d.LogString()\n}\n\n// LogString returns a domain for logging.\n// For IDNA names, the string is the slash-separated Unicode and ASCII name.\n// For ASCII-only domain names, just the ASCII string is returned.\nfunc (d Domain) LogString() string {\n\tif d.Unicode == \"\" {\n\t\treturn d.ASCII\n\t}\n\treturn d.Unicode + \"/\" + d.ASCII\n}\n\n// IsZero returns if this is an empty Domain.\nfunc (d Domain) IsZero() bool {\n\treturn d == Domain{}\n}\n\n// ParseDomain parses a domain name that can consist of ASCII-only labels or U\n// labels (unicode).\n// Names are IDN-canonicalized and lower-cased.\n// Characters in unicode can be replaced by equivalents. E.g. \"\u24c7\" to \"r\". This\n// means you should only compare parsed domain names, never unparsed strings\n// directly.\nfunc ParseDomain(s string) (Domain, error) {\n\tif strings.HasSuffix(s, \".\") {\n\t\treturn Domain{}, errTrailingDot\n\t}\n\n\tascii, err := idna.Lookup.ToASCII(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to ascii: %v\", errIDNA, err)\n\t}\n\tunicode, err := idna.Lookup.ToUnicode(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to unicode: %w\", errIDNA, err)\n\t}\n\t// todo: should we cause errors for unicode domains that were not in\n\t// canonical form? we are now accepting all kinds of obscure spellings\n\t// for even a basic ASCII domain name.\n\t// Also see https://daniel.haxx.se/blog/2022/12/14/idn-is-crazy/\n\tif ascii == unicode {\n\t\treturn Domain{ascii, \"\"}, nil\n\t}\n\treturn Domain{ascii, unicode}, nil\n}\n\n// ParseDomainLax parses a domain like ParseDomain, but allows labels with\n// underscores if the entire domain name is ASCII-only non-IDNA and Pedantic mode\n// is not enabled. Used for interoperability, e.g. domains may specify MX\n// targets with underscores.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// IsNotFound returns whether an error is an adns.DNSError with IsNotFound set.\n// IsNotFound means the requested type does not exist for the given domain (a\n// nodata or nxdomain response). It doesn't not necessarily mean no other types for\n// that name exist.\n//\n// A DNS server can respond to a lookup with an error \"nxdomain\" to indicate a\n// name does not exist (at all), or with a success status with an empty list.\n// The adns resolver (just like the Go resolver) returns an IsNotFound error for\n// both cases, there is no need to explicitly check for zero entries.\nfunc IsNotFound(err error) bool {\n\tvar dnsErr *adns.DNSError\n\treturn err != nil && errors.As(err, &dnsErr) && dnsErr.IsNotFound\n}\n\n```\n# Function Signature: func ParseDomainLax(s string) (Domain, error) {\n# Natural Language Annotations: // ParseDomainLax parses a domain like ParseDomain, but allows labels with\n// underscores if the entire domain name is ASCII-only non-IDNA and Pedantic mode\n// is not enabled. Used for interoperability, e.g. domains may specify MX\n// targets with underscores.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseDomainLax(s string) (Domain, error) {\n\tif strings.HasSuffix(s, \".\") {\n\t\treturn Domain{}, errTrailingDot\n\t}\n\n\tcontainsUnderscore := strings.Contains(s, \"_\")\n\tif containsUnderscore && Pedantic {\n\t\treturn Domain{}, errUnderscore\n\t}\n\n\tasciiProcessed, err := idna.Lookup.ToASCII(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to ascii: %v\", errIDNA, err)\n\t}\n\t\n\tif containsUnderscore && !strings.EqualFold(asciiProcessed, s) {\n\t\treturn Domain{}, errUnderscore\n\t}\n\n\tunicodeProcessed, err := idna.Lookup.ToUnicode(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to unicode: %v\", errIDNA, err)\n\t}\n\n\tif asciiProcessed == unicodeProcessed {\n\t\treturn Domain{asciiProcessed, \"\"}, nil\n\t}\n\n\treturn Domain{asciiProcessed, unicodeProcessed}, nil\n}\n```"]}, "mox-dmarcdb/eval.go-AddEvaluation": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n# Natural Language Annotations: // AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\nfunc AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n    // Validate the Evaluation object\n    if e == nil {\n        return errors.New(\"evaluation is nil\")\n    }\n    \n    // Set the IntervalHours based on aggregateReportingIntervalSeconds\n    e.IntervalHours = intervalHours(aggregateReportingIntervalSeconds)\n    \n    // Add the evaluation to the database\n    if err := EvalDB.Insert(ctx, e); err != nil {\n        metricReportError.Inc()  // Increment error metric\n        return fmt.Errorf(\"inserting evaluation into database: %w\", err)\n    }\n    \n    metricReport.Inc()  // Increment report metric\n    return nil\n}\n```"]}, "mox-dmarcdb/eval.go-EvaluationStats": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n# Natural Language Annotations: // EvaluationStats returns evaluation counts and report-sending status per domain.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// EvaluationStats returns evaluation counts and report-sending status per domain.\nfunc EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n\tevaluations, err := Evaluations(ctx)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to retrieve evaluations: %v\", err)\n\t}\n\n\tdomainStats := make(map[string]EvaluationStat)\n\n\tfor _, eval := range evaluations {\n\t\tstat, exists := domainStats[eval.PolicyDomain]\n\n\t\tif !exists {\n\t\t\tstat = EvaluationStat{\n\t\t\t\tDomain: dns.Domain(eval.PolicyDomain),\n\t\t\t}\n\t\t}\n\n\t\tstat.Dispositions = append(stat.Dispositions, string(eval.Disposition))\n\t\tstat.Count++\n\n\t\t// SendReport is true if at least one evaluation for the domain is not optional\n\t\tif !eval.Optional {\n\t\t\tstat.SendReport = true\n\t\t}\n\n\t\tdomainStats[eval.PolicyDomain] = stat\n\t}\n\n\treturn domainStats, nil\n}\n```"]}, "mox-dmarcdb/eval.go-EvaluationsDomain": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n# Natural Language Annotations: // EvaluationsDomain returns all evaluations for a domain.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.FilterEqual(\"PolicyDomain\", domain.Name)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n```"]}, "mox-dmarcdb/eval.go-RemoveEvaluationsDomain": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n# Natural Language Annotations: // RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here's a Go function implementation for `RemoveEvaluationsDomain` based on the provided natural language annotations and context from the surrounding code:\n\n```go\nfunc RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n\t// This function removes evaluations for a specified domain, ensuring that they are not included in future aggregate reports.\n\t\n\t// Create a query on the EvalDB to target the specific evaluations for the given domain.\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\t\n\t// Use FilterNonzero to ensure we target evaluations specifically for this domain.\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain.String()})\n\t\n\t// Execute the delete operation to remove the evaluations from the database.\n\t_, err := q.Delete()\n\t\n\t// Check if there was an error during the deletion process and return it.\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to remove evaluations for domain %s: %w\", domain, err)\n\t}\n\t\n\t// Return nil if no error occurs, indicating successful deletion.\n\treturn nil\n}\n```\n\nThis function queries the database for all evaluations associated with a specific domain and deletes them, ensuring they won't be included in future aggregate reports. The function handles the context and potential errors from the database operation."]}, "mox-dmarcdb/reports.go-AddReport": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/reports.go:\n```\npackage dmarcdb\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n)\n\nvar (\n\tReportsDBTypes = []any{DomainFeedback{}} // Types stored in DB.\n\tReportsDB      *bstore.DB                // Exported for backups.\n)\n\nvar (\n\tmetricEvaluated = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_policy_evaluated_total\",\n\t\t\tHelp: \"Number of policy evaluations.\",\n\t\t},\n\t\t// We only register validated domains for which we have a config.\n\t\t[]string{\"domain\", \"disposition\", \"dkim\", \"spf\"},\n\t)\n\tmetricDKIM = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_dkim_result_total\",\n\t\t\tHelp: \"Number of DKIM results.\",\n\t\t},\n\t\t[]string{\"result\"},\n\t)\n\tmetricSPF = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_spf_result_total\",\n\t\t\tHelp: \"Number of SPF results.\",\n\t\t},\n\t\t[]string{\"result\"},\n\t)\n)\n\n// DomainFeedback is a single report stored in the database.\ntype DomainFeedback struct {\n\tID int64\n\t// Domain where DMARC DNS record was found, could be organizational domain.\n\tDomain string `bstore:\"index\"`\n\t// Domain in From-header.\n\tFromDomain string `bstore:\"index\"`\n\tdmarcrpt.Feedback\n}\n\n// AddReport adds a DMARC aggregate feedback report from an email to the database,\n// and updates prometheus metrics.\n//\n// fromDomain is the domain in the report message From header.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Records returns all reports in the database.\nfunc Records(ctx context.Context) ([]DomainFeedback, error) {\n\treturn bstore.QueryDB[DomainFeedback](ctx, ReportsDB).List()\n}\n\n// RecordID returns the report for the ID.\nfunc RecordID(ctx context.Context, id int64) (DomainFeedback, error) {\n\te := DomainFeedback{ID: id}\n\terr := ReportsDB.Get(ctx, &e)\n\treturn e, err\n}\n\n// RecordsPeriodDomain returns the reports overlapping start and end, for the given\n// domain. If domain is empty, all records match for domain.\nfunc RecordsPeriodDomain(ctx context.Context, start, end time.Time, domain string) ([]DomainFeedback, error) {\n\ts := start.Unix()\n\te := end.Unix()\n\n\tq := bstore.QueryDB[DomainFeedback](ctx, ReportsDB)\n\tif domain != \"\" {\n\t\tq.FilterNonzero(DomainFeedback{Domain: domain})\n\t}\n\tq.FilterFn(func(d DomainFeedback) bool {\n\t\tm := d.Feedback.ReportMetadata.DateRange\n\t\treturn m.Begin >= s && m.Begin < e || m.End > s && m.End <= e\n\t})\n\treturn q.List()\n}\n\n```\n# Function Signature: func AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain) error {\n# Natural Language Annotations: // AddReport adds a DMARC aggregate feedback report from an email to the database,\n// and updates prometheus metrics.\n//\n// fromDomain is the domain in the report message From header.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain) error {\n\t// Check for nil feedback\n\tif f == nil {\n\t\treturn fmt.Errorf(\"feedback is nil\")\n\t}\n\n\t// Initialize DomainFeedback\n\tdf := DomainFeedback{\n\t\tDomain:        f.ReportMetadata.OrgDomain,\n\t\tFromDomain:    string(fromDomain),\n\t\tFeedback:      *f,\n\t}\n\n\t// Insert the feedback report into the database\n\tif err := ReportsDB.Insert(ctx, &df); err != nil {\n\t\treturn fmt.Errorf(\"failed to insert report: %w\", err)\n\t}\n\n\t// Update Prometheus metrics\n\tfor _, record := range f.Records {\n\t\tdisposition := record.Row.PolicyEvaluated.Disposition\n\t\tdkimResult := record.AuthResults.DKIM.Result\n\t\tspfResult := record.AuthResults.SPF.Result\n\n\t\tmetricEvaluated.WithLabelValues(df.Domain, disposition, dkimResult, spfResult).Inc()\n\t\tmetricDKIM.WithLabelValues(dkimResult).Inc()\n\t\tmetricSPF.WithLabelValues(spfResult).Inc()\n\t}\n\n\treturn nil\n}\n```"]}, "mox-junk/bloom.go-NewBloom": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/bloom.go:\n```\npackage junk\n\nimport (\n\t\"errors\"\n\t\"os\"\n\n\t\"golang.org/x/crypto/blake2b\"\n)\n\n// see https://en.wikipedia.org/wiki/Bloom_filter\n\nvar errWidth = errors.New(\"k and width wider than 256 bits and width not more than 32\")\nvar errPowerOfTwo = errors.New(\"data not a power of two\")\n\n// Bloom is a bloom filter.\ntype Bloom struct {\n\tdata     []byte\n\tk        int // Number of bits we store/lookup in the bloom filter per value.\n\tw        int // Number of bits needed to address a single bit position.\n\tmodified bool\n}\n\nfunc bloomWidth(fileSize int) int {\n\tw := 0\n\tfor bits := uint32(fileSize * 8); bits > 1; bits >>= 1 {\n\t\tw++\n\t}\n\treturn w\n}\n\n// BloomValid returns an error if the bloom file parameters are not correct.\nfunc BloomValid(fileSize int, k int) error {\n\t_, err := bloomValid(fileSize, k)\n\treturn err\n}\n\nfunc bloomValid(fileSize, k int) (int, error) {\n\tw := bloomWidth(fileSize)\n\tif 1<<w != fileSize*8 {\n\t\treturn 0, errPowerOfTwo\n\t}\n\tif k*w > 256 || w > 32 {\n\t\treturn 0, errWidth\n\t}\n\treturn w, nil\n}\n\n// NewBloom returns a bloom filter with given initial data.\n//\n// The number of bits in data must be a power of 2.\n// K is the number of \"hashes\" (bits) to store/lookup for each value stored.\n// Width is calculated as the number of bits needed to represent a single bit/hash\n// position in the data.\n//\n// For each value stored/looked up, a hash over the value is calculated. The hash\n// is split into \"k\" values that are \"width\" bits wide, each used to lookup a bit.\n// K * width must not exceed 256.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (b *Bloom) Add(s string) {\n\th := hash([]byte(s), b.w)\n\tfor i := 0; i < b.k; i++ {\n\t\tb.set(h.nextPos())\n\t}\n}\n\nfunc (b *Bloom) Has(s string) bool {\n\th := hash([]byte(s), b.w)\n\tfor i := 0; i < b.k; i++ {\n\t\tif !b.has(h.nextPos()) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (b *Bloom) Bytes() []byte {\n\treturn b.data\n}\n\nfunc (b *Bloom) Modified() bool {\n\treturn b.modified\n}\n\n// Ones returns the number of ones.\nfunc (b *Bloom) Ones() (n int) {\n\tfor _, d := range b.data {\n\t\tfor i := 0; i < 8; i++ {\n\t\t\tif d&1 != 0 {\n\t\t\t\tn++\n\t\t\t}\n\t\t\td >>= 1\n\t\t}\n\t}\n\treturn n\n}\n\nfunc (b *Bloom) Write(path string) error {\n\tf, err := os.OpenFile(path, os.O_CREATE|os.O_WRONLY, 0660)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif _, err := f.Write(b.data); err != nil {\n\t\tf.Close()\n\t\treturn err\n\t}\n\tif err := f.Close(); err != nil {\n\t\treturn err\n\t}\n\tb.modified = false\n\treturn nil\n}\n\nfunc (b *Bloom) has(p int) bool {\n\tv := b.data[p>>3] >> (7 - (p & 7))\n\treturn v&1 != 0\n}\n\nfunc (b *Bloom) set(p int) {\n\tby := p >> 3\n\tbi := p & 0x7\n\tvar v byte = 1 << (7 - bi)\n\tif b.data[by]&v == 0 {\n\t\tb.data[by] |= v\n\t\tb.modified = true\n\t}\n}\n\ntype bits struct {\n\twidth int    // Number of bits for each position.\n\tbuf   []byte // Remaining bytes to use for next position.\n\tcur   uint64 // Bits to read next position from. Replenished from buf.\n\tncur  int    // Number of bits available in cur. We consume the highest bits first.\n}\n\nfunc hash(v []byte, width int) *bits {\n\tbuf := blake2b.Sum256(v)\n\treturn &bits{width: width, buf: buf[:]}\n}\n\n// nextPos returns the next bit position.\nfunc (b *bits) nextPos() (v int) {\n\tif b.width > b.ncur {\n\t\tfor len(b.buf) > 0 && b.ncur < 64-8 {\n\t\t\tb.cur <<= 8\n\t\t\tb.cur |= uint64(b.buf[0])\n\t\t\tb.ncur += 8\n\t\t\tb.buf = b.buf[1:]\n\t\t}\n\t}\n\tv = int((b.cur >> (b.ncur - b.width)) & ((1 << b.width) - 1))\n\tb.ncur -= b.width\n\treturn v\n}\n\n```\n# Function Signature: func NewBloom(data []byte, k int) (*Bloom, error) {\n# Natural Language Annotations: // NewBloom returns a bloom filter with given initial data.\n//\n// The number of bits in data must be a power of 2.\n// K is the number of \"hashes\" (bits) to store/lookup for each value stored.\n// Width is calculated as the number of bits needed to represent a single bit/hash\n// position in the data.\n//\n// For each value stored/looked up, a hash over the value is calculated. The hash\n// is split into \"k\" values that are \"width\" bits wide, each used to lookup a bit.\n// K * width must not exceed 256.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewBloom(data []byte, k int) (*Bloom, error) {\n    // Calculate the size of the bloom filter in bits\n    fileSize := len(data)\n\n    // Validate the data size: it must be a power of 2\n    if (fileSize&(fileSize-1)) != 0 || fileSize == 0 {\n        return nil, errPowerOfTwo\n    }\n\n    // Calculate the width based on fileSize\n    w, err := bloomValid(fileSize, k)\n    if err != nil {\n        return nil, err\n    }\n\n    // Create the Bloom filter instance\n    bloom := &Bloom {\n        data: data,\n        k: k,\n        w: w,\n        modified: false,\n    }\n\n    return bloom, nil\n}\n```"]}, "mox-junk/filter.go-NewFilter": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n# Natural Language Annotations: // NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n\t// Check if the bloom filter file already exists\n\tif _, err := os.Stat(bloomPath); err == nil {\n\t\treturn nil, fmt.Errorf(\"bloom filter file already exists at %s\", bloomPath)\n\t}\n\n\t// Create a new bloom filter\n\tbloom, err := newBloom(bloomPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"creating new bloom filter: %w\", err)\n\t}\n\n\t// Check if the database file already exists\n\tif _, err := os.Stat(dbPath); err == nil {\n\t\treturn nil, fmt.Errorf(\"database file already exists at %s\", dbPath)\n\t}\n\n\t// Create a new database\n\tdb, err := newDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"creating new database: %w\", err)\n\t}\n\n\t// Initialize a new Filter instance\n\tfilter := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     make(map[string]word),\n\t\tchanged:   make(map[string]word),\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t\tisNew:     true, // Mark the filter as new until the first save\n\t}\n\n\t// Return the new Filter\n\treturn filter, nil\n}\n\nfunc newBloom(path string) (*Bloom, error) {\n\t// Initialize a new bloom filter with the required size and parameters\n\t// This function should be implemented to set up a new Bloom filter correctly.\n\t// Here, a stub example is returned.\n\treturn &Bloom{}, nil\n}\n```"]}, "mox-junk/filter.go-ClassifyMessagePath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n# Natural Language Annotations: // ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\nfunc (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tfile, err := os.Open(path)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, fmt.Errorf(\"opening file: %w\", err)\n\t}\n\tdefer file.Close()\n\n\tfileInfo, err := file.Stat()\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, fmt.Errorf(\"getting file info: %w\", err)\n\t}\n\n\treturn f.ClassifyMessageReader(ctx, file, fileInfo.Size())\n}\n```"]}, "mox-junk/filter.go-TrainDirs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n# Natural Language Annotations: // TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// TrainDirs trains and saves a filter with mail messages from different types of directories.\nfunc (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\ttrainDirectory := func(dir string, files []string, isHam bool) error {\n\t\tn, malformed, err := f.TrainDir(dir, files, isHam)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tf.log.Debug(\"trained directory\",\n\t\t\tslog.Any(\"dir\", dir),\n\t\t\tslog.Any(\"trained\", n),\n\t\t\tslog.Any(\"malformed\", malformed),\n\t\t\tslog.Any(\"isHam\", isHam))\n\t\treturn nil\n\t}\n\n\t// Train ham messages\n\tif err := trainDirectory(hamDir, hamFiles, true); err != nil {\n\t\treturn err\n\t}\n\n\t// Train sent messages\n\tif err := trainDirectory(sentDir, sentFiles, true); err != nil {\n\t\treturn err\n\t}\n\n\t// Train spam messages\n\tif err := trainDirectory(spamDir, spamFiles, false); err != nil {\n\t\treturn err\n\t}\n\n\tif f.modified {\n\t\tif err := f.Save(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n```"]}, "mox-webmail/message.go-parseListPostAddress": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/webmail/message.go:\n```\npackage webmail\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"net/url\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\n// todo: we should have all needed information for messageItem in store.Message (perhaps some data in message.Part) for fast access, not having to parse the on-disk message file.\n\nvar wordDecoder = mime.WordDecoder{\n\tCharsetReader: func(charset string, r io.Reader) (io.Reader, error) {\n\t\tswitch strings.ToLower(charset) {\n\t\tcase \"\", \"us-ascii\", \"utf-8\":\n\t\t\treturn r, nil\n\t\t}\n\t\tenc, _ := ianaindex.MIME.Encoding(charset)\n\t\tif enc == nil {\n\t\t\tenc, _ = ianaindex.IANA.Encoding(charset)\n\t\t}\n\t\tif enc == nil {\n\t\t\treturn r, fmt.Errorf(\"unknown charset %q\", charset)\n\t\t}\n\t\treturn enc.NewDecoder().Reader(r), nil\n\t},\n}\n\n// Attempt q/b-word-decode name, coming from Content-Type \"name\" field or\n// Content-Disposition \"filename\" field.\n//\n// RFC 2231 specify an encoding for non-ascii values in mime header parameters. But\n// it appears common practice to instead just q/b-word encode the values.\n// Thunderbird and gmail.com do this for the Content-Type \"name\" parameter.\n// gmail.com also does that for the Content-Disposition \"filename\" parameter, where\n// Thunderbird uses the RFC 2231-defined encoding. Go's mime.ParseMediaType parses\n// the mechanism specified in RFC 2231 only. The value for \"name\" we get here would\n// already be decoded properly for standards-compliant headers, like\n// \"filename*0*=UTF-8\u201d%...; filename*1*=%.... We'll look for Q/B-word encoding\n// markers (\"=?\"-prefix or \"?=\"-suffix) and try to decode if present. This would\n// only cause trouble for filenames having this prefix/suffix.\nfunc tryDecodeParam(log mlog.Log, name string) string {\n\tif name == \"\" || !strings.HasPrefix(name, \"=?\") && !strings.HasSuffix(name, \"?=\") {\n\t\treturn name\n\t}\n\t// todo: find where this is allowed. it seems quite common. perhaps we should remove the pedantic check?\n\tif mox.Pedantic {\n\t\tlog.Debug(\"attachment contains rfc2047 q/b-word-encoded mime parameter instead of rfc2231-encoded\", slog.String(\"name\", name))\n\t\treturn name\n\t}\n\ts, err := wordDecoder.DecodeHeader(name)\n\tif err != nil {\n\t\tlog.Debugx(\"q/b-word decoding mime parameter\", err, slog.String(\"name\", name))\n\t\treturn name\n\t}\n\treturn s\n}\n\n// todo: mime.FormatMediaType does not wrap long lines. should do it ourselves, and split header into several parts (if commonly supported).\n\nfunc messageItem(log mlog.Log, m store.Message, state *msgState) (MessageItem, error) {\n\tpm, err := parsedMessage(log, m, state, false, true)\n\tif err != nil {\n\t\treturn MessageItem{}, fmt.Errorf(\"parsing message %d for item: %v\", m.ID, err)\n\t}\n\t// Clear largish unused data.\n\tm.MsgPrefix = nil\n\tm.ParsedBuf = nil\n\treturn MessageItem{m, pm.envelope, pm.attachments, pm.isSigned, pm.isEncrypted, pm.firstLine, true}, nil\n}\n\n// formatFirstLine returns a line the client can display next to the subject line\n// in a mailbox. It will replace quoted text, and any prefixing \"On ... write:\"\n// line with \"[...]\" so only new and useful information will be displayed.\n// Trailing signatures are not included.\nfunc formatFirstLine(r io.Reader) (string, error) {\n\t// We look quite a bit of lines ahead for trailing signatures with trailing empty lines.\n\tvar lines []string\n\tscanner := bufio.NewScanner(r)\n\tensureLines := func() {\n\t\tfor len(lines) < 10 && scanner.Scan() {\n\t\t\tlines = append(lines, strings.TrimSpace(scanner.Text()))\n\t\t}\n\t}\n\tensureLines()\n\n\tisSnipped := func(s string) bool {\n\t\treturn s == \"[...]\" || s == \"[\u2026]\" || s == \"...\"\n\t}\n\n\tnextLineQuoted := func(i int) bool {\n\t\tif i+1 < len(lines) && lines[i+1] == \"\" {\n\t\t\ti++\n\t\t}\n\t\treturn i+1 < len(lines) && (strings.HasPrefix(lines[i+1], \">\") || isSnipped(lines[i+1]))\n\t}\n\n\t// Remainder is signature if we see a line with only and minimum 2 dashes, and\n\t// there are no more empty lines, and there aren't more than 5 lines left.\n\tisSignature := func() bool {\n\t\tif len(lines) == 0 || !strings.HasPrefix(lines[0], \"--\") || strings.Trim(strings.TrimSpace(lines[0]), \"-\") != \"\" {\n\t\t\treturn false\n\t\t}\n\t\tl := lines[1:]\n\t\tfor len(l) > 0 && l[len(l)-1] == \"\" {\n\t\t\tl = l[:len(l)-1]\n\t\t}\n\t\tif len(l) >= 5 {\n\t\t\treturn false\n\t\t}\n\t\tfor _, line := range l {\n\t\t\tif line == \"\" {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\tresult := \"\"\n\n\tresultSnipped := func() bool {\n\t\treturn strings.HasSuffix(result, \"[...]\\n\") || strings.HasSuffix(result, \"[\u2026]\")\n\t}\n\n\t// Quick check for initial wrapped \"On ... wrote:\" line.\n\tif len(lines) > 3 && strings.HasPrefix(lines[0], \"On \") && !strings.HasSuffix(lines[0], \"wrote:\") && strings.HasSuffix(lines[1], \":\") && nextLineQuoted(1) {\n\t\tresult = \"[...]\\n\"\n\t\tlines = lines[3:]\n\t\tensureLines()\n\t}\n\n\tfor ; len(lines) > 0 && !isSignature(); ensureLines() {\n\t\tline := lines[0]\n\t\tif strings.HasPrefix(line, \">\") {\n\t\t\tif !resultSnipped() {\n\t\t\t\tresult += \"[...]\\n\"\n\t\t\t}\n\t\t\tlines = lines[1:]\n\t\t\tcontinue\n\t\t}\n\t\tif line == \"\" {\n\t\t\tlines = lines[1:]\n\t\t\tcontinue\n\t\t}\n\t\t// Check for a \"On <date>, <person> wrote:\", we require digits before a quoted\n\t\t// line, with an optional empty line in between. If we don't have any text yet, we\n\t\t// don't require the digits.\n\t\tif strings.HasSuffix(line, \":\") && (strings.ContainsAny(line, \"0123456789\") || result == \"\") && nextLineQuoted(0) {\n\t\t\tif !resultSnipped() {\n\t\t\t\tresult += \"[...]\\n\"\n\t\t\t}\n\t\t\tlines = lines[1:]\n\t\t\tcontinue\n\t\t}\n\t\t// Skip possibly duplicate snipping by author.\n\t\tif !isSnipped(line) || !resultSnipped() {\n\t\t\tresult += line + \"\\n\"\n\t\t}\n\t\tlines = lines[1:]\n\t\tif len(result) > 250 {\n\t\t\tbreak\n\t\t}\n\t}\n\tif len(result) > 250 {\n\t\tresult = result[:230] + \"...\"\n\t}\n\treturn result, scanner.Err()\n}\n\nfunc parsedMessage(log mlog.Log, m store.Message, state *msgState, full, msgitem bool) (pm ParsedMessage, rerr error) {\n\tif full || msgitem {\n\t\tif !state.ensurePart(m, true) {\n\t\t\treturn pm, state.err\n\t\t}\n\t\tif full {\n\t\t\tpm.Part = *state.part\n\t\t}\n\t} else {\n\t\tif !state.ensurePart(m, false) {\n\t\t\treturn pm, state.err\n\t\t}\n\t}\n\n\t// todo: we should store this form in message.Part, requires a data structure update.\n\n\tconvertAddrs := func(l []message.Address) []MessageAddress {\n\t\tr := make([]MessageAddress, len(l))\n\t\tfor i, a := range l {\n\t\t\td, err := dns.ParseDomain(a.Host)\n\t\t\tlog.Check(err, \"parsing domain\")\n\t\t\tif err != nil {\n\t\t\t\td = dns.Domain{ASCII: a.Host}\n\t\t\t}\n\t\t\tr[i] = MessageAddress{a.Name, a.User, d}\n\t\t}\n\t\treturn r\n\t}\n\n\tif full || msgitem {\n\t\tenv := MessageEnvelope{}\n\t\tif state.part.Envelope != nil {\n\t\t\te := *state.part.Envelope\n\t\t\tenv.Date = e.Date\n\t\t\tenv.Subject = e.Subject\n\t\t\tenv.InReplyTo = e.InReplyTo\n\t\t\tenv.MessageID = e.MessageID\n\t\t\tenv.From = convertAddrs(e.From)\n\t\t\tenv.Sender = convertAddrs(e.Sender)\n\t\t\tenv.ReplyTo = convertAddrs(e.ReplyTo)\n\t\t\tenv.To = convertAddrs(e.To)\n\t\t\tenv.CC = convertAddrs(e.CC)\n\t\t\tenv.BCC = convertAddrs(e.BCC)\n\t\t}\n\t\tpm.envelope = env\n\t}\n\n\tif full && state.part.BodyOffset > 0 {\n\t\thdrs, err := state.part.Header()\n\t\tif err != nil {\n\t\t\treturn ParsedMessage{}, fmt.Errorf(\"parsing headers: %v\", err)\n\t\t}\n\t\tpm.Headers = hdrs\n\n\t\tpm.ListReplyAddress = parseListPostAddress(hdrs.Get(\"List-Post\"))\n\t} else {\n\t\tpm.Headers = map[string][]string{}\n\t}\n\n\tpm.Texts = []string{}\n\n\t// We track attachments from multipart/mixed differently from other attachments.\n\t// The others are often inline, sometimes just some logo's in HTML alternative\n\t// messages. We want to have our mixed attachments at the start of the list, but\n\t// our descent-first parsing would result in inline messages first in the typical\n\t// message.\n\tvar attachmentsMixed []Attachment\n\tvar attachmentsOther []Attachment\n\n\taddAttachment := func(a Attachment, isMixed bool) {\n\t\tif isMixed {\n\t\t\tattachmentsMixed = append(attachmentsMixed, a)\n\t\t} else {\n\t\t\tattachmentsOther = append(attachmentsOther, a)\n\t\t}\n\t}\n\n\t// todo: how should we handle messages where a user prefers html, and we want to show it, but it's a DSN that also has textual-only parts? e.g. gmail's dsn where the first part is multipart/related with multipart/alternative, and second part is the regular message/delivery-status. we want to display both the html and the text.\n\n\tvar usePart func(p message.Part, index int, parent *message.Part, path []int, parentMixed bool)\n\tusePart = func(p message.Part, index int, parent *message.Part, path []int, parentMixed bool) {\n\t\tmt := p.MediaType + \"/\" + p.MediaSubType\n\t\tnewParentMixed := mt == \"MULTIPART/MIXED\"\n\t\tfor i, sp := range p.Parts {\n\t\t\tif mt == \"MULTIPART/SIGNED\" && i >= 1 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tusePart(sp, i, &p, append(append([]int{}, path...), i), newParentMixed)\n\t\t}\n\t\tswitch mt {\n\t\tcase \"TEXT/PLAIN\", \"/\":\n\t\t\t// Don't include if Content-Disposition attachment.\n\t\t\tif full || msgitem {\n\t\t\t\t// todo: should have this, and perhaps all content-* headers, preparsed in message.Part?\n\t\t\t\th, err := p.Header()\n\t\t\t\tlog.Check(err, \"parsing attachment headers\", slog.Int64(\"msgid\", m.ID))\n\t\t\t\tcp := h.Get(\"Content-Disposition\")\n\t\t\t\tif cp != \"\" {\n\t\t\t\t\tdisp, params, err := mime.ParseMediaType(cp)\n\t\t\t\t\tlog.Check(err, \"parsing content-disposition\", slog.String(\"cp\", cp))\n\t\t\t\t\tif strings.EqualFold(disp, \"attachment\") {\n\t\t\t\t\t\tname := tryDecodeParam(log, p.ContentTypeParams[\"name\"])\n\t\t\t\t\t\tif name == \"\" {\n\t\t\t\t\t\t\tname = tryDecodeParam(log, params[\"filename\"])\n\t\t\t\t\t\t}\n\t\t\t\t\t\taddAttachment(Attachment{path, name, p}, parentMixed)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif full {\n\t\t\t\tbuf, err := io.ReadAll(&moxio.LimitReader{R: p.ReaderUTF8OrBinary(), Limit: 2 * 1024 * 1024})\n\t\t\t\tif err != nil {\n\t\t\t\t\trerr = fmt.Errorf(\"reading text part: %v\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tpm.Texts = append(pm.Texts, string(buf))\n\t\t\t}\n\t\t\tif msgitem && pm.firstLine == \"\" {\n\t\t\t\tpm.firstLine, rerr = formatFirstLine(p.ReaderUTF8OrBinary())\n\t\t\t\tif rerr != nil {\n\t\t\t\t\trerr = fmt.Errorf(\"reading text for first line snippet: %v\", rerr)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"TEXT/HTML\":\n\t\t\tpm.HasHTML = true\n\n\t\tdefault:\n\t\t\t// todo: see if there is a common nesting messages that are both signed and encrypted.\n\t\t\tif parent == nil && mt == \"MULTIPART/SIGNED\" {\n\t\t\t\tpm.isSigned = true\n\t\t\t}\n\t\t\tif parent == nil && mt == \"MULTIPART/ENCRYPTED\" {\n\t\t\t\tpm.isEncrypted = true\n\t\t\t}\n\t\t\t// todo: possibly do not include anything below multipart/alternative that starts with text/html, they may be cids. perhaps have a separate list of attachments for the text vs html version?\n\t\t\tif p.MediaType != \"MULTIPART\" {\n\t\t\t\tvar parentct string\n\t\t\t\tif parent != nil {\n\t\t\t\t\tparentct = parent.MediaType + \"/\" + parent.MediaSubType\n\t\t\t\t}\n\n\t\t\t\t// Recognize DSNs.\n\t\t\t\tif parentct == \"MULTIPART/REPORT\" && index == 1 && (mt == \"MESSAGE/GLOBAL-DELIVERY-STATUS\" || mt == \"MESSAGE/DELIVERY-STATUS\") {\n\t\t\t\t\tif full {\n\t\t\t\t\t\tbuf, err := io.ReadAll(&moxio.LimitReader{R: p.ReaderUTF8OrBinary(), Limit: 1024 * 1024})\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\trerr = fmt.Errorf(\"reading text part: %v\", err)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t\tpm.Texts = append(pm.Texts, string(buf))\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif parentct == \"MULTIPART/REPORT\" && index == 2 && (mt == \"MESSAGE/GLOBAL-HEADERS\" || mt == \"TEXT/RFC822-HEADERS\") {\n\t\t\t\t\tif full {\n\t\t\t\t\t\tbuf, err := io.ReadAll(&moxio.LimitReader{R: p.ReaderUTF8OrBinary(), Limit: 1024 * 1024})\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\trerr = fmt.Errorf(\"reading text part: %v\", err)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t\tpm.Texts = append(pm.Texts, string(buf))\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif parentct == \"MULTIPART/REPORT\" && index == 2 && (mt == \"MESSAGE/GLOBAL\" || mt == \"TEXT/RFC822\") {\n\t\t\t\t\taddAttachment(Attachment{path, \"original.eml\", p}, parentMixed)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tname := tryDecodeParam(log, p.ContentTypeParams[\"name\"])\n\t\t\t\tif name == \"\" && (full || msgitem) {\n\t\t\t\t\t// todo: should have this, and perhaps all content-* headers, preparsed in message.Part?\n\t\t\t\t\th, err := p.Header()\n\t\t\t\t\tlog.Check(err, \"parsing attachment headers\", slog.Int64(\"msgid\", m.ID))\n\t\t\t\t\tcp := h.Get(\"Content-Disposition\")\n\t\t\t\t\tif cp != \"\" {\n\t\t\t\t\t\t_, params, err := mime.ParseMediaType(cp)\n\t\t\t\t\t\tlog.Check(err, \"parsing content-disposition\", slog.String(\"cp\", cp))\n\t\t\t\t\t\tname = tryDecodeParam(log, params[\"filename\"])\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\taddAttachment(Attachment{path, name, p}, parentMixed)\n\t\t\t}\n\t\t}\n\t}\n\tusePart(*state.part, -1, nil, []int{}, false)\n\n\tpm.attachments = []Attachment{}\n\tpm.attachments = append(pm.attachments, attachmentsMixed...)\n\tpm.attachments = append(pm.attachments, attachmentsOther...)\n\n\tif rerr == nil {\n\t\tpm.ID = m.ID\n\t}\n\treturn\n}\n\n// parses List-Post header, returning an address if it could be found, and nil otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func parseListPostAddress(s string) *MessageAddress {\n# Natural Language Annotations: // parses List-Post header, returning an address if it could be found, and nil otherwise.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc parseListPostAddress(s string) *MessageAddress {\n\t// Check if the header string is empty, return nil if so\n\tif s == \"\" {\n\t\treturn nil\n\t}\n\n\t// Check for the presence of a '<' character which indicates the start of an address\n\tstartIdx := strings.Index(s, \"<\")\n\tendIdx := strings.Index(s, \">\")\n\n\t// Only proceed if both '<' and '>' are found and have valid positions\n\tif startIdx != -1 && endIdx != -1 && startIdx < endIdx {\n\t\taddressStr := s[startIdx+1 : endIdx] // Extract the email address portion\n\n\t\t// Parse the address to break it into components\n\t\taddrElements := strings.Split(addressStr, \"@\")\n\t\tif len(addrElements) == 2 {\n\t\t\tuser := addrElements[0]\n\t\t\thost := addrElements[1]\n\n\t\t\t// Attempt to parse the domain part of the address\n\t\t\tdomain, err := dns.ParseDomain(host)\n\t\t\tif err != nil {\n\t\t\t\t// If parsing fails, fallback to using the host as a plain ASCII string\n\t\t\t\tdomain = dns.Domain{ASCII: host}\n\t\t\t}\n\n\t\t\t// Construct and return a MessageAddress instance\n\t\t\treturn &MessageAddress{\n\t\t\t\tName: \"\", // No name part from List-Post\n\t\t\t\tUser: user,\n\t\t\t\tDomain: domain,\n\t\t\t}\n\t\t}\n\t}\n\n\t// Return nil if the address could not be parsed\n\treturn nil\n}\n```"]}, "mox-webmail/webmail.go-sanitizeNode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/webmail/webmail.go:\n```\n// Package webmail implements a webmail client, serving html/js and providing an API for message actions and SSE endpoint for receiving real-time updates.\npackage webmail\n\n// todo: should we be serving the messages/parts on a separate (sub)domain for user-content? to limit damage if the csp rules aren't enough.\n\nimport (\n\t\"archive/zip\"\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"strings\"\n\n\t_ \"embed\"\n\n\t\"golang.org/x/net/html\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\t\"github.com/mjl-/sherpa\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/store\"\n\t\"github.com/mjl-/mox/webauth\"\n\t\"github.com/mjl-/mox/webops\"\n)\n\nvar pkglog = mlog.New(\"webmail\", nil)\n\ntype ctxKey string\n\n// We pass the request to the sherpa handler so the TLS info can be used for\n// the Received header in submitted messages. Most API calls need just the\n// account name.\nvar requestInfoCtxKey ctxKey = \"requestInfo\"\n\ntype requestInfo struct {\n\tLog          mlog.Log\n\tLoginAddress string\n\tAccount      *store.Account // Nil only for methods Login and LoginPrep.\n\tSessionToken store.SessionToken\n\tResponse     http.ResponseWriter\n\tRequest      *http.Request // For Proto and TLS connection state during message submit.\n}\n\n//go:embed webmail.html\nvar webmailHTML []byte\n\n//go:embed webmail.js\nvar webmailJS []byte\n\n//go:embed msg.html\nvar webmailmsgHTML []byte\n\n//go:embed msg.js\nvar webmailmsgJS []byte\n\n//go:embed text.html\nvar webmailtextHTML []byte\n\n//go:embed text.js\nvar webmailtextJS []byte\n\nvar (\n\t// Similar between ../webmail/webmail.go:/metricSubmission and ../smtpserver/server.go:/metricSubmission and ../webapisrv/server.go:/metricSubmission\n\tmetricSubmission = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_webmail_submission_total\",\n\t\t\tHelp: \"Webmail message submission results, known values (those ending with error are server errors): ok, badfrom, messagelimiterror, recipientlimiterror, queueerror, storesenterror.\",\n\t\t},\n\t\t[]string{\n\t\t\t\"result\",\n\t\t},\n\t)\n\tmetricServerErrors = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_webmail_errors_total\",\n\t\t\tHelp: \"Webmail server errors, known values: dkimsign, submit.\",\n\t\t},\n\t\t[]string{\n\t\t\t\"error\",\n\t\t},\n\t)\n\tmetricSSEConnections = promauto.NewGauge(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"mox_webmail_sse_connections\",\n\t\t\tHelp: \"Number of active webmail SSE connections.\",\n\t\t},\n\t)\n)\n\nfunc xcheckf(ctx context.Context, err error, format string, args ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\tmsg := fmt.Sprintf(format, args...)\n\terrmsg := fmt.Sprintf(\"%s: %s\", msg, err)\n\tpkglog.WithContext(ctx).Errorx(msg, err)\n\tcode := \"server:error\"\n\tif errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {\n\t\tcode = \"user:error\"\n\t}\n\tpanic(&sherpa.Error{Code: code, Message: errmsg})\n}\n\nfunc xcheckuserf(ctx context.Context, err error, format string, args ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\tmsg := fmt.Sprintf(format, args...)\n\terrmsg := fmt.Sprintf(\"%s: %s\", msg, err)\n\tpkglog.WithContext(ctx).Errorx(msg, err)\n\tpanic(&sherpa.Error{Code: \"user:error\", Message: errmsg})\n}\n\nfunc xdbwrite(ctx context.Context, acc *store.Account, fn func(tx *bstore.Tx)) {\n\terr := acc.DB.Write(ctx, func(tx *bstore.Tx) error {\n\t\tfn(tx)\n\t\treturn nil\n\t})\n\txcheckf(ctx, err, \"transaction\")\n}\n\nfunc xdbread(ctx context.Context, acc *store.Account, fn func(tx *bstore.Tx)) {\n\terr := acc.DB.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfn(tx)\n\t\treturn nil\n\t})\n\txcheckf(ctx, err, \"transaction\")\n}\n\nvar webmailFile = &mox.WebappFile{\n\tHTML:     webmailHTML,\n\tJS:       webmailJS,\n\tHTMLPath: filepath.FromSlash(\"webmail/webmail.html\"),\n\tJSPath:   filepath.FromSlash(\"webmail/webmail.js\"),\n}\n\n// Serve content, either from a file, or return the fallback data. Caller\n// should already have set the content-type. We use this to return a file from\n// the local file system (during development), or embedded in the binary (when\n// deployed).\nfunc serveContentFallback(log mlog.Log, w http.ResponseWriter, r *http.Request, path string, fallback []byte) {\n\tf, err := os.Open(path)\n\tif err == nil {\n\t\tdefer f.Close()\n\t\tst, err := f.Stat()\n\t\tif err == nil {\n\t\t\thttp.ServeContent(w, r, \"\", st.ModTime(), f)\n\t\t\treturn\n\t\t}\n\t}\n\thttp.ServeContent(w, r, \"\", mox.FallbackMtime(log), bytes.NewReader(fallback))\n}\n\nfunc init() {\n\tmox.NewWebmailHandler = func(maxMsgSize int64, basePath string, isForwarded bool, accountPath string) http.Handler {\n\t\treturn http.HandlerFunc(Handler(maxMsgSize, basePath, isForwarded, accountPath))\n\t}\n}\n\n// Handler returns a handler for the webmail endpoints, customized for the max\n// message size coming from the listener and cookiePath.\nfunc Handler(maxMessageSize int64, cookiePath string, isForwarded bool, accountPath string) func(w http.ResponseWriter, r *http.Request) {\n\tsh, err := makeSherpaHandler(maxMessageSize, cookiePath, isForwarded)\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\tif err != nil {\n\t\t\thttp.Error(w, \"500 - internal server error - cannot handle requests\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\thandle(sh, isForwarded, accountPath, w, r)\n\t}\n}\n\nfunc handle(apiHandler http.Handler, isForwarded bool, accountPath string, w http.ResponseWriter, r *http.Request) {\n\tctx := r.Context()\n\tlog := pkglog.WithContext(ctx).With(slog.String(\"userauth\", \"\"))\n\n\t// Server-sent event connection, for all initial data (list of mailboxes), list of\n\t// messages, and all events afterwards. Authenticated through a token in the query\n\t// string, which it got from a Token API call.\n\tif r.URL.Path == \"/events\" {\n\t\tserveEvents(ctx, log, accountPath, w, r)\n\t\treturn\n\t}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\terr, ok := x.(*sherpa.Error)\n\t\tif !ok {\n\t\t\tlog.WithContext(ctx).Error(\"handle panic\", slog.Any(\"err\", x))\n\t\t\tdebug.PrintStack()\n\t\t\tmetrics.PanicInc(metrics.Webmailhandle)\n\t\t\tpanic(x)\n\t\t}\n\t\tif strings.HasPrefix(err.Code, \"user:\") {\n\t\t\tlog.Debugx(\"webmail user error\", err)\n\t\t\thttp.Error(w, \"400 - bad request - \"+err.Message, http.StatusBadRequest)\n\t\t} else {\n\t\t\tlog.Errorx(\"webmail server error\", err)\n\t\t\thttp.Error(w, \"500 - internal server error - \"+err.Message, http.StatusInternalServerError)\n\t\t}\n\t}()\n\n\tswitch r.URL.Path {\n\tcase \"/\":\n\t\tswitch r.Method {\n\t\tcase \"GET\", \"HEAD\":\n\t\t\th := w.Header()\n\t\t\th.Set(\"X-Frame-Options\", \"deny\")\n\t\t\th.Set(\"Referrer-Policy\", \"same-origin\")\n\t\t\twebmailFile.Serve(ctx, log, w, r)\n\t\tdefault:\n\t\t\thttp.Error(w, \"405 - method not allowed - use get\", http.StatusMethodNotAllowed)\n\t\t}\n\t\treturn\n\n\tcase \"/msg.js\", \"/text.js\":\n\t\tswitch r.Method {\n\t\tdefault:\n\t\t\thttp.Error(w, \"405 - method not allowed - use get\", http.StatusMethodNotAllowed)\n\t\t\treturn\n\t\tcase \"GET\", \"HEAD\":\n\t\t}\n\n\t\tpath := filepath.Join(\"webmail\", r.URL.Path[1:])\n\t\tvar fallback = webmailmsgJS\n\t\tif r.URL.Path == \"/text.js\" {\n\t\t\tfallback = webmailtextJS\n\t\t}\n\n\t\tw.Header().Set(\"Content-Type\", \"application/javascript; charset=utf-8\")\n\t\tserveContentFallback(log, w, r, path, fallback)\n\t\treturn\n\t}\n\n\tisAPI := strings.HasPrefix(r.URL.Path, \"/api/\")\n\t// Only allow POST for calls, they will not work cross-domain without CORS.\n\tif isAPI && r.URL.Path != \"/api/\" && r.Method != \"POST\" {\n\t\thttp.Error(w, \"405 - method not allowed - use post\", http.StatusMethodNotAllowed)\n\t\treturn\n\t}\n\n\tvar loginAddress, accName string\n\tvar sessionToken store.SessionToken\n\t// All other URLs, except the login endpoint require some authentication.\n\tif r.URL.Path != \"/api/LoginPrep\" && r.URL.Path != \"/api/Login\" {\n\t\tvar ok bool\n\t\tisExport := r.URL.Path == \"/export\"\n\t\trequireCSRF := isAPI || isExport\n\t\taccName, sessionToken, loginAddress, ok = webauth.Check(ctx, log, webauth.Accounts, \"webmail\", isForwarded, w, r, isAPI, requireCSRF, isExport)\n\t\tif !ok {\n\t\t\t// Response has been written already.\n\t\t\treturn\n\t\t}\n\t}\n\n\tif isAPI {\n\t\tvar acc *store.Account\n\t\tif accName != \"\" {\n\t\t\tlog = log.With(slog.String(\"account\", accName))\n\t\t\tvar err error\n\t\t\tacc, err = store.OpenAccount(log, accName)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorx(\"open account\", err)\n\t\t\t\thttp.Error(w, \"500 - internal server error - error opening account\", http.StatusInternalServerError)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\terr := acc.Close()\n\t\t\t\tlog.Check(err, \"closing account\")\n\t\t\t}()\n\t\t}\n\t\treqInfo := requestInfo{log, loginAddress, acc, sessionToken, w, r}\n\t\tctx = context.WithValue(ctx, requestInfoCtxKey, reqInfo)\n\t\tapiHandler.ServeHTTP(w, r.WithContext(ctx))\n\t\treturn\n\t}\n\n\t// We are now expecting the following URLs:\n\t// .../export\n\t// .../msg/<msgid>/{attachments.zip,parsedmessage.js,raw}\n\t// .../msg/<msgid>/{,msg}{text,html,htmlexternal}\n\t// .../msg/<msgid>/{view,viewtext,download}/<partid>\n\n\tif r.URL.Path == \"/export\" {\n\t\twebops.Export(log, accName, w, r)\n\t\treturn\n\t}\n\n\tif !strings.HasPrefix(r.URL.Path, \"/msg/\") {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\n\tt := strings.Split(r.URL.Path[len(\"/msg/\"):], \"/\")\n\tif len(t) < 2 {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\n\tid, err := strconv.ParseInt(t[0], 10, 64)\n\tif err != nil || id == 0 {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\n\t// Many of the requests need either a message or a parsed part. Make it easy to\n\t// fetch/prepare and cleanup. We only do all the work when the request seems legit\n\t// (valid HTTP route and method).\n\txprepare := func() (acc *store.Account, m store.Message, msgr *store.MsgReader, p message.Part, cleanup func(), ok bool) {\n\t\tif r.Method != \"GET\" {\n\t\t\thttp.Error(w, \"405 - method not allowed - post required\", http.StatusMethodNotAllowed)\n\t\t\treturn\n\t\t}\n\n\t\tdefer func() {\n\t\t\tif ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif msgr != nil {\n\t\t\t\terr := msgr.Close()\n\t\t\t\tlog.Check(err, \"closing message reader\")\n\t\t\t\tmsgr = nil\n\t\t\t}\n\t\t\tif acc != nil {\n\t\t\t\terr := acc.Close()\n\t\t\t\tlog.Check(err, \"closing account\")\n\t\t\t\tacc = nil\n\t\t\t}\n\t\t}()\n\n\t\tvar err error\n\n\t\tacc, err = store.OpenAccount(log, accName)\n\t\txcheckf(ctx, err, \"open account\")\n\n\t\tm = store.Message{ID: id}\n\t\terr = acc.DB.Get(ctx, &m)\n\t\tif err == bstore.ErrAbsent || err == nil && m.Expunged {\n\t\t\thttp.NotFound(w, r)\n\t\t\treturn\n\t\t}\n\t\txcheckf(ctx, err, \"get message\")\n\n\t\tmsgr = acc.MessageReader(m)\n\n\t\tp, err = m.LoadPart(msgr)\n\t\txcheckf(ctx, err, \"load parsed message\")\n\n\t\tcleanup = func() {\n\t\t\terr := msgr.Close()\n\t\t\tlog.Check(err, \"closing message reader\")\n\t\t\terr = acc.Close()\n\t\t\tlog.Check(err, \"closing account\")\n\t\t}\n\t\tok = true\n\t\treturn\n\t}\n\n\th := w.Header()\n\n\t// We set a Content-Security-Policy header that is as strict as possible, depending\n\t// on the type of message/part/html/js. We have to be careful because we are\n\t// returning data that is coming in from external places. E.g. HTML could contain\n\t// javascripts that we don't want to execute, especially not on our domain. We load\n\t// resources in an iframe. The CSP policy starts out  with default-src 'none' to\n\t// disallow loading anything, then start allowing what is safe, such as inlined\n\t// datauri images and inline styles. Data can only be loaded when the request is\n\t// coming from the same origin (so other sites cannot include resources\n\t// (messages/parts)).\n\t//\n\t// We want to load resources in sandbox-mode, causing the page to be loaded as from\n\t// a different origin. If sameOrigin is set, we have a looser CSP policy:\n\t// allow-same-origin is set so resources are loaded as coming from this same\n\t// origin. This is needed for the msg* endpoints that render a message, where we\n\t// load the message body in a separate iframe again (with stricter CSP again),\n\t// which we need to access for its inner height. If allowSelfScript is also set\n\t// (for \"msgtext\"), the CSP leaves out the sandbox entirely.\n\t//\n\t// If allowExternal is set, we allow loading image, media (audio/video), styles and\n\t// fronts from external URLs as well as inline URI's. By default we don't allow any\n\t// loading of content, except inlined images (we do that ourselves for images\n\t// embedded in the email), and we allow inline styles (which are safely constrained\n\t// to an iframe).\n\t//\n\t// If allowSelfScript is set, inline scripts and scripts from our origin are\n\t// allowed. Used to display a message including header. The header is rendered with\n\t// javascript, the content is rendered in a separate iframe with a CSP that doesn't\n\t// have allowSelfScript.\n\theaders := func(sameOrigin, allowExternal, allowSelfScript, allowSelfImg bool) {\n\t\t// allow-popups is needed to make opening links in new tabs work.\n\t\tsb := \"sandbox allow-popups allow-popups-to-escape-sandbox; \"\n\t\tif sameOrigin && allowSelfScript {\n\t\t\t// Sandbox with both allow-same-origin and allow-script would not provide security,\n\t\t\t// and would give warning in console about that.\n\t\t\tsb = \"\"\n\t\t} else if sameOrigin {\n\t\t\tsb = \"sandbox allow-popups allow-popups-to-escape-sandbox allow-same-origin; \"\n\t\t}\n\t\tscript := \"\"\n\t\tif allowSelfScript {\n\t\t\tscript = \"; script-src 'unsafe-inline' 'self'; frame-src 'self'; connect-src 'self'\"\n\t\t}\n\t\tvar csp string\n\t\tif allowExternal {\n\t\t\tcsp = sb + \"frame-ancestors 'self'; default-src 'none'; img-src data: http: https: 'unsafe-inline'; style-src 'unsafe-inline' data: http: https:; font-src data: http: https: 'unsafe-inline'; media-src 'unsafe-inline' data: http: https:\" + script\n\t\t} else if allowSelfImg {\n\t\t\tcsp = sb + \"frame-ancestors 'self'; default-src 'none'; img-src data: 'self'; style-src 'unsafe-inline'\" + script\n\t\t} else {\n\t\t\tcsp = sb + \"frame-ancestors 'self'; default-src 'none'; img-src data:; style-src 'unsafe-inline'\" + script\n\t\t}\n\t\th.Set(\"Content-Security-Policy\", csp)\n\t\th.Set(\"X-Frame-Options\", \"sameorigin\") // Duplicate with CSP, but better too much than too little.\n\t\th.Set(\"X-Content-Type-Options\", \"nosniff\")\n\t\th.Set(\"Referrer-Policy\", \"no-referrer\")\n\t}\n\n\tswitch {\n\tcase len(t) == 2 && t[1] == \"attachments.zip\":\n\t\tacc, m, msgr, p, cleanup, ok := xprepare()\n\t\tif !ok {\n\t\t\treturn\n\t\t}\n\t\tdefer cleanup()\n\t\tstate := msgState{acc: acc, m: m, msgr: msgr, part: &p}\n\t\t// note: state is cleared by cleanup\n\n\t\tmi, err := messageItem(log, m, &state)\n\t\txcheckf(ctx, err, \"parsing message\")\n\n\t\theaders(false, false, false, false)\n\t\th.Set(\"Content-Type\", \"application/zip\")\n\t\th.Set(\"Cache-Control\", \"no-store, max-age=0\")\n\t\tvar subjectSlug string\n\t\tif p.Envelope != nil {\n\t\t\ts := p.Envelope.Subject\n\t\t\ts = strings.ToLower(s)\n\t\t\ts = regexp.MustCompile(\"[^a-z0-9_.-]\").ReplaceAllString(s, \"-\")\n\t\t\ts = regexp.MustCompile(\"--*\").ReplaceAllString(s, \"-\")\n\t\t\ts = strings.TrimLeft(s, \"-\")\n\t\t\ts = strings.TrimRight(s, \"-\")\n\t\t\tif s != \"\" {\n\t\t\t\ts = \"-\" + s\n\t\t\t}\n\t\t\tsubjectSlug = s\n\t\t}\n\t\tfilename := fmt.Sprintf(\"email-%d-attachments-%s%s.zip\", m.ID, m.Received.Format(\"20060102-150405\"), subjectSlug)\n\t\tcd := mime.FormatMediaType(\"attachment\", map[string]string{\"filename\": filename})\n\t\th.Set(\"Content-Disposition\", cd)\n\n\t\tzw := zip.NewWriter(w)\n\t\tnames := map[string]bool{}\n\t\tfor _, a := range mi.Attachments {\n\t\t\tap := a.Part\n\t\t\tname := tryDecodeParam(log, ap.ContentTypeParams[\"name\"])\n\t\t\tif name == \"\" {\n\t\t\t\t// We don't check errors, this is all best-effort.\n\t\t\t\th, _ := ap.Header()\n\t\t\t\tdisposition := h.Get(\"Content-Disposition\")\n\t\t\t\t_, params, _ := mime.ParseMediaType(disposition)\n\t\t\t\tname = tryDecodeParam(log, params[\"filename\"])\n\t\t\t}\n\t\t\tif name != \"\" {\n\t\t\t\tname = filepath.Base(name)\n\t\t\t}\n\t\t\tmt := strings.ToLower(ap.MediaType + \"/\" + ap.MediaSubType)\n\t\t\tif name == \"\" || names[name] {\n\t\t\t\text := filepath.Ext(name)\n\t\t\t\tif ext == \"\" {\n\t\t\t\t\t// Handle just a few basic types.\n\t\t\t\t\textensions := map[string]string{\n\t\t\t\t\t\t\"text/plain\":      \".txt\",\n\t\t\t\t\t\t\"text/html\":       \".html\",\n\t\t\t\t\t\t\"image/jpeg\":      \".jpg\",\n\t\t\t\t\t\t\"image/png\":       \".png\",\n\t\t\t\t\t\t\"image/gif\":       \".gif\",\n\t\t\t\t\t\t\"application/zip\": \".zip\",\n\t\t\t\t\t}\n\t\t\t\t\text = extensions[mt]\n\t\t\t\t\tif ext == \"\" {\n\t\t\t\t\t\text = \".bin\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tvar stem string\n\t\t\t\tif name != \"\" && strings.HasSuffix(name, ext) {\n\t\t\t\t\tstem = strings.TrimSuffix(name, ext)\n\t\t\t\t} else {\n\t\t\t\t\tstem = \"attachment\"\n\t\t\t\t\tfor _, index := range a.Path {\n\t\t\t\t\t\tstem += fmt.Sprintf(\"-%d\", index)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tname = stem + ext\n\t\t\t\tseq := 0\n\t\t\t\tfor names[name] {\n\t\t\t\t\tseq++\n\t\t\t\t\tname = stem + fmt.Sprintf(\"-%d\", seq) + ext\n\t\t\t\t}\n\t\t\t}\n\t\t\tnames[name] = true\n\n\t\t\tfh := zip.FileHeader{\n\t\t\t\tName:     name,\n\t\t\t\tModified: m.Received,\n\t\t\t}\n\t\t\tnodeflate := map[string]bool{\n\t\t\t\t\"application/x-bzip2\":          true,\n\t\t\t\t\"application/zip\":              true,\n\t\t\t\t\"application/x-zip-compressed\": true,\n\t\t\t\t\"application/gzip\":             true,\n\t\t\t\t\"application/x-gzip\":           true,\n\t\t\t\t\"application/vnd.rar\":          true,\n\t\t\t\t\"application/x-rar-compressed\": true,\n\t\t\t\t\"application/x-7z-compressed\":  true,\n\t\t\t}\n\t\t\t// Sniff content-type as well for compressed data.\n\t\t\tbuf := make([]byte, 512)\n\t\t\tn, _ := io.ReadFull(ap.Reader(), buf)\n\t\t\tvar sniffmt string\n\t\t\tif n > 0 {\n\t\t\t\tsniffmt = strings.ToLower(http.DetectContentType(buf[:n]))\n\t\t\t}\n\t\t\tdeflate := ap.MediaType != \"VIDEO\" && ap.MediaType != \"AUDIO\" && (ap.MediaType != \"IMAGE\" || ap.MediaSubType == \"BMP\") && !nodeflate[mt] && !nodeflate[sniffmt]\n\t\t\tif deflate {\n\t\t\t\tfh.Method = zip.Deflate\n\t\t\t}\n\t\t\t// We cannot return errors anymore: we have already sent an application/zip header.\n\t\t\tif zf, err := zw.CreateHeader(&fh); err != nil {\n\t\t\t\tlog.Check(err, \"adding to zip file\")\n\t\t\t\treturn\n\t\t\t} else if _, err := io.Copy(zf, ap.Reader()); err != nil {\n\t\t\t\tlog.Check(err, \"writing to zip file\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\terr = zw.Close()\n\t\tlog.Check(err, \"final write to zip file\")\n\n\t// Raw display of a message, as text/plain.\n\tcase len(t) == 2 && t[1] == \"raw\":\n\t\t_, _, msgr, p, cleanup, ok := xprepare()\n\t\tif !ok {\n\t\t\treturn\n\t\t}\n\t\tdefer cleanup()\n\n\t\t// We intentially use text/plain. We certainly don't want to return a format that\n\t\t// browsers or users would think of executing. We do set the charset if available\n\n```\n# Function Signature: func sanitizeNode(node *html.Node) {\n# Natural Language Annotations: // sanitizeNode removes script elements, on* attributes, javascript: href\n// attributes, adds target=\"_blank\" to all links and to a base tag.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc sanitizeNode(node *html.Node) {\n\t// Define a recursive function to walk through each HTML node\n\tvar sanitize func(*html.Node)\n\tsanitize = func(n *html.Node) {\n\t\t// If the current node is an element node, perform modifications\n\t\tif n.Type == html.ElementNode {\n\t\t\t// Remove script elements\n\t\t\tif n.Data == \"script\" {\n\t\t\t\tn.Parent.RemoveChild(n)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Iterate through attributes\n\t\t\tvar attrs []html.Attribute\n\t\t\tfor _, attr := range n.Attr {\n\t\t\t\t// Remove on* attributes (e.g., onclick, onmouseover)\n\t\t\t\tif strings.HasPrefix(attr.Key, \"on\") {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// Remove javascript: href attributes\n\t\t\t\tif attr.Key == \"href\" && strings.HasPrefix(strings.ToLower(attr.Val), \"javascript:\") {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tattrs = append(attrs, attr)\n\t\t\t}\n\t\t\tn.Attr = attrs\n\n\t\t\t// Add target=\"_blank\" to all links and to a base tag\n\t\t\tif n.Data == \"a\" || n.Data == \"base\" {\n\t\t\t\thasTarget := false\n\t\t\t\tfor _, attr := range n.Attr {\n\t\t\t\t\tif attr.Key == \"target\" {\n\t\t\t\t\t\thasTarget = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !hasTarget {\n\t\t\t\t\tn.Attr = append(n.Attr, html.Attribute{Key: \"target\", Val: \"_blank\"})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Recursively sanitize child nodes\n\t\tfor c := n.FirstChild; c != nil; c = c.NextSibling {\n\t\t\tsanitize(c)\n\t\t}\n\t}\n\n\t// Start sanitizing from the provided node\n\tsanitize(node)\n}\n```"]}, "mox-dnsbl/dnsbl.go-Lookup": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dnsbl/dnsbl.go:\n```\n// Package dnsbl implements DNS block lists (RFC 5782), for checking incoming messages from sources without reputation.\n//\n// A DNS block list contains IP addresses that should be blocked. The DNSBL is\n// queried using DNS \"A\" lookups. The DNSBL starts at a \"zone\", e.g.\n// \"dnsbl.example\". To look up whether an IP address is listed, a DNS name is\n// composed: For 10.11.12.13, that name would be \"13.12.11.10.dnsbl.example\". If\n// the lookup returns \"record does not exist\", the IP is not listed. If an IP\n// address is returned, the IP is listed. If an IP is listed, an additional TXT\n// lookup is done for more information about the block. IPv6 addresses are also\n// looked up with an DNS \"A\" lookup of a name similar to an IPv4 address, but with\n// 4-bit hexadecimal dot-separated characters, in reverse.\n//\n// The health of a DNSBL \"zone\" can be check through a lookup of 127.0.0.1\n// (must not be present) and 127.0.0.2 (must be present).\npackage dnsbl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/stub\"\n)\n\nvar (\n\tMetricLookup stub.HistogramVec = stub.HistogramVecIgnore{}\n)\n\nvar ErrDNS = errors.New(\"dnsbl: dns error\") // Temporary error.\n\n// Status is the result of a DNSBL lookup.\ntype Status string\n\nvar (\n\tStatusTemperr Status = \"temperror\" // Temporary failure.\n\tStatusPass    Status = \"pass\"      // Not present in block list.\n\tStatusFail    Status = \"fail\"      // Present in block list.\n)\n\n// Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n// Users of a DNSBL should periodically check if the DNSBL is still operating\n// properly.\n// For temporary errors, ErrDNS is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n# Natural Language Annotations: // Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n\t// Validate the IP is not nil\n\tif ip == nil {\n\t\treturn StatusTemperr, \"\", errors.New(\"invalid IP address\")\n\t}\n\n\t// Reverse the IP address for the DNSBL lookup\n\tvar lookupName string\n\tif ip.To4() != nil {\n\t\t// Handle IPv4 address\n\t\tlookupName = fmt.Sprintf(\"%d.%d.%d.%d.%s\",\n\t\t\tip[15], ip[14], ip[13], ip[12], zone)\n\t} else {\n\t\t// Handle IPv6 address by converting it to 4-bit nibbles\n\t\tfor i := len(ip) - 1; i >= 0; i-- {\n\t\t\tlookupName += fmt.Sprintf(\"%x.%x.\", ip[i]&0xf, ip[i]>>4)\n\t\t}\n\t\tlookupName += string(zone)\n\t}\n\n\t// Perform DNS A record lookup\n\taddrs, err := resolver.LookupIPAddr(ctx, lookupName)\n\tif err != nil {\n\t\telog.ErrorCtx(ctx, \"DNS lookup failed\", \"lookupName\", lookupName, \"error\", err)\n\t\treturn StatusTemperr, \"\", ErrDNS\n\t}\n\n\t// Determine the status based on the DNS lookup result\n\tif len(addrs) == 0 {\n\t\t// No record found, IP not listed in DNSBL\n\t\treturn StatusPass, \"\", nil\n\t}\n\n\t// IP is listed, perform additional TXT lookup for explanation\n\ttxtRecords, err := resolver.LookupTXT(ctx, lookupName)\n\tif err != nil {\n\t\telog.WarnCtx(ctx, \"TXT lookup failed\", \"lookupName\", lookupName, \"error\", err)\n\t\treturn StatusFail, \"Listed with no additional explanation\", nil\n\t}\n\n\t// Join TXT records for explanation\n\trexplanation = strings.Join(txtRecords, \"; \")\n\treturn StatusFail, rexplanation, nil\n}\n```"]}, "mox-dnsbl/dnsbl.go-CheckHealth": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dnsbl/dnsbl.go:\n```\n// Package dnsbl implements DNS block lists (RFC 5782), for checking incoming messages from sources without reputation.\n//\n// A DNS block list contains IP addresses that should be blocked. The DNSBL is\n// queried using DNS \"A\" lookups. The DNSBL starts at a \"zone\", e.g.\n// \"dnsbl.example\". To look up whether an IP address is listed, a DNS name is\n// composed: For 10.11.12.13, that name would be \"13.12.11.10.dnsbl.example\". If\n// the lookup returns \"record does not exist\", the IP is not listed. If an IP\n// address is returned, the IP is listed. If an IP is listed, an additional TXT\n// lookup is done for more information about the block. IPv6 addresses are also\n// looked up with an DNS \"A\" lookup of a name similar to an IPv4 address, but with\n// 4-bit hexadecimal dot-separated characters, in reverse.\n//\n// The health of a DNSBL \"zone\" can be check through a lookup of 127.0.0.1\n// (must not be present) and 127.0.0.2 (must be present).\npackage dnsbl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/stub\"\n)\n\nvar (\n\tMetricLookup stub.HistogramVec = stub.HistogramVecIgnore{}\n)\n\nvar ErrDNS = errors.New(\"dnsbl: dns error\") // Temporary error.\n\n// Status is the result of a DNSBL lookup.\ntype Status string\n\nvar (\n\tStatusTemperr Status = \"temperror\" // Temporary failure.\n\tStatusPass    Status = \"pass\"      // Not present in block list.\n\tStatusFail    Status = \"fail\"      // Present in block list.\n)\n\n// Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n// Users of a DNSBL should periodically check if the DNSBL is still operating\n// properly.\n// For temporary errors, ErrDNS is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n# Natural Language Annotations: // CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n// Users of a DNSBL should periodically check if the DNSBL is still operating\n// properly.\n// For temporary errors, ErrDNS is returned.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n\t// Define the IP addresses used to check the health of the DNSBL zone.\n\tconst (\n\t\tpresentIP   = \"127.0.0.2\"\n\t\tabsentIP    = \"127.0.0.1\"\n\t)\n\n\t// Helper function to perform DNS A record lookup and log the outcome.\n\tperformLookup := func(ip string, shouldExist bool) error {\n\t\t// Construct the DNSBL domain name using the IP address and zone.\n\t\tdnsblName := fmt.Sprintf(\"%s.%s\", ip, zone)\n\n\t\t// Perform the DNS A record lookup\n\t\taddresses, err := resolver.LookupIPAddr(ctx, dnsblName)\n\t\tif err != nil {\n\t\t\t// Log the error if any occurred during the DNS query\n\t\t\telog.Error(fmt.Sprintf(\"error looking up %s: %v\", dnsblName, err))\n\t\t\treturn ErrDNS\n\t\t}\n\n\t\tif shouldExist != (len(addresses) > 0) {\n\t\t\t// Log the unexpected outcome of the A record lookup\n\t\t\telog.Error(fmt.Sprintf(\"unexpected result for %s: expected present=%v, got addresses=%v\", dnsblName, shouldExist, addresses))\n\t\t\tif shouldExist {\n\t\t\t\treturn fmt.Errorf(\"expected %s to be present in DNSBL\", dnsblName)\n\t\t\t}\n\t\t\treturn fmt.Errorf(\"expected %s to be absent from DNSBL\", dnsblName)\n\t\t}\n\n\t\t// Log the success of the DNSBL health check lookup.\n\t\telog.Info(fmt.Sprintf(\"check for %s succeeded: present=%v\", dnsblName, shouldExist))\n\t\treturn nil\n\t}\n\n\t// Check that 127.0.0.2 is present in the DNSBL zone\n\tif err := performLookup(presentIP, true); err != nil {\n\t\treturn err\n\t}\n\n\t// Check that 127.0.0.1 is absent from the DNSBL zone\n\tif err := performLookup(absentIP, false); err != nil {\n\t\treturn err\n\t}\n\n\t// If both checks pass without errors, the DNSBL zone is considered healthy.\n\treturn nil\n}\n```\n"]}, "mox-dmarcrpt/parse.go-ParseMessageReport": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcrpt/parse.go:\n```\n// Package dmarcrpt parses DMARC aggregate feedback reports.\npackage dmarcrpt\n\nimport (\n\t\"archive/zip\"\n\t\"bytes\"\n\t\"compress/gzip\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxio\"\n)\n\nvar ErrNoReport = errors.New(\"no dmarc aggregate report found in message\")\n\n// ParseReport parses an XML aggregate feedback report.\n// The maximum report size is 20MB.\nfunc ParseReport(r io.Reader) (*Feedback, error) {\n\tr = &moxio.LimitReader{R: r, Limit: 20 * 1024 * 1024}\n\tvar feedback Feedback\n\td := xml.NewDecoder(r)\n\tif err := d.Decode(&feedback); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &feedback, nil\n}\n\n// ParseMessageReport parses an aggregate feedback report from a mail message. The\n// maximum message size is 15MB, the maximum report size after decompression is\n// 20MB.\n\n\n\n\n\n\n\n\n\n\n\nfunc parseMessageReport(log mlog.Log, p message.Part) (*Feedback, error) {\n\t// Pretty much any mime structure is allowed. ../rfc/7489:1861\n\t// In practice, some parties will send the report as the only (non-multipart)\n\t// content of the message.\n\n\tif p.MediaType != \"MULTIPART\" {\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treport, err := parseMessageReport(log, *sp)\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(p message.Part) (*Feedback, error) {\n\tct := strings.ToLower(p.MediaType + \"/\" + p.MediaSubType)\n\tr := p.Reader()\n\n\t// If no (useful) content-type is set, try to detect it.\n\tif ct == \"\" || ct == \"application/octet-stream\" {\n\t\tdata := make([]byte, 512)\n\t\tn, err := io.ReadFull(r, data)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t} else if err != nil && err != io.ErrUnexpectedEOF {\n\t\t\treturn nil, fmt.Errorf(\"reading application/octet-stream for content-type detection: %v\", err)\n\t\t}\n\t\tdata = data[:n]\n\t\tct = http.DetectContentType(data)\n\t\tr = io.MultiReader(bytes.NewReader(data), r)\n\t}\n\n\tswitch ct {\n\tcase \"application/zip\":\n\t\t// Google sends messages with direct application/zip content-type.\n\t\treturn parseZip(r)\n\tcase \"application/gzip\", \"application/x-gzip\":\n\t\tgzr, err := gzip.NewReader(r)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip xml report: %s\", err)\n\t\t}\n\t\treturn ParseReport(gzr)\n\tcase \"text/xml\", \"application/xml\":\n\t\treturn ParseReport(r)\n\t}\n\treturn nil, ErrNoReport\n}\n\nfunc parseZip(r io.Reader) (*Feedback, error) {\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading feedback: %s\", err)\n\t}\n\tzr, err := zip.NewReader(bytes.NewReader(buf), int64(len(buf)))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing zip file: %s\", err)\n\t}\n\tif len(zr.File) != 1 {\n\t\treturn nil, fmt.Errorf(\"zip contains %d files, expected 1\", len(zr.File))\n\t}\n\tf, err := zr.File[0].Open()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"opening file in zip: %s\", err)\n\t}\n\tdefer f.Close()\n\treturn ParseReport(f)\n}\n\n```\n# Function Signature: func ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n# Natural Language Annotations: // ParseMessageReport parses an aggregate feedback report from a mail message. The\n// maximum message size is 15MB, the maximum report size after decompression is\n// 20MB.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n\t// Wrap the input reader with a LimitReader to restrict the maximum read size to 15MB.\n\tlimitedReader := io.NewSectionReader(r, 0, 15*1024*1024)\n\n\t// Parse the message part using the message package.\n\tmsg, err := message.Parse(limitedReader)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to parse message: %w\", err)\n\t}\n\n\t// Use mlog to create a log component for logging purposes.\n\tlog := mlog.New(elog, \"ParseMessageReport\")\n\n\t// Attempt to parse each part of the message to find a valid DMARC report.\n\tfor i, part := range msg.Parts {\n\t\telog.Debug(\"Parsing part\", \"index\", i, \"mediaType\", part.MediaType, \"mediaSubType\", part.MediaSubType)\n\t\tif feedback, err := parseMessageReport(log, part); err == nil {\n\t\t\treturn feedback, nil\n\t\t} else if err != ErrNoReport {\n\t\t\telog.Error(\"Failed to parse message part\", \"error\", err)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Return an error if no valid DMARC report was found in the message parts.\n\treturn nil, ErrNoReport\n}\n```"]}, "mox-mox-/lifecycle.go-Register": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/mox-/lifecycle.go:\n```\npackage mox\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\n// We start up as root, bind to sockets, open private key/cert files and fork and\n// exec as unprivileged user. During startup as root, we gather the fd's for the\n// listen addresses in passedListeners and files in passedFiles, and pass their\n// addresses and paths in environment variables to the new process.\nvar passedListeners = map[string]*os.File{} // Listen address to file descriptor.\nvar passedFiles = map[string][]*os.File{}   // Path to file descriptors.\n\n// RestorePassedFiles reads addresses from $MOX_SOCKETS and paths from $MOX_FILES\n// and prepares an os.File for each file descriptor, which are used by later calls\n// of Listen or opening files.\nfunc RestorePassedFiles() {\n\ts := os.Getenv(\"MOX_SOCKETS\")\n\tif s == \"\" {\n\t\tvar linuxhint string\n\t\tif runtime.GOOS == \"linux\" {\n\t\t\tlinuxhint = \" If you updated from v0.0.1, update the mox.service file to start as root (privileges are dropped): ./mox config printservice >mox.service && sudo systemctl daemon-reload && sudo systemctl restart mox.\"\n\t\t}\n\t\tpkglog.Fatal(\"mox must be started as root, and will drop privileges after binding required sockets (missing environment variable MOX_SOCKETS).\" + linuxhint)\n\t}\n\n\t// 0,1,2 are stdin,stdout,stderr, 3 is the first passed fd (first listeners, then files).\n\tvar o uintptr = 3\n\tfor _, addr := range strings.Split(s, \",\") {\n\t\tpassedListeners[addr] = os.NewFile(o, addr)\n\t\to++\n\t}\n\n\tfiles := os.Getenv(\"MOX_FILES\")\n\tif files == \"\" {\n\t\treturn\n\t}\n\tfor _, path := range strings.Split(files, \",\") {\n\t\tpassedFiles[path] = append(passedFiles[path], os.NewFile(o, path))\n\t\to++\n\t}\n}\n\n// CleanupPassedFiles closes the listening socket file descriptors and files passed\n// in by the parent process. To be called by the unprivileged child after listeners\n// have been recreated (they dup the file descriptor), and by the privileged\n// process after starting its child.\nfunc CleanupPassedFiles() {\n\tfor _, f := range passedListeners {\n\t\terr := f.Close()\n\t\tpkglog.Check(err, \"closing listener socket file descriptor\")\n\t}\n\tfor _, fl := range passedFiles {\n\t\tfor _, f := range fl {\n\t\t\terr := f.Close()\n\t\t\tpkglog.Check(err, \"closing path file descriptor\")\n\t\t}\n\t}\n}\n\n// For privileged file descriptor operations (listen and opening privileged files),\n// perform them immediately, regardless of running as root or other user, in case\n// ForkExecUnprivileged is not used.\nvar FilesImmediate bool\n\n// Listen returns a newly created network listener when starting as root, and\n// otherwise (not root) returns a network listener from a file descriptor that was\n// passed by the parent root process.\nfunc Listen(network, addr string) (net.Listener, error) {\n\tif os.Getuid() != 0 && !FilesImmediate {\n\t\tf, ok := passedListeners[addr]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"no file descriptor for listener %s\", addr)\n\t\t}\n\t\tln, err := net.FileListener(f)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"making network listener from file descriptor for address %s: %v\", addr, err)\n\t\t}\n\t\treturn ln, nil\n\t}\n\n\tif _, ok := passedListeners[addr]; ok {\n\t\treturn nil, fmt.Errorf(\"duplicate listener: %s\", addr)\n\t}\n\n\tln, err := net.Listen(network, addr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// On windows, we cannot duplicate a socket. We don't need to for mox localserve\n\t// with FilesImmediate.\n\tif !FilesImmediate {\n\t\ttcpln, ok := ln.(*net.TCPListener)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"listener not a tcp listener, but %T, for network %s, address %s\", ln, network, addr)\n\t\t}\n\t\tf, err := tcpln.File()\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"dup listener: %v\", err)\n\t\t}\n\t\tpassedListeners[addr] = f\n\t}\n\treturn ln, err\n}\n\n// Open a privileged file, such as a TLS private key. When running as root\n// (during startup), the file is opened and the file descriptor is stored.\n// These file descriptors are passed to the unprivileged process. When in the\n// unprivileged processed, we lookup a passed file descriptor.\n// The same calls should be made in the privileged and unprivileged process.\nfunc OpenPrivileged(path string) (*os.File, error) {\n\tif os.Getuid() != 0 && !FilesImmediate {\n\t\tfl := passedFiles[path]\n\t\tif len(fl) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"no file descriptor for file %s\", path)\n\t\t}\n\t\tf := fl[0]\n\t\tpassedFiles[path] = fl[1:]\n\t\treturn f, nil\n\t}\n\n\tf, err := os.Open(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpassedFiles[path] = append(passedFiles[path], f)\n\n\t// Open again, the caller will be closing this file.\n\treturn os.Open(path)\n}\n\n// Shutdown is canceled when a graceful shutdown is initiated. SMTP, IMAP, periodic\n// processes should check this before starting a new operation. If this context is\n// canaceled, the operation should not be started, and new connections/commands should\n// receive a message that the service is currently not available.\nvar Shutdown context.Context\nvar ShutdownCancel func()\n\n// This context should be used as parent by most operations. It is canceled 1\n// second after graceful shutdown was initiated with the cancelation of the\n// Shutdown context. This should abort active operations.\n//\n// Operations typically have context timeouts, 30s for single i/o like DNS queries,\n// and 1 minute for operations with more back and forth. These are set through a\n// context.WithTimeout based on this context, so those contexts are still canceled\n// when shutting down.\n//\n// HTTP servers don't get graceful shutdown, their connections are just aborted.\n// todo: should shut down http connections as well, and shut down the listener and/or return 503 for new requests.\nvar Context context.Context\nvar ContextCancel func()\n\n// Connections holds all active protocol sockets (smtp, imap). They will be given\n// an immediate read/write deadline shortly after initiating mox shutdown, after\n// which the connections get 1 more second for error handling before actual\n// shutdown.\nvar Connections = &connections{\n\tconns:  map[net.Conn]connKind{},\n\tgauges: map[connKind]prometheus.GaugeFunc{},\n\tactive: map[connKind]int64{},\n}\n\ntype connKind struct {\n\tprotocol string\n\tlistener string\n}\n\ntype connections struct {\n\tsync.Mutex\n\tconns  map[net.Conn]connKind\n\tdones  []chan struct{}\n\tgauges map[connKind]prometheus.GaugeFunc\n\n\tactiveMutex sync.Mutex\n\tactive      map[connKind]int64\n}\n\n// Register adds a connection for receiving an immediate i/o deadline on shutdown.\n// When the connection is closed, Remove must be called to cancel the registration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a connection for shutdown.\nfunc (c *connections) Unregister(nc net.Conn) {\n\tc.Lock()\n\tdefer c.Unlock()\n\tck := c.conns[nc]\n\n\tdefer func() {\n\t\tc.activeMutex.Lock()\n\t\tc.active[ck]--\n\t\tc.activeMutex.Unlock()\n\t}()\n\n\tdelete(c.conns, nc)\n\tif len(c.conns) > 0 {\n\t\treturn\n\t}\n\tfor _, done := range c.dones {\n\t\tdone <- struct{}{}\n\t}\n\tc.dones = nil\n}\n\n// Shutdown sets an immediate i/o deadline on all open registered sockets. Called\n// some time after mox shutdown is initiated.\n// The deadline will cause i/o's to be aborted, which should result in the\n// connection being unregistered.\nfunc (c *connections) Shutdown() {\n\tnow := time.Now()\n\tc.Lock()\n\tdefer c.Unlock()\n\tfor nc := range c.conns {\n\t\tif err := nc.SetDeadline(now); err != nil {\n\t\t\tpkglog.Errorx(\"setting immediate read/write deadline for shutdown\", err)\n\t\t}\n\t}\n}\n\n// Done returns a new channel on which a value is sent when no more sockets are\n// open, which could be immediate.\nfunc (c *connections) Done() chan struct{} {\n\tc.Lock()\n\tdefer c.Unlock()\n\tdone := make(chan struct{}, 1)\n\tif len(c.conns) == 0 {\n\t\tdone <- struct{}{}\n\t\treturn done\n\t}\n\tc.dones = append(c.dones, done)\n\treturn done\n}\n\n```\n# Function Signature: func (c *connections) Register(nc net.Conn, protocol, listener string) {\n# Natural Language Annotations: // Register adds a connection for receiving an immediate i/o deadline on shutdown.\n// When the connection is closed, Remove must be called to cancel the registration.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *connections) Register(nc net.Conn, protocol, listener string) {\n\tc.Lock()\n\tdefer c.Unlock()\n\t\n\t// Register the connection with its associated protocol and listener.\n\tc.conns[nc] = connKind{protocol: protocol, listener: listener}\n\n\t// Update the active connection count for this protocol and listener.\n\tc.activeMutex.Lock()\n\tc.active[connKind{protocol: protocol, listener: listener}]++\n\tc.activeMutex.Unlock()\n}\n```"]}, "mox-smtpclient/dial.go-Dial": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtpclient/dial.go:\n```\npackage smtpclient\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// DialHook can be used during tests to override the regular dialer from being used.\nvar DialHook func(ctx context.Context, dialer Dialer, timeout time.Duration, addr string, laddr net.Addr) (net.Conn, error)\n\nfunc dial(ctx context.Context, dialer Dialer, timeout time.Duration, addr string, laddr net.Addr) (net.Conn, error) {\n\t// todo: see if we can remove this function and DialHook in favor of the Dialer interface.\n\n\tif DialHook != nil {\n\t\treturn DialHook(ctx, dialer, timeout, addr, laddr)\n\t}\n\n\t// If this is a net.Dialer, use its settings and add the timeout and localaddr.\n\t// This is the typical case, but SOCKS5 support can use a different dialer.\n\tif d, ok := dialer.(*net.Dialer); ok {\n\t\tnd := *d\n\t\tnd.Timeout = timeout\n\t\tnd.LocalAddr = laddr\n\t\treturn nd.DialContext(ctx, \"tcp\", addr)\n\t}\n\treturn dialer.DialContext(ctx, \"tcp\", addr)\n}\n\n// Dialer is used to dial mail servers, an interface to facilitate testing.\ntype Dialer interface {\n\tDialContext(ctx context.Context, network, addr string) (c net.Conn, err error)\n}\n\n// Dial connects to host by dialing ips, taking previous attempts in dialedIPs into\n// accounts (for greylisting, blocklisting and ipv4/ipv6).\n//\n// If the previous attempt used IPv4, this attempt will use IPv6 (useful in case\n// one of the IPs is in a DNSBL).\n//\n// The second attempt for an address family we prefer the same IP as earlier, to\n// increase our chances if remote is doing greylisting.\n//\n// Dial updates dialedIPs, callers may want to save it so it can be taken into\n// account for future delivery attempts.\n//\n// The first matching protocol family from localIPs is set for the local side\n// of the TCP connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n# Natural Language Annotations: // Dial connects to host by dialing ips, taking previous attempts in dialedIPs into\n// accounts (for greylisting, blocklisting and ipv4/ipv6).\n//\n// If the previous attempt used IPv4, this attempt will use IPv6 (useful in case\n// one of the IPs is in a DNSBL).\n//\n// The second attempt for an address family we prefer the same IP as earlier, to\n// increase our chances if remote is doing greylisting.\n//\n// Dial updates dialedIPs, callers may want to save it so it can be taken into\n// account for future delivery attempts.\n//\n// The first matching protocol family from localIPs is set for the local side\n// of the TCP connection.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n\t// Helper function to classify IP addresses.\n\tpreferIPv4 := func() bool {\n\t\tif previousIPs, found := dialedIPs[host.Domain]; found {\n\t\t\tfor _, prevIP := range previousIPs {\n\t\t\t\tif prevIP.To4() != nil {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\tvar targetIPs []net.IP\n\tif preferIPv4() {\n\t\t// Prioritize IPv4\n\t\tfor _, ip := range ips {\n\t\t\tif ip.To4() != nil {\n\t\t\t\ttargetIPs = append(targetIPs, ip)\n\t\t\t}\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif ip.To4() == nil {\n\t\t\t\ttargetIPs = append(targetIPs, ip)\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// Prioritize IPv6\n\t\tfor _, ip := range ips {\n\t\t\tif ip.To4() == nil {\n\t\t\t\ttargetIPs = append(targetIPs, ip)\n\t\t\t}\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif ip.To4() != nil {\n\t\t\t\ttargetIPs = append(targetIPs, ip)\n\t\t\t}\n\t\t}\n\t}\n\n\tfor _, ip := range targetIPs {\n\t\taddr := fmt.Sprintf(\"%s:%d\", ip.String(), port)\n\n\t\t// Select local IP\n\t\tvar laddr net.Addr\n\t\tfor _, localIP := range localIPs {\n\t\t\tif (ip.To4() != nil && localIP.To4() != nil) || (ip.To4() == nil && localIP.To4() == nil) {\n\t\t\t\tladdr = &net.TCPAddr{IP: localIP}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\telog.Info(\"Dial attempt\", \"addr\", addr, \"localAddr\", laddr)\n\n\t\tc, err := dial(ctx, dialer, 0, addr, laddr)\n\t\tif err == nil {\n\t\t\tdialedIPs[host.Domain] = append(dialedIPs[host.Domain], ip)\n\t\t\treturn c, ip, nil\n\t\t}\n\n\t\telog.Error(\"Dial failed\", \"addr\", addr, \"error\", err)\n\t\trerr = err\n\t}\n\n\treturn nil, nil, rerr\n}\n```"]}, "mox-smtpclient/gather.go-GatherIPs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtpclient/gather.go:\n```\npackage smtpclient\n\nimport (\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"crypto/sha512\"\n\t\"crypto/x509\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/adns\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\terrCNAMELoop  = errors.New(\"cname loop\")\n\terrCNAMELimit = errors.New(\"too many cname records\")\n\terrDNS        = errors.New(\"dns lookup error\")\n\terrNoMail     = errors.New(\"domain does not accept email as indicated with single dot for mx record\")\n)\n\n// GatherDestinations looks up the hosts to deliver email to a domain (\"next-hop\").\n// If it is an IP address, it is the only destination to try. Otherwise CNAMEs of\n// the domain are followed. Then MX records for the expanded CNAME are looked up.\n// If no MX record is present, the original domain is returned. If an MX record is\n// present but indicates the domain does not accept email, ErrNoMail is returned.\n// If valid MX records were found, the MX target hosts are returned.\n//\n// haveMX indicates if an MX record was found.\n//\n// origNextHopAuthentic indicates if the DNS record for the initial domain name was\n// DNSSEC secure (CNAME, MX).\n//\n// expandedNextHopAuthentic indicates if the DNS records after following CNAMEs were\n// DNSSEC secure.\n//\n// These authentic results are needed for DANE, to determine where to look up TLSA\n// records, and which names to allow in the remote TLS certificate. If MX records\n// were found, both the original and expanded next-hops must be authentic for DANE\n// to be option. For a non-IP with no MX records found, the authentic result can\n// be used to decide which of the names to use as TLSA base domain.\nfunc GatherDestinations(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, origNextHop dns.IPDomain) (haveMX, origNextHopAuthentic, expandedNextHopAuthentic bool, expandedNextHop dns.Domain, hosts []dns.IPDomain, permanent bool, err error) {\n\t// ../rfc/5321:3824\n\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// IP addresses are dialed directly, and don't have TLSA records.\n\tif len(origNextHop.IP) > 0 {\n\t\treturn false, false, false, expandedNextHop, []dns.IPDomain{origNextHop}, false, nil\n\t}\n\n\t// We start out assuming the result is authentic. Updated with each lookup.\n\torigNextHopAuthentic = true\n\texpandedNextHopAuthentic = true\n\n\t// We start out delivering to the recipient domain. We follow CNAMEs.\n\trcptDomain := origNextHop.Domain\n\t// Domain we are actually delivering to, after following CNAME record(s).\n\texpandedNextHop = rcptDomain\n\t// Keep track of CNAMEs we have followed, to detect loops.\n\tdomainsSeen := map[string]bool{}\n\tfor i := 0; ; i++ {\n\t\tif domainsSeen[expandedNextHop.ASCII] {\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s: already saw %s\", errCNAMELoop, rcptDomain, expandedNextHop)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\t\tdomainsSeen[expandedNextHop.ASCII] = true\n\n\t\t// note: The Go resolver returns the requested name if the domain has no CNAME\n\t\t// record but has a host record.\n\t\tif i == 16 {\n\t\t\t// We have a maximum number of CNAME records we follow. There is no hard limit for\n\t\t\t// DNS, and you might think folks wouldn't configure CNAME chains at all, but for\n\t\t\t// (non-mail) domains, CNAME chains of 10 records have been encountered according\n\t\t\t// to the internet.\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s, last resolved domain %s\", errCNAMELimit, rcptDomain, expandedNextHop)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\n\t\t// Do explicit CNAME lookup. Go's LookupMX also resolves CNAMEs, but we want to\n\t\t// know the final name, and we're interested in learning if the first vs later\n\t\t// results were DNSSEC-(in)secure.\n\t\t// ../rfc/5321:3838 ../rfc/3974:197\n\t\tcctx, ccancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer ccancel()\n\t\tcname, cnameResult, err := resolver.LookupCNAME(cctx, expandedNextHop.ASCII+\".\")\n\t\tccancel()\n\t\tif i == 0 {\n\t\t\torigNextHopAuthentic = origNextHopAuthentic && cnameResult.Authentic\n\t\t}\n\t\texpandedNextHopAuthentic = expandedNextHopAuthentic && cnameResult.Authentic\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\terr = fmt.Errorf(\"%w: cname lookup for %s: %v\", errDNS, expandedNextHop, err)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\t\tif err == nil && cname != expandedNextHop.ASCII+\".\" {\n\t\t\td, err := dns.ParseDomain(strings.TrimSuffix(cname, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\t\terr = fmt.Errorf(\"%w: parsing cname domain %s: %v\", errDNS, expandedNextHop, err)\n\t\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t\t}\n\t\t\texpandedNextHop = d\n\t\t\t// Start again with new domain.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Not a CNAME, so lookup MX record.\n\t\tmctx, mcancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer mcancel()\n\t\t// Note: LookupMX can return an error and still return records: Invalid records are\n\t\t// filtered out and an error returned. We must process any records that are valid.\n\t\t// Only if all are unusable will we return an error. ../rfc/5321:3851\n\t\tmxl, mxResult, err := resolver.LookupMX(mctx, expandedNextHop.ASCII+\".\")\n\t\tmcancel()\n\t\tif i == 0 {\n\t\t\torigNextHopAuthentic = origNextHopAuthentic && mxResult.Authentic\n\t\t}\n\t\texpandedNextHopAuthentic = expandedNextHopAuthentic && mxResult.Authentic\n\t\tif err != nil && len(mxl) == 0 {\n\t\t\tif !dns.IsNotFound(err) {\n\t\t\t\terr = fmt.Errorf(\"%w: mx lookup for %s: %v\", errDNS, expandedNextHop, err)\n\t\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t\t}\n\n\t\t\t// No MX record, attempt delivery directly to host. ../rfc/5321:3842\n\t\t\thosts = []dns.IPDomain{{Domain: expandedNextHop}}\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, hosts, false, nil\n\t\t} else if err != nil {\n\t\t\tlog.Infox(\"mx record has some invalid records, keeping only the valid mx records\", err)\n\t\t}\n\n\t\t// ../rfc/7505:122\n\t\tif err == nil && len(mxl) == 1 && mxl[0].Host == \".\" {\n\t\t\t// Note: Depending on MX record TTL, this record may be replaced with a more\n\t\t\t// receptive MX record before our final delivery attempt. But it's clearly the\n\t\t\t// explicit desire not to be bothered with email delivery attempts, so mark failure\n\t\t\t// as permanent.\n\t\t\treturn true, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, true, errNoMail\n\t\t}\n\n\t\t// The Go resolver already sorts by preference, randomizing records of same\n\t\t// preference. ../rfc/5321:3885\n\t\tfor _, mx := range mxl {\n\t\t\t// Parsing lax (unless pedantic mode) for MX targets with underscores as seen in the wild.\n\t\t\thost, err := dns.ParseDomainLax(strings.TrimSuffix(mx.Host, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// note: should not happen because Go resolver already filters these out.\n\t\t\t\terr = fmt.Errorf(\"%w: invalid host name in mx record %q: %v\", errDNS, mx.Host, err)\n\t\t\t\treturn true, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, true, err\n\t\t\t}\n\t\t\thosts = append(hosts, dns.IPDomain{Domain: host})\n\t\t}\n\t\tif len(hosts) > 0 {\n\t\t\terr = nil\n\t\t}\n\t\treturn true, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, hosts, false, err\n\t}\n}\n\n// GatherIPs looks up the IPs to try for connecting to host, with the IPs ordered\n// to take previous attempts into account. For use with DANE, the CNAME-expanded\n// name is returned, and whether the DNS responses were authentic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GatherTLSA looks up TLSA record for either expandedHost or host, and returns\n// records usable for DANE with SMTP, and host names to allow in DANE-TA\n// certificate name verification.\n//\n// If no records are found, this isn't necessarily an error. It can just indicate\n// the domain/host does not opt-in to DANE, and nil records and a nil error are\n// returned.\n//\n// Only usable records are returned. If any record was found, DANE is required and\n// this is indicated with daneRequired. If no usable records remain, the caller\n// must do TLS, but not verify the remote TLS certificate.\n//\n// Returned values are always meaningful, also when an error was returned.\nfunc GatherTLSA(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, host dns.Domain, expandedAuthentic bool, expandedHost dns.Domain) (daneRequired bool, daneRecords []adns.TLSA, tlsaBaseDomain dns.Domain, err error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// ../rfc/7672:912\n\t// This function is only called when the lookup of host was authentic.\n\n\tvar l []adns.TLSA\n\n\ttlsaBaseDomain = host\n\tif host == expandedHost || !expandedAuthentic {\n\t\tl, err = lookupTLSACNAME(ctx, log, resolver, 25, \"tcp\", host)\n\t} else if expandedAuthentic {\n\t\t// ../rfc/7672:934\n\t\ttlsaBaseDomain = expandedHost\n\t\tl, err = lookupTLSACNAME(ctx, log, resolver, 25, \"tcp\", expandedHost)\n\t\tif err == nil && len(l) == 0 {\n\t\t\ttlsaBaseDomain = host\n\t\t\tl, err = lookupTLSACNAME(ctx, log, resolver, 25, \"tcp\", host)\n\t\t}\n\t}\n\tif len(l) == 0 || err != nil {\n\t\tdaneRequired = err != nil\n\t\tlog.Debugx(\"gathering tlsa records failed\", err, slog.Bool(\"danerequired\", daneRequired), slog.Any(\"basedomain\", tlsaBaseDomain))\n\t\treturn daneRequired, nil, tlsaBaseDomain, err\n\t}\n\tdaneRequired = len(l) > 0\n\tl = filterUsableTLSARecords(log, l)\n\tlog.Debug(\"tlsa records exist\",\n\t\tslog.Bool(\"danerequired\", daneRequired),\n\t\tslog.Any(\"records\", l),\n\t\tslog.Any(\"basedomain\", tlsaBaseDomain))\n\treturn daneRequired, l, tlsaBaseDomain, err\n}\n\n// lookupTLSACNAME composes a TLSA domain name to lookup, follows CNAMEs and looks\n// up TLSA records. no TLSA records exist, a nil error is returned as it means\n// the host does not opt-in to DANE.\nfunc lookupTLSACNAME(ctx context.Context, log mlog.Log, resolver dns.Resolver, port int, protocol string, host dns.Domain) (l []adns.TLSA, rerr error) {\n\tname := fmt.Sprintf(\"_%d._%s.%s\", port, protocol, host.ASCII+\".\")\n\tfor i := 0; ; i++ {\n\t\tcname, result, err := resolver.LookupCNAME(ctx, name)\n\t\tif dns.IsNotFound(err) {\n\t\t\tif !result.Authentic {\n\t\t\t\tlog.Debugx(\"cname nxdomain result during tlsa lookup not authentic, not doing dane for host\", err, slog.Any(\"host\", host), slog.String(\"name\", name))\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn nil, fmt.Errorf(\"looking up cname for tlsa candidate base domain: %w\", err)\n\t\t} else if !result.Authentic {\n\t\t\tlog.Debugx(\"cname result during tlsa lookup not authentic, not doing dane for host\", err, slog.Any(\"host\", host), slog.String(\"name\", name))\n\t\t\treturn nil, nil\n\t\t}\n\t\tif i == 10 {\n\t\t\treturn nil, fmt.Errorf(\"looking up cname for tlsa candidate base domain: %w\", errCNAMELimit)\n\t\t}\n\t\tname = strings.TrimSuffix(cname, \".\") + \".\"\n\t}\n\tvar result adns.Result\n\tvar err error\n\tl, result, err = resolver.LookupTLSA(ctx, 0, \"\", name)\n\tif dns.IsNotFound(err) || err == nil && len(l) == 0 {\n\t\tlog.Debugx(\"no tlsa records for host, not doing dane\", err,\n\t\t\tslog.Any(\"host\", host),\n\t\t\tslog.String(\"name\", name),\n\t\t\tslog.Bool(\"authentic\", result.Authentic))\n\t\treturn nil, nil\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"looking up tlsa records for tlsa candidate base domain: %w\", err)\n\t} else if !result.Authentic {\n\t\tlog.Debugx(\"tlsa lookup not authentic, not doing dane for host\", err, slog.Any(\"host\", host), slog.String(\"name\", name))\n\t\treturn nil, nil\n\t}\n\treturn l, nil\n}\n\nfunc filterUsableTLSARecords(log mlog.Log, l []adns.TLSA) []adns.TLSA {\n\t// Gather \"usable\" records. ../rfc/7672:708\n\to := 0\n\tfor _, r := range l {\n\t\t// A record is not usable when we don't recognize parameters. ../rfc/6698:649\n\n\t\tswitch r.Usage {\n\t\tcase adns.TLSAUsageDANETA, adns.TLSAUsageDANEEE:\n\t\tdefault:\n\t\t\t// We can regard PKIX-TA and PKIX-EE as \"unusable\" with SMTP DANE. ../rfc/7672:1304\n\t\t\tcontinue\n\t\t}\n\t\tswitch r.Selector {\n\t\tcase adns.TLSASelectorCert, adns.TLSASelectorSPKI:\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\t\tswitch r.MatchType {\n\t\tcase adns.TLSAMatchTypeFull:\n\t\t\tif r.Selector == adns.TLSASelectorCert {\n\t\t\t\tif _, err := x509.ParseCertificate(r.CertAssoc); err != nil {\n\t\t\t\t\tlog.Debugx(\"parsing certificate in dane tlsa record, ignoring\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else if r.Selector == adns.TLSASelectorSPKI {\n\t\t\t\tif _, err := x509.ParsePKIXPublicKey(r.CertAssoc); err != nil {\n\t\t\t\t\tlog.Debugx(\"parsing certificate in dane tlsa record, ignoring\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\tcase adns.TLSAMatchTypeSHA256:\n\t\t\tif len(r.CertAssoc) != sha256.Size {\n\t\t\t\tlog.Debug(\"dane tlsa record with wrong data size for sha2-256\", slog.Int(\"got\", len(r.CertAssoc)), slog.Int(\"expect\", sha256.Size))\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase adns.TLSAMatchTypeSHA512:\n\t\t\tif len(r.CertAssoc) != sha512.Size {\n\t\t\t\tlog.Debug(\"dane tlsa record with wrong data size for sha2-512\", slog.Int(\"got\", len(r.CertAssoc)), slog.Int(\"expect\", sha512.Size))\n\t\t\t\tcontinue\n\t\t\t}\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\n\t\tl[o] = r\n\t\to++\n\t}\n\treturn l[:o]\n}\n\n// GatherTLSANames returns the allowed names in TLS certificates for verification\n// with PKIX-* or DANE-TA. The first name should be used for SNI.\n//\n// If there was no MX record, the next-hop domain parameters (i.e. the original\n// email destination host, and its CNAME-expanded host, that has MX records) are\n// ignored and only the base domain parameters are taken into account.\nfunc GatherTLSANames(haveMX, expandedNextHopAuthentic, expandedTLSABaseDomainAuthentic bool, origNextHop, expandedNextHop, origTLSABaseDomain, expandedTLSABaseDomain dns.Domain) []dns.Domain {\n\t// Gather the names to check against TLS certificate. ../rfc/7672:1318\n\tif !haveMX {\n\t\t// ../rfc/7672:1336\n\t\tif !expandedTLSABaseDomainAuthentic || origTLSABaseDomain == expandedTLSABaseDomain {\n\t\t\treturn []dns.Domain{origTLSABaseDomain}\n\t\t}\n\t\treturn []dns.Domain{expandedTLSABaseDomain, origTLSABaseDomain}\n\t} else if expandedNextHopAuthentic {\n\t\t// ../rfc/7672:1326\n\t\tvar l []dns.Domain\n\t\tif expandedTLSABaseDomainAuthentic {\n\t\t\tl = []dns.Domain{expandedTLSABaseDomain}\n\t\t}\n\t\tif expandedTLSABaseDomain != origTLSABaseDomain {\n\t\t\tl = append(l, origTLSABaseDomain)\n\t\t}\n\t\tl = append(l, origNextHop)\n\t\tif origNextHop != expandedNextHop {\n\t\t\tl = append(l, expandedNextHop)\n\t\t}\n\t\treturn l\n\t} else {\n\t\t// We don't attempt DANE after insecure MX, but behaviour for it is specified.\n\t\t// ../rfc/7672:1332\n\t\treturn []dns.Domain{origNextHop}\n\t}\n}\n\n```\n# Function Signature: func GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n# Natural Language Annotations: // GatherIPs looks up the IPs to try for connecting to host, with the IPs ordered\n// to take previous attempts into account. For use with DANE, the CNAME-expanded\n// name is returned, and whether the DNS responses were authentic.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// Initialize return values\n\tauthentic = true\n\texpandedAuthentic = true\n\texpandedHost = host.Domain\n\n\t// To keep track of CNAME chains and prevent loops\n\tdomainsSeen := map[string]bool{}\n\n\tname := host.Domain.ASCII\n\tfor i := 0; ; i++ {\n\t\tif domainsSeen[name] {\n\t\t\trerr = fmt.Errorf(\"%w: host %s: already seen %s\", errCNAMELoop, host.Domain, name)\n\t\t\treturn false, expandedAuthentic, expandedHost, nil, false, rerr\n\t\t}\n\t\tdomainsSeen[name] = true\n\n\t\t// Perform a CNAME lookup\n\t\tcctx, ccancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer ccancel()\n\t\tcname, result, err := resolver.LookupCNAME(cctx, name+\".\")\n\t\tccancel()\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\trerr = fmt.Errorf(\"%w: cname lookup for %s: %v\", errDNS, name, err)\n\t\t\treturn false, expandedAuthentic, expandedHost, nil, false, rerr\n\t\t}\n\n\t\tif i == 0 {\n\t\t\tauthentic = authentic && result.Authentic\n\t\t}\n\t\texpandedAuthentic = expandedAuthentic && result.Authentic\n\n\t\tif err == nil && cname != name+\".\" {\n\t\t\t// Not a direct hostname, follow the CNAME\n\t\t\tname = strings.TrimSuffix(cname, \".\")\n\t\t\td, err := dns.ParseDomain(name)\n\t\t\tif err != nil {\n\t\t\t\trerr = fmt.Errorf(\"%w: parsing cname domain %s: %v\", errDNS, name, err)\n\t\t\t\treturn false, expandedAuthentic, expandedHost, nil, false, rerr\n\t\t\t}\n\t\t\texpandedHost = d\n\t\t\tcontinue\n\t\t}\n\n\t\t// Perform an IP lookup\n\t\tipctx, ipcancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer ipcancel()\n\t\tips, result, err = resolver.LookupIP(ipctx, network, name+\".\")\n\t\tipcancel()\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\trerr = fmt.Errorf(\"%w: ip lookup for %s: %v\", errDNS, name, err)\n\t\t\treturn false, expandedAuthentic, expandedHost, nil, false, rerr\n\t\t}\n\n\t\tif i == 0 {\n\t\t\tauthentic = authentic && result.Authentic\n\t\t}\n\t\texpandedAuthentic = expandedAuthentic && result.Authentic\n\n\t\tif len(ips) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Account for previously dialed IPs in order\n\t\tprevDialedIPs, hasPrev := dialedIPs[name]\n\t\tif hasPrev {\n\t\t\tipsSet := make(map[string]net.IP)\n\t\t\tfor _, ip := range ips {\n\t\t\t\tipsSet[ip.String()] = ip\n\t\t\t}\n\n\t\t\t// Remove previously dialed IPs from the main list\n\t\t\tfor _, ip := range prevDialedIPs {\n\t\t\t\tdelete(ipsSet, ip.String())\n\t\t\t}\n\n\t\t\t// Reorder IPs by placing previously dialed ones at the end\n\t\t\tnewOrder := append(prevDialedIPs, ips[:0]...)\n\t\t\tfor _, ip := range ipsSet {\n\t\t\t\tnewOrder = append(newOrder, ip)\n\t\t\t}\n\t\t\tips = newOrder\n\t\t}\n\n\t\tdualstack = len(ips) > 1\n\t\treturn authentic, expandedAuthentic, expandedHost, ips, dualstack, nil\n\t}\n}\n```"]}, "mox-tlsrpt/report.go-TLSFailureDetails": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/tlsrpt/report.go:\n```\npackage tlsrpt\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/adns\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxio\"\n)\n\nvar ErrNoReport = errors.New(\"no tlsrpt report found\")\n\n// ../rfc/8460:628\n\n// Report is a TLSRPT report.\ntype Report struct {\n\tOrganizationName string\n\tDateRange        TLSRPTDateRange\n\tContactInfo      string\n\tReportID         string\n\tPolicies         []Result\n}\n\n// ReportJSON is a TLS report with field names as used in the specification. These field names are inconvenient to use in JavaScript, so after parsing a ReportJSON is turned into a Report.\ntype ReportJSON struct {\n\tOrganizationName string              `json:\"organization-name\"`\n\tDateRange        TLSRPTDateRangeJSON `json:\"date-range\"`\n\tContactInfo      string              `json:\"contact-info\"` // Email address.\n\tReportID         string              `json:\"report-id\"`\n\tPolicies         []ResultJSON        `json:\"policies\"`\n}\n\nfunc convertSlice[T interface{ Convert() S }, S any](l []T) []S {\n\tif l == nil {\n\t\treturn nil\n\t}\n\tr := make([]S, len(l))\n\tfor i, e := range l {\n\t\tr[i] = e.Convert()\n\t}\n\treturn r\n}\n\nfunc (v Report) Convert() ReportJSON {\n\treturn ReportJSON{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[Result, ResultJSON](v.Policies)}\n}\n\nfunc (v ReportJSON) Convert() Report {\n\treturn Report{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[ResultJSON, Result](v.Policies)}\n}\n\n// Merge combines the counts and failure details of results into the report.\n// Policies are merged if identical and added otherwise. Same for failure details\n// within a result.\nfunc (r *Report) Merge(results ...Result) {\nMerge:\n\tfor _, nr := range results {\n\t\tfor i, p := range r.Policies {\n\t\t\tif !p.Policy.equal(nr.Policy) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tr.Policies[i].Add(nr.Summary.TotalSuccessfulSessionCount, nr.Summary.TotalFailureSessionCount, nr.FailureDetails...)\n\t\t\tcontinue Merge\n\t\t}\n\n\t\tr.Policies = append(r.Policies, nr)\n\t}\n}\n\n// Add increases the success/failure counts of a result, and adds any failure\n// details.\nfunc (r *Result) Add(success, failure int64, fds ...FailureDetails) {\n\tr.Summary.TotalSuccessfulSessionCount += success\n\tr.Summary.TotalFailureSessionCount += failure\n\n\t// In smtpclient we can compensate with a negative success, after failed read after\n\t// successful handshake. Sanity check that we never get negative counts.\n\tif r.Summary.TotalSuccessfulSessionCount < 0 {\n\t\tr.Summary.TotalSuccessfulSessionCount = 0\n\t}\n\tif r.Summary.TotalFailureSessionCount < 0 {\n\t\tr.Summary.TotalFailureSessionCount = 0\n\t}\n\nMerge:\n\tfor _, nfd := range fds {\n\t\tfor i, fd := range r.FailureDetails {\n\t\t\tif !fd.equalKey(nfd) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfd.FailedSessionCount += nfd.FailedSessionCount\n\t\t\tr.FailureDetails[i] = fd\n\t\t\tcontinue Merge\n\t\t}\n\t\tr.FailureDetails = append(r.FailureDetails, nfd)\n\t}\n}\n\n// Add is a convenience function for merging making a Result and merging it into\n// the report.\nfunc (r *Report) Add(policy ResultPolicy, success, failure int64, fds ...FailureDetails) {\n\tr.Merge(Result{policy, Summary{success, failure}, fds})\n}\n\n// TLSAPolicy returns a policy for DANE.\nfunc TLSAPolicy(records []adns.TLSA, tlsaBaseDomain dns.Domain) ResultPolicy {\n\t// The policy domain is the TLSA base domain. ../rfc/8460:251\n\n\tl := make([]string, len(records))\n\tfor i, r := range records {\n\t\tl[i] = r.Record()\n\t}\n\tsort.Strings(l) // For consistent equals.\n\treturn ResultPolicy{\n\t\tType:   TLSA,\n\t\tString: l,\n\t\tDomain: tlsaBaseDomain.ASCII,\n\t\tMXHost: []string{},\n\t}\n}\n\nfunc MakeResult(policyType PolicyType, domain dns.Domain, fds ...FailureDetails) Result {\n\tif fds == nil {\n\t\tfds = []FailureDetails{}\n\t}\n\treturn Result{\n\t\tPolicy:         ResultPolicy{Type: policyType, Domain: domain.ASCII, String: []string{}, MXHost: []string{}},\n\t\tFailureDetails: fds,\n\t}\n}\n\n// note: with TLSRPT prefix to prevent clash in sherpadoc types.\ntype TLSRPTDateRange struct {\n\tStart time.Time\n\tEnd   time.Time\n}\n\nfunc (v TLSRPTDateRange) Convert() TLSRPTDateRangeJSON {\n\treturn TLSRPTDateRangeJSON(v)\n}\n\ntype TLSRPTDateRangeJSON struct {\n\tStart time.Time `json:\"start-datetime\"`\n\tEnd   time.Time `json:\"end-datetime\"`\n}\n\nfunc (v TLSRPTDateRangeJSON) Convert() TLSRPTDateRange {\n\treturn TLSRPTDateRange(v)\n}\n\n// UnmarshalJSON is defined on the date range, not the individual time.Time fields\n// because it is easier to keep the unmodified time.Time fields stored in the\n// database.\nfunc (dr *TLSRPTDateRangeJSON) UnmarshalJSON(buf []byte) error {\n\tvar v struct {\n\t\tStart xtime `json:\"start-datetime\"`\n\t\tEnd   xtime `json:\"end-datetime\"`\n\t}\n\tif err := json.Unmarshal(buf, &v); err != nil {\n\t\treturn err\n\t}\n\tdr.Start = time.Time(v.Start)\n\tdr.End = time.Time(v.End)\n\treturn nil\n}\n\n// xtime and its UnmarshalJSON exists to work around a specific invalid date-time encoding seen in the wild.\ntype xtime time.Time\n\nfunc (x *xtime) UnmarshalJSON(buf []byte) error {\n\tvar t time.Time\n\terr := t.UnmarshalJSON(buf)\n\tif err == nil {\n\t\t*x = xtime(t)\n\t\treturn nil\n\t}\n\n\t// Microsoft is sending reports with invalid start-datetime/end-datetime (missing\n\t// timezone, ../rfc/8460:682 ../rfc/3339:415). We compensate.\n\tvar s string\n\tif err := json.Unmarshal(buf, &s); err != nil {\n\t\treturn err\n\t}\n\tt, err = time.Parse(\"2006-01-02T15:04:05\", s)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*x = xtime(t)\n\treturn nil\n}\n\ntype Result struct {\n\tPolicy         ResultPolicy\n\tSummary        Summary\n\tFailureDetails []FailureDetails\n}\n\nfunc (r Result) Convert() ResultJSON {\n\treturn ResultJSON{ResultPolicyJSON(r.Policy), SummaryJSON(r.Summary), convertSlice[FailureDetails, FailureDetailsJSON](r.FailureDetails)}\n}\n\ntype ResultJSON struct {\n\tPolicy         ResultPolicyJSON     `json:\"policy\"`\n\tSummary        SummaryJSON          `json:\"summary\"`\n\tFailureDetails []FailureDetailsJSON `json:\"failure-details\"`\n}\n\nfunc (r ResultJSON) Convert() Result {\n\treturn Result{ResultPolicy(r.Policy), Summary(r.Summary), convertSlice[FailureDetailsJSON, FailureDetails](r.FailureDetails)}\n}\n\n// todo spec: ../rfc/8460:437 says policy is a string, with rules for turning dane records into a single string. perhaps a remnant of an earlier version (for mtasts a single string would have made more sense). i doubt the intention is to always have a single element in policy-string (though the field name is singular).\n\ntype ResultPolicy struct {\n\tType   PolicyType\n\tString []string\n\tDomain string // ASCII/A-labels, ../rfc/8460:704\n\tMXHost []string\n}\n\ntype ResultPolicyJSON struct {\n\tType   PolicyType `json:\"policy-type\"`\n\tString []string   `json:\"policy-string\"`\n\tDomain string     `json:\"policy-domain\"`\n\tMXHost []string   `json:\"mx-host\"` // Example in RFC has errata, it originally was a single string. ../rfc/8460-eid6241 ../rfc/8460:1779\n}\n\n// PolicyType indicates the policy success/failure results are for.\ntype PolicyType string\n\nconst (\n\t// For DANE, against a mail host (not recipient domain).\n\tTLSA PolicyType = \"tlsa\"\n\n\t// For MTA-STS, against a recipient domain (not a mail host).\n\tSTS PolicyType = \"sts\"\n\n\t// Recipient domain did not have MTA-STS policy, or mail host (TSLA base domain)\n\t// did not have DANE TLSA records.\n\tNoPolicyFound PolicyType = \"no-policy-found\"\n\t// todo spec: ../rfc/8460:440 ../rfc/8460:697 suggest to replace with values like \"no-sts-found\" and \"no-tlsa-found\" to make it explicit which policy isn't found. also easier to implement, because you don't have to handle leaving out an sts no-policy-found result for a mail host when a tlsa policy is present.\n)\n\nfunc (rp ResultPolicy) equal(orp ResultPolicy) bool {\n\treturn rp.Type == orp.Type && slices.Equal(rp.String, orp.String) && rp.Domain == orp.Domain && slices.Equal(rp.MXHost, orp.MXHost)\n}\n\ntype Summary struct {\n\tTotalSuccessfulSessionCount int64\n\tTotalFailureSessionCount    int64\n}\n\ntype SummaryJSON struct {\n\tTotalSuccessfulSessionCount int64 `json:\"total-successful-session-count\"`\n\tTotalFailureSessionCount    int64 `json:\"total-failure-session-count\"`\n}\n\n// ResultType represents a TLS error.\ntype ResultType string\n\n// ../rfc/8460:1377\n// https://www.iana.org/assignments/starttls-validation-result-types/starttls-validation-result-types.xhtml\n\nconst (\n\tResultSTARTTLSNotSupported    ResultType = \"starttls-not-supported\"\n\tResultCertificateHostMismatch ResultType = \"certificate-host-mismatch\"\n\tResultCertificateExpired      ResultType = \"certificate-expired\"\n\tResultTLSAInvalid             ResultType = \"tlsa-invalid\"\n\tResultDNSSECInvalid           ResultType = \"dnssec-invalid\"\n\tResultDANERequired            ResultType = \"dane-required\"\n\tResultCertificateNotTrusted   ResultType = \"certificate-not-trusted\"\n\tResultSTSPolicyInvalid        ResultType = \"sts-policy-invalid\"\n\tResultSTSWebPKIInvalid        ResultType = \"sts-webpki-invalid\"\n\tResultValidationFailure       ResultType = \"validation-failure\" // Other error.\n\tResultSTSPolicyFetch          ResultType = \"sts-policy-fetch-error\"\n)\n\n// todo spec: ../rfc/8460:719 more of these fields should be optional. some sts failure details, like failed policy fetches, won't have an ip or mx, the failure happens earlier in the delivery process.\n\ntype FailureDetails struct {\n\tResultType            ResultType\n\tSendingMTAIP          string\n\tReceivingMXHostname   string\n\tReceivingMXHelo       string\n\tReceivingIP           string\n\tFailedSessionCount    int64\n\tAdditionalInformation string\n\tFailureReasonCode     string\n}\n\nfunc (v FailureDetails) Convert() FailureDetailsJSON { return FailureDetailsJSON(v) }\n\ntype FailureDetailsJSON struct {\n\tResultType            ResultType `json:\"result-type\"`\n\tSendingMTAIP          string     `json:\"sending-mta-ip\"`\n\tReceivingMXHostname   string     `json:\"receiving-mx-hostname\"`\n\tReceivingMXHelo       string     `json:\"receiving-mx-helo,omitempty\"`\n\tReceivingIP           string     `json:\"receiving-ip\"`\n\tFailedSessionCount    int64      `json:\"failed-session-count\"`\n\tAdditionalInformation string     `json:\"additional-information\"`\n\tFailureReasonCode     string     `json:\"failure-reason-code\"`\n}\n\nfunc (v FailureDetailsJSON) Convert() FailureDetails { return FailureDetails(v) }\n\n// equalKey returns whether FailureDetails have the same values, expect for\n// FailedSessionCount. Useful for aggregating FailureDetails.\nfunc (fd FailureDetails) equalKey(ofd FailureDetails) bool {\n\tfd.FailedSessionCount = 0\n\tofd.FailedSessionCount = 0\n\treturn fd == ofd\n}\n\n// Details is a convenience function to compose a FailureDetails.\nfunc Details(t ResultType, r string) FailureDetails {\n\treturn FailureDetails{ResultType: t, FailedSessionCount: 1, FailureReasonCode: r}\n}\n\nvar invalidReasons = map[x509.InvalidReason]string{\n\tx509.NotAuthorizedToSign:           \"not-authorized-to-sign\",\n\tx509.Expired:                       \"certificate-expired\",\n\tx509.CANotAuthorizedForThisName:    \"ca-not-authorized-for-this-name\",\n\tx509.TooManyIntermediates:          \"too-many-intermediates\",\n\tx509.IncompatibleUsage:             \"incompatible-key-usage\",\n\tx509.NameMismatch:                  \"parent-subject-child-issuer-mismatch\",\n\tx509.NameConstraintsWithoutSANs:    \"name-constraint-without-sans\",\n\tx509.UnconstrainedName:             \"unconstrained-name\",\n\tx509.TooManyConstraints:            \"too-many-constraints\",\n\tx509.CANotAuthorizedForExtKeyUsage: \"ca-not-authorized-for-ext-key-usage\",\n}\n\n// TLSFailureDetails turns errors encountered during TLS handshakes into a result\n// type and failure reason code for use with FailureDetails.\n//\n// Errors from crypto/tls, including local and remote alerts, from crypto/x509,\n// and generic i/o and timeout errors are recognized.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Parse parses a Report.\n// The maximum size is 20MB.\nfunc Parse(r io.Reader) (*ReportJSON, error) {\n\tr = &moxio.LimitReader{R: r, Limit: 20 * 1024 * 1024}\n\tvar report ReportJSON\n\tif err := json.NewDecoder(r).Decode(&report); err != nil {\n\t\treturn nil, err\n\t}\n\t// note: there may be leftover data, we ignore it.\n\treturn &report, nil\n}\n\n// ParseMessage parses a Report from a mail message.\n// The maximum size of the message is 15MB, the maximum size of the\n// decompressed report is 20MB.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc parseMessageReport(log mlog.Log, p message.Part, allow bool) (*ReportJSON, error) {\n\tif p.MediaType != \"MULTIPART\" {\n\t\tif !allow {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif p.MediaSubType == \"REPORT\" && p.ContentTypeParams[\"report-type\"] != \"tlsrpt\" {\n\t\t\treturn nil, fmt.Errorf(\"unknown report-type parameter %q\", p.ContentTypeParams[\"report-type\"])\n\t\t}\n\t\treport, err := parseMessageReport(log, *sp, p.MediaSubType == \"REPORT\")\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(p message.Part) (*ReportJSON, error) {\n\tmt := strings.ToLower(p.MediaType + \"/\" + p.MediaSubType)\n\tswitch mt {\n\tcase \"application/tlsrpt+json\":\n\t\treturn Parse(p.Reader())\n\tcase \"application/tlsrpt+gzip\":\n\t\tgzr, err := gzip.NewReader(p.Reader())\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip TLSRPT report: %s\", err)\n\t\t}\n\t\treturn Parse(gzr)\n\t}\n\treturn nil, ErrNoReport\n}\n\n```\n# Function Signature: func TLSFailureDetails(err error) (ResultType, string) {\n# Natural Language Annotations: // TLSFailureDetails turns errors encountered during TLS handshakes into a result\n// type and failure reason code for use with FailureDetails.\n//\n// Errors from crypto/tls, including local and remote alerts, from crypto/x509,\n// and generic i/o and timeout errors are recognized.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc TLSFailureDetails(err error) (ResultType, string) {\n\tif err == nil {\n\t\treturn \"\", \"\"\n\t}\n\n\tvar resultType ResultType\n\tvar reasonCode string\n\n\tswitch err := err.(type) {\n\tcase tls.RecordHeaderError:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"tls-record-header-error\"\n\tcase *tls.Alert:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"tls-alert-\" + err.String()\n\tcase *x509.CertificateInvalidError:\n\t\tresultType = ResultCertificateNotTrusted\n\t\tif reason, found := invalidReasons[err.Reason]; found {\n\t\t\treasonCode = reason\n\t\t} else {\n\t\t\treasonCode = \"certificate-invalid\"\n\t\t}\n\tcase *x509.HostnameError:\n\t\tresultType = ResultCertificateHostMismatch\n\t\treasonCode = \"certificate-hostname-mismatch\"\n\tcase *net.OpError:\n\t\tif err.Timeout() {\n\t\t\tresultType = ResultValidationFailure\n\t\t\treasonCode = \"network-timeout\"\n\t\t} else {\n\t\t\tresultType = ResultValidationFailure\n\t\t\treasonCode = \"network-io-error\"\n\t\t}\n\tdefault:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"unknown-error\"\n\t}\n\n\treturn resultType, reasonCode\n}\n```"]}, "mox-tlsrpt/report.go-ParseMessage": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/tlsrpt/report.go:\n```\npackage tlsrpt\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/adns\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxio\"\n)\n\nvar ErrNoReport = errors.New(\"no tlsrpt report found\")\n\n// ../rfc/8460:628\n\n// Report is a TLSRPT report.\ntype Report struct {\n\tOrganizationName string\n\tDateRange        TLSRPTDateRange\n\tContactInfo      string\n\tReportID         string\n\tPolicies         []Result\n}\n\n// ReportJSON is a TLS report with field names as used in the specification. These field names are inconvenient to use in JavaScript, so after parsing a ReportJSON is turned into a Report.\ntype ReportJSON struct {\n\tOrganizationName string              `json:\"organization-name\"`\n\tDateRange        TLSRPTDateRangeJSON `json:\"date-range\"`\n\tContactInfo      string              `json:\"contact-info\"` // Email address.\n\tReportID         string              `json:\"report-id\"`\n\tPolicies         []ResultJSON        `json:\"policies\"`\n}\n\nfunc convertSlice[T interface{ Convert() S }, S any](l []T) []S {\n\tif l == nil {\n\t\treturn nil\n\t}\n\tr := make([]S, len(l))\n\tfor i, e := range l {\n\t\tr[i] = e.Convert()\n\t}\n\treturn r\n}\n\nfunc (v Report) Convert() ReportJSON {\n\treturn ReportJSON{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[Result, ResultJSON](v.Policies)}\n}\n\nfunc (v ReportJSON) Convert() Report {\n\treturn Report{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[ResultJSON, Result](v.Policies)}\n}\n\n// Merge combines the counts and failure details of results into the report.\n// Policies are merged if identical and added otherwise. Same for failure details\n// within a result.\nfunc (r *Report) Merge(results ...Result) {\nMerge:\n\tfor _, nr := range results {\n\t\tfor i, p := range r.Policies {\n\t\t\tif !p.Policy.equal(nr.Policy) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tr.Policies[i].Add(nr.Summary.TotalSuccessfulSessionCount, nr.Summary.TotalFailureSessionCount, nr.FailureDetails...)\n\t\t\tcontinue Merge\n\t\t}\n\n\t\tr.Policies = append(r.Policies, nr)\n\t}\n}\n\n// Add increases the success/failure counts of a result, and adds any failure\n// details.\nfunc (r *Result) Add(success, failure int64, fds ...FailureDetails) {\n\tr.Summary.TotalSuccessfulSessionCount += success\n\tr.Summary.TotalFailureSessionCount += failure\n\n\t// In smtpclient we can compensate with a negative success, after failed read after\n\t// successful handshake. Sanity check that we never get negative counts.\n\tif r.Summary.TotalSuccessfulSessionCount < 0 {\n\t\tr.Summary.TotalSuccessfulSessionCount = 0\n\t}\n\tif r.Summary.TotalFailureSessionCount < 0 {\n\t\tr.Summary.TotalFailureSessionCount = 0\n\t}\n\nMerge:\n\tfor _, nfd := range fds {\n\t\tfor i, fd := range r.FailureDetails {\n\t\t\tif !fd.equalKey(nfd) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfd.FailedSessionCount += nfd.FailedSessionCount\n\t\t\tr.FailureDetails[i] = fd\n\t\t\tcontinue Merge\n\t\t}\n\t\tr.FailureDetails = append(r.FailureDetails, nfd)\n\t}\n}\n\n// Add is a convenience function for merging making a Result and merging it into\n// the report.\nfunc (r *Report) Add(policy ResultPolicy, success, failure int64, fds ...FailureDetails) {\n\tr.Merge(Result{policy, Summary{success, failure}, fds})\n}\n\n// TLSAPolicy returns a policy for DANE.\nfunc TLSAPolicy(records []adns.TLSA, tlsaBaseDomain dns.Domain) ResultPolicy {\n\t// The policy domain is the TLSA base domain. ../rfc/8460:251\n\n\tl := make([]string, len(records))\n\tfor i, r := range records {\n\t\tl[i] = r.Record()\n\t}\n\tsort.Strings(l) // For consistent equals.\n\treturn ResultPolicy{\n\t\tType:   TLSA,\n\t\tString: l,\n\t\tDomain: tlsaBaseDomain.ASCII,\n\t\tMXHost: []string{},\n\t}\n}\n\nfunc MakeResult(policyType PolicyType, domain dns.Domain, fds ...FailureDetails) Result {\n\tif fds == nil {\n\t\tfds = []FailureDetails{}\n\t}\n\treturn Result{\n\t\tPolicy:         ResultPolicy{Type: policyType, Domain: domain.ASCII, String: []string{}, MXHost: []string{}},\n\t\tFailureDetails: fds,\n\t}\n}\n\n// note: with TLSRPT prefix to prevent clash in sherpadoc types.\ntype TLSRPTDateRange struct {\n\tStart time.Time\n\tEnd   time.Time\n}\n\nfunc (v TLSRPTDateRange) Convert() TLSRPTDateRangeJSON {\n\treturn TLSRPTDateRangeJSON(v)\n}\n\ntype TLSRPTDateRangeJSON struct {\n\tStart time.Time `json:\"start-datetime\"`\n\tEnd   time.Time `json:\"end-datetime\"`\n}\n\nfunc (v TLSRPTDateRangeJSON) Convert() TLSRPTDateRange {\n\treturn TLSRPTDateRange(v)\n}\n\n// UnmarshalJSON is defined on the date range, not the individual time.Time fields\n// because it is easier to keep the unmodified time.Time fields stored in the\n// database.\nfunc (dr *TLSRPTDateRangeJSON) UnmarshalJSON(buf []byte) error {\n\tvar v struct {\n\t\tStart xtime `json:\"start-datetime\"`\n\t\tEnd   xtime `json:\"end-datetime\"`\n\t}\n\tif err := json.Unmarshal(buf, &v); err != nil {\n\t\treturn err\n\t}\n\tdr.Start = time.Time(v.Start)\n\tdr.End = time.Time(v.End)\n\treturn nil\n}\n\n// xtime and its UnmarshalJSON exists to work around a specific invalid date-time encoding seen in the wild.\ntype xtime time.Time\n\nfunc (x *xtime) UnmarshalJSON(buf []byte) error {\n\tvar t time.Time\n\terr := t.UnmarshalJSON(buf)\n\tif err == nil {\n\t\t*x = xtime(t)\n\t\treturn nil\n\t}\n\n\t// Microsoft is sending reports with invalid start-datetime/end-datetime (missing\n\t// timezone, ../rfc/8460:682 ../rfc/3339:415). We compensate.\n\tvar s string\n\tif err := json.Unmarshal(buf, &s); err != nil {\n\t\treturn err\n\t}\n\tt, err = time.Parse(\"2006-01-02T15:04:05\", s)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*x = xtime(t)\n\treturn nil\n}\n\ntype Result struct {\n\tPolicy         ResultPolicy\n\tSummary        Summary\n\tFailureDetails []FailureDetails\n}\n\nfunc (r Result) Convert() ResultJSON {\n\treturn ResultJSON{ResultPolicyJSON(r.Policy), SummaryJSON(r.Summary), convertSlice[FailureDetails, FailureDetailsJSON](r.FailureDetails)}\n}\n\ntype ResultJSON struct {\n\tPolicy         ResultPolicyJSON     `json:\"policy\"`\n\tSummary        SummaryJSON          `json:\"summary\"`\n\tFailureDetails []FailureDetailsJSON `json:\"failure-details\"`\n}\n\nfunc (r ResultJSON) Convert() Result {\n\treturn Result{ResultPolicy(r.Policy), Summary(r.Summary), convertSlice[FailureDetailsJSON, FailureDetails](r.FailureDetails)}\n}\n\n// todo spec: ../rfc/8460:437 says policy is a string, with rules for turning dane records into a single string. perhaps a remnant of an earlier version (for mtasts a single string would have made more sense). i doubt the intention is to always have a single element in policy-string (though the field name is singular).\n\ntype ResultPolicy struct {\n\tType   PolicyType\n\tString []string\n\tDomain string // ASCII/A-labels, ../rfc/8460:704\n\tMXHost []string\n}\n\ntype ResultPolicyJSON struct {\n\tType   PolicyType `json:\"policy-type\"`\n\tString []string   `json:\"policy-string\"`\n\tDomain string     `json:\"policy-domain\"`\n\tMXHost []string   `json:\"mx-host\"` // Example in RFC has errata, it originally was a single string. ../rfc/8460-eid6241 ../rfc/8460:1779\n}\n\n// PolicyType indicates the policy success/failure results are for.\ntype PolicyType string\n\nconst (\n\t// For DANE, against a mail host (not recipient domain).\n\tTLSA PolicyType = \"tlsa\"\n\n\t// For MTA-STS, against a recipient domain (not a mail host).\n\tSTS PolicyType = \"sts\"\n\n\t// Recipient domain did not have MTA-STS policy, or mail host (TSLA base domain)\n\t// did not have DANE TLSA records.\n\tNoPolicyFound PolicyType = \"no-policy-found\"\n\t// todo spec: ../rfc/8460:440 ../rfc/8460:697 suggest to replace with values like \"no-sts-found\" and \"no-tlsa-found\" to make it explicit which policy isn't found. also easier to implement, because you don't have to handle leaving out an sts no-policy-found result for a mail host when a tlsa policy is present.\n)\n\nfunc (rp ResultPolicy) equal(orp ResultPolicy) bool {\n\treturn rp.Type == orp.Type && slices.Equal(rp.String, orp.String) && rp.Domain == orp.Domain && slices.Equal(rp.MXHost, orp.MXHost)\n}\n\ntype Summary struct {\n\tTotalSuccessfulSessionCount int64\n\tTotalFailureSessionCount    int64\n}\n\ntype SummaryJSON struct {\n\tTotalSuccessfulSessionCount int64 `json:\"total-successful-session-count\"`\n\tTotalFailureSessionCount    int64 `json:\"total-failure-session-count\"`\n}\n\n// ResultType represents a TLS error.\ntype ResultType string\n\n// ../rfc/8460:1377\n// https://www.iana.org/assignments/starttls-validation-result-types/starttls-validation-result-types.xhtml\n\nconst (\n\tResultSTARTTLSNotSupported    ResultType = \"starttls-not-supported\"\n\tResultCertificateHostMismatch ResultType = \"certificate-host-mismatch\"\n\tResultCertificateExpired      ResultType = \"certificate-expired\"\n\tResultTLSAInvalid             ResultType = \"tlsa-invalid\"\n\tResultDNSSECInvalid           ResultType = \"dnssec-invalid\"\n\tResultDANERequired            ResultType = \"dane-required\"\n\tResultCertificateNotTrusted   ResultType = \"certificate-not-trusted\"\n\tResultSTSPolicyInvalid        ResultType = \"sts-policy-invalid\"\n\tResultSTSWebPKIInvalid        ResultType = \"sts-webpki-invalid\"\n\tResultValidationFailure       ResultType = \"validation-failure\" // Other error.\n\tResultSTSPolicyFetch          ResultType = \"sts-policy-fetch-error\"\n)\n\n// todo spec: ../rfc/8460:719 more of these fields should be optional. some sts failure details, like failed policy fetches, won't have an ip or mx, the failure happens earlier in the delivery process.\n\ntype FailureDetails struct {\n\tResultType            ResultType\n\tSendingMTAIP          string\n\tReceivingMXHostname   string\n\tReceivingMXHelo       string\n\tReceivingIP           string\n\tFailedSessionCount    int64\n\tAdditionalInformation string\n\tFailureReasonCode     string\n}\n\nfunc (v FailureDetails) Convert() FailureDetailsJSON { return FailureDetailsJSON(v) }\n\ntype FailureDetailsJSON struct {\n\tResultType            ResultType `json:\"result-type\"`\n\tSendingMTAIP          string     `json:\"sending-mta-ip\"`\n\tReceivingMXHostname   string     `json:\"receiving-mx-hostname\"`\n\tReceivingMXHelo       string     `json:\"receiving-mx-helo,omitempty\"`\n\tReceivingIP           string     `json:\"receiving-ip\"`\n\tFailedSessionCount    int64      `json:\"failed-session-count\"`\n\tAdditionalInformation string     `json:\"additional-information\"`\n\tFailureReasonCode     string     `json:\"failure-reason-code\"`\n}\n\nfunc (v FailureDetailsJSON) Convert() FailureDetails { return FailureDetails(v) }\n\n// equalKey returns whether FailureDetails have the same values, expect for\n// FailedSessionCount. Useful for aggregating FailureDetails.\nfunc (fd FailureDetails) equalKey(ofd FailureDetails) bool {\n\tfd.FailedSessionCount = 0\n\tofd.FailedSessionCount = 0\n\treturn fd == ofd\n}\n\n// Details is a convenience function to compose a FailureDetails.\nfunc Details(t ResultType, r string) FailureDetails {\n\treturn FailureDetails{ResultType: t, FailedSessionCount: 1, FailureReasonCode: r}\n}\n\nvar invalidReasons = map[x509.InvalidReason]string{\n\tx509.NotAuthorizedToSign:           \"not-authorized-to-sign\",\n\tx509.Expired:                       \"certificate-expired\",\n\tx509.CANotAuthorizedForThisName:    \"ca-not-authorized-for-this-name\",\n\tx509.TooManyIntermediates:          \"too-many-intermediates\",\n\tx509.IncompatibleUsage:             \"incompatible-key-usage\",\n\tx509.NameMismatch:                  \"parent-subject-child-issuer-mismatch\",\n\tx509.NameConstraintsWithoutSANs:    \"name-constraint-without-sans\",\n\tx509.UnconstrainedName:             \"unconstrained-name\",\n\tx509.TooManyConstraints:            \"too-many-constraints\",\n\tx509.CANotAuthorizedForExtKeyUsage: \"ca-not-authorized-for-ext-key-usage\",\n}\n\n// TLSFailureDetails turns errors encountered during TLS handshakes into a result\n// type and failure reason code for use with FailureDetails.\n//\n// Errors from crypto/tls, including local and remote alerts, from crypto/x509,\n// and generic i/o and timeout errors are recognized.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Parse parses a Report.\n// The maximum size is 20MB.\nfunc Parse(r io.Reader) (*ReportJSON, error) {\n\tr = &moxio.LimitReader{R: r, Limit: 20 * 1024 * 1024}\n\tvar report ReportJSON\n\tif err := json.NewDecoder(r).Decode(&report); err != nil {\n\t\treturn nil, err\n\t}\n\t// note: there may be leftover data, we ignore it.\n\treturn &report, nil\n}\n\n// ParseMessage parses a Report from a mail message.\n// The maximum size of the message is 15MB, the maximum size of the\n// decompressed report is 20MB.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc parseMessageReport(log mlog.Log, p message.Part, allow bool) (*ReportJSON, error) {\n\tif p.MediaType != \"MULTIPART\" {\n\t\tif !allow {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif p.MediaSubType == \"REPORT\" && p.ContentTypeParams[\"report-type\"] != \"tlsrpt\" {\n\t\t\treturn nil, fmt.Errorf(\"unknown report-type parameter %q\", p.ContentTypeParams[\"report-type\"])\n\t\t}\n\t\treport, err := parseMessageReport(log, *sp, p.MediaSubType == \"REPORT\")\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(p message.Part) (*ReportJSON, error) {\n\tmt := strings.ToLower(p.MediaType + \"/\" + p.MediaSubType)\n\tswitch mt {\n\tcase \"application/tlsrpt+json\":\n\t\treturn Parse(p.Reader())\n\tcase \"application/tlsrpt+gzip\":\n\t\tgzr, err := gzip.NewReader(p.Reader())\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip TLSRPT report: %s\", err)\n\t\t}\n\t\treturn Parse(gzr)\n\t}\n\treturn nil, ErrNoReport\n}\n\n```\n# Function Signature: func ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n# Natural Language Annotations: // ParseMessage parses a Report from a mail message.\n// The maximum size of the message is 15MB, the maximum size of the\n// decompressed report is 20MB.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n    const maxMessageSize = 15 * 1024 * 1024 // 15MB\n    const maxReportSize = 20 * 1024 * 1024 // 20MB\n\n    // Create a section reader to allow reading from the reader with a max size\n    sectionReader := io.NewSectionReader(r, 0, maxMessageSize)\n\n    // Parse the mail message\n    msg, err := message.Read(sectionReader)\n    if err != nil {\n        return nil, err\n    }\n\n    // Start parsing the message and check for TLSRPT report\n    report, err := parseMessageReport(mlog.New(elog), *msg.Body, false)\n    if err != nil {\n        return nil, err\n    }\n\n    return report, nil\n}\n```\n\nThis function uses the `io.ReaderAt` to create a section reader that ensures the read does not exceed the maximum message size of 15MB. It then parses the message and attempts to extract a TLS Report by leveraging the existing `parseMessageReport` method. This code assumes that the `message.Read` function is available and correctly reads the message structure from the input reader. The logging and error handling are managed by the provided `slog.Logger`."]}, "mox-spf/received.go-Header": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/spf/received.go:\n```\npackage spf\n\nimport (\n\t\"net\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n)\n\n// ../rfc/7208:2083\n\n// Received represents a Received-SPF header with the SPF verify results, to be\n// prepended to a message.\n//\n// Example:\n//\n//\tReceived-SPF: pass (mybox.example.org: domain of\n//\t myname@example.com designates 192.0.2.1 as permitted sender)\n//\t receiver=mybox.example.org; client-ip=192.0.2.1;\n//\t envelope-from=\"myname@example.com\"; helo=foo.example.com;\ntype Received struct {\n\tResult       Status\n\tComment      string       // Additional free-form information about the verification result. Optional. Included in message header comment inside \"()\".\n\tClientIP     net.IP       // IP address of remote SMTP client, \"client-ip=\".\n\tEnvelopeFrom string       // Sender mailbox, typically SMTP MAIL FROM, but will be set to \"postmaster\" at SMTP EHLO if MAIL FROM is empty, \"envelop-from=\".\n\tHelo         dns.IPDomain // IP or host name from EHLO or HELO command, \"helo=\".\n\tProblem      string       // Optional. \"problem=\"\n\tReceiver     string       // Hostname of receiving mail server, \"receiver=\".\n\tIdentity     Identity     // The identity that was checked, \"mailfrom\" or \"helo\", for \"identity=\".\n\tMechanism    string       // Mechanism that caused the result, can be \"default\". Optional.\n}\n\n// Identity that was verified.\ntype Identity string\n\nconst (\n\tReceivedMailFrom Identity = \"mailfrom\"\n\tReceivedHELO     Identity = \"helo\"\n)\n\nfunc receivedValueEncode(s string) string {\n\tif s == \"\" {\n\t\treturn quotedString(\"\")\n\t}\n\tfor i, c := range s {\n\t\tif c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c > 0x7f {\n\t\t\tcontinue\n\t\t}\n\t\t// ../rfc/5322:679\n\t\tconst atext = \"!#$%&'*+-/=?^_`{|}~\"\n\t\tif strings.IndexByte(atext, byte(c)) >= 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif c != '.' || (i == 0 || i+1 == len(s)) {\n\t\t\treturn quotedString(s)\n\t\t}\n\t}\n\treturn s\n}\n\n// ../rfc/5322:736\nfunc quotedString(s string) string {\n\tw := &strings.Builder{}\n\tw.WriteByte('\"')\n\tfor _, c := range s {\n\t\tif c > ' ' && c < 0x7f && c != '\"' && c != '\\\\' || c > 0x7f || c == ' ' || c == '\\t' {\n\t\t\t// We allow utf-8. This should only be needed when the destination address has an\n\t\t\t// utf8 localpart, in which case we are already doing smtputf8.\n\t\t\t// We also allow unescaped space and tab. This is FWS, and the name of ABNF\n\t\t\t// production \"qcontent\" implies the FWS is not part of the string, but escaping\n\t\t\t// space and tab leads to ugly strings. ../rfc/5322:743\n\t\t\tw.WriteRune(c)\n\t\t\tcontinue\n\t\t}\n\t\tswitch c {\n\t\tcase ' ', '\\t', '\"', '\\\\':\n\t\t\tw.WriteByte('\\\\')\n\t\t\tw.WriteRune(c)\n\t\t}\n\t}\n\tw.WriteByte('\"')\n\treturn w.String()\n}\n\n// Header returns a Received-SPF header including trailing crlf that can be\n// prepended to an incoming message.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (r Received) Header() string {\n# Natural Language Annotations: // Header returns a Received-SPF header including trailing crlf that can be\n// prepended to an incoming message.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (r Received) Header() string {\n\tvar parts []string\n\n\tparts = append(parts, \"Received-SPF: \"+string(r.Result))\n\tif r.Comment != \"\" {\n\t\tparts = append(parts, \"(\"+r.Comment+\")\")\n\t}\n\tparts = append(parts, \" receiver=\"+receivedValueEncode(r.Receiver))\n\tparts = append(parts, \" client-ip=\"+r.ClientIP.String())\n\tparts = append(parts, \" envelope-from=\"+receivedValueEncode(r.EnvelopeFrom))\n\tparts = append(parts, \" helo=\"+receivedValueEncode(string(r.Helo)))\n\tparts = append(parts, \" identity=\"+string(r.Identity))\n\n\tif r.Problem != \"\" {\n\t\tparts = append(parts, \" problem=\"+receivedValueEncode(r.Problem))\n\t}\n\n\tif r.Mechanism != \"\" && r.Mechanism != \"default\" {\n\t\tparts = append(parts, \" mechanism=\"+r.Mechanism)\n\t}\n\n\treturn strings.Join(parts, \";\") + \"\\r\\n\"\n}\n```"]}, "mox-spf/spf.go-Evaluate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/spf/spf.go:\n```\n// Package spf implements Sender Policy Framework (SPF, RFC 7208) for verifying\n// remote mail server IPs with their published records.\n//\n// With SPF a domain can publish a policy as a DNS TXT record describing which IPs\n// are allowed to send email with SMTP with the domain in the MAIL FROM command,\n// and how to treat SMTP transactions coming from other IPs.\npackage spf\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/stub\"\n)\n\n// The net package always returns DNS names in absolute, lower-case form. We make\n// sure we make names absolute when looking up. For verifying, we do not want to\n// verify names relative to our local search domain.\n\nvar (\n\tMetricVerify stub.HistogramVec = stub.HistogramVecIgnore{}\n)\n\n// cross-link rfc and errata\n// ../rfc/7208-eid5436 ../rfc/7208:2043\n// ../rfc/7208-eid6721 ../rfc/7208:1928\n// ../rfc/7208-eid5227 ../rfc/7208:1297\n// ../rfc/7208-eid6595 ../rfc/7208:984\n\nvar (\n\t// Lookup errors.\n\tErrName            = errors.New(\"spf: bad domain name\")\n\tErrNoRecord        = errors.New(\"spf: no txt record\")\n\tErrMultipleRecords = errors.New(\"spf: multiple spf txt records in dns\")\n\tErrDNS             = errors.New(\"spf: lookup of dns record\")\n\tErrRecordSyntax    = errors.New(\"spf: malformed spf txt record\")\n\n\t// Evaluation errors.\n\tErrTooManyDNSRequests = errors.New(\"spf: too many dns requests\")\n\tErrTooManyVoidLookups = errors.New(\"spf: too many void lookups\")\n\tErrMacroSyntax        = errors.New(\"spf: bad macro syntax\")\n)\n\nconst (\n\t// Maximum number of DNS requests to execute. This excludes some requests, such as\n\t// lookups of MX host results.\n\tdnsRequestsMax = 10\n\n\t// Maximum number of DNS lookups that result in no records before a StatusPermerror\n\t// is returned. This limit aims to prevent abuse.\n\tvoidLookupsMax = 2\n)\n\n// Status is the result of an SPF verification.\ntype Status string\n\n// ../rfc/7208:517\n// ../rfc/7208:1836\n\nconst (\n\tStatusNone      Status = \"none\"      // E.g. no DNS domain name in session, or no SPF record in DNS.\n\tStatusNeutral   Status = \"neutral\"   // Explicit statement that nothing is said about the IP, \"?\" qualifier. None and Neutral must be treated the same.\n\tStatusPass      Status = \"pass\"      // IP is authorized.\n\tStatusFail      Status = \"fail\"      // IP is exlicitly not authorized. \"-\" qualifier.\n\tStatusSoftfail  Status = \"softfail\"  // Weak statement that IP is probably not authorized, \"~\" qualifier.\n\tStatusTemperror Status = \"temperror\" // Trying again later may succeed, e.g. for temporary DNS lookup error.\n\tStatusPermerror Status = \"permerror\" // Error requiring some intervention to correct. E.g. invalid DNS record.\n)\n\n// Args are the parameters to the SPF verification algorithm (\"check_host\" in the RFC).\n//\n// All fields should be set as they can be required for macro expansions.\ntype Args struct {\n\t// RemoteIP will be checked as sender for email.\n\tRemoteIP net.IP\n\n\t// Address from SMTP MAIL FROM command. Zero values for a null reverse path (used for DSNs).\n\tMailFromLocalpart smtp.Localpart\n\tMailFromDomain    dns.Domain\n\n\t// HelloDomain is from the SMTP EHLO/HELO command.\n\tHelloDomain dns.IPDomain\n\n\tLocalIP       net.IP\n\tLocalHostname dns.Domain\n\n\t// Explanation string to use for failure. In case of \"include\", where explanation\n\t// from original domain must be used.\n\t// May be set for recursive calls.\n\texplanation *string\n\n\t// Domain to validate.\n\tdomain dns.Domain\n\n\t// Effective sender. Equal to MailFrom if non-zero, otherwise set to \"postmaster\" at HelloDomain.\n\tsenderLocalpart smtp.Localpart\n\tsenderDomain    dns.Domain\n\n\t// To enforce the limit on lookups. Initialized automatically if nil.\n\tdnsRequests *int\n\tvoidLookups *int\n}\n\n// Mocked for testing expanding \"t\" macro.\nvar timeNow = time.Now\n\n// Lookup looks up and parses an SPF TXT record for domain.\n//\n// Authentic indicates if the DNS results were DNSSEC-verified.\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, domain dns.Domain) (rstatus Status, rtxt string, rrecord *Record, authentic bool, rerr error) {\n\tlog := mlog.New(\"spf\", elog)\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf lookup result\", rerr,\n\t\t\tslog.Any(\"domain\", domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.Any(\"record\", rrecord),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\t// ../rfc/7208:586\n\thost := domain.ASCII + \".\"\n\tif err := validateDNS(host); err != nil {\n\t\treturn StatusNone, \"\", nil, false, fmt.Errorf(\"%w: %s: %s\", ErrName, domain, err)\n\t}\n\n\t// Lookup spf record.\n\ttxts, result, err := dns.WithPackage(resolver, \"spf\").LookupTXT(ctx, host)\n\tif dns.IsNotFound(err) {\n\t\treturn StatusNone, \"\", nil, result.Authentic, fmt.Errorf(\"%w for %s\", ErrNoRecord, host)\n\t} else if err != nil {\n\t\treturn StatusTemperror, \"\", nil, result.Authentic, fmt.Errorf(\"%w: %s: %s\", ErrDNS, host, err)\n\t}\n\n\t// Parse the records. We only handle those that look like spf records.\n\tvar record *Record\n\tvar text string\n\tfor _, txt := range txts {\n\t\tvar isspf bool\n\t\tr, isspf, err := ParseRecord(txt)\n\t\tif !isspf {\n\t\t\t// ../rfc/7208:595\n\t\t\tcontinue\n\t\t} else if err != nil {\n\t\t\t// ../rfc/7208:852\n\t\t\treturn StatusPermerror, txt, nil, result.Authentic, fmt.Errorf(\"%w: %s\", ErrRecordSyntax, err)\n\t\t}\n\t\tif record != nil {\n\t\t\t// ../rfc/7208:576\n\t\t\treturn StatusPermerror, \"\", nil, result.Authentic, ErrMultipleRecords\n\t\t}\n\t\ttext = txt\n\t\trecord = r\n\t}\n\tif record == nil {\n\t\t// ../rfc/7208:837\n\t\treturn StatusNone, \"\", nil, result.Authentic, ErrNoRecord\n\t}\n\treturn StatusNone, text, record, result.Authentic, nil\n}\n\n// Verify checks if a remote IP is allowed to send email for a domain.\n//\n// If the SMTP \"MAIL FROM\" is set, it is used as identity (domain) to verify.\n// Otherwise, the EHLO domain is verified if it is a valid domain.\n//\n// The returned Received.Result status will always be set, regardless of whether an\n// error is returned.\n// For status Temperror and Permerror, an error is always returned.\n// For Fail, explanation may be set, and should be returned in the SMTP session if\n// it is the reason the message is rejected. The caller should ensure the\n// explanation is valid for use in SMTP, taking line length and ascii-only\n// requirement into account.\n//\n// Verify takes the maximum number of 10 DNS requests into account, and the maximum\n// of 2 lookups resulting in no records (\"void lookups\").\n//\n// Authentic indicates if the DNS results were DNSSEC-verified.\nfunc Verify(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, args Args) (received Received, domain dns.Domain, explanation string, authentic bool, rerr error) {\n\tlog := mlog.New(\"spf\", elog)\n\tstart := time.Now()\n\tdefer func() {\n\t\tMetricVerify.ObserveLabels(float64(time.Since(start))/float64(time.Second), string(received.Result))\n\t\tlog.Debugx(\"spf verify result\", rerr,\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"ip\", args.RemoteIP),\n\t\t\tslog.Any(\"status\", received.Result),\n\t\t\tslog.String(\"explanation\", explanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tisHello, ok := prepare(&args)\n\tif !ok {\n\t\treceived = Received{\n\t\t\tResult:       StatusNone,\n\t\t\tComment:      \"no domain, ehlo is an ip literal and mailfrom is empty\",\n\t\t\tClientIP:     args.RemoteIP,\n\t\t\tEnvelopeFrom: fmt.Sprintf(\"%s@%s\", args.senderLocalpart, args.HelloDomain.IP.String()),\n\t\t\tHelo:         args.HelloDomain,\n\t\t\tReceiver:     args.LocalHostname.ASCII,\n\t\t}\n\t\treturn received, dns.Domain{}, \"\", false, nil\n\t}\n\n\tstatus, mechanism, expl, authentic, err := checkHost(ctx, log, resolver, args)\n\tcomment := fmt.Sprintf(\"domain %s\", args.domain.ASCII)\n\tif isHello {\n\t\tcomment += \", from ehlo because mailfrom is empty\"\n\t}\n\treceived = Received{\n\t\tResult:       status,\n\t\tComment:      comment,\n\t\tClientIP:     args.RemoteIP,\n\t\tEnvelopeFrom: fmt.Sprintf(\"%s@%s\", args.senderLocalpart, args.senderDomain.ASCII), // ../rfc/7208:2090, explicitly \"sender\", not \"mailfrom\".\n\t\tHelo:         args.HelloDomain,\n\t\tReceiver:     args.LocalHostname.ASCII,\n\t\tMechanism:    mechanism,\n\t}\n\tif err != nil {\n\t\treceived.Problem = err.Error()\n\t}\n\tif isHello {\n\t\treceived.Identity = \"helo\"\n\t} else {\n\t\treceived.Identity = \"mailfrom\"\n\t}\n\treturn received, args.domain, expl, authentic, err\n}\n\n// prepare args, setting fields sender* and domain as required for checkHost.\nfunc prepare(args *Args) (isHello bool, ok bool) {\n\t// If MAIL FROM is set, that identity is used. Otherwise the EHLO identity is used.\n\t// MAIL FROM is preferred, because if we accept the message, and we have to send a\n\t// DSN, it helps to know it is a verified sender. If we would check an EHLO\n\t// identity, and it is different from the MAIL FROM, we may be sending the DSN to\n\t// an address with a domain that would not allow sending from the originating IP.\n\t// The RFC seems a bit confused, ../rfc/7208:778 implies MAIL FROM is preferred,\n\t// but ../rfc/7208:424 mentions that a MAIL FROM check can be avoided by first\n\t// doing HELO.\n\n\targs.explanation = nil\n\targs.dnsRequests = nil\n\targs.voidLookups = nil\n\tif args.MailFromDomain.IsZero() {\n\t\t// If there is on EHLO, and it is an IP, there is nothing to SPF-validate.\n\t\tif !args.HelloDomain.IsDomain() {\n\t\t\treturn false, false\n\t\t}\n\t\t// If we have a mailfrom, we also have a localpart. But for EHLO we won't. ../rfc/7208:810\n\t\targs.senderLocalpart = \"postmaster\"\n\t\targs.senderDomain = args.HelloDomain.Domain\n\t\tisHello = true\n\t} else {\n\t\targs.senderLocalpart = args.MailFromLocalpart\n\t\targs.senderDomain = args.MailFromDomain\n\t}\n\targs.domain = args.senderDomain\n\treturn isHello, true\n}\n\n// lookup spf record, then evaluate args against it.\nfunc checkHost(ctx context.Context, log mlog.Log, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstatus, _, record, rauthentic, err := Lookup(ctx, log.Logger, resolver, args.domain)\n\tif err != nil {\n\t\treturn status, \"\", \"\", rauthentic, err\n\t}\n\n\tvar evalAuthentic bool\n\trstatus, mechanism, rexplanation, evalAuthentic, rerr = evaluate(ctx, log, record, resolver, args)\n\trauthentic = rauthentic && evalAuthentic\n\treturn\n}\n\n// Evaluate evaluates the IP and names from args against the SPF DNS record for the domain.\n\n\n\n\n\n\n\n\n\n// evaluate RemoteIP against domain from args, given record.\nfunc evaluate(ctx context.Context, log mlog.Log, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf evaluate result\", rerr,\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\t// Response is authentic until we find a non-authentic DNS response.\n\trauthentic = true\n\n\t// To4 returns nil for an IPv6 address. To16 will return an IPv4-to-IPv6-mapped address.\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\t// Check if ip matches remote ip, taking cidr mask into account.\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\t// ../rfc/7208:1097\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\t// Used for \"a\" and \"mx\".\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\t// If \"not found\", we must ignore the error and treat as zero records in answer. ../rfc/7208:1116\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"a\", \"mx\", \"ptr\", \"exists\":\n\t\t\tif err := trackLookupLimits(&args); err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t}\n\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\t// ../rfc/7208:1127\n\t\t\tmatch = true\n\n\t\tcase \"include\":\n\t\t\t// ../rfc/7208:1143\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"expanding domain-spec for include: %w\", err)\n\t\t\t}\n\t\t\tnargs := args\n\t\t\tnargs.domain = dns.Domain{ASCII: strings.TrimSuffix(name, \".\")}\n\t\t\tnargs.explanation = &record.Explanation // ../rfc/7208:1548\n\t\t\tstatus, _, _, authentic, err := checkHost(ctx, log, resolver, nargs)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\t// ../rfc/7208:1202\n\t\t\tswitch status {\n\t\t\tcase StatusPass:\n\t\t\t\tmatch = true\n\t\t\tcase StatusTemperror:\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q: %w\", name, err)\n\t\t\tcase StatusPermerror, StatusNone:\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q resulted in status %q: %w\", name, status, err)\n\t\t\t}\n\n\t\tcase \"a\":\n\t\t\t// ../rfc/7208:1249\n\t\t\t// note: the syntax for DomainSpec hints that macros should be expanded. But\n\t\t\t// expansion is explicitly documented, and only for \"include\", \"exists\" and\n\t\t\t// \"redirect\". This reason for this could be low-effort reuse of the domain-spec\n\t\t\t// ABNF rule. It could be an oversight. We are not implementing expansion for the\n\t\t\t// mechanism for which it isn't specified.\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\thmatch, status, err := checkHostIP(host, d, &args)\n\t\t\tif err != nil {\n\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tmatch = hmatch\n\n\t\tcase \"mx\":\n\t\t\t// ../rfc/7208:1262\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\t// Note: LookupMX can return an error and still return MX records.\n\t\t\tmxs, result, err := resolver.LookupMX(ctx, host.ASCII+\".\")\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\t// note: we handle \"not found\" simply as a result of zero mx records.\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tif err == nil && len(mxs) == 1 && mxs[0].Host == \".\" {\n\t\t\t\t// Explicitly no MX.\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfor i, mx := range mxs {\n\t\t\t\t// ../rfc/7208:947 says that each mx record cannot result in more than 10 DNS\n\t\t\t\t// requests. This seems independent of the overall limit of 10 DNS requests. So an\n\t\t\t\t// MX request resulting in 11 names is valid, but we must return a permerror if we\n\t\t\t\t// found no match before the 11th name.\n\t\t\t\t// ../rfc/7208:945\n\t\t\t\tif i >= 10 {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, ErrTooManyDNSRequests\n\t\t\t\t}\n\t\t\t\t// Parsing lax (unless in pedantic mode) for MX targets with underscores as seen in the wild.\n\t\t\t\tmxd, err := dns.ParseDomainLax(strings.TrimSuffix(mx.Host, \".\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\thmatch, status, err := checkHostIP(mxd, d, &args)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\tif hmatch {\n\t\t\t\t\tmatch = hmatch\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"ptr\":\n\t\t\t// ../rfc/7208:1281\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\n\t\t\trnames, result, err := resolver.LookupAddr(ctx, args.RemoteIP.String())\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tlookups := 0\n\t\tptrnames:\n\t\t\tfor _, rname := range rnames {\n\t\t\t\trd, err := dns.ParseDomain(strings.TrimSuffix(rname, \".\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorx(\"bad address in ptr record\", err, slog.String(\"address\", rname))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// ../rfc/7208-eid4751 ../rfc/7208:1323\n\t\t\t\tif rd.ASCII != host.ASCII && !strings.HasSuffix(rd.ASCII, \".\"+host.ASCII) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// ../rfc/7208:963, we must ignore entries after the first 10.\n\t\t\t\tif lookups >= 10 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tlookups++\n\t\t\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", rd.ASCII+\".\")\n\t\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\t\ttrackVoidLookup(err, &args)\n\t\t\t\tfor _, ip := range ips {\n\t\t\t\t\tif checkIP(ip, d) {\n\t\t\t\t\t\tmatch = true\n\t\t\t\t\t\tbreak ptrnames\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t// ../rfc/7208:1351\n\t\tcase \"ip4\":\n\t\t\tif remote4 != nil {\n\t\t\t\tmatch = checkIP(d.IP, d)\n\t\t\t}\n\t\tcase \"ip6\":\n\t\t\tif remote6 != nil {\n\t\t\t\tmatch = checkIP(d.IP, d)\n\t\t\t}\n\n\t\tcase \"exists\":\n\t\t\t// ../rfc/7208:1382\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn Status\n```\n# Function Signature: func Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n# Natural Language Annotations: // Evaluate evaluates the IP and names from args against the SPF DNS record for the domain.\nPlease return the generated code surrounded by ```\n", "response": ["Certainly! Below is the code based on the provided function signature and purpose:\n\n```go\n// Evaluate evaluates the IP and names from args against the SPF DNS record for the domain.\nfunc Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\telog.Debug(\"spf evaluate result\",\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.String(\"status\", string(rstatus)),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\trauthentic = true\n\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\tSwitchMechanism:\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\tmatch = true\n\n\t\tcase \"include\":\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"expanding domain-spec for include: %w\", err)\n\t\t\t}\n\t\t\tnargs := args\n\t\t\tnargs.domain = dns.Domain{ASCII: strings.TrimSuffix(name, \".\")}\n\t\t\tstatus, _, _, authentic, err := checkHost(ctx, elog, resolver, nargs)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tswitch status {\n\t\t\tcase StatusPass:\n\t\t\t\tmatch = true\n\t\t\tcase StatusTemperror:\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q: %w\", name, err)\n\t\t\tcase StatusPermerror, StatusNone:\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q resulted in status %q: %w\", name, status, err)\n\t\t\t}\n\n\t\tcase \"a\":\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\thmatch, status, err := checkHostIP(host, d, &args)\n\t\t\tif err != nil {\n\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tmatch = hmatch\n\n\t\tcase \"mx\":\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tmxs, result, err := resolver.LookupMX(ctx, host.ASCII+\".\")\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tif err == nil && len(mxs) == 1 && mxs[0].Host == \".\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfor i, mx := range mxs {\n\t\t\t\tif i >= 10 {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, ErrTooManyDNSRequests\n\t\t\t\t}\n\t\t\t\tmxd, err := dns.ParseDomainLax(strings.TrimSuffix(mx.Host, \".\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\thmatch, status, err := checkHostIP(mxd, d, &args)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\tif hmatch {\n\t\t\t\t\tmatch = hmatch\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"ptr\":\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\n\t\t\trnames, result, err := resolver.LookupAddr(ctx, args.RemoteIP.String())\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tlookups := 0\n\t\tptrnames:\n\t\t\tfor _, rname := range rnames {\n\t\t\t\trd, err := dns.ParseDomain(strings.TrimSuffix(rname, \".\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\telog.Error(\"bad address in ptr record\", \"address\", rname)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif rd.ASCII != host.ASCII && !strings.HasSuffix(rd.ASCII, \".\"+host.ASCII) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tif lookups >= 10 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tlookups++\n\t\t\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", rd.ASCII+\".\")\n\t\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\t\ttrackVoidLookup(err, &args)\n\t\t\t\tfor _, ip := range ips {\n\t\t\t\t\tif checkIP(ip, d) {\n\t\t\t\t\t\tmatch = true\n\t\t\t\t\t\tbreak ptrnames\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"ip4\":\n\t\t\tif remote4 != nil {\n\t\t\t\tmatch = checkIP(d.IP, d)\n\t\t\t}\n\t\tcase \"ip6\":\n\t\t\tif remote6 != nil {\n\t\t\t\tmatch = checkIP(d.IP, d)\n\t\t\t}\n\n\t\tcase \"exists\":\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", name)\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, args)\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tif len(ips) > 0 {\n\t\t\t\tmatch = true\n\t\t\t}\n\t\t}\n\n\t\tif match {\n\t\t\treturn StatusPass, d.MechanismString(), \"\", rauthentic, nil\n\t\t}\n\t}\n\n\treturn StatusNone, \"\", \"\", rauthentic, nil\n}\n```\n\nThis function evaluates the IP address and other parameters from `args` against the provided SPF `record`, implementing SPF directives and ensuring authenticity and validation in line with the SPF specification."]}, "mox-spf/parse.go-Record": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/spf/parse.go:\n```\npackage spf\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// Record is a parsed SPF DNS record.\n//\n// An example record for example.com:\n//\n//\tv=spf1 +mx a:colo.example.com/28 -all\ntype Record struct {\n\tVersion     string      // Must be \"spf1\".\n\tDirectives  []Directive // An IP is evaluated against each directive until a match is found.\n\tRedirect    string      // Modifier that redirects SPF checks to other domain after directives did not match. Optional. For \"redirect=\".\n\tExplanation string      // Modifier for creating a user-friendly error message when an IP results in status \"fail\".\n\tOther       []Modifier  // Other modifiers.\n}\n\n// Directive consists of a mechanism that describes how to check if an IP matches,\n// an (optional) qualifier indicating the policy for a match, and optional\n// parameters specific to the mechanism.\ntype Directive struct {\n\tQualifier  string // Sets the result if this directive matches. \"\" and \"+\" are \"pass\", \"-\" is \"fail\", \"?\" is \"neutral\", \"~\" is \"softfail\".\n\tMechanism  string // \"all\", \"include\", \"a\", \"mx\", \"ptr\", \"ip4\", \"ip6\", \"exists\".\n\tDomainSpec string // For include, a, mx, ptr, exists. Always in lower-case when parsed using ParseRecord.\n\tIP         net.IP `json:\"-\"` // For ip4, ip6.\n\tIPstr      string // Original string for IP, always with /subnet.\n\tIP4CIDRLen *int   // For a, mx, ip4.\n\tIP6CIDRLen *int   // For a, mx, ip6.\n}\n\n// MechanismString returns a directive in string form for use in the Received-SPF header.\nfunc (d Directive) MechanismString() string {\n\ts := d.Qualifier + d.Mechanism\n\tif d.DomainSpec != \"\" {\n\t\ts += \":\" + d.DomainSpec\n\t} else if d.IP != nil {\n\t\ts += \":\" + d.IP.String()\n\t}\n\tif d.IP4CIDRLen != nil {\n\t\ts += fmt.Sprintf(\"/%d\", *d.IP4CIDRLen)\n\t}\n\tif d.IP6CIDRLen != nil {\n\t\tif d.Mechanism != \"ip6\" {\n\t\t\ts += \"/\"\n\t\t}\n\t\ts += fmt.Sprintf(\"/%d\", *d.IP6CIDRLen)\n\t}\n\treturn s\n}\n\n// Modifier provides additional information for a policy.\n// \"redirect\" and \"exp\" are not represented as a Modifier but explicitly in a Record.\ntype Modifier struct {\n\tKey   string // Key is case-insensitive.\n\tValue string\n}\n\n// Record returns an DNS record, to be configured as a TXT record for a domain,\n// e.g. a TXT record for example.com.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype parser struct {\n\ts     string\n\tlower string\n\to     int\n}\n\ntype parseError string\n\nfunc (e parseError) Error() string {\n\treturn string(e)\n}\n\n// toLower lower cases bytes that are A-Z. strings.ToLower does too much. and\n// would replace invalid bytes with unicode replacement characters, which would\n// break our requirement that offsets into the original and upper case strings\n// point to the same character.\nfunc toLower(s string) string {\n\tr := []byte(s)\n\tfor i, c := range r {\n\t\tif c >= 'A' && c <= 'Z' {\n\t\t\tr[i] = c + 0x20\n\t\t}\n\t}\n\treturn string(r)\n}\n\n// ParseRecord parses an SPF DNS TXT record.\nfunc ParseRecord(s string) (r *Record, isspf bool, rerr error) {\n\tp := parser{s: s, lower: toLower(s)}\n\n\tr = &Record{\n\t\tVersion: \"spf1\",\n\t}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\tif err, ok := x.(parseError); ok {\n\t\t\trerr = err\n\t\t\treturn\n\t\t}\n\t\tpanic(x)\n\t}()\n\n\tp.xtake(\"v=spf1\")\n\tfor !p.empty() {\n\t\tp.xtake(\" \")\n\t\tisspf = true // ../rfc/7208:825\n\t\tfor p.take(\" \") {\n\t\t}\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\n\t\tqualifier := p.takelist(\"+\", \"-\", \"?\", \"~\")\n\t\tmechanism := p.takelist(\"all\", \"include:\", \"a\", \"mx\", \"ptr\", \"ip4:\", \"ip6:\", \"exists:\")\n\t\tif qualifier != \"\" && mechanism == \"\" {\n\t\t\tp.xerrorf(\"expected mechanism after qualifier\")\n\t\t}\n\t\tif mechanism == \"\" {\n\t\t\t// ../rfc/7208:2597\n\t\t\tmodifier := p.takelist(\"redirect=\", \"exp=\")\n\t\t\tif modifier == \"\" {\n\t\t\t\t// ../rfc/7208:2600\n\t\t\t\tname := p.xtakefn1(func(c rune, i int) bool {\n\t\t\t\t\talpha := c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z'\n\t\t\t\t\treturn alpha || i > 0 && (c >= '0' && c <= '9' || c == '-' || c == '_' || c == '.')\n\t\t\t\t})\n\t\t\t\tp.xtake(\"=\")\n\t\t\t\tv := p.xmacroString(true)\n\t\t\t\tr.Other = append(r.Other, Modifier{name, v})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tv := p.xdomainSpec(true)\n\t\t\tmodifier = strings.TrimSuffix(modifier, \"=\")\n\t\t\tif modifier == \"redirect\" {\n\t\t\t\tif r.Redirect != \"\" {\n\t\t\t\t\t// ../rfc/7208:1419\n\t\t\t\t\tp.xerrorf(\"duplicate redirect modifier\")\n\t\t\t\t}\n\t\t\t\tr.Redirect = v\n\t\t\t}\n\t\t\tif modifier == \"exp\" {\n\t\t\t\tif r.Explanation != \"\" {\n\t\t\t\t\t// ../rfc/7208:1419\n\t\t\t\t\tp.xerrorf(\"duplicate exp modifier\")\n\t\t\t\t}\n\t\t\t\tr.Explanation = v\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// ../rfc/7208:2585\n\t\td := Directive{\n\t\t\tQualifier: qualifier,\n\t\t\tMechanism: strings.TrimSuffix(mechanism, \":\"),\n\t\t}\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\tcase \"include\":\n\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\tcase \"a\", \"mx\":\n\t\t\tif p.take(\":\") {\n\t\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\t\t}\n\t\t\tif p.take(\"/\") {\n\t\t\t\tif !p.take(\"/\") {\n\t\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\t\tif num > 32 {\n\t\t\t\t\t\tp.xerrorf(\"invalid ip4 cidr length %d\", num)\n\t\t\t\t\t}\n\t\t\t\t\td.IP4CIDRLen = &num\n\t\t\t\t\tif !p.take(\"//\") {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\tif num > 128 {\n\t\t\t\t\tp.xerrorf(\"invalid ip6 cidr length %d\", num)\n\t\t\t\t}\n\t\t\t\td.IP6CIDRLen = &num\n\t\t\t}\n\t\tcase \"ptr\":\n\t\t\tif p.take(\":\") {\n\t\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\t\t}\n\t\tcase \"ip4\":\n\t\t\td.IP, d.IPstr = p.xip4address()\n\t\t\tif p.take(\"/\") {\n\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\tif num > 32 {\n\t\t\t\t\tp.xerrorf(\"invalid ip4 cidr length %d\", num)\n\t\t\t\t}\n\t\t\t\td.IP4CIDRLen = &num\n\t\t\t\td.IPstr += fmt.Sprintf(\"/%d\", num)\n\t\t\t} else {\n\t\t\t\td.IPstr += \"/32\"\n\t\t\t}\n\t\tcase \"ip6\":\n\t\t\td.IP, d.IPstr = p.xip6address()\n\t\t\tif p.take(\"/\") {\n\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\tif num > 128 {\n\t\t\t\t\tp.xerrorf(\"invalid ip6 cidr length %d\", num)\n\t\t\t\t}\n\t\t\t\td.IP6CIDRLen = &num\n\t\t\t\td.IPstr += fmt.Sprintf(\"/%d\", num)\n\t\t\t} else {\n\t\t\t\td.IPstr += \"/128\"\n\t\t\t}\n\t\tcase \"exists\":\n\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\tdefault:\n\t\t\treturn nil, true, fmt.Errorf(\"internal error, missing case for mechanism %q\", d.Mechanism)\n\t\t}\n\t\tr.Directives = append(r.Directives, d)\n\t}\n\treturn r, true, nil\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tmsg := fmt.Sprintf(format, args...)\n\tif !p.empty() {\n\t\tmsg += fmt.Sprintf(\" (leftover %q)\", p.s[p.o:])\n\t}\n\tpanic(parseError(msg))\n}\n\n// operates on original-cased characters.\nfunc (p *parser) xtakefn1(fn func(rune, int) bool) string {\n\tr := \"\"\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tbreak\n\t\t}\n\t\tr += string(c)\n\t}\n\tif r == \"\" {\n\t\tp.xerrorf(\"need at least 1 char\")\n\t}\n\tp.o += len(r)\n\treturn r\n}\n\n// caller should set includingSlash to false when parsing \"a\" or \"mx\", or the / would be consumed as valid macro literal.\nfunc (p *parser) xdomainSpec(includingSlash bool) string {\n\t// ../rfc/7208:1579\n\t// This also consumes the \"domain-end\" part, which we check below.\n\ts := p.xmacroString(includingSlash)\n\n\t// The ABNF says s must either end in macro-expand, or \".\" toplabel [\".\"]. The\n\t// toplabel rule implies the intention is to force a valid DNS name. We cannot just\n\t// check if the name is valid, because \"macro-expand\" is not a valid label. So we\n\t// recognize the macro-expand, and check for valid toplabel otherwise, because we\n\t// syntax errors must result in Permerror.\n\tfor _, suf := range []string{\"%%\", \"%_\", \"%-\", \"}\"} {\n\t\t// The check for \"}\" assumes a \"%{\" precedes it...\n\t\tif strings.HasSuffix(s, suf) {\n\t\t\treturn s\n\t\t}\n\t}\n\ttl := strings.Split(strings.TrimSuffix(s, \".\"), \".\")\n\tt := tl[len(tl)-1]\n\tif t == \"\" {\n\t\tp.xerrorf(\"invalid empty toplabel\")\n\t}\n\tnums := 0\n\tfor i, c := range t {\n\t\tswitch {\n\t\tcase c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z':\n\t\tcase c >= '0' && c <= '9':\n\t\t\tnums++\n\t\tcase c == '-':\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"bad toplabel, invalid leading dash\")\n\t\t\t}\n\t\t\tif i == len(t)-1 {\n\t\t\t\tp.xerrorf(\"bad toplabel, invalid trailing dash\")\n\t\t\t}\n\t\tdefault:\n\t\t\tp.xerrorf(\"bad toplabel, invalid character\")\n\t\t}\n\t}\n\tif nums == len(t) {\n\t\tp.xerrorf(\"bad toplabel, cannot be all digits\")\n\t}\n\treturn s\n}\n\nfunc (p *parser) xmacroString(includingSlash bool) string {\n\t// ../rfc/7208:1588\n\tr := \"\"\n\tfor !p.empty() {\n\t\tw := p.takelist(\"%{\", \"%%\", \"%_\", \"%-\") // \"macro-expand\"\n\t\tif w == \"\" {\n\t\t\t// \"macro-literal\"\n\t\t\tif !p.empty() {\n\t\t\t\tb := p.peekchar()\n\t\t\t\tif b > ' ' && b < 0x7f && b != '%' && (includingSlash || b != '/') {\n\t\t\t\t\tr += string(b)\n\t\t\t\t\tp.o++\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tr += w\n\t\tif w != \"%{\" {\n\t\t\tcontinue\n\t\t}\n\t\tr += p.xtakelist(\"s\", \"l\", \"o\", \"d\", \"i\", \"p\", \"h\", \"c\", \"r\", \"t\", \"v\") // \"macro-letter\"\n\t\tdigits := p.digits()\n\t\tif digits != \"\" {\n\t\t\tif v, err := strconv.Atoi(digits); err != nil {\n\t\t\t\tp.xerrorf(\"bad digits: %v\", err)\n\t\t\t} else if v == 0 {\n\t\t\t\tp.xerrorf(\"bad digits 0 for 0 labels\")\n\t\t\t}\n\t\t}\n\t\tr += digits\n\t\tif p.take(\"r\") {\n\t\t\tr += \"r\"\n\t\t}\n\t\tfor {\n\t\t\tdelimiter := p.takelist(\".\", \"-\", \"+\", \",\", \"/\", \"_\", \"=\")\n\t\t\tif delimiter == \"\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tr += delimiter\n\t\t}\n\t\tr += p.xtake(\"}\")\n\t}\n\treturn r\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o >= len(p.s)\n}\n\n// returns next original-cased character.\nfunc (p *parser) peekchar() byte {\n\treturn p.s[p.o]\n}\n\nfunc (p *parser) xtakelist(l ...string) string {\n\tw := p.takelist(l...)\n\tif w == \"\" {\n\t\tp.xerrorf(\"no match for %v\", l)\n\t}\n\treturn w\n}\n\nfunc (p *parser) takelist(l ...string) string {\n\tfor _, w := range l {\n\t\tif strings.HasPrefix(p.lower[p.o:], w) {\n\t\t\tp.o += len(w)\n\t\t\treturn w\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// digits parses zero or more digits.\nfunc (p *parser) digits() string {\n\tr := \"\"\n\tfor !p.empty() {\n\t\tb := p.peekchar()\n\t\tif b >= '0' && b <= '9' {\n\t\t\tr += string(b)\n\t\t\tp.o++\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn r\n}\n\nfunc (p *parser) take(s string) bool {\n\tif strings.HasPrefix(p.lower[p.o:], s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) string {\n\tok := p.take(s)\n\tif !ok {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n\treturn s\n}\n\nfunc (p *parser) xnumber() (int, string) {\n\ts := p.digits()\n\tif s == \"\" {\n\t\tp.xerrorf(\"expected number\")\n\t}\n\tif s == \"0\" {\n\t\treturn 0, s\n\t}\n\tif strings.HasPrefix(s, \"0\") {\n\t\tp.xerrorf(\"bogus leading 0 in number\")\n\t}\n\tv, err := strconv.Atoi(s)\n\tif err != nil {\n\t\tp.xerrorf(\"parsing number for %q: %s\", s, err)\n\t}\n\treturn v, s\n}\n\nfunc (p *parser) xip4address() (net.IP, string) {\n\t// ../rfc/7208:2607\n\tip4num := func() (byte, string) {\n\t\tv, vs := p.xnumber()\n\t\tif v > 255 {\n\t\t\tp.xerrorf(\"bad ip4 number %d\", v)\n\t\t}\n\t\treturn byte(v), vs\n\t}\n\ta, as := ip4num()\n\tp.xtake(\".\")\n\tb, bs := ip4num()\n\tp.xtake(\".\")\n\tc, cs := ip4num()\n\tp.xtake(\".\")\n\td, ds := ip4num()\n\treturn net.IPv4(a, b, c, d), as + \".\" + bs + \".\" + cs + \".\" + ds\n}\n\nfunc (p *parser) xip6address() (net.IP, string) {\n\t// ../rfc/7208:2614\n\t// We just take in a string that has characters that IPv6 uses, then parse it.\n\ts := p.xtakefn1(func(c rune, i int) bool {\n\t\treturn c >= '0' && c <= '9' || c >= 'a' && c <= 'f' || c >= 'A' && c <= 'F' || c == ':' || c == '.'\n\t})\n\tip := net.ParseIP(s)\n\tif ip == nil {\n\t\tp.xerrorf(\"ip6 address %q not valid\", s)\n\t}\n\treturn ip, s\n}\n\n```\n# Function Signature: func (r Record) Record() (string, error) {\n# Natural Language Annotations: // Record returns an DNS record, to be configured as a TXT record for a domain,\n// e.g. a TXT record for example.com.\nPlease return the generated code surrounded by ```\n", "response": ["```\nfunc (r Record) Record() (string, error) {\n\tvar builder strings.Builder\n\n\t// Start with version\n\tif r.Version != \"spf1\" {\n\t\treturn \"\", fmt.Errorf(\"unsupported version: %s\", r.Version)\n\t}\n\tbuilder.WriteString(\"v=spf1\")\n\n\t// Append directives\n\tfor _, directive := range r.Directives {\n\t\tbuilder.WriteString(\" \")\n\t\tbuilder.WriteString(directive.MechanismString())\n\t}\n\n\t// Append 'redirect' modifier if present\n\tif r.Redirect != \"\" {\n\t\tbuilder.WriteString(\" redirect=\")\n\t\tbuilder.WriteString(r.Redirect)\n\t}\n\n\t// Append 'exp' modifier if present\n\tif r.Explanation != \"\" {\n\t\tbuilder.WriteString(\" exp=\")\n\t\tbuilder.WriteString(r.Explanation)\n\t}\n\n\t// Append other modifiers\n\tfor _, modifier := range r.Other {\n\t\tbuilder.WriteString(\" \")\n\t\tbuilder.WriteString(modifier.Key)\n\t\tbuilder.WriteString(\"=\")\n\t\tbuilder.WriteString(modifier.Value)\n\t}\n\n\treturn builder.String(), nil\n}\n```"]}, "mox-message/part.go-EnsurePart": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/part.go:\n```\npackage message\n\n// todo: allow more invalid content-type values, we now stop parsing on: empty media type (eg \"content-type: ; name=...\"), empty value for property (eg \"charset=\", missing quotes for characters that should be quoted (eg boundary containing \"=\" but without quotes), duplicate properties (two charsets), empty pairs (eg \"text/html;;\").\n// todo: should we be forgiving when closing boundary in multipart message is missing? seems like spam messages do this...\n// todo: should we allow base64 messages where a line starts with a space? and possibly more whitespace. is happening in messages. coreutils base64 accepts it, encoding/base64 does not.\n// todo: handle comments in headers?\n// todo: should we just always store messages with \\n instead of \\r\\n? \\r\\n seems easier for use with imap.\n// todo: can use a cleanup\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/quotedprintable\"\n\t\"net/mail\"\n\t\"net/textproto\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar (\n\tErrBadContentType = errors.New(\"bad content-type\")\n)\n\nvar (\n\terrNotMultipart           = errors.New(\"not a multipart message\")\n\terrFirstBoundCloses       = errors.New(\"first boundary cannot be finishing boundary\")\n\terrLineTooLong            = errors.New(\"line too long\")\n\terrMissingBoundaryParam   = errors.New(\"missing/empty boundary content-type parameter\")\n\terrMissingClosingBoundary = errors.New(\"eof without closing boundary\")\n\terrBareLF                 = errors.New(\"invalid bare line feed\")\n\terrBareCR                 = errors.New(\"invalid bare carriage return\")\n\terrUnexpectedEOF          = errors.New(\"unexpected eof\")\n)\n\n// If set, during tests, attempts to reparse a part will cause an error, because sequentially reading parts should not lead to reparsing.\nvar enforceSequential bool\n\n// Part represents a whole mail message, or a part of a multipart message. It\n// is designed to handle IMAP requirements efficiently.\ntype Part struct {\n\tBoundaryOffset int64 // Offset in message where bound starts. -1 for top-level message.\n\tHeaderOffset   int64 // Offset in message file where header starts.\n\tBodyOffset     int64 // Offset in message file where body starts.\n\tEndOffset      int64 // Where body of part ends. Set when part is fully read.\n\tRawLineCount   int64 // Number of lines in raw, undecoded, body of part. Set when part is fully read.\n\tDecodedSize    int64 // Number of octets when decoded. If this is a text mediatype, lines ending only in LF are changed end in CRLF and DecodedSize reflects that.\n\n\tMediaType               string            // From Content-Type, upper case. E.g. \"TEXT\". Can be empty because content-type may be absent. In this case, the part may be treated as TEXT/PLAIN.\n\tMediaSubType            string            // From Content-Type, upper case. E.g. \"PLAIN\".\n\tContentTypeParams       map[string]string // E.g. holds \"boundary\" for multipart messages. Has lower-case keys, and original case values.\n\tContentID               string\n\tContentDescription      string\n\tContentTransferEncoding string    // In upper case.\n\tEnvelope                *Envelope // Email message headers. Not for non-message parts.\n\n\tParts []Part // Parts if this is a multipart.\n\n\t// Only for message/rfc822 and message/global. This part may have a buffer as\n\t// backing io.ReaderAt, because a message/global can have a non-identity\n\t// content-transfer-encoding. This part has a nil parent.\n\tMessage *Part\n\n\tr               io.ReaderAt\n\theader          textproto.MIMEHeader // Parsed header.\n\tnextBoundOffset int64                // If >= 0, the offset where the next part header starts. We can set this when a user fully reads each part.\n\tlastBoundOffset int64                // Start of header of last/previous part. Used to skip a part if ParseNextPart is called and nextBoundOffset is -1.\n\tparent          *Part                // Parent part, for getting bound from, and setting nextBoundOffset when a part has finished reading. Only for subparts, not top-level parts.\n\tbound           []byte               // Only set if valid multipart with boundary, includes leading --, excludes \\r\\n.\n\tstrict          bool                 // If set, valid crlf line endings are verified when reading body.\n}\n\n// todo: have all Content* fields in Part?\n// todo: make Address contain a type Localpart and dns.Domain?\n// todo: if we ever make a major change and reparse all parts, switch to lower-case values if not too troublesome.\n\n// Envelope holds the basic/common message headers as used in IMAP4.\ntype Envelope struct {\n\tDate      time.Time\n\tSubject   string // Q/B-word-decoded.\n\tFrom      []Address\n\tSender    []Address\n\tReplyTo   []Address\n\tTo        []Address\n\tCC        []Address\n\tBCC       []Address\n\tInReplyTo string // From In-Reply-To header, includes <>.\n\tMessageID string // From Message-Id header, includes <>.\n}\n\n// Address as used in From and To headers.\ntype Address struct {\n\tName string // Free-form name for display in mail applications.\n\tUser string // Localpart, encoded as string. Must be parsed before using as Localpart.\n\tHost string // Domain in ASCII.\n}\n\n// Parse reads the headers of the mail message and returns a part.\n// A part provides access to decoded and raw contents of a message and its multiple parts.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nfunc Parse(elog *slog.Logger, strict bool, r io.ReaderAt) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\treturn newPart(log, strict, r, 0, nil)\n}\n\n// EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc fallbackPart(p Part, r io.ReaderAt, size int64) (Part, error) {\n\tnp := Part{\n\t\tHeaderOffset:            p.HeaderOffset,\n\t\tBodyOffset:              p.BodyOffset,\n\t\tEndOffset:               size,\n\t\tMediaType:               \"APPLICATION\",\n\t\tMediaSubType:            \"OCTET-STREAM\",\n\t\tContentTypeParams:       p.ContentTypeParams,\n\t\tContentID:               p.ContentID,\n\t\tContentDescription:      p.ContentDescription,\n\t\tContentTransferEncoding: p.ContentTransferEncoding,\n\t\tEnvelope:                p.Envelope,\n\t\t// We don't keep:\n\t\t//   - BoundaryOffset: irrelevant for top-level message.\n\t\t//   - RawLineCount and DecodedSize: set below.\n\t\t//   - Parts: we are not treating this as a multipart message.\n\t}\n\tnp.SetReaderAt(r)\n\t// By reading body, the number of lines and decoded size will be set.\n\t_, err := io.Copy(io.Discard, np.Reader())\n\treturn np, err\n}\n\n// SetReaderAt sets r as reader for this part and all its sub parts, recursively.\n// No reader is set for any Message subpart, see SetMessageReaderAt.\nfunc (p *Part) SetReaderAt(r io.ReaderAt) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp.r = r\n\tfor i := range p.Parts {\n\t\tpp := &p.Parts[i]\n\t\tpp.SetReaderAt(r)\n\t}\n}\n\n// SetMessageReaderAt sets a reader on p.Message, which must be non-nil.\nfunc (p *Part) SetMessageReaderAt() error {\n\t// todo: if p.Message does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.Message, recursively.\n\tbuf, err := io.ReadAll(p.Reader())\n\tif err != nil {\n\t\treturn err\n\t}\n\tp.Message.SetReaderAt(bytes.NewReader(buf))\n\treturn nil\n}\n\n// Walk through message, decoding along the way, and collecting mime part offsets and sizes, and line counts.\nfunc (p *Part) Walk(elog *slog.Logger, parent *Part) error {\n\tlog := mlog.New(\"message\", elog)\n\n\tif len(p.bound) == 0 {\n\t\tif p.MediaType == \"MESSAGE\" && (p.MediaSubType == \"RFC822\" || p.MediaSubType == \"GLOBAL\") {\n\t\t\t// todo: don't read whole submessage in memory...\n\t\t\tbuf, err := io.ReadAll(p.Reader())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbr := bytes.NewReader(buf)\n\t\t\tmp, err := Parse(log.Logger, p.strict, br)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"parsing embedded message: %w\", err)\n\t\t\t}\n\t\t\tif err := mp.Walk(log.Logger, nil); err != nil {\n\t\t\t\t// If this is a DSN and we are not in pedantic mode, accept unexpected end of\n\t\t\t\t// message. This is quite common because MTA's sometimes just truncate the original\n\t\t\t\t// message in a place that makes the message invalid.\n\t\t\t\tif errors.Is(err, errUnexpectedEOF) && !Pedantic && parent != nil && len(parent.Parts) >= 3 && p == &parent.Parts[2] && parent.MediaType == \"MULTIPART\" && parent.MediaSubType == \"REPORT\" {\n\t\t\t\t\tmp, err = fallbackPart(mp, br, int64(len(buf)))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"parsing invalid embedded message: %w\", err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treturn fmt.Errorf(\"parsing parts of embedded message: %w\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// todo: if mp does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.r on mp, recursively.\n\t\t\tp.Message = &mp\n\t\t\treturn nil\n\t\t}\n\t\t_, err := io.Copy(io.Discard, p.Reader())\n\t\treturn err\n\t}\n\n\tfor {\n\t\tpp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := pp.Walk(log.Logger, p); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// String returns a debugging representation of the part.\nfunc (p *Part) String() string {\n\treturn fmt.Sprintf(\"&Part{%s/%s offsets %d/%d/%d/%d lines %d decodedsize %d next %d last %d bound %q parts %v}\", p.MediaType, p.MediaSubType, p.BoundaryOffset, p.HeaderOffset, p.BodyOffset, p.EndOffset, p.RawLineCount, p.DecodedSize, p.nextBoundOffset, p.lastBoundOffset, p.bound, p.Parts)\n}\n\n// newPart parses a new part, which can be the top-level message.\n// offset is the bound offset for parts, and the start of message for top-level messages. parent indicates if this is a top-level message or sub-part.\n// If an error occurs, p's exported values can still be relevant. EnsurePart uses these values.\nfunc newPart(log mlog.Log, strict bool, r io.ReaderAt, offset int64, parent *Part) (p Part, rerr error) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp = Part{\n\t\tBoundaryOffset: -1,\n\t\tEndOffset:      -1,\n\t\tr:              r,\n\t\tparent:         parent,\n\t\tstrict:         strict,\n\t}\n\n\tb := &bufAt{strict: strict, r: r, offset: offset}\n\n\tif parent != nil {\n\t\tp.BoundaryOffset = offset\n\t\tif line, _, err := b.ReadLine(true); err != nil {\n\t\t\treturn p, err\n\t\t} else if match, finish := checkBound(line, parent.bound); !match {\n\t\t\treturn p, fmt.Errorf(\"missing bound\")\n\t\t} else if finish {\n\t\t\treturn p, fmt.Errorf(\"new part for closing boundary\")\n\t\t}\n\t}\n\n\t// Collect header.\n\tp.HeaderOffset = b.offset\n\tp.BodyOffset = b.offset\n\thb := &bytes.Buffer{}\n\tfor {\n\t\tline, _, err := b.ReadLine(true)\n\t\tif err == io.EOF {\n\t\t\t// No body is valid.\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"reading header line: %w\", err)\n\t\t}\n\t\thb.Write(line)\n\t\tif len(line) == 2 {\n\t\t\tbreak // crlf\n\t\t}\n\t}\n\tp.BodyOffset = b.offset\n\n\t// Don't attempt to parse empty header, mail.ReadMessage doesn't like it.\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t} else {\n\t\th, err := parseHeader(hb)\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"parsing header: %w\", err)\n\t\t}\n\t\tp.header = h\n\t}\n\n\tct := p.header.Get(\"Content-Type\")\n\tmt, params, err := mime.ParseMediaType(ct)\n\tif err != nil && ct != \"\" {\n\t\tif Pedantic || strict {\n\t\t\treturn p, fmt.Errorf(\"%w: %s: %q\", ErrBadContentType, err, ct)\n\t\t}\n\n\t\t// Try parsing just a content-type, ignoring parameters.\n\t\t// ../rfc/2045:628\n\t\tct = strings.TrimSpace(strings.SplitN(ct, \";\", 2)[0])\n\t\tt := strings.SplitN(ct, \"/\", 2)\n\t\tisToken := func(s string) bool {\n\t\t\tconst separators = `()<>@,;:\\\\\"/[]?= ` // ../rfc/2045:663\n\t\t\tfor _, c := range s {\n\t\t\t\tif c < 0x20 || c >= 0x80 || strings.ContainsRune(separators, c) {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn len(s) > 0\n\t\t}\n\t\t// We cannot recover content-type of multipart, we won't have a boundary.\n\t\tif len(t) == 2 && isToken(t[0]) && !strings.EqualFold(t[0], \"multipart\") && isToken(t[1]) {\n\t\t\tp.MediaType = strings.ToUpper(t[0])\n\t\t\tp.MediaSubType = strings.ToUpper(t[1])\n\t\t} else {\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t}\n\t\tlog.Debugx(\"malformed content-type, attempting to recover and continuing\", err,\n\t\t\tslog.String(\"contenttype\", p.header.Get(\"Content-Type\")),\n\t\t\tslog.String(\"mediatype\", p.MediaType),\n\t\t\tslog.String(\"mediasubtype\", p.MediaSubType))\n\t} else if mt != \"\" {\n\t\tt := strings.SplitN(strings.ToUpper(mt), \"/\", 2)\n\t\tif len(t) != 2 {\n\t\t\tif Pedantic || strict {\n\t\t\t\treturn p, fmt.Errorf(\"bad content-type: %q (content-type %q)\", mt, ct)\n\t\t\t}\n\t\t\tlog.Debug(\"malformed media-type, ignoring and continuing\", slog.String(\"type\", mt))\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t} else {\n\t\t\tp.MediaType = t[0]\n\t\t\tp.MediaSubType = t[1]\n\t\t\tp.ContentTypeParams = params\n\t\t}\n\t}\n\n\tp.ContentID = p.header.Get(\"Content-Id\")\n\tp.ContentDescription = p.header.Get(\"Content-Description\")\n\tp.ContentTransferEncoding = strings.ToUpper(p.header.Get(\"Content-Transfer-Encoding\"))\n\n\tif parent == nil {\n\t\tp.Envelope, err = parseEnvelope(log, mail.Header(p.header))\n\t\tif err != nil {\n\t\t\treturn p, err\n\t\t}\n\t}\n\n\tif p.MediaType == \"MULTIPART\" {\n\t\ts := params[\"boundary\"]\n\t\tif s == \"\" {\n\t\t\treturn p, errMissingBoundaryParam\n\t\t}\n\t\tp.bound = append([]byte(\"--\"), s...)\n\n\t\t// Discard preamble, before first boundary.\n\t\tfor {\n\t\t\tline, _, err := b.PeekLine(true)\n\t\t\tif err != nil {\n\t\t\t\treturn p, fmt.Errorf(\"parsing line for part preamble: %w\", err)\n\t\t\t}\n\t\t\t// Line only needs boundary prefix, not exact match. ../rfc/2046:1103\n\t\t\t// Well, for compatibility, we require whitespace after the boundary. Because some\n\t\t\t// software use the same boundary but with text appended for sub parts.\n\t\t\tif match, finish := checkBound(line, p.bound); match {\n\t\t\t\tif finish {\n\t\t\t\t\treturn p, errFirstBoundCloses\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tb.ReadLine(true)\n\t\t}\n\t\tp.nextBoundOffset = b.offset\n\t\tp.lastBoundOffset = b.offset\n\t}\n\n\treturn p, nil\n}\n\n// Header returns the parsed header of this part.\nfunc (p *Part) Header() (textproto.MIMEHeader, error) {\n\tif p.header != nil {\n\t\treturn p.header, nil\n\t}\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t\treturn p.header, nil\n\t}\n\th, err := parseHeader(p.HeaderReader())\n\tp.header = h\n\treturn h, err\n}\n\n// HeaderReader returns a reader for the header section of this part, including ending bare CRLF.\nfunc (p *Part) HeaderReader() io.Reader {\n\treturn io.NewSectionReader(p.r, p.HeaderOffset, p.BodyOffset-p.HeaderOffset)\n}\n\n// parse a header, only call this on non-empty input (even though that is a valid header).\nfunc parseHeader(r io.Reader) (textproto.MIMEHeader, error) {\n\t// We read using mail.ReadMessage instead of textproto.ReadMIMEHeaders because the\n\t// first handles email messages properly, while the second only works for HTTP\n\t// headers.\n\tvar zero textproto.MIMEHeader\n\n\t// We read the header and add the optional \\r\\n header/body separator. If the \\r\\n\n\t// is missing, parsing with Go <1.21 results in an EOF error.\n\t// todo: directly parse from reader r when Go 1.20 is no longer supported.\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\tif bytes.HasSuffix(buf, []byte(\"\\r\\n\")) && !bytes.HasSuffix(buf, []byte(\"\\r\\n\\r\\n\")) {\n\t\tbuf = append(buf, \"\\r\\n\"...)\n\t}\n\tmsg, err := mail.ReadMessage(bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\treturn textproto.MIMEHeader(msg.Header), nil\n}\n\nvar wordDecoder = mime.WordDecoder{\n\tCharsetReader: func(charset string, r io.Reader) (io.Reader, error) {\n\t\tswitch strings.ToLower(charset) {\n\t\tcase \"\", \"us-ascii\", \"utf-8\":\n\t\t\treturn r, nil\n\t\t}\n\t\tenc, _ := ianaindex.MIME.Encoding(charset)\n\t\tif enc == nil {\n\t\t\tenc, _ = ianaindex.IANA.Encoding(charset)\n\t\t}\n\t\tif enc == nil {\n\t\t\treturn r, fmt.Errorf(\"unknown charset %q\", charset)\n\t\t}\n\t\treturn enc.NewDecoder().Reader(r), nil\n\t},\n}\n\nfunc parseEnvelope(log mlog.Log, h mail.Header) (*Envelope, error) {\n\tdate, _ := h.Date()\n\n\t// We currently marshal this field to JSON. But JSON cannot represent all\n\t// time.Time. Time zone of 24:00 was seen in the wild. We won't try for extreme\n\t// years, but we can readjust timezones.\n\t// todo: remove this once we no longer store using json.\n\t_, offset := date.Zone()\n\tif date.Year() > 9999 {\n\t\tdate = time.Time{}\n\t} else if offset <= -24*3600 || offset >= 24*3600 {\n\t\tdate = time.Unix(date.Unix(), 0).UTC()\n\t}\n\n\tsubject := h.Get(\"Subject\")\n\tif s, err := wordDecoder.DecodeHeader(subject); err == nil {\n\t\tsubject = s\n\t}\n\n\tenv := &Envelope{\n\t\tdate,\n\t\tsubject,\n\t\tparseAddressList(log, h, \"from\"),\n\t\tparseAddressList(log, h, \"sender\"),\n\t\tparseAddressList(log, h, \"reply-to\"),\n\t\tparseAddressList(log, h, \"to\"),\n\t\tparseAddressList(log, h, \"cc\"),\n\t\tparseAddressList(log, h, \"bcc\"),\n\t\th.Get(\"In-Reply-To\"),\n\t\th.Get(\"Message-Id\"),\n\t}\n\treturn env, nil\n}\n\nfunc parseAddressList(log mlog.Log, h mail.Header, k string) []Address {\n\t// todo: possibly work around ios mail generating incorrect q-encoded \"phrases\" with unencoded double quotes? ../rfc/2047:382\n\tl, err := h.AddressList(k)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tvar r []Address\n\tfor _, a := range l {\n\t\t// todo: parse more fully according to ../rfc/5322:959\n\t\tvar user, host string\n\t\taddr, err := smtp.ParseAddress(a.Address)\n\t\tif err != nil {\n\t\t\tlog.Infox(\"parsing address (continuing)\", err, slog.Any(\"address\", a.Address))\n\t\t} else {\n\t\t\tuser = addr.Localpart.String()\n\t\t\thost = addr.Domain.ASCII\n\t\t}\n\t\tr = append(r, Address{a.Name, user, host})\n\t}\n\treturn r\n}\n\n// ParseNextPart parses the next (sub)part of this multipart message.\n// ParseNextPart returns io.EOF and a nil part when there are no more parts.\n// Only used for initial parsing of message. Once parsed, use p.Parts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// IsDSN returns whether the MIME structure of the part is a DSN.\nfunc (p *Part) IsDSN() bool {\n\treturn p.MediaType == \"MULTIPART\" &&\n\t\tp.MediaSubType == \"REPORT\" &&\n\t\tlen(p.Parts) >= 2 &&\n\t\tp.Parts[1].MediaType == \"MESSAGE\" &&\n\t\t(p.Parts[1].MediaSubType == \"DELIVERY-STATUS\" || p.Parts[1].MediaSubType == \"GLOBAL-DELIVERY-STATUS\")\n}\n\n// Reader returns a reader for the decoded body content.\nfunc (p *Part) Reader() io.Reader {\n\treturn p.bodyReader(p.RawReader())\n}\n\n// ReaderUTF8OrBinary returns a reader for the decoded body content, transformed to\n// utf-8 for known mime/iana encodings (only if they aren't us-ascii or utf-8\n// already). For unknown or missing character sets/encodings, the original reader\n// is returned.\nfunc (p *Part) ReaderUTF8OrBinary() io.Reader {\n\treturn DecodeReader(p.ContentType\n```\n# Function Signature: func EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n# Natural Language Annotations: // EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n\t// Attempt to parse the message part using the existing Parse function\n\tpart, err := Parse(elog, strict, r)\n\n\t// If there's no error, return the successfully parsed part\n\tif err == nil {\n\t\treturn part, nil\n\t}\n\n\t// In case of error, log the issue and create a fallback part as application/octet-stream\n\tlog := mlog.New(\"message\", elog)\n\tlog.Error(\"parse error, fallback to application/octet-stream\", err)\n\n\t// Create a fallback part with the given reader and size\n\tfallback, fallbackErr := fallbackPart(part, r, size)\n\n\t// Return the fallback part even if there's an error during its creation\n\tif fallbackErr != nil {\n\t\tlog.Error(\"error creating fallback part\", fallbackErr)\n\t}\n\n\treturn fallback, nil\n}\n```"]}, "mox-message/part.go-ParseNextPart": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/part.go:\n```\npackage message\n\n// todo: allow more invalid content-type values, we now stop parsing on: empty media type (eg \"content-type: ; name=...\"), empty value for property (eg \"charset=\", missing quotes for characters that should be quoted (eg boundary containing \"=\" but without quotes), duplicate properties (two charsets), empty pairs (eg \"text/html;;\").\n// todo: should we be forgiving when closing boundary in multipart message is missing? seems like spam messages do this...\n// todo: should we allow base64 messages where a line starts with a space? and possibly more whitespace. is happening in messages. coreutils base64 accepts it, encoding/base64 does not.\n// todo: handle comments in headers?\n// todo: should we just always store messages with \\n instead of \\r\\n? \\r\\n seems easier for use with imap.\n// todo: can use a cleanup\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/quotedprintable\"\n\t\"net/mail\"\n\t\"net/textproto\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar (\n\tErrBadContentType = errors.New(\"bad content-type\")\n)\n\nvar (\n\terrNotMultipart           = errors.New(\"not a multipart message\")\n\terrFirstBoundCloses       = errors.New(\"first boundary cannot be finishing boundary\")\n\terrLineTooLong            = errors.New(\"line too long\")\n\terrMissingBoundaryParam   = errors.New(\"missing/empty boundary content-type parameter\")\n\terrMissingClosingBoundary = errors.New(\"eof without closing boundary\")\n\terrBareLF                 = errors.New(\"invalid bare line feed\")\n\terrBareCR                 = errors.New(\"invalid bare carriage return\")\n\terrUnexpectedEOF          = errors.New(\"unexpected eof\")\n)\n\n// If set, during tests, attempts to reparse a part will cause an error, because sequentially reading parts should not lead to reparsing.\nvar enforceSequential bool\n\n// Part represents a whole mail message, or a part of a multipart message. It\n// is designed to handle IMAP requirements efficiently.\ntype Part struct {\n\tBoundaryOffset int64 // Offset in message where bound starts. -1 for top-level message.\n\tHeaderOffset   int64 // Offset in message file where header starts.\n\tBodyOffset     int64 // Offset in message file where body starts.\n\tEndOffset      int64 // Where body of part ends. Set when part is fully read.\n\tRawLineCount   int64 // Number of lines in raw, undecoded, body of part. Set when part is fully read.\n\tDecodedSize    int64 // Number of octets when decoded. If this is a text mediatype, lines ending only in LF are changed end in CRLF and DecodedSize reflects that.\n\n\tMediaType               string            // From Content-Type, upper case. E.g. \"TEXT\". Can be empty because content-type may be absent. In this case, the part may be treated as TEXT/PLAIN.\n\tMediaSubType            string            // From Content-Type, upper case. E.g. \"PLAIN\".\n\tContentTypeParams       map[string]string // E.g. holds \"boundary\" for multipart messages. Has lower-case keys, and original case values.\n\tContentID               string\n\tContentDescription      string\n\tContentTransferEncoding string    // In upper case.\n\tEnvelope                *Envelope // Email message headers. Not for non-message parts.\n\n\tParts []Part // Parts if this is a multipart.\n\n\t// Only for message/rfc822 and message/global. This part may have a buffer as\n\t// backing io.ReaderAt, because a message/global can have a non-identity\n\t// content-transfer-encoding. This part has a nil parent.\n\tMessage *Part\n\n\tr               io.ReaderAt\n\theader          textproto.MIMEHeader // Parsed header.\n\tnextBoundOffset int64                // If >= 0, the offset where the next part header starts. We can set this when a user fully reads each part.\n\tlastBoundOffset int64                // Start of header of last/previous part. Used to skip a part if ParseNextPart is called and nextBoundOffset is -1.\n\tparent          *Part                // Parent part, for getting bound from, and setting nextBoundOffset when a part has finished reading. Only for subparts, not top-level parts.\n\tbound           []byte               // Only set if valid multipart with boundary, includes leading --, excludes \\r\\n.\n\tstrict          bool                 // If set, valid crlf line endings are verified when reading body.\n}\n\n// todo: have all Content* fields in Part?\n// todo: make Address contain a type Localpart and dns.Domain?\n// todo: if we ever make a major change and reparse all parts, switch to lower-case values if not too troublesome.\n\n// Envelope holds the basic/common message headers as used in IMAP4.\ntype Envelope struct {\n\tDate      time.Time\n\tSubject   string // Q/B-word-decoded.\n\tFrom      []Address\n\tSender    []Address\n\tReplyTo   []Address\n\tTo        []Address\n\tCC        []Address\n\tBCC       []Address\n\tInReplyTo string // From In-Reply-To header, includes <>.\n\tMessageID string // From Message-Id header, includes <>.\n}\n\n// Address as used in From and To headers.\ntype Address struct {\n\tName string // Free-form name for display in mail applications.\n\tUser string // Localpart, encoded as string. Must be parsed before using as Localpart.\n\tHost string // Domain in ASCII.\n}\n\n// Parse reads the headers of the mail message and returns a part.\n// A part provides access to decoded and raw contents of a message and its multiple parts.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nfunc Parse(elog *slog.Logger, strict bool, r io.ReaderAt) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\treturn newPart(log, strict, r, 0, nil)\n}\n\n// EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc fallbackPart(p Part, r io.ReaderAt, size int64) (Part, error) {\n\tnp := Part{\n\t\tHeaderOffset:            p.HeaderOffset,\n\t\tBodyOffset:              p.BodyOffset,\n\t\tEndOffset:               size,\n\t\tMediaType:               \"APPLICATION\",\n\t\tMediaSubType:            \"OCTET-STREAM\",\n\t\tContentTypeParams:       p.ContentTypeParams,\n\t\tContentID:               p.ContentID,\n\t\tContentDescription:      p.ContentDescription,\n\t\tContentTransferEncoding: p.ContentTransferEncoding,\n\t\tEnvelope:                p.Envelope,\n\t\t// We don't keep:\n\t\t//   - BoundaryOffset: irrelevant for top-level message.\n\t\t//   - RawLineCount and DecodedSize: set below.\n\t\t//   - Parts: we are not treating this as a multipart message.\n\t}\n\tnp.SetReaderAt(r)\n\t// By reading body, the number of lines and decoded size will be set.\n\t_, err := io.Copy(io.Discard, np.Reader())\n\treturn np, err\n}\n\n// SetReaderAt sets r as reader for this part and all its sub parts, recursively.\n// No reader is set for any Message subpart, see SetMessageReaderAt.\nfunc (p *Part) SetReaderAt(r io.ReaderAt) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp.r = r\n\tfor i := range p.Parts {\n\t\tpp := &p.Parts[i]\n\t\tpp.SetReaderAt(r)\n\t}\n}\n\n// SetMessageReaderAt sets a reader on p.Message, which must be non-nil.\nfunc (p *Part) SetMessageReaderAt() error {\n\t// todo: if p.Message does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.Message, recursively.\n\tbuf, err := io.ReadAll(p.Reader())\n\tif err != nil {\n\t\treturn err\n\t}\n\tp.Message.SetReaderAt(bytes.NewReader(buf))\n\treturn nil\n}\n\n// Walk through message, decoding along the way, and collecting mime part offsets and sizes, and line counts.\nfunc (p *Part) Walk(elog *slog.Logger, parent *Part) error {\n\tlog := mlog.New(\"message\", elog)\n\n\tif len(p.bound) == 0 {\n\t\tif p.MediaType == \"MESSAGE\" && (p.MediaSubType == \"RFC822\" || p.MediaSubType == \"GLOBAL\") {\n\t\t\t// todo: don't read whole submessage in memory...\n\t\t\tbuf, err := io.ReadAll(p.Reader())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbr := bytes.NewReader(buf)\n\t\t\tmp, err := Parse(log.Logger, p.strict, br)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"parsing embedded message: %w\", err)\n\t\t\t}\n\t\t\tif err := mp.Walk(log.Logger, nil); err != nil {\n\t\t\t\t// If this is a DSN and we are not in pedantic mode, accept unexpected end of\n\t\t\t\t// message. This is quite common because MTA's sometimes just truncate the original\n\t\t\t\t// message in a place that makes the message invalid.\n\t\t\t\tif errors.Is(err, errUnexpectedEOF) && !Pedantic && parent != nil && len(parent.Parts) >= 3 && p == &parent.Parts[2] && parent.MediaType == \"MULTIPART\" && parent.MediaSubType == \"REPORT\" {\n\t\t\t\t\tmp, err = fallbackPart(mp, br, int64(len(buf)))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"parsing invalid embedded message: %w\", err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treturn fmt.Errorf(\"parsing parts of embedded message: %w\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// todo: if mp does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.r on mp, recursively.\n\t\t\tp.Message = &mp\n\t\t\treturn nil\n\t\t}\n\t\t_, err := io.Copy(io.Discard, p.Reader())\n\t\treturn err\n\t}\n\n\tfor {\n\t\tpp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := pp.Walk(log.Logger, p); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// String returns a debugging representation of the part.\nfunc (p *Part) String() string {\n\treturn fmt.Sprintf(\"&Part{%s/%s offsets %d/%d/%d/%d lines %d decodedsize %d next %d last %d bound %q parts %v}\", p.MediaType, p.MediaSubType, p.BoundaryOffset, p.HeaderOffset, p.BodyOffset, p.EndOffset, p.RawLineCount, p.DecodedSize, p.nextBoundOffset, p.lastBoundOffset, p.bound, p.Parts)\n}\n\n// newPart parses a new part, which can be the top-level message.\n// offset is the bound offset for parts, and the start of message for top-level messages. parent indicates if this is a top-level message or sub-part.\n// If an error occurs, p's exported values can still be relevant. EnsurePart uses these values.\nfunc newPart(log mlog.Log, strict bool, r io.ReaderAt, offset int64, parent *Part) (p Part, rerr error) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp = Part{\n\t\tBoundaryOffset: -1,\n\t\tEndOffset:      -1,\n\t\tr:              r,\n\t\tparent:         parent,\n\t\tstrict:         strict,\n\t}\n\n\tb := &bufAt{strict: strict, r: r, offset: offset}\n\n\tif parent != nil {\n\t\tp.BoundaryOffset = offset\n\t\tif line, _, err := b.ReadLine(true); err != nil {\n\t\t\treturn p, err\n\t\t} else if match, finish := checkBound(line, parent.bound); !match {\n\t\t\treturn p, fmt.Errorf(\"missing bound\")\n\t\t} else if finish {\n\t\t\treturn p, fmt.Errorf(\"new part for closing boundary\")\n\t\t}\n\t}\n\n\t// Collect header.\n\tp.HeaderOffset = b.offset\n\tp.BodyOffset = b.offset\n\thb := &bytes.Buffer{}\n\tfor {\n\t\tline, _, err := b.ReadLine(true)\n\t\tif err == io.EOF {\n\t\t\t// No body is valid.\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"reading header line: %w\", err)\n\t\t}\n\t\thb.Write(line)\n\t\tif len(line) == 2 {\n\t\t\tbreak // crlf\n\t\t}\n\t}\n\tp.BodyOffset = b.offset\n\n\t// Don't attempt to parse empty header, mail.ReadMessage doesn't like it.\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t} else {\n\t\th, err := parseHeader(hb)\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"parsing header: %w\", err)\n\t\t}\n\t\tp.header = h\n\t}\n\n\tct := p.header.Get(\"Content-Type\")\n\tmt, params, err := mime.ParseMediaType(ct)\n\tif err != nil && ct != \"\" {\n\t\tif Pedantic || strict {\n\t\t\treturn p, fmt.Errorf(\"%w: %s: %q\", ErrBadContentType, err, ct)\n\t\t}\n\n\t\t// Try parsing just a content-type, ignoring parameters.\n\t\t// ../rfc/2045:628\n\t\tct = strings.TrimSpace(strings.SplitN(ct, \";\", 2)[0])\n\t\tt := strings.SplitN(ct, \"/\", 2)\n\t\tisToken := func(s string) bool {\n\t\t\tconst separators = `()<>@,;:\\\\\"/[]?= ` // ../rfc/2045:663\n\t\t\tfor _, c := range s {\n\t\t\t\tif c < 0x20 || c >= 0x80 || strings.ContainsRune(separators, c) {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn len(s) > 0\n\t\t}\n\t\t// We cannot recover content-type of multipart, we won't have a boundary.\n\t\tif len(t) == 2 && isToken(t[0]) && !strings.EqualFold(t[0], \"multipart\") && isToken(t[1]) {\n\t\t\tp.MediaType = strings.ToUpper(t[0])\n\t\t\tp.MediaSubType = strings.ToUpper(t[1])\n\t\t} else {\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t}\n\t\tlog.Debugx(\"malformed content-type, attempting to recover and continuing\", err,\n\t\t\tslog.String(\"contenttype\", p.header.Get(\"Content-Type\")),\n\t\t\tslog.String(\"mediatype\", p.MediaType),\n\t\t\tslog.String(\"mediasubtype\", p.MediaSubType))\n\t} else if mt != \"\" {\n\t\tt := strings.SplitN(strings.ToUpper(mt), \"/\", 2)\n\t\tif len(t) != 2 {\n\t\t\tif Pedantic || strict {\n\t\t\t\treturn p, fmt.Errorf(\"bad content-type: %q (content-type %q)\", mt, ct)\n\t\t\t}\n\t\t\tlog.Debug(\"malformed media-type, ignoring and continuing\", slog.String(\"type\", mt))\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t} else {\n\t\t\tp.MediaType = t[0]\n\t\t\tp.MediaSubType = t[1]\n\t\t\tp.ContentTypeParams = params\n\t\t}\n\t}\n\n\tp.ContentID = p.header.Get(\"Content-Id\")\n\tp.ContentDescription = p.header.Get(\"Content-Description\")\n\tp.ContentTransferEncoding = strings.ToUpper(p.header.Get(\"Content-Transfer-Encoding\"))\n\n\tif parent == nil {\n\t\tp.Envelope, err = parseEnvelope(log, mail.Header(p.header))\n\t\tif err != nil {\n\t\t\treturn p, err\n\t\t}\n\t}\n\n\tif p.MediaType == \"MULTIPART\" {\n\t\ts := params[\"boundary\"]\n\t\tif s == \"\" {\n\t\t\treturn p, errMissingBoundaryParam\n\t\t}\n\t\tp.bound = append([]byte(\"--\"), s...)\n\n\t\t// Discard preamble, before first boundary.\n\t\tfor {\n\t\t\tline, _, err := b.PeekLine(true)\n\t\t\tif err != nil {\n\t\t\t\treturn p, fmt.Errorf(\"parsing line for part preamble: %w\", err)\n\t\t\t}\n\t\t\t// Line only needs boundary prefix, not exact match. ../rfc/2046:1103\n\t\t\t// Well, for compatibility, we require whitespace after the boundary. Because some\n\t\t\t// software use the same boundary but with text appended for sub parts.\n\t\t\tif match, finish := checkBound(line, p.bound); match {\n\t\t\t\tif finish {\n\t\t\t\t\treturn p, errFirstBoundCloses\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tb.ReadLine(true)\n\t\t}\n\t\tp.nextBoundOffset = b.offset\n\t\tp.lastBoundOffset = b.offset\n\t}\n\n\treturn p, nil\n}\n\n// Header returns the parsed header of this part.\nfunc (p *Part) Header() (textproto.MIMEHeader, error) {\n\tif p.header != nil {\n\t\treturn p.header, nil\n\t}\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t\treturn p.header, nil\n\t}\n\th, err := parseHeader(p.HeaderReader())\n\tp.header = h\n\treturn h, err\n}\n\n// HeaderReader returns a reader for the header section of this part, including ending bare CRLF.\nfunc (p *Part) HeaderReader() io.Reader {\n\treturn io.NewSectionReader(p.r, p.HeaderOffset, p.BodyOffset-p.HeaderOffset)\n}\n\n// parse a header, only call this on non-empty input (even though that is a valid header).\nfunc parseHeader(r io.Reader) (textproto.MIMEHeader, error) {\n\t// We read using mail.ReadMessage instead of textproto.ReadMIMEHeaders because the\n\t// first handles email messages properly, while the second only works for HTTP\n\t// headers.\n\tvar zero textproto.MIMEHeader\n\n\t// We read the header and add the optional \\r\\n header/body separator. If the \\r\\n\n\t// is missing, parsing with Go <1.21 results in an EOF error.\n\t// todo: directly parse from reader r when Go 1.20 is no longer supported.\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\tif bytes.HasSuffix(buf, []byte(\"\\r\\n\")) && !bytes.HasSuffix(buf, []byte(\"\\r\\n\\r\\n\")) {\n\t\tbuf = append(buf, \"\\r\\n\"...)\n\t}\n\tmsg, err := mail.ReadMessage(bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\treturn textproto.MIMEHeader(msg.Header), nil\n}\n\nvar wordDecoder = mime.WordDecoder{\n\tCharsetReader: func(charset string, r io.Reader) (io.Reader, error) {\n\t\tswitch strings.ToLower(charset) {\n\t\tcase \"\", \"us-ascii\", \"utf-8\":\n\t\t\treturn r, nil\n\t\t}\n\t\tenc, _ := ianaindex.MIME.Encoding(charset)\n\t\tif enc == nil {\n\t\t\tenc, _ = ianaindex.IANA.Encoding(charset)\n\t\t}\n\t\tif enc == nil {\n\t\t\treturn r, fmt.Errorf(\"unknown charset %q\", charset)\n\t\t}\n\t\treturn enc.NewDecoder().Reader(r), nil\n\t},\n}\n\nfunc parseEnvelope(log mlog.Log, h mail.Header) (*Envelope, error) {\n\tdate, _ := h.Date()\n\n\t// We currently marshal this field to JSON. But JSON cannot represent all\n\t// time.Time. Time zone of 24:00 was seen in the wild. We won't try for extreme\n\t// years, but we can readjust timezones.\n\t// todo: remove this once we no longer store using json.\n\t_, offset := date.Zone()\n\tif date.Year() > 9999 {\n\t\tdate = time.Time{}\n\t} else if offset <= -24*3600 || offset >= 24*3600 {\n\t\tdate = time.Unix(date.Unix(), 0).UTC()\n\t}\n\n\tsubject := h.Get(\"Subject\")\n\tif s, err := wordDecoder.DecodeHeader(subject); err == nil {\n\t\tsubject = s\n\t}\n\n\tenv := &Envelope{\n\t\tdate,\n\t\tsubject,\n\t\tparseAddressList(log, h, \"from\"),\n\t\tparseAddressList(log, h, \"sender\"),\n\t\tparseAddressList(log, h, \"reply-to\"),\n\t\tparseAddressList(log, h, \"to\"),\n\t\tparseAddressList(log, h, \"cc\"),\n\t\tparseAddressList(log, h, \"bcc\"),\n\t\th.Get(\"In-Reply-To\"),\n\t\th.Get(\"Message-Id\"),\n\t}\n\treturn env, nil\n}\n\nfunc parseAddressList(log mlog.Log, h mail.Header, k string) []Address {\n\t// todo: possibly work around ios mail generating incorrect q-encoded \"phrases\" with unencoded double quotes? ../rfc/2047:382\n\tl, err := h.AddressList(k)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tvar r []Address\n\tfor _, a := range l {\n\t\t// todo: parse more fully according to ../rfc/5322:959\n\t\tvar user, host string\n\t\taddr, err := smtp.ParseAddress(a.Address)\n\t\tif err != nil {\n\t\t\tlog.Infox(\"parsing address (continuing)\", err, slog.Any(\"address\", a.Address))\n\t\t} else {\n\t\t\tuser = addr.Localpart.String()\n\t\t\thost = addr.Domain.ASCII\n\t\t}\n\t\tr = append(r, Address{a.Name, user, host})\n\t}\n\treturn r\n}\n\n// ParseNextPart parses the next (sub)part of this multipart message.\n// ParseNextPart returns io.EOF and a nil part when there are no more parts.\n// Only used for initial parsing of message. Once parsed, use p.Parts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// IsDSN returns whether the MIME structure of the part is a DSN.\nfunc (p *Part) IsDSN() bool {\n\treturn p.MediaType == \"MULTIPART\" &&\n\t\tp.MediaSubType == \"REPORT\" &&\n\t\tlen(p.Parts) >= 2 &&\n\t\tp.Parts[1].MediaType == \"MESSAGE\" &&\n\t\t(p.Parts[1].MediaSubType == \"DELIVERY-STATUS\" || p.Parts[1].MediaSubType == \"GLOBAL-DELIVERY-STATUS\")\n}\n\n// Reader returns a reader for the decoded body content.\nfunc (p *Part) Reader() io.Reader {\n\treturn p.bodyReader(p.RawReader())\n}\n\n// ReaderUTF8OrBinary returns a reader for the decoded body content, transformed to\n// utf-8 for known mime/iana encodings (only if they aren't us-ascii or utf-8\n// already). For unknown or missing character sets/encodings, the original reader\n// is returned.\nfunc (p *Part) ReaderUTF8OrBinary() io.Reader {\n\treturn DecodeReader(p.ContentType\n```\n# Function Signature: func (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n# Natural Language Annotations: // ParseNextPart parses the next (sub)part of this multipart message.\n// ParseNextPart returns io.EOF and a nil part when there are no more parts.\n// Only used for initial parsing of message. Once parsed, use p.Parts.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\n\tif len(p.bound) == 0 {\n\t\treturn nil, errNotMultipart\n\t}\n\n\tb := &bufAt{strict: p.strict, r: p.r, offset: p.nextBoundOffset}\n\n\tfor {\n\t\tline, _, err := b.ReadLine(true)\n\t\tif err == io.EOF {\n\t\t\treturn nil, io.EOF\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"reading line for next part: %w\", err)\n\t\t}\n\n\t\tif match, finish := checkBound(line, p.bound); match {\n\t\t\tif finish {\n\t\t\t\tp.nextBoundOffset = -1\n\t\t\t\treturn nil, io.EOF\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t}\n\n\tpart, err := newPart(log, p.strict, p.r, b.Offset(), p)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpart.lastBoundOffset = b.offset\n\tpart.nextBoundOffset = b.offset + 2\n\tpart.HeaderOffset = b.offset + 2\n\tpart.BodyOffset = b.offset + 2\n\tp.nextBoundOffset = part.nextBoundOffset\n\n\tp.Parts = append(p.Parts, part)\n\n\treturn &p.Parts[len(p.Parts)-1], nil\n}\n```"]}, "mox-message/messageid.go-MessageIDCanonical": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/messageid.go:\n```\npackage message\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\nvar errBadMessageID = errors.New(\"not a message-id\")\n\n// MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func MessageIDCanonical(s string) (string, bool, error) {\n# Natural Language Annotations: // MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MessageIDCanonical(s string) (string, bool, error) {\n\t// Trim unnecessary whitespaces\n\ts = strings.TrimSpace(s)\n\n\t// Check if the message-id is enclosed in <>\n\tif len(s) < 2 || s[0] != '<' || s[len(s)-1] != '>' {\n\t\treturn \"\", false, errBadMessageID\n\t}\n\n\t// Remove the enclosing <>\n\ts = s[1 : len(s)-1]\n\n\t// Lowercase the message-id\n\ts = strings.ToLower(s)\n\n\t// Split localpart and domain\n\tparts := strings.Split(s, \"@\")\n\tif len(parts) != 2 {\n\t\t// If the message-id could not be parsed as address (localpart \"@\" domain)\n\t\treturn s, true, nil\n\t}\n\n\t// Return the canonical message-id\n\treturn s, false, nil\n}\n```"]}, "mox-message/decode.go-DecodeReader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/decode.go:\n```\npackage message\n\nimport (\n\t\"io\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n)\n\n// DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DecodeReader(charset string, r io.Reader) io.Reader {\n# Natural Language Annotations: // DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage message\n\nimport (\n\t\"io\"\n\n\t\"golang.org/x/text/transform\"\n\t\"golang.org/x/text/encoding/ianaindex\"\n)\n\n// DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\nfunc DecodeReader(charset string, r io.Reader) io.Reader {\n\t// If the charset is empty or \"us-ascii\" or \"utf-8\", return the original reader\n\tif charset == \"\" || charset == \"us-ascii\" || charset == \"utf-8\" {\n\t\treturn r\n\t}\n\n\t// Lookup the encoding corresponding to the charset\n\tencoding, err := ianaindex.IANA.Encoding(charset)\n\tif err != nil || encoding == nil {\n\t\t// If the encoding is unknown, return the original reader\n\t\treturn r\n\t}\n\n\t// Create a transformer for decoding\n\tdecoder := encoding.NewDecoder()\n\t// Return a transformed reader using the decoder\n\treturn transform.NewReader(r, decoder)\n}\n```"]}, "mox-message/referencedids.go-ReferencedIDs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/referencedids.go:\n```\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n# Natural Language Annotations: // ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nfunc ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n\t// A map is used to ensure unique canonicalized IDs\n\tidsMap := make(map[string]struct{})\n\n\t// Helper function to process a list of headers\n\tprocessHeaders := func(headers []string) {\n\t\tfor _, header := range headers {\n\t\t\t// Split the header by spaces to get individual IDs\n\t\t\tids := strings.Fields(header)\n\t\t\tfor _, id := range ids {\n\t\t\t\t// Canonicalize the message ID\n\t\t\t\tcanonicalID := smtp.MessageIDCanonical(id)\n\t\t\t\t// If the canonical ID is not empty, add it to the map\n\t\t\t\tif canonicalID != \"\" {\n\t\t\t\t\tidsMap[canonicalID] = struct{}{}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Process the References headers first\n\tprocessHeaders(references)\n\n\t// If no IDs are found in References, process the In-Reply-To headers\n\tif len(idsMap) == 0 {\n\t\tprocessHeaders(inReplyTo)\n\t}\n\n\t// Convert the map keys to a slice of strings\n\tresult := make([]string, 0, len(idsMap))\n\tfor id := range idsMap {\n\t\tresult = append(result, id)\n\t}\n\n\treturn result, nil\n}\n```"]}, "mox-message/authresults.go-ParseAuthResults": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/authresults.go:\n```\npackage message\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// ../rfc/8601:577\n\n// Authentication-Results header, see RFC 8601.\ntype AuthResults struct {\n\tHostname string\n\t// Optional version of Authentication-Results header, assumed \"1\" when absent,\n\t// which is common.\n\tVersion string\n\tComment string       // If not empty, header comment without \"()\", added after Hostname.\n\tMethods []AuthMethod // Can be empty, in case of \"none\".\n}\n\n// ../rfc/8601:598\n\n// AuthMethod is a result for one authentication method.\n//\n// Example encoding in the header: \"spf=pass smtp.mailfrom=example.net\".\ntype AuthMethod struct {\n\t// E.g. \"dkim\", \"spf\", \"iprev\", \"auth\".\n\tMethod  string\n\tVersion string // For optional method version. \"1\" is implied when missing, which is common.\n\tResult  string // Each method has a set of known values, e.g. \"pass\", \"temperror\", etc.\n\tComment string // Optional, message header comment.\n\tReason  string // Optional.\n\tProps   []AuthProp\n}\n\n// ../rfc/8601:606\n\n// AuthProp describes properties for an authentication method.\n// Each method has a set of known properties.\n// Encoded in the header as \"type.property=value\", e.g. \"smtp.mailfrom=example.net\"\n// for spf.\ntype AuthProp struct {\n\t// Valid values maintained at https://www.iana.org/assignments/email-auth/email-auth.xhtml\n\tType     string\n\tProperty string\n\tValue    string\n\t// Whether value is address-like (localpart@domain, or domain). Or another value,\n\t// which is subject to escaping.\n\tIsAddrLike bool\n\tComment    string // If not empty, header comment without \"()\", added after Value.\n}\n\n// MakeAuthProp is a convenient way to make an AuthProp.\nfunc MakeAuthProp(typ, property, value string, isAddrLike bool, Comment string) AuthProp {\n\treturn AuthProp{typ, property, value, isAddrLike, Comment}\n}\n\n// todo future: we could store fields as dns.Domain, and when we encode as non-ascii also add the ascii version as a comment.\n\n// Header returns an Authentication-Results header, possibly spanning multiple\n// lines, always ending in crlf.\nfunc (h AuthResults) Header() string {\n\t// Escaping of values: ../rfc/8601:684 ../rfc/2045:661\n\n\toptComment := func(s string) string {\n\t\tif s != \"\" {\n\t\t\treturn \" (\" + s + \")\"\n\t\t}\n\t\treturn s\n\t}\n\n\tw := &HeaderWriter{}\n\tw.Add(\"\", \"Authentication-Results:\"+optComment(h.Comment)+\" \"+value(h.Hostname, false)+\";\")\n\tfor i, m := range h.Methods {\n\t\tw.Newline()\n\n\t\ttokens := []string{}\n\t\taddf := func(format string, args ...any) {\n\t\t\ts := fmt.Sprintf(format, args...)\n\t\t\ttokens = append(tokens, s)\n\t\t}\n\t\taddf(\"%s=%s\", m.Method, m.Result)\n\t\tif m.Comment != \"\" && (m.Reason != \"\" || len(m.Props) > 0) {\n\t\t\taddf(\"(%s)\", m.Comment)\n\t\t}\n\t\tif m.Reason != \"\" {\n\t\t\taddf(\"reason=%s\", value(m.Reason, false))\n\t\t}\n\t\tfor _, p := range m.Props {\n\t\t\tv := value(p.Value, p.IsAddrLike)\n\t\t\taddf(\"%s.%s=%s%s\", p.Type, p.Property, v, optComment(p.Comment))\n\t\t}\n\t\tfor j, t := range tokens {\n\t\t\tvar sep string\n\t\t\tif j > 0 {\n\t\t\t\tsep = \" \"\n\t\t\t}\n\t\t\tif j == len(tokens)-1 && i < len(h.Methods)-1 {\n\t\t\t\tt += \";\"\n\t\t\t}\n\t\t\tw.Add(sep, t)\n\t\t}\n\t}\n\treturn w.String()\n}\n\nfunc value(s string, isAddrLike bool) string {\n\tquote := s == \"\"\n\tfor _, c := range s {\n\t\t// utf-8 does not have to be quoted. ../rfc/6532:242\n\t\t// Characters outside of tokens do. ../rfc/2045:661\n\t\tif c <= ' ' || c == 0x7f || (c == '@' && !isAddrLike) || strings.ContainsRune(`()<>,;:\\\\\"/[]?= `, c) {\n\t\t\tquote = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !quote {\n\t\treturn s\n\t}\n\tr := `\"`\n\tfor _, c := range s {\n\t\tif c == '\"' || c == '\\\\' {\n\t\t\tr += \"\\\\\"\n\t\t}\n\t\tr += string(c)\n\t}\n\tr += `\"`\n\treturn r\n}\n\n// ParseAuthResults parses a Authentication-Results header value.\n//\n// Comments are not populated in the returned AuthResults.\n// Both crlf and lf line-endings are accepted. The input string must end with\n// either crlf or lf.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype parser struct {\n\ts     string\n\tlower string // Like s, but with ascii characters lower-cased (utf-8 offsets preserved).\n\to     int\n}\n\ntype parseError struct{ err error }\n\nfunc (p *parser) recover(err *error) {\n\tx := recover()\n\tif x == nil {\n\t\treturn\n\t}\n\tperr, ok := x.(parseError)\n\tif ok {\n\t\t*err = perr.err\n\t\treturn\n\t}\n\tpanic(x)\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(parseError{fmt.Errorf(format, args...)})\n}\n\nfunc (p *parser) end() bool {\n\treturn p.s[p.o:] == \"\\r\\n\" || p.s[p.o:] == \"\\n\"\n}\n\n// ../rfc/5322:599\nfunc (p *parser) cfws() {\n\tp.fws()\n\tfor p.prefix(\"(\") {\n\t\tp.xcomment()\n\t}\n\tp.fws()\n}\n\nfunc (p *parser) fws() {\n\tfor p.take(\" \") || p.take(\"\\t\") {\n\t}\n\topts := []string{\"\\n \", \"\\n\\t\", \"\\r\\n \", \"\\r\\n\\t\"}\n\tfor _, o := range opts {\n\t\tif p.take(o) {\n\t\t\tbreak\n\t\t}\n\t}\n\tfor p.take(\" \") || p.take(\"\\t\") {\n\t}\n}\n\nfunc (p *parser) xcomment() {\n\tp.xtake(\"(\")\n\tp.fws()\n\tfor !p.take(\")\") {\n\t\tif p.empty() {\n\t\t\tp.xerrorf(\"unexpected end in comment\")\n\t\t}\n\t\tif p.prefix(\"(\") {\n\t\t\tp.xcomment()\n\t\t\tp.fws()\n\t\t\tcontinue\n\t\t}\n\t\tp.take(`\\`)\n\t\tif c := p.s[p.o]; c > ' ' && c < 0x7f {\n\t\t\tp.o++\n\t\t} else {\n\t\t\tp.xerrorf(\"bad character %c in comment\", c)\n\t\t}\n\t\tp.fws()\n\t}\n}\n\nfunc (p *parser) prefix(s string) bool {\n\treturn strings.HasPrefix(p.lower[p.o:], s)\n}\n\nfunc (p *parser) xvalue() string {\n\tif p.prefix(`\"`) {\n\t\treturn p.xquotedString()\n\t}\n\treturn p.xtakefn1(\"value token\", func(c rune, i int) bool {\n\t\t// ../rfc/2045:661\n\t\t// todo: token cannot contain utf-8? not updated in ../rfc/6532. however, we also use it for the localpart & domain parsing, so we'll allow it.\n\t\treturn c > ' ' && !strings.ContainsRune(`()<>@,;:\\\\\"/[]?= `, c)\n\t})\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == 0 {\n\t\tp.o = len(p.s)\n\t} else {\n\t\tp.o += o\n\t}\n\treturn r\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"bad escaped char %c in quoted string\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\tif c >= ' ' && c != '\\\\' && c != '\"' {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid quoted string, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) digits() string {\n\to := p.o\n\tfor o < len(p.s) && p.s[o] >= '0' && p.s[o] <= '9' {\n\t\to++\n\t}\n\tp.o = o\n\treturn p.s[o:p.o]\n}\n\nfunc (p *parser) xdigits() string {\n\ts := p.digits()\n\tif s == \"\" {\n\t\tp.xerrorf(\"expected digits, remaining %q\", p.s[p.o:])\n\t}\n\treturn s\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.prefix(s) {\n\t\tp.xerrorf(\"expected %q, remaining %q\", s, p.s[p.o:])\n\t}\n\tp.o += len(s)\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o >= len(p.s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.prefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtakefn1(what string, fn func(c rune, i int) bool) string {\n\tif p.empty() {\n\t\tp.xerrorf(\"need at least one char for %s\", what)\n\t}\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"expected at least one char for %s, remaining %q\", what, p.s[p.o:])\n\t\t\t}\n\t\t\ts := p.s[p.o : p.o+i]\n\t\t\tp.o += i\n\t\t\treturn s\n\t\t}\n\t}\n\ts := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn s\n}\n\n// ../rfc/5321:2287\nfunc (p *parser) xkeyword(isResult bool) string {\n\ts := strings.ToLower(p.xtakefn1(\"keyword\", func(c rune, i int) bool {\n\t\t// Yahoo sends results like \"dkim=perm_fail\".\n\t\treturn c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '-' || isResult && !Pedantic && c == '_'\n\t}))\n\tif s == \"-\" {\n\t\tp.xerrorf(\"missing keyword\")\n\t} else if strings.HasSuffix(s, \"-\") {\n\t\tp.o--\n\t\ts = s[:len(s)-1]\n\t}\n\treturn s\n}\n\nfunc (p *parser) xmethodspec(methodKeyword string) (string, string, string) {\n\tp.cfws()\n\tvar methodDigits string\n\tif p.take(\"/\") {\n\t\tmethodDigits = p.xdigits()\n\t\tp.cfws()\n\t}\n\tp.xtake(\"=\")\n\tp.cfws()\n\tresult := p.xkeyword(true)\n\treturn methodKeyword, methodDigits, result\n}\n\nfunc (p *parser) xpropspec() (ap AuthProp) {\n\tap.Type = p.xkeyword(false)\n\tp.cfws()\n\tp.xtake(\".\")\n\tp.cfws()\n\tif p.take(\"mailfrom\") {\n\t\tap.Property = \"mailfrom\"\n\t} else if p.take(\"rcptto\") {\n\t\tap.Property = \"rcptto\"\n\t} else {\n\t\tap.Property = p.xkeyword(false)\n\t}\n\tp.cfws()\n\tp.xtake(\"=\")\n\tap.IsAddrLike, ap.Value = p.xpvalue()\n\treturn\n}\n\n// method keyword has been parsed, method-version not yet.\nfunc (p *parser) xresinfo(methodKeyword string) (am AuthMethod) {\n\tp.cfws()\n\tam.Method, am.Version, am.Result = p.xmethodspec(methodKeyword)\n\tp.cfws()\n\tif p.take(\"reason\") {\n\t\tp.cfws()\n\t\tp.xtake(\"=\")\n\t\tp.cfws()\n\t\tam.Reason = p.xvalue()\n\t}\n\tp.cfws()\n\tfor !p.prefix(\";\") && !p.end() {\n\t\tam.Props = append(am.Props, p.xpropspec())\n\t\tp.cfws()\n\t}\n\treturn\n}\n\n// todo: could keep track whether this is a localpart.\nfunc (p *parser) xpvalue() (bool, string) {\n\tp.cfws()\n\tif p.take(\"@\") {\n\t\t// Bare domain.\n\t\tdom, _ := p.xdomain()\n\t\treturn true, \"@\" + dom\n\t}\n\ts := p.xvalue()\n\tif p.take(\"@\") {\n\t\tdom, _ := p.xdomain()\n\t\ts += \"@\" + dom\n\t\treturn true, s\n\t}\n\treturn false, s\n}\n\n// ../rfc/5321:2291\nfunc (p *parser) xdomain() (string, dns.Domain) {\n\ts := p.xsubdomain()\n\tfor p.take(\".\") {\n\t\ts += \".\" + p.xsubdomain()\n\t}\n\td, err := dns.ParseDomain(s)\n\tif err != nil {\n\t\tp.xerrorf(\"parsing domain name %q: %s\", s, err)\n\t}\n\tif len(s) > 255 {\n\t\t// ../rfc/5321:3491\n\t\tp.xerrorf(\"domain longer than 255 octets\")\n\t}\n\treturn s, d\n}\n\n// ../rfc/5321:2303\n// ../rfc/5321:2303 ../rfc/6531:411\nfunc (p *parser) xsubdomain() string {\n\treturn p.xtakefn1(\"subdomain\", func(c rune, i int) bool {\n\t\treturn c >= '0' && c <= '9' || c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || i > 0 && c == '-' || c > 0x7f\n\t})\n}\n\n```\n# Function Signature: func ParseAuthResults(s string) (ar AuthResults, err error) {\n# Natural Language Annotations: // ParseAuthResults parses a Authentication-Results header value.\n//\n// Comments are not populated in the returned AuthResults.\n// Both crlf and lf line-endings are accepted. The input string must end with\n// either crlf or lf.\nPlease return the generated code surrounded by ```\n", "response": ["To implement the `ParseAuthResults` function in Go, we'll use a parser to process the given string according to the provided data and comments. The function will not populate comments, will accept CRLF and LF line-endings, and will ensure the input string ends with either CRLF or LF. Here's the implementation:\n\n```go\nfunc ParseAuthResults(s string) (ar AuthResults, err error) {\n\tdefer func() {\n\t\tif recoverErr := recover(); recoverErr != nil {\n\t\t\tif parseErr, ok := recoverErr.(parseError); ok {\n\t\t\t\terr = parseErr.err\n\t\t\t} else {\n\t\t\t\tpanic(recoverErr)\n\t\t\t}\n\t\t}\n\t}()\n\n\tp := &parser{s: s, lower: strings.ToLower(s)}\n\tp.cfws()\n\n\t// Parse the hostname part\n\tar.Hostname = p.xvalue()\n\tp.cfws()\n\tp.xtake(\";\")\n\n\t// Parse each method\n\tfor !p.end() {\n\t\tp.cfws()\n\t\tmethod := p.xkeyword(false)\n\t\tif p.prefix(\"=\") {\n\t\t\t// Parse the rest of the method specifics\n\t\t\tauthMethod := p.xresinfo(method)\n\t\t\tar.Methods = append(ar.Methods, authMethod)\n\t\t}\n\n\t\tp.cfws()\n\t\t\n\t\tif p.take(\";\") {\n\t\t\t// Handle delimiter between methods\n\t\t\tcontinue\n\t\t}\n\n\t\tif p.end() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif !p.end() {\n\t\tp.xerrorf(\"unexpected data at end of input\")\n\t}\n\n\treturn ar, nil\n}\n```\n\nThis implementation defines `ParseAuthResults` to parse an \"Authentication-Results\" header string and populate an `AuthResults` struct, excluding comments and allowing either CRLF or LF line endings. It uses the helper functions from the parser struct to correctly interpret the structure of the header fields."]}, "mox-store/msgreader.go-FileMsgReader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/store/msgreader.go:\n```\npackage store\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n)\n\n// MsgReader provides access to a message. Reads return the \"msg_prefix\" in the\n// database (typically received headers), followed by the on-disk msg file\n// contents. MsgReader is an io.Reader, io.ReaderAt and io.Closer.\ntype MsgReader struct {\n\tprefix []byte   // First part of the message. Typically contains received headers.\n\tpath   string   // To on-disk message file.\n\tsize   int64    // Total size of message, including prefix and contents from path.\n\toffset int64    // Current reading offset.\n\tf      *os.File // Opened path, automatically opened after prefix has been read.\n\terr    error    // If set, error to return for reads. Sets io.EOF for readers, but ReadAt ignores them.\n}\n\nvar errMsgClosed = errors.New(\"msg is closed\")\n\n// FileMsgReader makes a MsgReader for an open file.\n// If initialization fails, reads will return the error.\n// Only call close on the returned MsgReader if you want to close msgFile.\n\n\n\n\n\n\n\n\n\n\n\n// Read reads data from the msg, taking prefix and on-disk msg file into account.\n// The read offset is adjusted after the read.\nfunc (m *MsgReader) Read(buf []byte) (int, error) {\n\treturn m.read(buf, m.offset, false)\n}\n\n// ReadAt reads data from the msg, taking prefix and on-disk msg file into account.\n// The read offset is not affected by ReadAt.\nfunc (m *MsgReader) ReadAt(buf []byte, off int64) (n int, err error) {\n\treturn m.read(buf, off, true)\n}\n\n// read always fill buf as far as possible, for ReadAt semantics.\nfunc (m *MsgReader) read(buf []byte, off int64, pread bool) (int, error) {\n\t// If a reader has consumed the file and reached EOF, further ReadAt must not return eof.\n\tif m.err != nil && (!pread || m.err != io.EOF) {\n\t\treturn 0, m.err\n\t}\n\tvar o int\n\tfor o < len(buf) {\n\t\t// First attempt to read from m.prefix.\n\t\tpn := int64(len(m.prefix)) - off\n\t\tif pn > 0 {\n\t\t\tn := len(buf)\n\t\t\tif int64(n) > pn {\n\t\t\t\tn = int(pn)\n\t\t\t}\n\t\t\tcopy(buf[o:], m.prefix[int(off):int(off)+n])\n\t\t\to += n\n\t\t\toff += int64(n)\n\t\t\tif !pread {\n\t\t\t\tm.offset += int64(n)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Now we need to read from file. Ensure it is open.\n\t\tif m.f == nil {\n\t\t\tf, err := os.Open(m.path)\n\t\t\tif err != nil {\n\t\t\t\tm.err = err\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tm.f = f\n\t\t}\n\t\tn, err := m.f.ReadAt(buf[o:], off-int64(len(m.prefix)))\n\t\tif !pread && n > 0 {\n\t\t\tm.offset += int64(n)\n\t\t}\n\t\tif !pread || err != io.EOF {\n\t\t\tm.err = err\n\t\t}\n\t\tif n > 0 {\n\t\t\to += n\n\t\t\toff += int64(n)\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tif off > m.size && (m.err == nil || m.err == io.EOF) {\n\t\t\t\terr = fmt.Errorf(\"on-disk message larger than expected (off %d, size %d)\", off, m.size)\n\t\t\t\tm.err = err\n\t\t\t}\n\t\t\treturn o, err\n\t\t}\n\t\tif n <= 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\tif off > m.size && (m.err == nil || m.err == io.EOF) {\n\t\tm.err = fmt.Errorf(\"on-disk message larger than expected (off %d, size %d, prefix %d)\", off, m.size, len(m.prefix))\n\t}\n\treturn o, m.err\n}\n\n// Close ensures the msg file is closed. Further reads will fail.\nfunc (m *MsgReader) Close() error {\n\tif m.f != nil {\n\t\tif err := m.f.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm.f = nil\n\t}\n\tif m.err == errMsgClosed {\n\t\treturn m.err\n\t}\n\tm.err = errMsgClosed\n\treturn nil\n}\n\n// Reset rewinds the offset and clears error conditions, making it usable as a fresh reader.\nfunc (m *MsgReader) Reset() {\n\tm.offset = 0\n\tm.err = nil\n}\n\n// Size returns the total size of the contents of the message.\nfunc (m *MsgReader) Size() int64 {\n\treturn m.size\n}\n\n```\n# Function Signature: func FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n# Natural Language Annotations: // FileMsgReader makes a MsgReader for an open file.\n// If initialization fails, reads will return the error.\n// Only call close on the returned MsgReader if you want to close msgFile.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n\t// Get the file size\n\tfileInfo, err := msgFile.Stat()\n\tif err != nil {\n\t\t// If there is an error getting the file info, create a MsgReader with the error set\n\t\treturn &MsgReader{\n\t\t\tprefix: prefix,\n\t\t\tpath:   msgFile.Name(),\n\t\t\tf:      msgFile,\n\t\t\terr:    err,\n\t\t}\n\t}\n\n\t// Create and return a MsgReader with the provided prefix, msgFile, and computed size\n\treturn &MsgReader{\n\t\tprefix: prefix,\n\t\tpath:   msgFile.Name(),\n\t\tsize:   int64(len(prefix)) + fileInfo.Size(),\n\t\tf:      msgFile,\n\t}\n}\n```"]}, "mox-store/import.go-ParseDovecotKeywordsFlags": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/store/import.go:\n```\npackage store\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// MsgSource is implemented by readers for mailbox file formats.\ntype MsgSource interface {\n\t// Return next message, or io.EOF when there are no more.\n\tNext() (*Message, *os.File, string, error)\n}\n\n// MboxReader reads messages from an mbox file, implementing MsgSource.\ntype MboxReader struct {\n\tlog        mlog.Log\n\tcreateTemp func(log mlog.Log, pattern string) (*os.File, error)\n\tpath       string\n\tline       int\n\tr          *bufio.Reader\n\tprevempty  bool\n\tnonfirst   bool\n\teof        bool\n\tfromLine   string // \"From \"-line for this message.\n\theader     bool   // Now in header section.\n}\n\nfunc NewMboxReader(log mlog.Log, createTemp func(log mlog.Log, pattern string) (*os.File, error), filename string, r io.Reader) *MboxReader {\n\treturn &MboxReader{\n\t\tlog:        log,\n\t\tcreateTemp: createTemp,\n\t\tpath:       filename,\n\t\tline:       1,\n\t\tr:          bufio.NewReader(r),\n\t}\n}\n\n// Position returns \"<filename>:<lineno>\" for the current position.\nfunc (mr *MboxReader) Position() string {\n\treturn fmt.Sprintf(\"%s:%d\", mr.path, mr.line)\n}\n\n// Next returns the next message read from the mbox file. The file is a temporary\n// file and must be removed/consumed. The third return value is the position in the\n// file.\nfunc (mr *MboxReader) Next() (*Message, *os.File, string, error) {\n\tif mr.eof {\n\t\treturn nil, nil, \"\", io.EOF\n\t}\n\n\tfrom := []byte(\"From \")\n\n\tif !mr.nonfirst {\n\t\tmr.header = true\n\t\t// First read, we're at the beginning of the file.\n\t\tline, err := mr.r.ReadBytes('\\n')\n\t\tif err == io.EOF {\n\t\t\treturn nil, nil, \"\", io.EOF\n\t\t}\n\t\tmr.line++\n\n\t\tif !bytes.HasPrefix(line, from) {\n\t\t\treturn nil, nil, mr.Position(), fmt.Errorf(`first line does not start with \"From \"`)\n\t\t}\n\t\tmr.nonfirst = true\n\t\tmr.fromLine = strings.TrimSpace(string(line))\n\t}\n\n\tf, err := mr.createTemp(mr.log, \"mboxreader\")\n\tif err != nil {\n\t\treturn nil, nil, mr.Position(), err\n\t}\n\tdefer func() {\n\t\tif f != nil {\n\t\t\tCloseRemoveTempFile(mr.log, f, \"message after mbox read error\")\n\t\t}\n\t}()\n\n\tfromLine := mr.fromLine\n\tbf := bufio.NewWriter(f)\n\tvar flags Flags\n\tkeywords := map[string]bool{}\n\tvar size int64\n\tfor {\n\t\tline, err := mr.r.ReadBytes('\\n')\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn nil, nil, mr.Position(), fmt.Errorf(\"reading from mbox: %v\", err)\n\t\t}\n\t\tif len(line) > 0 {\n\t\t\tmr.line++\n\t\t\t// We store data with crlf, adjust any imported messages with bare newlines.\n\t\t\tif !bytes.HasSuffix(line, []byte(\"\\r\\n\")) {\n\t\t\t\tline = append(line[:len(line)-1], \"\\r\\n\"...)\n\t\t\t}\n\n\t\t\tif mr.header {\n\t\t\t\t// See https://doc.dovecot.org/admin_manual/mailbox_formats/mbox/\n\t\t\t\tif bytes.HasPrefix(line, []byte(\"Status:\")) {\n\t\t\t\t\ts := strings.TrimSpace(strings.SplitN(string(line), \":\", 2)[1])\n\t\t\t\t\tfor _, c := range s {\n\t\t\t\t\t\tswitch c {\n\t\t\t\t\t\tcase 'R':\n\t\t\t\t\t\t\tflags.Seen = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if bytes.HasPrefix(line, []byte(\"X-Status:\")) {\n\t\t\t\t\ts := strings.TrimSpace(strings.SplitN(string(line), \":\", 2)[1])\n\t\t\t\t\tfor _, c := range s {\n\t\t\t\t\t\tswitch c {\n\t\t\t\t\t\tcase 'A':\n\t\t\t\t\t\t\tflags.Answered = true\n\t\t\t\t\t\tcase 'F':\n\t\t\t\t\t\t\tflags.Flagged = true\n\t\t\t\t\t\tcase 'T':\n\t\t\t\t\t\t\tflags.Draft = true\n\t\t\t\t\t\tcase 'D':\n\t\t\t\t\t\t\tflags.Deleted = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if bytes.HasPrefix(line, []byte(\"X-Keywords:\")) {\n\t\t\t\t\ts := strings.TrimSpace(strings.SplitN(string(line), \":\", 2)[1])\n\t\t\t\t\tfor _, t := range strings.Split(s, \",\") {\n\t\t\t\t\t\tword := strings.ToLower(strings.TrimSpace(t))\n\t\t\t\t\t\tswitch word {\n\t\t\t\t\t\tcase \"forwarded\", \"$forwarded\":\n\t\t\t\t\t\t\tflags.Forwarded = true\n\t\t\t\t\t\tcase \"junk\", \"$junk\":\n\t\t\t\t\t\t\tflags.Junk = true\n\t\t\t\t\t\tcase \"notjunk\", \"$notjunk\", \"nonjunk\", \"$nonjunk\":\n\t\t\t\t\t\t\tflags.Notjunk = true\n\t\t\t\t\t\tcase \"phishing\", \"$phishing\":\n\t\t\t\t\t\t\tflags.Phishing = true\n\t\t\t\t\t\tcase \"mdnsent\", \"$mdnsent\":\n\t\t\t\t\t\t\tflags.MDNSent = true\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tif err := CheckKeyword(word); err == nil {\n\t\t\t\t\t\t\t\tkeywords[word] = true\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif bytes.Equal(line, []byte(\"\\r\\n\")) {\n\t\t\t\tmr.header = false\n\t\t\t}\n\n\t\t\t// Next mail message starts at bare From word.\n\t\t\tif mr.prevempty && bytes.HasPrefix(line, from) {\n\t\t\t\tmr.fromLine = strings.TrimSpace(string(line))\n\t\t\t\tmr.header = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif bytes.HasPrefix(line, []byte(\">\")) && bytes.HasPrefix(bytes.TrimLeft(line, \">\"), []byte(\"From \")) {\n\t\t\t\tline = line[1:]\n\t\t\t}\n\t\t\tn, err := bf.Write(line)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, mr.Position(), fmt.Errorf(\"writing message to file: %v\", err)\n\t\t\t}\n\t\t\tsize += int64(n)\n\t\t\tmr.prevempty = bytes.Equal(line, []byte(\"\\r\\n\"))\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tmr.eof = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif err := bf.Flush(); err != nil {\n\t\treturn nil, nil, mr.Position(), fmt.Errorf(\"flush: %v\", err)\n\t}\n\n\tm := &Message{Flags: flags, Keywords: maps.Keys(keywords), Size: size}\n\n\tif t := strings.SplitN(fromLine, \" \", 3); len(t) == 3 {\n\t\tlayouts := []string{time.ANSIC, time.UnixDate, time.RubyDate}\n\t\tfor _, l := range layouts {\n\t\t\tt, err := time.Parse(l, t[2])\n\t\t\tif err == nil {\n\t\t\t\tm.Received = t\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// Prevent cleanup by defer.\n\tmf := f\n\tf = nil\n\n\treturn m, mf, mr.Position(), nil\n}\n\ntype MaildirReader struct {\n\tlog          mlog.Log\n\tcreateTemp   func(log mlog.Log, pattern string) (*os.File, error)\n\tnewf, curf   *os.File\n\tf            *os.File // File we are currently reading from. We first read newf, then curf.\n\tdir          string   // Name of directory for f. Can be empty on first call.\n\tentries      []os.DirEntry\n\tdovecotFlags []string // Lower-case flags/keywords.\n}\n\nfunc NewMaildirReader(log mlog.Log, createTemp func(log mlog.Log, pattern string) (*os.File, error), newf, curf *os.File) *MaildirReader {\n\tmr := &MaildirReader{\n\t\tlog:        log,\n\t\tcreateTemp: createTemp,\n\t\tnewf:       newf,\n\t\tcurf:       curf,\n\t\tf:          newf,\n\t}\n\n\t// Best-effort parsing of dovecot keywords.\n\tkf, err := os.Open(filepath.Join(filepath.Dir(newf.Name()), \"dovecot-keywords\"))\n\tif err == nil {\n\t\tmr.dovecotFlags, err = ParseDovecotKeywordsFlags(kf, log)\n\t\tlog.Check(err, \"parsing dovecot keywords file\")\n\t\terr = kf.Close()\n\t\tlog.Check(err, \"closing dovecot-keywords file\")\n\t}\n\n\treturn mr\n}\n\nfunc (mr *MaildirReader) Next() (*Message, *os.File, string, error) {\n\tif mr.dir == \"\" {\n\t\tmr.dir = mr.f.Name()\n\t}\n\n\tif len(mr.entries) == 0 {\n\t\tvar err error\n\t\tmr.entries, err = mr.f.ReadDir(100)\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn nil, nil, \"\", err\n\t\t}\n\t\tif len(mr.entries) == 0 {\n\t\t\tif mr.f == mr.curf {\n\t\t\t\treturn nil, nil, \"\", io.EOF\n\t\t\t}\n\t\t\tmr.f = mr.curf\n\t\t\tmr.dir = \"\"\n\t\t\treturn mr.Next()\n\t\t}\n\t}\n\n\tp := filepath.Join(mr.dir, mr.entries[0].Name())\n\tmr.entries = mr.entries[1:]\n\tsf, err := os.Open(p)\n\tif err != nil {\n\t\treturn nil, nil, p, fmt.Errorf(\"open message in maildir: %s\", err)\n\t}\n\tdefer func() {\n\t\terr := sf.Close()\n\t\tmr.log.Check(err, \"closing message file after error\")\n\t}()\n\tf, err := mr.createTemp(mr.log, \"maildirreader\")\n\tif err != nil {\n\t\treturn nil, nil, p, err\n\t}\n\tdefer func() {\n\t\tif f != nil {\n\t\t\tname := f.Name()\n\t\t\terr := f.Close()\n\t\t\tmr.log.Check(err, \"closing temporary message file after maildir read error\")\n\t\t\terr = os.Remove(name)\n\t\t\tmr.log.Check(err, \"removing temporary message file after maildir read error\", slog.String(\"path\", name))\n\t\t}\n\t}()\n\n\t// Copy data, changing bare \\n into \\r\\n.\n\tr := bufio.NewReader(sf)\n\tw := bufio.NewWriter(f)\n\tvar size int64\n\tfor {\n\t\tline, err := r.ReadBytes('\\n')\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn nil, nil, p, fmt.Errorf(\"reading message: %v\", err)\n\t\t}\n\t\tif len(line) > 0 {\n\t\t\tif !bytes.HasSuffix(line, []byte(\"\\r\\n\")) {\n\t\t\t\tline = append(line[:len(line)-1], \"\\r\\n\"...)\n\t\t\t}\n\n\t\t\tif n, err := w.Write(line); err != nil {\n\t\t\t\treturn nil, nil, p, fmt.Errorf(\"writing message: %v\", err)\n\t\t\t} else {\n\t\t\t\tsize += int64(n)\n\t\t\t}\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t}\n\tif err := w.Flush(); err != nil {\n\t\treturn nil, nil, p, fmt.Errorf(\"writing message: %v\", err)\n\t}\n\n\t// Take received time from filename, falling back to mtime for maildirs\n\t// reconstructed some other sources of message files.\n\tvar received time.Time\n\tt := strings.SplitN(filepath.Base(sf.Name()), \".\", 3)\n\tif v, err := strconv.ParseInt(t[0], 10, 64); len(t) == 3 && err == nil {\n\t\treceived = time.Unix(v, 0)\n\t} else if fi, err := sf.Stat(); err == nil {\n\t\treceived = fi.ModTime()\n\t}\n\n\t// Parse flags. See https://cr.yp.to/proto/maildir.html.\n\tflags := Flags{}\n\tkeywords := map[string]bool{}\n\tt = strings.SplitN(filepath.Base(sf.Name()), \":2,\", 2)\n\tif len(t) == 2 {\n\t\tfor _, c := range t[1] {\n\t\t\tswitch c {\n\t\t\tcase 'P':\n\t\t\t\t// Passed, doesn't map to a common IMAP flag.\n\t\t\tcase 'R':\n\t\t\t\tflags.Answered = true\n\t\t\tcase 'S':\n\t\t\t\tflags.Seen = true\n\t\t\tcase 'T':\n\t\t\t\tflags.Deleted = true\n\t\t\tcase 'D':\n\t\t\t\tflags.Draft = true\n\t\t\tcase 'F':\n\t\t\t\tflags.Flagged = true\n\t\t\tdefault:\n\t\t\t\tif c >= 'a' && c <= 'z' {\n\t\t\t\t\tindex := int(c - 'a')\n\t\t\t\t\tif index >= len(mr.dovecotFlags) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tkw := mr.dovecotFlags[index]\n\t\t\t\t\tswitch kw {\n\t\t\t\t\tcase \"$forwarded\", \"forwarded\":\n\t\t\t\t\t\tflags.Forwarded = true\n\t\t\t\t\tcase \"$junk\", \"junk\":\n\t\t\t\t\t\tflags.Junk = true\n\t\t\t\t\tcase \"$notjunk\", \"notjunk\", \"nonjunk\":\n\t\t\t\t\t\tflags.Notjunk = true\n\t\t\t\t\tcase \"$mdnsent\", \"mdnsent\":\n\t\t\t\t\t\tflags.MDNSent = true\n\t\t\t\t\tcase \"$phishing\", \"phishing\":\n\t\t\t\t\t\tflags.Phishing = true\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tkeywords[kw] = true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tm := &Message{Received: received, Flags: flags, Keywords: maps.Keys(keywords), Size: size}\n\n\t// Prevent cleanup by defer.\n\tmf := f\n\tf = nil\n\n\treturn m, mf, p, nil\n}\n\n// ParseDovecotKeywordsFlags attempts to parse a dovecot-keywords file. It only\n// returns valid flags/keywords, as lower-case. If an error is encountered and\n// returned, any keywords that were found are still returned. The returned list has\n// both system/well-known flags and custom keywords.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n# Natural Language Annotations: // ParseDovecotKeywordsFlags attempts to parse a dovecot-keywords file. It only\n// returns valid flags/keywords, as lower-case. If an error is encountered and\n// returned, any keywords that were found are still returned. The returned list has\n// both system/well-known flags and custom keywords.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n\tvar keywords []string\n\tscanner := bufio.NewScanner(r)\n\tlineNumber := 0\n\n\tfor scanner.Scan() {\n\t\tline := scanner.Text()\n\t\tlineNumber++\n\t\t// Trim spaces and check for empty lines or comments\n\t\tline = strings.TrimSpace(line)\n\t\tif len(line) == 0 || strings.HasPrefix(line, \"#\") {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Convert to lower-case as specified\n\t\tkeyword := strings.ToLower(line)\n\t\t// Validate the keyword using a fictional validation function\n\t\tif err := CheckKeyword(keyword); err == nil {\n\t\t\tkeywords = append(keywords, keyword)\n\t\t} else {\n\t\t\tlog.Error(\"invalid keyword\", slog.String(\"keyword\", keyword), slog.Int(\"line\", lineNumber), slog.Error(err))\n\t\t}\n\t}\n\n\tif err := scanner.Err(); err != nil {\n\t\tlog.Error(\"error reading dovecot-keywords file\", slog.Error(err))\n\t\treturn keywords, err\n\t}\n\n\treturn keywords, nil\n}\n```\n"]}, "mox-store/account.go-MessageRuleset": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/store/account.go:\n```\n/*\nPackage store implements storage for accounts, their mailboxes, IMAP\nsubscriptions and messages, and broadcasts updates (e.g. mail delivery) to\ninterested sessions (e.g. IMAP connections).\n\nLayout of storage for accounts:\n\n\t<DataDir>/accounts/<name>/index.db\n\t<DataDir>/accounts/<name>/msg/[a-zA-Z0-9_-]+/<id>\n\nIndex.db holds tables for user information, mailboxes, and messages. Messages\nare stored in the msg/ subdirectory, each in their own file. The on-disk message\ndoes not contain headers generated during an incoming SMTP transaction, such as\nReceived and Authentication-Results headers. Those are in the database to\nprevent having to rewrite incoming messages (e.g. Authentication-Result for DKIM\nsignatures can only be determined after having read the message). Messages must\nbe read through MsgReader, which transparently adds the prefix from the\ndatabase.\n*/\npackage store\n\n// todo: make up a function naming scheme that indicates whether caller should broadcast changes.\n\nimport (\n\t\"context\"\n\t\"crypto/md5\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/sha1\"\n\t\"crypto/sha256\"\n\t\"encoding\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/bcrypt\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/config\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/scram\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// If true, each time an account is closed its database file is checked for\n// consistency. If an inconsistency is found, panic is called. Set by default\n// because of all the packages with tests, the mox main function sets it to\n// false again.\nvar CheckConsistencyOnClose = true\n\nvar (\n\tErrUnknownMailbox     = errors.New(\"no such mailbox\")\n\tErrUnknownCredentials = errors.New(\"credentials not found\")\n\tErrAccountUnknown     = errors.New(\"no such account\")\n\tErrOverQuota          = errors.New(\"account over quota\")\n)\n\nvar DefaultInitialMailboxes = config.InitialMailboxes{\n\tSpecialUse: config.SpecialUseMailboxes{\n\t\tSent:    \"Sent\",\n\t\tArchive: \"Archive\",\n\t\tTrash:   \"Trash\",\n\t\tDraft:   \"Drafts\",\n\t\tJunk:    \"Junk\",\n\t},\n}\n\ntype SCRAM struct {\n\tSalt           []byte\n\tIterations     int\n\tSaltedPassword []byte\n}\n\n// CRAMMD5 holds HMAC ipad and opad hashes that are initialized with the first\n// block with (a derivation of) the key/password, so we don't store the password in plain\n// text.\ntype CRAMMD5 struct {\n\tIpad hash.Hash\n\tOpad hash.Hash\n}\n\n// BinaryMarshal is used by bstore to store the ipad/opad hash states.\nfunc (c CRAMMD5) MarshalBinary() ([]byte, error) {\n\tif c.Ipad == nil || c.Opad == nil {\n\t\treturn nil, nil\n\t}\n\n\tipad, err := c.Ipad.(encoding.BinaryMarshaler).MarshalBinary()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"marshal ipad: %v\", err)\n\t}\n\topad, err := c.Opad.(encoding.BinaryMarshaler).MarshalBinary()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"marshal opad: %v\", err)\n\t}\n\tbuf := make([]byte, 2+len(ipad)+len(opad))\n\tipadlen := uint16(len(ipad))\n\tbuf[0] = byte(ipadlen >> 8)\n\tbuf[1] = byte(ipadlen >> 0)\n\tcopy(buf[2:], ipad)\n\tcopy(buf[2+len(ipad):], opad)\n\treturn buf, nil\n}\n\n// BinaryUnmarshal is used by bstore to restore the ipad/opad hash states.\nfunc (c *CRAMMD5) UnmarshalBinary(buf []byte) error {\n\tif len(buf) == 0 {\n\t\t*c = CRAMMD5{}\n\t\treturn nil\n\t}\n\tif len(buf) < 2 {\n\t\treturn fmt.Errorf(\"short buffer\")\n\t}\n\tipadlen := int(uint16(buf[0])<<8 | uint16(buf[1])<<0)\n\tif len(buf) < 2+ipadlen {\n\t\treturn fmt.Errorf(\"buffer too short for ipadlen\")\n\t}\n\tipad := md5.New()\n\topad := md5.New()\n\tif err := ipad.(encoding.BinaryUnmarshaler).UnmarshalBinary(buf[2 : 2+ipadlen]); err != nil {\n\t\treturn fmt.Errorf(\"unmarshal ipad: %v\", err)\n\t}\n\tif err := opad.(encoding.BinaryUnmarshaler).UnmarshalBinary(buf[2+ipadlen:]); err != nil {\n\t\treturn fmt.Errorf(\"unmarshal opad: %v\", err)\n\t}\n\t*c = CRAMMD5{ipad, opad}\n\treturn nil\n}\n\n// Password holds credentials in various forms, for logging in with SMTP/IMAP.\ntype Password struct {\n\tHash        string  // bcrypt hash for IMAP LOGIN, SASL PLAIN and HTTP basic authentication.\n\tCRAMMD5     CRAMMD5 // For SASL CRAM-MD5.\n\tSCRAMSHA1   SCRAM   // For SASL SCRAM-SHA-1.\n\tSCRAMSHA256 SCRAM   // For SASL SCRAM-SHA-256.\n}\n\n// Subjectpass holds the secret key used to sign subjectpass tokens.\ntype Subjectpass struct {\n\tEmail string // Our destination address (canonical, with catchall localpart stripped).\n\tKey   string\n}\n\n// NextUIDValidity is a singleton record in the database with the next UIDValidity\n// to use for the next mailbox.\ntype NextUIDValidity struct {\n\tID   int // Just a single record with ID 1.\n\tNext uint32\n}\n\n// SyncState track ModSeqs.\ntype SyncState struct {\n\tID int // Just a single record with ID 1.\n\n\t// Last used, next assigned will be one higher. The first value we hand out is 2.\n\t// That's because 0 (the default value for old existing messages, from before the\n\t// Message.ModSeq field) is special in IMAP, so we return it as 1.\n\tLastModSeq ModSeq `bstore:\"nonzero\"`\n\n\t// Highest ModSeq of expunged record that we deleted. When a clients synchronizes\n\t// and requests changes based on a modseq before this one, we don't have the\n\t// history to provide information about deletions. We normally keep these expunged\n\t// records around, but we may periodically truly delete them to reclaim storage\n\t// space. Initially set to -1 because we don't want to match with any ModSeq in the\n\t// database, which can be zero values.\n\tHighestDeletedModSeq ModSeq\n}\n\n// Mailbox is collection of messages, e.g. Inbox or Sent.\ntype Mailbox struct {\n\tID int64\n\n\t// \"Inbox\" is the name for the special IMAP \"INBOX\". Slash separated\n\t// for hierarchy.\n\tName string `bstore:\"nonzero,unique\"`\n\n\t// If UIDs are invalidated, e.g. when renaming a mailbox to a previously existing\n\t// name, UIDValidity must be changed. Used by IMAP for synchronization.\n\tUIDValidity uint32\n\n\t// UID likely to be assigned to next message. Used by IMAP to detect messages\n\t// delivered to a mailbox.\n\tUIDNext UID\n\n\tSpecialUse\n\n\t// Keywords as used in messages. Storing a non-system keyword for a message\n\t// automatically adds it to this list. Used in the IMAP FLAGS response. Only\n\t// \"atoms\" are allowed (IMAP syntax), keywords are case-insensitive, only stored in\n\t// lower case (for JMAP), sorted.\n\tKeywords []string\n\n\tHaveCounts    bool // Whether MailboxCounts have been initialized.\n\tMailboxCounts      // Statistics about messages, kept up to date whenever a change happens.\n}\n\n// MailboxCounts tracks statistics about messages for a mailbox.\ntype MailboxCounts struct {\n\tTotal   int64 // Total number of messages, excluding \\Deleted. For JMAP.\n\tDeleted int64 // Number of messages with \\Deleted flag. Used for IMAP message count that includes messages with \\Deleted.\n\tUnread  int64 // Messages without \\Seen, excluding those with \\Deleted, for JMAP.\n\tUnseen  int64 // Messages without \\Seen, including those with \\Deleted, for IMAP.\n\tSize    int64 // Number of bytes for all messages.\n}\n\nfunc (mc MailboxCounts) String() string {\n\treturn fmt.Sprintf(\"%d total, %d deleted, %d unread, %d unseen, size %d bytes\", mc.Total, mc.Deleted, mc.Unread, mc.Unseen, mc.Size)\n}\n\n// Add increases mailbox counts mc with those of delta.\nfunc (mc *MailboxCounts) Add(delta MailboxCounts) {\n\tmc.Total += delta.Total\n\tmc.Deleted += delta.Deleted\n\tmc.Unread += delta.Unread\n\tmc.Unseen += delta.Unseen\n\tmc.Size += delta.Size\n}\n\n// Add decreases mailbox counts mc with those of delta.\nfunc (mc *MailboxCounts) Sub(delta MailboxCounts) {\n\tmc.Total -= delta.Total\n\tmc.Deleted -= delta.Deleted\n\tmc.Unread -= delta.Unread\n\tmc.Unseen -= delta.Unseen\n\tmc.Size -= delta.Size\n}\n\n// SpecialUse identifies a specific role for a mailbox, used by clients to\n// understand where messages should go.\ntype SpecialUse struct {\n\tArchive bool\n\tDraft   bool\n\tJunk    bool\n\tSent    bool\n\tTrash   bool\n}\n\n// CalculateCounts calculates the full current counts for messages in the mailbox.\nfunc (mb *Mailbox) CalculateCounts(tx *bstore.Tx) (mc MailboxCounts, err error) {\n\tq := bstore.QueryTx[Message](tx)\n\tq.FilterNonzero(Message{MailboxID: mb.ID})\n\tq.FilterEqual(\"Expunged\", false)\n\terr = q.ForEach(func(m Message) error {\n\t\tmc.Add(m.MailboxCounts())\n\t\treturn nil\n\t})\n\treturn\n}\n\n// ChangeSpecialUse returns a change for special-use flags, for broadcasting to\n// other connections.\nfunc (mb Mailbox) ChangeSpecialUse() ChangeMailboxSpecialUse {\n\treturn ChangeMailboxSpecialUse{mb.ID, mb.Name, mb.SpecialUse}\n}\n\n// ChangeKeywords returns a change with new keywords for a mailbox (e.g. after\n// setting a new keyword on a message in the mailbox), for broadcasting to other\n// connections.\nfunc (mb Mailbox) ChangeKeywords() ChangeMailboxKeywords {\n\treturn ChangeMailboxKeywords{mb.ID, mb.Name, mb.Keywords}\n}\n\n// KeywordsChanged returns whether the keywords in a mailbox have changed.\nfunc (mb Mailbox) KeywordsChanged(origmb Mailbox) bool {\n\tif len(mb.Keywords) != len(origmb.Keywords) {\n\t\treturn true\n\t}\n\t// Keywords are stored sorted.\n\tfor i, kw := range mb.Keywords {\n\t\tif origmb.Keywords[i] != kw {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// CountsChange returns a change with mailbox counts.\nfunc (mb Mailbox) ChangeCounts() ChangeMailboxCounts {\n\treturn ChangeMailboxCounts{mb.ID, mb.Name, mb.MailboxCounts}\n}\n\n// Subscriptions are separate from existence of mailboxes.\ntype Subscription struct {\n\tName string\n}\n\n// Flags for a mail message.\ntype Flags struct {\n\tSeen      bool\n\tAnswered  bool\n\tFlagged   bool\n\tForwarded bool\n\tJunk      bool\n\tNotjunk   bool\n\tDeleted   bool\n\tDraft     bool\n\tPhishing  bool\n\tMDNSent   bool\n}\n\n// FlagsAll is all flags set, for use as mask.\nvar FlagsAll = Flags{true, true, true, true, true, true, true, true, true, true}\n\n// Validation of \"message From\" domain.\ntype Validation uint8\n\nconst (\n\tValidationUnknown   Validation = 0\n\tValidationStrict    Validation = 1 // Like DMARC, with strict policies.\n\tValidationDMARC     Validation = 2 // Actual DMARC policy.\n\tValidationRelaxed   Validation = 3 // Like DMARC, with relaxed policies.\n\tValidationPass      Validation = 4 // For SPF.\n\tValidationNeutral   Validation = 5 // For SPF.\n\tValidationTemperror Validation = 6\n\tValidationPermerror Validation = 7\n\tValidationFail      Validation = 8\n\tValidationSoftfail  Validation = 9  // For SPF.\n\tValidationNone      Validation = 10 // E.g. No records.\n)\n\n// Message stored in database and per-message file on disk.\n//\n// Contents are always the combined data from MsgPrefix and the on-disk file named\n// based on ID.\n//\n// Messages always have a header section, even if empty. Incoming messages without\n// header section must get an empty header section added before inserting.\ntype Message struct {\n\t// ID, unchanged over lifetime, determines path to on-disk msg file.\n\t// Set during deliver.\n\tID int64\n\n\tUID       UID   `bstore:\"nonzero\"` // UID, for IMAP. Set during deliver.\n\tMailboxID int64 `bstore:\"nonzero,unique MailboxID+UID,index MailboxID+Received,index MailboxID+ModSeq,ref Mailbox\"`\n\n\t// Modification sequence, for faster syncing with IMAP QRESYNC and JMAP.\n\t// ModSeq is the last modification. CreateSeq is the Seq the message was inserted,\n\t// always <= ModSeq. If Expunged is set, the message has been removed and should not\n\t// be returned to the user. In this case, ModSeq is the Seq where the message is\n\t// removed, and will never be changed again.\n\t// We have an index on both ModSeq (for JMAP that synchronizes per account) and\n\t// MailboxID+ModSeq (for IMAP that synchronizes per mailbox).\n\t// The index on CreateSeq helps efficiently finding created messages for JMAP.\n\t// The value of ModSeq is special for IMAP. Messages that existed before ModSeq was\n\t// added have 0 as value. But modseq 0 in IMAP is special, so we return it as 1. If\n\t// we get modseq 1 from a client, the IMAP server will translate it to 0. When we\n\t// return modseq to clients, we turn 0 into 1.\n\tModSeq    ModSeq `bstore:\"index\"`\n\tCreateSeq ModSeq `bstore:\"index\"`\n\tExpunged  bool\n\n\t// If set, this message was delivered to a Rejects mailbox. When it is moved to a\n\t// different mailbox, its MailboxOrigID is set to the destination mailbox and this\n\t// flag cleared.\n\tIsReject bool\n\n\t// If set, this is a forwarded message (through a ruleset with IsForward). This\n\t// causes fields used during junk analysis to be moved to their Orig variants, and\n\t// masked IP fields cleared, so they aren't used in junk classifications for\n\t// incoming messages. This ensures the forwarded messages don't cause negative\n\t// reputation for the forwarding mail server, which may also be sending regular\n\t// messages.\n\tIsForward bool\n\n\t// MailboxOrigID is the mailbox the message was originally delivered to. Typically\n\t// Inbox or Rejects, but can also be a mailbox configured in a Ruleset, or\n\t// Postmaster, TLS/DMARC reporting addresses. MailboxOrigID is not changed when the\n\t// message is moved to another mailbox, e.g. Archive/Trash/Junk. Used for\n\t// per-mailbox reputation.\n\t//\n\t// MailboxDestinedID is normally 0, but when a message is delivered to the Rejects\n\t// mailbox, it is set to the intended mailbox according to delivery rules,\n\t// typically that of Inbox. When such a message is moved out of Rejects, the\n\t// MailboxOrigID is corrected by setting it to MailboxDestinedID. This ensures the\n\t// message is used for reputation calculation for future deliveries to that\n\t// mailbox.\n\t//\n\t// These are not bstore references to prevent having to update all messages in a\n\t// mailbox when the original mailbox is removed. Use of these fields requires\n\t// checking if the mailbox still exists.\n\tMailboxOrigID     int64\n\tMailboxDestinedID int64\n\n\tReceived time.Time `bstore:\"default now,index\"`\n\n\t// Full IP address of remote SMTP server. Empty if not delivered over SMTP. The\n\t// masked IPs are used to classify incoming messages. They are left empty for\n\t// messages matching a ruleset for forwarded messages.\n\tRemoteIP        string\n\tRemoteIPMasked1 string `bstore:\"index RemoteIPMasked1+Received\"` // For IPv4 /32, for IPv6 /64, for reputation.\n\tRemoteIPMasked2 string `bstore:\"index RemoteIPMasked2+Received\"` // For IPv4 /26, for IPv6 /48.\n\tRemoteIPMasked3 string `bstore:\"index RemoteIPMasked3+Received\"` // For IPv4 /21, for IPv6 /32.\n\n\t// Only set if present and not an IP address. Unicode string. Empty for forwarded\n\t// messages.\n\tEHLODomain        string         `bstore:\"index EHLODomain+Received\"`\n\tMailFrom          string         // With localpart and domain. Can be empty.\n\tMailFromLocalpart smtp.Localpart // SMTP \"MAIL FROM\", can be empty.\n\t// Only set if it is a domain, not an IP. Unicode string. Empty for forwarded\n\t// messages, but see OrigMailFromDomain.\n\tMailFromDomain  string         `bstore:\"index MailFromDomain+Received\"`\n\tRcptToLocalpart smtp.Localpart // SMTP \"RCPT TO\", can be empty.\n\tRcptToDomain    string         // Unicode string.\n\n\t// Parsed \"From\" message header, used for reputation along with domain validation.\n\tMsgFromLocalpart smtp.Localpart\n\tMsgFromDomain    string `bstore:\"index MsgFromDomain+Received\"`    // Unicode string.\n\tMsgFromOrgDomain string `bstore:\"index MsgFromOrgDomain+Received\"` // Unicode string.\n\n\t// Simplified statements of the Validation fields below, used for incoming messages\n\t// to check reputation.\n\tEHLOValidated     bool\n\tMailFromValidated bool\n\tMsgFromValidated  bool\n\n\tEHLOValidation     Validation // Validation can also take reverse IP lookup into account, not only SPF.\n\tMailFromValidation Validation // Can have SPF-specific validations like ValidationSoftfail.\n\tMsgFromValidation  Validation // Desirable validations: Strict, DMARC, Relaxed. Will not be just Pass.\n\n\t// Domains with verified DKIM signatures. Unicode string. For forwarded messages, a\n\t// DKIM domain that matched a ruleset's verified domain is left out, but included\n\t// in OrigDKIMDomains.\n\tDKIMDomains []string `bstore:\"index DKIMDomains+Received\"`\n\n\t// For forwarded messages,\n\tOrigEHLODomain  string\n\tOrigDKIMDomains []string\n\n\t// Canonicalized Message-Id, always lower-case and normalized quoting, without\n\t// <>'s. Empty if missing. Used for matching message threads, and to prevent\n\t// duplicate reject delivery.\n\tMessageID string `bstore:\"index\"`\n\t// lower-case: ../rfc/5256:495\n\n\t// For matching threads in case there is no References/In-Reply-To header. It is\n\t// lower-cased, white-space collapsed, mailing list tags and re/fwd tags removed.\n\tSubjectBase string `bstore:\"index\"`\n\t// ../rfc/5256:90\n\n\t// Hash of message. For rejects delivery in case there is no Message-ID, only set\n\t// when delivered as reject.\n\tMessageHash []byte\n\n\t// ID of message starting this thread.\n\tThreadID int64 `bstore:\"index\"`\n\t// IDs of parent messages, from closest parent to the root message. Parent messages\n\t// may be in a different mailbox, or may no longer exist. ThreadParentIDs must\n\t// never contain the message id itself (a cycle), and parent messages must\n\t// reference the same ancestors.\n\tThreadParentIDs []int64\n\t// ThreadMissingLink is true if there is no match with a direct parent. E.g. first\n\t// ID in ThreadParentIDs is not the direct ancestor (an intermediate message may\n\t// have been deleted), or subject-based matching was done.\n\tThreadMissingLink bool\n\t// If set, newly delivered child messages are automatically marked as read. This\n\t// field is copied to new child messages. Changes are propagated to the webmail\n\t// client.\n\tThreadMuted bool\n\t// If set, this (sub)thread is collapsed in the webmail client, for threading mode\n\t// \"on\" (mode \"unread\" ignores it). This field is copied to new child message.\n\t// Changes are propagated to the webmail client.\n\tThreadCollapsed bool\n\n\t// If received message was known to match a mailing list rule (with modified junk\n\t// filtering).\n\tIsMailingList bool\n\n\t// If this message is a DSN, generated by us or received. For DSNs, we don't look\n\t// at the subject when matching threads.\n\tDSN bool\n\n\tReceivedTLSVersion     uint16 // 0 if unknown, 1 if plaintext/no TLS, otherwise TLS cipher suite.\n\tReceivedTLSCipherSuite uint16\n\tReceivedRequireTLS     bool // Whether RequireTLS was known to be used for incoming delivery.\n\n\tFlags\n\t// For keywords other than system flags or the basic well-known $-flags. Only in\n\t// \"atom\" syntax (IMAP), they are case-insensitive, always stored in lower-case\n\t// (for JMAP), sorted.\n\tKeywords    []string `bstore:\"index\"`\n\tSize        int64\n\tTrainedJunk *bool  // If nil, no training done yet. Otherwise, true is trained as junk, false trained as nonjunk.\n\tMsgPrefix   []byte // Typically holds received headers and/or header separator.\n\n\t// ParsedBuf message structure. Currently saved as JSON of message.Part because bstore\n\t// cannot yet store recursive types. Created when first needed, and saved in the\n\t// database.\n\t// todo: once replaced with non-json storage, remove date fixup in ../message/part.go.\n\tParsedBuf []byte\n}\n\n// MailboxCounts returns the delta to counts this message means for its\n// mailbox.\nfunc (m Message) MailboxCounts() (mc MailboxCounts) {\n\tif m.Expunged\n```\n# Function Signature: func MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n# Natural Language Annotations: // MessageRuleset returns the first ruleset (if any) that matches the message\n// represented by msgPrefix and msgFile, with smtp and validation fields from m.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// MessageRuleset returns the first ruleset (if any) that matches the message\n// represented by msgPrefix and msgFile, with SMTP and validation fields from m.\nfunc MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n\t// Create a new reader combining the msgPrefix and the content of the msgFile.\n\tmsgReader := io.MultiReader(bytes.NewReader(msgPrefix), msgFile)\n\n\t// Iterate over each ruleset in the given destination.\n\tfor _, ruleset := range dest.Rulesets {\n\t\t// Check if the current ruleset matches the message.\n\t\t// Perform checks based on SMTP fields and validation fields in the Message struct.\n\t\tif ruleset.Matches(msgReader, m) {\n\t\t\t// Return the first matching ruleset.\n\t\t\treturn &ruleset\n\t\t}\n\t}\n\n\t// If no matching ruleset is found, return nil.\n\treturn nil\n}\n```\n\nThis function iterates through a list of rulesets in a given destination configuration. It attempts to match each ruleset against the given message data, which includes fields from the message, the message prefix, and the message file contents. If a match is found, the function returns that ruleset. If no match is found, it returns `nil`."]}, "mox-smtp/address.go-ParseAddress": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtp/address.go:\n```\npackage smtp\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"golang.org/x/text/unicode/norm\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar ErrBadAddress = errors.New(\"invalid email address\")\n\n// Localpart is a decoded local part of an email address, before the \"@\".\n// For quoted strings, values do not hold the double quote or escaping backslashes.\n// An empty string can be a valid localpart.\n// Localparts are in Unicode NFC.\ntype Localpart string\n\n// String returns a packed representation of an address, with proper escaping/quoting, for use in SMTP.\nfunc (lp Localpart) String() string {\n\t// See ../rfc/5321:2322 ../rfc/6531:414\n\t// First we try as dot-string. If not possible we make a quoted-string.\n\tdotstr := true\n\tt := strings.Split(string(lp), \".\")\n\tfor _, e := range t {\n\t\tfor _, c := range e {\n\t\t\tif c >= '0' && c <= '9' || c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c > 0x7f {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdotstr = false\n\t\t\tbreak\n\t\t}\n\t\tdotstr = dotstr && len(e) > 0\n\t}\n\tdotstr = dotstr && len(t) > 0\n\tif dotstr {\n\t\treturn string(lp)\n\t}\n\n\t// Make quoted-string.\n\tr := `\"`\n\tfor _, b := range lp {\n\t\tif b == '\"' || b == '\\\\' {\n\t\t\tr += \"\\\\\" + string(b)\n\t\t} else {\n\t\t\tr += string(b)\n\t\t}\n\t}\n\tr += `\"`\n\treturn r\n}\n\n// LogString returns the localpart as string for use in smtp, and an escaped\n// representation if it has non-ascii characters.\nfunc (lp Localpart) LogString() string {\n\ts := lp.String()\n\tqs := strconv.QuoteToASCII(s)\n\tif qs != `\"`+s+`\"` {\n\t\ts = \"/\" + qs\n\t}\n\treturn s\n}\n\n// DSNString returns the localpart as string for use in a DSN.\n// utf8 indicates if the remote MTA supports utf8 messaging. If not, the 7bit DSN\n// encoding for \"utf-8-addr-xtext\" from RFC 6533 is used.\nfunc (lp Localpart) DSNString(utf8 bool) string {\n\tif utf8 {\n\t\treturn lp.String()\n\t}\n\t// ../rfc/6533:259\n\tr := \"\"\n\tfor _, c := range lp {\n\t\tif c > 0x20 && c < 0x7f && c != '\\\\' && c != '+' && c != '=' {\n\t\t\tr += string(c)\n\t\t} else {\n\t\t\tr += fmt.Sprintf(`\\x{%x}`, c)\n\t\t}\n\t}\n\treturn r\n}\n\n// IsInternational returns if this is an internationalized local part, i.e. has\n// non-ASCII characters.\nfunc (lp Localpart) IsInternational() bool {\n\tfor _, c := range lp {\n\t\tif c > 0x7f {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Address is a parsed email address.\ntype Address struct {\n\tLocalpart Localpart\n\tDomain    dns.Domain // todo: shouldn't we accept an ip address here too? and merge this type into smtp.Path.\n}\n\n// NewAddress returns an address.\nfunc NewAddress(localpart Localpart, domain dns.Domain) Address {\n\treturn Address{localpart, domain}\n}\n\nfunc (a Address) Path() Path {\n\treturn Path{Localpart: a.Localpart, IPDomain: dns.IPDomain{Domain: a.Domain}}\n}\n\nfunc (a Address) IsZero() bool {\n\treturn a == Address{}\n}\n\n// Pack returns the address in string form. If smtputf8 is true, the domain is\n// formatted with non-ASCII characters. If localpart has non-ASCII characters,\n// they are returned regardless of smtputf8.\nfunc (a Address) Pack(smtputf8 bool) string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.XName(smtputf8)\n}\n\n// String returns the address in string form with non-ASCII characters.\nfunc (a Address) String() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.Name()\n}\n\n// LogString returns the address with with utf-8 in localpart and/or domain. In\n// case of an IDNA domain and/or quotable characters in the localpart, an address\n// with quoted/escaped localpart and ASCII domain is also returned.\nfunc (a Address) LogString() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\ts := a.Pack(true)\n\tlp := a.Localpart.String()\n\tqlp := strconv.QuoteToASCII(lp)\n\tescaped := qlp != `\"`+lp+`\"`\n\tif a.Domain.Unicode != \"\" || escaped {\n\t\tif escaped {\n\t\t\tlp = qlp\n\t\t}\n\t\ts += \"/\" + lp + \"@\" + a.Domain.ASCII\n\t}\n\treturn s\n}\n\n// ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar ErrBadLocalpart = errors.New(\"invalid localpart\")\n\n// ParseLocalpart parses the local part.\n// UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\nfunc parseLocalPart(s string) (localpart Localpart, remain string, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, p.remainder(), nil\n}\n\ntype parser struct {\n\ts string\n\to int\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(fmt.Errorf(format, args...))\n}\n\nfunc (p *parser) hasPrefix(s string) bool {\n\treturn strings.HasPrefix(p.s[p.o:], s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.hasPrefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.take(s) {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o == len(p.s)\n}\n\nfunc (p *parser) xtaken(n int) string {\n\tr := p.s[p.o : p.o+n]\n\tp.o += n\n\treturn r\n}\n\nfunc (p *parser) remainder() string {\n\tr := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn r\n}\n\n// todo: reduce duplication between implementations: ../smtp/address.go:/xlocalpart ../dkim/parser.go:/xlocalpart ../smtpserver/parse.go:/xlocalpart\nfunc (p *parser) xlocalpart() Localpart {\n\t// ../rfc/5321:2316\n\tvar s string\n\tif p.hasPrefix(`\"`) {\n\t\ts = p.xquotedString()\n\t} else {\n\t\ts = p.xatom()\n\t\tfor p.take(\".\") {\n\t\t\ts += \".\" + p.xatom()\n\t\t}\n\t}\n\t// In the wild, some services use large localparts for generated (bounce) addresses.\n\tif Pedantic && len(s) > 64 || len(s) > 128 {\n\t\t// ../rfc/5321:3486\n\t\tp.xerrorf(\"localpart longer than 64 octets\")\n\t}\n\treturn Localpart(norm.NFC.String(s))\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"invalid localpart, bad escaped char %c\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\t// todo: should we be accepting utf8 for quoted strings?\n\t\tif c >= ' ' && c < 0x7f && c != '\\\\' && c != '\"' || c > 0x7f {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid localpart, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == 0 {\n\t\tp.o = len(p.s)\n\t} else {\n\t\tp.o += o\n\t}\n\treturn r\n}\n\nfunc (p *parser) takefn1(what string, fn func(c rune, i int) bool) string {\n\tif p.empty() {\n\t\tp.xerrorf(\"need at least one char for %s\", what)\n\t}\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"expected at least one char for %s, got char %c\", what, c)\n\t\t\t}\n\t\t\treturn p.xtaken(i)\n\t\t}\n\t}\n\treturn p.remainder()\n}\n\nfunc (p *parser) xatom() string {\n\treturn p.takefn1(\"atom\", func(c rune, i int) bool {\n\t\tswitch c {\n\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\treturn true\n\t\t}\n\t\treturn isalphadigit(c) || c > 0x7f\n\t})\n}\n\nfunc isalpha(c rune) bool {\n\treturn c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z'\n}\n\nfunc isdigit(c rune) bool {\n\treturn c >= '0' && c <= '9'\n}\n\nfunc isalphadigit(c rune) bool {\n\treturn isalpha(c) || isdigit(c)\n}\n\n```\n# Function Signature: func ParseAddress(s string) (address Address, err error) {\n# Natural Language Annotations: // ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseAddress(s string) (address Address, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = ErrBadAddress\n\t\t}\n\t}()\n\n\tp := &parser{s: s}\n\n\t// Parse the local part\n\tlocalpart, remain, err := parseLocalPart(p.s)\n\tif err != nil {\n\t\treturn Address{}, ErrBadAddress\n\t}\n\n\t// Separate domain part\n\tif !strings.HasPrefix(remain, \"@\") {\n\t\treturn Address{}, ErrBadAddress\n\t}\n\tdomain := remain[1:]\n\n\t// Parse domain using dns package\n\tdomainParsed, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn Address{}, ErrBadAddress\n\t}\n\n\treturn NewAddress(localpart, domainParsed), nil\n}\n```"]}, "mox-smtp/address.go-ParseLocalpart": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtp/address.go:\n```\npackage smtp\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"golang.org/x/text/unicode/norm\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar ErrBadAddress = errors.New(\"invalid email address\")\n\n// Localpart is a decoded local part of an email address, before the \"@\".\n// For quoted strings, values do not hold the double quote or escaping backslashes.\n// An empty string can be a valid localpart.\n// Localparts are in Unicode NFC.\ntype Localpart string\n\n// String returns a packed representation of an address, with proper escaping/quoting, for use in SMTP.\nfunc (lp Localpart) String() string {\n\t// See ../rfc/5321:2322 ../rfc/6531:414\n\t// First we try as dot-string. If not possible we make a quoted-string.\n\tdotstr := true\n\tt := strings.Split(string(lp), \".\")\n\tfor _, e := range t {\n\t\tfor _, c := range e {\n\t\t\tif c >= '0' && c <= '9' || c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c > 0x7f {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdotstr = false\n\t\t\tbreak\n\t\t}\n\t\tdotstr = dotstr && len(e) > 0\n\t}\n\tdotstr = dotstr && len(t) > 0\n\tif dotstr {\n\t\treturn string(lp)\n\t}\n\n\t// Make quoted-string.\n\tr := `\"`\n\tfor _, b := range lp {\n\t\tif b == '\"' || b == '\\\\' {\n\t\t\tr += \"\\\\\" + string(b)\n\t\t} else {\n\t\t\tr += string(b)\n\t\t}\n\t}\n\tr += `\"`\n\treturn r\n}\n\n// LogString returns the localpart as string for use in smtp, and an escaped\n// representation if it has non-ascii characters.\nfunc (lp Localpart) LogString() string {\n\ts := lp.String()\n\tqs := strconv.QuoteToASCII(s)\n\tif qs != `\"`+s+`\"` {\n\t\ts = \"/\" + qs\n\t}\n\treturn s\n}\n\n// DSNString returns the localpart as string for use in a DSN.\n// utf8 indicates if the remote MTA supports utf8 messaging. If not, the 7bit DSN\n// encoding for \"utf-8-addr-xtext\" from RFC 6533 is used.\nfunc (lp Localpart) DSNString(utf8 bool) string {\n\tif utf8 {\n\t\treturn lp.String()\n\t}\n\t// ../rfc/6533:259\n\tr := \"\"\n\tfor _, c := range lp {\n\t\tif c > 0x20 && c < 0x7f && c != '\\\\' && c != '+' && c != '=' {\n\t\t\tr += string(c)\n\t\t} else {\n\t\t\tr += fmt.Sprintf(`\\x{%x}`, c)\n\t\t}\n\t}\n\treturn r\n}\n\n// IsInternational returns if this is an internationalized local part, i.e. has\n// non-ASCII characters.\nfunc (lp Localpart) IsInternational() bool {\n\tfor _, c := range lp {\n\t\tif c > 0x7f {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Address is a parsed email address.\ntype Address struct {\n\tLocalpart Localpart\n\tDomain    dns.Domain // todo: shouldn't we accept an ip address here too? and merge this type into smtp.Path.\n}\n\n// NewAddress returns an address.\nfunc NewAddress(localpart Localpart, domain dns.Domain) Address {\n\treturn Address{localpart, domain}\n}\n\nfunc (a Address) Path() Path {\n\treturn Path{Localpart: a.Localpart, IPDomain: dns.IPDomain{Domain: a.Domain}}\n}\n\nfunc (a Address) IsZero() bool {\n\treturn a == Address{}\n}\n\n// Pack returns the address in string form. If smtputf8 is true, the domain is\n// formatted with non-ASCII characters. If localpart has non-ASCII characters,\n// they are returned regardless of smtputf8.\nfunc (a Address) Pack(smtputf8 bool) string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.XName(smtputf8)\n}\n\n// String returns the address in string form with non-ASCII characters.\nfunc (a Address) String() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.Name()\n}\n\n// LogString returns the address with with utf-8 in localpart and/or domain. In\n// case of an IDNA domain and/or quotable characters in the localpart, an address\n// with quoted/escaped localpart and ASCII domain is also returned.\nfunc (a Address) LogString() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\ts := a.Pack(true)\n\tlp := a.Localpart.String()\n\tqlp := strconv.QuoteToASCII(lp)\n\tescaped := qlp != `\"`+lp+`\"`\n\tif a.Domain.Unicode != \"\" || escaped {\n\t\tif escaped {\n\t\t\tlp = qlp\n\t\t}\n\t\ts += \"/\" + lp + \"@\" + a.Domain.ASCII\n\t}\n\treturn s\n}\n\n// ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar ErrBadLocalpart = errors.New(\"invalid localpart\")\n\n// ParseLocalpart parses the local part.\n// UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\nfunc parseLocalPart(s string) (localpart Localpart, remain string, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, p.remainder(), nil\n}\n\ntype parser struct {\n\ts string\n\to int\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(fmt.Errorf(format, args...))\n}\n\nfunc (p *parser) hasPrefix(s string) bool {\n\treturn strings.HasPrefix(p.s[p.o:], s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.hasPrefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.take(s) {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o == len(p.s)\n}\n\nfunc (p *parser) xtaken(n int) string {\n\tr := p.s[p.o : p.o+n]\n\tp.o += n\n\treturn r\n}\n\nfunc (p *parser) remainder() string {\n\tr := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn r\n}\n\n// todo: reduce duplication between implementations: ../smtp/address.go:/xlocalpart ../dkim/parser.go:/xlocalpart ../smtpserver/parse.go:/xlocalpart\nfunc (p *parser) xlocalpart() Localpart {\n\t// ../rfc/5321:2316\n\tvar s string\n\tif p.hasPrefix(`\"`) {\n\t\ts = p.xquotedString()\n\t} else {\n\t\ts = p.xatom()\n\t\tfor p.take(\".\") {\n\t\t\ts += \".\" + p.xatom()\n\t\t}\n\t}\n\t// In the wild, some services use large localparts for generated (bounce) addresses.\n\tif Pedantic && len(s) > 64 || len(s) > 128 {\n\t\t// ../rfc/5321:3486\n\t\tp.xerrorf(\"localpart longer than 64 octets\")\n\t}\n\treturn Localpart(norm.NFC.String(s))\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"invalid localpart, bad escaped char %c\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\t// todo: should we be accepting utf8 for quoted strings?\n\t\tif c >= ' ' && c < 0x7f && c != '\\\\' && c != '\"' || c > 0x7f {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid localpart, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == 0 {\n\t\tp.o = len(p.s)\n\t} else {\n\t\tp.o += o\n\t}\n\treturn r\n}\n\nfunc (p *parser) takefn1(what string, fn func(c rune, i int) bool) string {\n\tif p.empty() {\n\t\tp.xerrorf(\"need at least one char for %s\", what)\n\t}\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"expected at least one char for %s, got char %c\", what, c)\n\t\t\t}\n\t\t\treturn p.xtaken(i)\n\t\t}\n\t}\n\treturn p.remainder()\n}\n\nfunc (p *parser) xatom() string {\n\treturn p.takefn1(\"atom\", func(c rune, i int) bool {\n\t\tswitch c {\n\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\treturn true\n\t\t}\n\t\treturn isalphadigit(c) || c > 0x7f\n\t})\n}\n\nfunc isalpha(c rune) bool {\n\treturn c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z'\n}\n\nfunc isdigit(c rune) bool {\n\treturn c >= '0' && c <= '9'\n}\n\nfunc isalphadigit(c rune) bool {\n\treturn isalpha(c) || isdigit(c)\n}\n\n```\n# Function Signature: func ParseLocalpart(s string) (localpart Localpart, err error) {\n# Natural Language Annotations: // ParseLocalpart parses the local part.\n// UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseLocalpart(s string) (localpart Localpart, err error) {\n\tlp, _, err := parseLocalPart(s)\n\tif err != nil {\n\t\treturn \"\", ErrBadAddress\n\t}\n\treturn lp, nil\n}\n```"]}, "mox-smtp/data.go-DataWrite": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtp/data.go:\n```\npackage smtp\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"errors\"\n\t\"io\"\n)\n\nvar ErrCRLF = errors.New(\"invalid bare carriage return or newline\")\n\nvar errMissingCRLF = errors.New(\"missing crlf at end of message\")\n\n// DataWrite reads data (a mail message) from r, and writes it to smtp\n// connection w with dot stuffing, as required by the SMTP data command.\n//\n// Messages with bare carriage returns or bare newlines result in an error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar dotcrlf = []byte(\".\\r\\n\")\n\n// DataReader is an io.Reader that reads data from an SMTP DATA command, doing dot\n// unstuffing and returning io.EOF when a bare dot is received. Use NewDataReader.\n//\n// Bare carriage returns, and the sequences \"[^\\r]\\n.\" and \"\\n.\\n\" result in an\n// error.\ntype DataReader struct {\n\t// ../rfc/5321:2003\n\tr           *bufio.Reader\n\tplast, last byte\n\tbuf         []byte // From previous read.\n\terr         error  // Read error, for after r.buf is exhausted.\n\n\t// When we see invalid combinations of CR and LF, we keep reading, and report an\n\t// error at the final \"\\r\\n.\\r\\n\". We cannot just stop reading and return an error,\n\t// the SMTP protocol would become out of sync.\n\tbadcrlf bool\n}\n\n// NewDataReader returns an initialized DataReader.\nfunc NewDataReader(r *bufio.Reader) *DataReader {\n\treturn &DataReader{\n\t\tr: r,\n\t\t// Set up initial state to accept a message that is only \".\" and CRLF.\n\t\tplast: '\\r',\n\t\tlast:  '\\n',\n\t}\n}\n\n// Read implements io.Reader.\nfunc (r *DataReader) Read(p []byte) (int, error) {\n\twrote := 0\n\tfor len(p) > 0 {\n\t\t// Read until newline as long as it fits in the buffer.\n\t\tif len(r.buf) == 0 {\n\t\t\tif r.err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// todo: set a max length, eg 1000 octets including crlf excluding potential leading dot. ../rfc/5321:3512\n\t\t\tr.buf, r.err = r.r.ReadSlice('\\n')\n\t\t\tif r.err == bufio.ErrBufferFull {\n\t\t\t\tr.err = nil\n\t\t\t} else if r.err == io.EOF {\n\t\t\t\t// Mark EOF as bad for now. If we see the ending dotcrlf below, err becomes regular\n\t\t\t\t// io.EOF again.\n\t\t\t\tr.err = io.ErrUnexpectedEOF\n\t\t\t}\n\t\t}\n\t\tif len(r.buf) > 0 {\n\t\t\t// Reject bare \\r.\n\t\t\tfor i, c := range r.buf {\n\t\t\t\tif c == '\\r' && (i == len(r.buf) || r.buf[i+1] != '\\n') {\n\t\t\t\t\tr.badcrlf = true\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// We require crlf. A bare LF is not a line ending for the end of the SMTP\n\t\t\t// transaction. ../rfc/5321:2032\n\t\t\t// Bare newlines are accepted as message data, unless around a bare dot. The SMTP\n\t\t\t// server adds missing carriage returns. We don't reject bare newlines outright,\n\t\t\t// real-world messages like that occur.\n\t\t\tif r.plast == '\\r' && r.last == '\\n' {\n\t\t\t\tif bytes.Equal(r.buf, dotcrlf) {\n\t\t\t\t\tr.buf = nil\n\t\t\t\t\tr.err = io.EOF\n\t\t\t\t\tif r.badcrlf {\n\t\t\t\t\t\tr.err = ErrCRLF\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t} else if r.buf[0] == '.' {\n\t\t\t\t\t// Reject \"\\r\\n.\\n\".\n\t\t\t\t\tif len(r.buf) >= 2 && r.buf[1] == '\\n' {\n\t\t\t\t\t\tr.badcrlf = true\n\t\t\t\t\t}\n\t\t\t\t\tr.buf = r.buf[1:]\n\t\t\t\t}\n\t\t\t} else if r.last == '\\n' && (bytes.HasPrefix(r.buf, []byte(\".\\n\")) || bytes.HasPrefix(r.buf, []byte(\".\\r\\n\"))) {\n\t\t\t\t// Reject \"[^\\r]\\n.\\n\" and \"[^\\r]\\n.\\r\\n\"\n\t\t\t\tr.badcrlf = true\n\t\t\t}\n\t\t\tn := len(r.buf)\n\t\t\tif n > len(p) {\n\t\t\t\tn = len(p)\n\t\t\t}\n\t\t\tcopy(p, r.buf[:n])\n\t\t\tif n == 1 {\n\t\t\t\tr.plast, r.last = r.last, r.buf[0]\n\t\t\t} else if n > 1 {\n\t\t\t\tr.plast, r.last = r.buf[n-2], r.buf[n-1]\n\t\t\t}\n\t\t\tp = p[n:]\n\t\t\tr.buf = r.buf[n:]\n\t\t\twrote += n\n\t\t}\n\t}\n\treturn wrote, r.err\n}\n\n```\n# Function Signature: func DataWrite(w io.Writer, r io.Reader) error {\n# Natural Language Annotations: // DataWrite reads data (a mail message) from r, and writes it to smtp\n// connection w with dot stuffing, as required by the SMTP data command.\n//\n// Messages with bare carriage returns or bare newlines result in an error.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc DataWrite(w io.Writer, r io.Reader) error {\n    reader := bufio.NewReader(r)\n    var line []byte\n    var err error\n\n    for {\n        line, err = reader.ReadBytes('\\n')\n        if err != nil && err != io.EOF {\n            return err\n        }\n\n        // Check if line has a bare CR or LF\n        if len(line) > 0 && line[len(line)-1] == '\\n' {\n            if len(line) > 1 && line[len(line)-2] != '\\r' {\n                return ErrCRLF\n            } else if len(line) == 1 {\n                return ErrCRLF\n            }\n        } else if len(line) > 0 && line[len(line)-1] == '\\r' {\n            return ErrCRLF\n        }\n\n        // Perform dot-stuffing: if the line starts with a '.', add another '.'\n        if len(line) > 0 && line[0] == '.' {\n            line = append([]byte{'.'}, line...)\n        }\n\n        // Write line to writer\n        _, writeErr := w.Write(line)\n        if writeErr != nil {\n            return writeErr\n        }\n\n        if err == io.EOF {\n            break\n        }\n    }\n\n    // Ensure the message ends with \"\\r\\n.\\r\\n\"\n    _, err = w.Write(dotcrlf)\n    return err\n}\n```"]}, "mox-scram/scram.go-NewServer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n# Natural Language Annotations: // NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n\tp := newParser(clientFirst)\n\tdefer p.recover(&rerr)\n\n\t// Parse the client first message\n\tgs2header := p.xattrval()\n\tclientFirstBare := p.xraw()\n\tgs2part := strings.SplitN(gs2header, \",\", 4)\n\n\t// Extract username and authorize them\n\tusername, authz, cbData := \"\", \"\", []byte{}\n\tif strings.HasPrefix(gs2header, \"n,\") {\n\t\tusername = saslname(gs2part[2][2:])\n\t} else if strings.HasPrefix(gs2header, \"y,\") || strings.HasPrefix(gs2header, \"p=\") {\n\t\tif len(gs2part) < 4 {\n\t\t\treturn nil, fmt.Errorf(\"%w: invalid gs2header\", ErrProtocol)\n\t\t}\n\t\tusername = saslname(gs2part[2][2:])\n\t\tauthz = saslname(gs2part[3][2:])\n\t} else {\n\t\treturn nil, fmt.Errorf(\"%w: invalid gs2header\", ErrProtocol)\n\t}\n\n\t// Verify the protocol matches\n\tif p.take(\",m=\") {\n\t\tp.xerrorf(\"%w: %w\", ErrExtensionsNotSupported, ErrProtocol)\n\t}\n\n\tnonce := p.xnonce()\n\tif nonce == \"\" {\n\t\treturn nil, fmt.Errorf(\"%w: empty nonce\", ErrProtocol)\n\t}\n\n\tp.xempty()\n\n\t// Handle channel binding\n\tvar channelBinding []byte\n\tif cs != nil {\n\t\tvar err error\n\t\tchannelBinding, err = channelBindData(cs)\n\t\tif err != nil || (channelBindingRequired && channelBinding == nil) {\n\t\t\treturn nil, ErrChannelBindingNotSupported\n\t\t}\n\t}\n\n\t// Create the server object\n\tserver = &Server{\n\t\tAuthentication: username,\n\t\tAuthorization:  authz,\n\t\th:              h,\n\t\tclientFirstBare: string(clientFirstBare),\n\t\tgs2header:       gs2header,\n\t\tclientNonce:     nonce,\n\t\tchannelBinding:  channelBinding,\n\t}\n\n\treturn server, nil\n}\n```"]}, "mox-scram/scram.go-ServerFirst": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n# Natural Language Annotations: // ServerFirst returns the string to send back to the client. To be called after NewServer.\nPlease return the generated code surrounded by ```\n", "response": ["To implement the `ServerFirst` method for the `Server` struct based on the provided context, we'll need to construct a server-first message, which involves generating a nonce if necessary, using the provided salt and iteration count, and formatting the message string appropriately. Here's the implementation:\n\n```go\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\nfunc (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n\t// Generate server nonce if not overridden\n\tif s.serverNonceOverride == \"\" {\n\t\tserverNonce := MakeRandom()\n\t\ts.nonce = s.clientNonce + base64.StdEncoding.EncodeToString(serverNonce)\n\t} else {\n\t\ts.nonce = s.clientNonce + s.serverNonceOverride\n\t}\n\t\n\t// Check nonce length for safety\n\tif len(s.nonce) < len(s.clientNonce)+8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\n\t// Encode salt to base64\n\tencodedSalt := base64.StdEncoding.EncodeToString(salt)\n\n\t// Construct the server first message\n\t// Example format: \"r=<server_nounce>,s=<base64_salt>,i=<iterations>\"\n\ts.serverFirst = fmt.Sprintf(\"r=%s,s=%s,i=%d\", s.nonce, encodedSalt, iterations)\n\n\treturn s.serverFirst, nil\n}\n```\n\nThis method does the following:\n- It generates a server nonce and appends it to the client's nonce to form a full nonce, unless a `serverNonceOverride` is provided for testing with specific vectors.\n- Ensures the combined nonce length meets a minimum requirement to avoid weak nonces.\n- Encodes the provided salt to a base64 string.\n- Formats and returns the server's first message string, which includes the nonce, the base64-encoded salt, and the iteration count."]}, "mox-scram/scram.go-Finish": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n# Natural Language Annotations: // Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\nfunc (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n\tclientFinalStr := string(clientFinal)\n\tp := newParser(clientFinal)\n\tdefer p.recover(&rerr)\n\n\t// Parse clientFinal message\n\tcbind, err := p.xbase64(\"c=\")\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"invalid client final message: %w\", ErrInvalidEncoding)\n\t}\n\tp.xtake(\",\")\n\tclientNonce := p.xstring(\"r=\")\n\tp.xtake(\",\")\n\tclientProof, err := p.xbase64(\"p=\")\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"invalid client final message: %w\", ErrInvalidEncoding)\n\t}\n\tp.xempty()\n\n\t// Verify nonce\n\tif clientNonce != s.nonce {\n\t\treturn \"\", fmt.Errorf(\"%w: nonces do not match\", ErrProtocol)\n\t}\n\n\t// Verify channel binding\n\tif !bytes.Equal(cbind, append([]byte(s.gs2header), s.channelBinding...)) {\n\t\treturn \"\", fmt.Errorf(\"%w: channel bindings don't match\", ErrChannelBindingsDontMatch)\n\t}\n\n\t// Verify client proof\n\tclientKey := hmac0(s.h, saltedPassword, \"Client Key\")\n\th := s.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(s.h, storedKey, s.clientFirstBare+\",\"+s.serverFirst+\",\"+clientFinalStr)\n\txor(clientSig, clientKey) // Now should equal clientProof.\n\n\tif !bytes.Equal(clientSig, clientProof) {\n\t\treturn \"\", ErrInvalidProof\n\t}\n\n\t// If no error, compute server signature\n\tserverKey := hmac0(s.h, saltedPassword, \"Server Key\")\n\tserverSig := hmac0(s.h, serverKey, s.clientFirstBare+\",\"+s.serverFirst+\",\"+clientFinalStr)\n\n\treturn fmt.Sprintf(\"v=%s\", base64.StdEncoding.EncodeToString(serverSig)), nil\n}\n```"]}, "mox-scram/scram.go-ClientFirst": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (c *Client) ClientFirst() (clientFirst string, rerr error) {\n# Natural Language Annotations: // ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *Client) ClientFirst() (clientFirst string, rerr error) {\n\t// Generate a random nonce for the client\n\tclientNonce := MakeRandom()\n\tc.clientNonce = base64.StdEncoding.EncodeToString(clientNonce)\n\n\t// Construct the GS2 header\n\tc.gs2header = \"n,,\" // As per RFC 5802, this is standard if no channel binding is done/supported\n\n\t// Construct the client first bare message: \"n=<username>,r=<client nonce>\"\n\tc.clientFirstBare = fmt.Sprintf(\"n=%s,r=%s\", saslname(c.authc), c.clientNonce)\n\n\t// Construct the complete client first message with GS2 header: \"<GS2Header><ClientFirstBare>\"\n\tclientFirst = fmt.Sprintf(\"%s%s\", c.gs2header, c.clientFirstBare)\n\t\n\treturn clientFirst, nil\n}\n```"]}, "mox-scram/scram.go-ServerFinal": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n# Natural Language Annotations: // ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\nfunc (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n\tp := newParser(serverFinal)\n\tdefer p.recover(&rerr)\n\n\t// Check for extensions, as they're not supported.\n\tif p.take(\"v=\") {\n\t\tverifier := p.xbase64()\n\t\tserverKey := hmac0(c.h, c.saltedPassword, \"Server Key\")\n\t\th := c.h()\n\t\th.Write(serverKey)\n\t\tserverSig := h.Sum(nil)\n\n\t\tif !bytes.Equal(verifier, serverSig) {\n\t\t\treturn fmt.Errorf(\"%w: server failed to prove it knows the password\", ErrInvalidProof)\n\t\t}\n\t} else {\n\t\treturn fmt.Errorf(\"%w: expected server signature but not found\", ErrProtocol)\n\t}\n\n\t// After processing the signature, check for any leftover extensions.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\treturn nil\n}\n```"]}, "mox-autotls/autotls.go-SetAllowedHostnames": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/autotls/autotls.go:\n```\n// Package autotls automatically configures TLS (for SMTP, IMAP, HTTP) by\n// requesting certificates with ACME, typically from Let's Encrypt.\npackage autotls\n\n// We do tls-alpn-01, and also http-01. For DNS we would need a third party tool\n// with an API that can make the DNS changes, as we don't want to link in dozens of\n// bespoke API's for DNS record manipulation into mox.\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto\"\n\t\"crypto/ecdsa\"\n\t\"crypto/elliptic\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/acme\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/autocert\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxvar\"\n)\n\nvar (\n\tmetricCertput = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_autotls_certput_total\",\n\t\t\tHelp: \"Number of certificate store puts.\",\n\t\t},\n\t)\n)\n\n// Manager is in charge of a single ACME identity, and automatically requests\n// certificates for allowlisted hosts.\ntype Manager struct {\n\tACMETLSConfig *tls.Config // For serving HTTPS on port 443, which is required for certificate requests to succeed.\n\tTLSConfig     *tls.Config // For all TLS servers not used for validating ACME requests. Like SMTP and IMAP (including with STARTTLS) and HTTPS on ports other than 443.\n\tManager       *autocert.Manager\n\n\tshutdown <-chan struct{}\n\n\tsync.Mutex\n\thosts map[dns.Domain]struct{}\n}\n\n// Load returns an initialized autotls manager for \"name\" (used for the ACME key\n// file and requested certs and their keys). All files are stored within acmeDir.\n//\n// contactEmail must be a valid email address to which notifications about ACME can\n// be sent. directoryURL is the ACME starting point.\n//\n// eabKeyID and eabKey are for external account binding when making a new account,\n// which some ACME providers require.\n//\n// getPrivateKey is called to get the private key for the host and key type. It\n// can be used to deliver a specific (e.g. always the same) private key for a\n// host, or a newly generated key.\n//\n// When shutdown is closed, no new TLS connections can be created.\nfunc Load(name, acmeDir, contactEmail, directoryURL string, eabKeyID string, eabKey []byte, getPrivateKey func(host string, keyType autocert.KeyType) (crypto.Signer, error), shutdown <-chan struct{}) (*Manager, error) {\n\tif directoryURL == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty ACME directory URL\")\n\t}\n\tif contactEmail == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty contact email\")\n\t}\n\n\t// Load identity key if it exists. Otherwise, create a new key.\n\tp := filepath.Join(acmeDir, name+\".key\")\n\tvar key crypto.Signer\n\tf, err := os.Open(p)\n\tif f != nil {\n\t\tdefer f.Close()\n\t}\n\tif err != nil && os.IsNotExist(err) {\n\t\tkey, err = ecdsa.GenerateKey(elliptic.P256(), cryptorand.Reader)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"generating ecdsa identity key: %s\", err)\n\t\t}\n\t\tder, err := x509.MarshalPKCS8PrivateKey(key)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"marshal identity key: %s\", err)\n\t\t}\n\t\tblock := &pem.Block{\n\t\t\tType: \"PRIVATE KEY\",\n\t\t\tHeaders: map[string]string{\n\t\t\t\t\"Note\": fmt.Sprintf(\"PEM PKCS8 ECDSA private key generated for ACME provider %s by mox\", name),\n\t\t\t},\n\t\t\tBytes: der,\n\t\t}\n\t\tb := &bytes.Buffer{}\n\t\tif err := pem.Encode(b, block); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"pem encode: %s\", err)\n\t\t} else if err := os.WriteFile(p, b.Bytes(), 0660); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"writing identity key: %s\", err)\n\t\t}\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"open identity key file: %s\", err)\n\t} else {\n\t\tvar privKey any\n\t\tif buf, err := io.ReadAll(f); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"reading identity key: %s\", err)\n\t\t} else if p, _ := pem.Decode(buf); p == nil {\n\t\t\treturn nil, fmt.Errorf(\"no pem data\")\n\t\t} else if p.Type != \"PRIVATE KEY\" {\n\t\t\treturn nil, fmt.Errorf(\"got PEM block %q, expected \\\"PRIVATE KEY\\\"\", p.Type)\n\t\t} else if privKey, err = x509.ParsePKCS8PrivateKey(p.Bytes); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"parsing PKCS8 private key: %s\", err)\n\t\t}\n\t\tswitch k := privKey.(type) {\n\t\tcase *ecdsa.PrivateKey:\n\t\t\tkey = k\n\t\tcase *rsa.PrivateKey:\n\t\t\tkey = k\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unsupported private key type %T\", key)\n\t\t}\n\t}\n\n\tm := &autocert.Manager{\n\t\tCache:  dirCache(filepath.Join(acmeDir, \"keycerts\", name)),\n\t\tPrompt: autocert.AcceptTOS,\n\t\tEmail:  contactEmail,\n\t\tClient: &acme.Client{\n\t\t\tDirectoryURL: directoryURL,\n\t\t\tKey:          key,\n\t\t\tUserAgent:    \"mox/\" + moxvar.Version,\n\t\t},\n\t\tGetPrivateKey: getPrivateKey,\n\t\t// HostPolicy set below.\n\t}\n\t// If external account binding key is provided, use it for registering a new account.\n\t// todo: ideally the key and its id are provided temporarily by the admin when registering a new account. but we don't do that interactive setup yet. in the future, an interactive setup/quickstart would ask for the key once to register a new acme account.\n\tif eabKeyID != \"\" {\n\t\tm.ExternalAccountBinding = &acme.ExternalAccountBinding{\n\t\t\tKID: eabKeyID,\n\t\t\tKey: eabKey,\n\t\t}\n\t}\n\n\tloggingGetCertificate := func(hello *tls.ClientHelloInfo) (*tls.Certificate, error) {\n\t\tlog := mlog.New(\"autotls\", nil).WithContext(hello.Context())\n\n\t\t// We handle missing invalid hostnames/ip's by returning a nil certificate and nil\n\t\t// error, which crypto/tls turns into a TLS alert \"unrecognized name\", which can be\n\t\t// interpreted by clients as a hint that they are using the wrong hostname, or a\n\t\t// certificate is missing.\n\n\t\t// Handle missing SNI to prevent logging an error below.\n\t\t// At startup, during config initialization, we already adjust the tls config to\n\t\t// inject the listener hostname if there isn't one in the TLS client hello. This is\n\t\t// common for SMTP STARTTLS connections, which often do not care about the\n\t\t// verification of the certificate.\n\t\tif hello.ServerName == \"\" {\n\t\t\tlog.Debug(\"tls request without sni servername, rejecting\", slog.Any(\"localaddr\", hello.Conn.LocalAddr()), slog.Any(\"supportedprotos\", hello.SupportedProtos))\n\t\t\treturn nil, nil\n\t\t}\n\n\t\tcert, err := m.GetCertificate(hello)\n\t\tif err != nil && errors.Is(err, errHostNotAllowed) {\n\t\t\tlog.Debugx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t\treturn nil, nil\n\t\t} else if err != nil {\n\t\t\tlog.Errorx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t}\n\t\treturn cert, err\n\t}\n\n\tacmeTLSConfig := *m.TLSConfig()\n\tacmeTLSConfig.GetCertificate = loggingGetCertificate\n\n\ttlsConfig := tls.Config{\n\t\tGetCertificate: loggingGetCertificate,\n\t}\n\n\ta := &Manager{\n\t\tACMETLSConfig: &acmeTLSConfig,\n\t\tTLSConfig:     &tlsConfig,\n\t\tManager:       m,\n\t\tshutdown:      shutdown,\n\t\thosts:         map[dns.Domain]struct{}{},\n\t}\n\tm.HostPolicy = a.HostPolicy\n\treturn a, nil\n}\n\n// CertAvailable checks whether a non-expired ECDSA certificate is available in the\n// cache for host. No other checks than expiration are done.\nfunc (m *Manager) CertAvailable(ctx context.Context, log mlog.Log, host dns.Domain) (bool, error) {\n\tck := host.ASCII // Would be \"+rsa\" for rsa keys.\n\tdata, err := m.Manager.Cache.Get(ctx, ck)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\treturn false, nil\n\t} else if err != nil {\n\t\treturn false, fmt.Errorf(\"attempt to get certificate from cache: %v\", err)\n\t}\n\n\t// The cached keycert is of the form: private key, leaf certificate, intermediate certificates...\n\tprivb, rem := pem.Decode(data)\n\tif privb == nil {\n\t\treturn false, fmt.Errorf(\"missing private key in cached keycert file\")\n\t}\n\tpubb, _ := pem.Decode(rem)\n\tif pubb == nil {\n\t\treturn false, fmt.Errorf(\"missing certificate in cached keycert file\")\n\t} else if pubb.Type != \"CERTIFICATE\" {\n\t\treturn false, fmt.Errorf(\"second pem block is %q, expected CERTIFICATE\", pubb.Type)\n\t}\n\tcert, err := x509.ParseCertificate(pubb.Bytes)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing certificate from cached keycert file: %v\", err)\n\t}\n\t// We assume the certificate has a matching hostname, and is properly CA-signed. We\n\t// only check the expiration time.\n\tif time.Until(cert.NotBefore) > 0 || time.Since(cert.NotAfter) > 0 {\n\t\treturn false, nil\n\t}\n\treturn true, nil\n}\n\n// SetAllowedHostnames sets a new list of allowed hostnames for automatic TLS.\n// After setting the host names, a goroutine is start to check that new host names\n// are fully served by publicIPs (only if non-empty and there is no unspecified\n// address in the list). If no, log an error with a warning that ACME validation\n// may fail.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Hostnames returns the allowed host names for use with ACME.\nfunc (m *Manager) Hostnames() []dns.Domain {\n\tm.Lock()\n\tdefer m.Unlock()\n\tvar l []dns.Domain\n\tfor h := range m.hosts {\n\t\tl = append(l, h)\n\t}\n\treturn l\n}\n\nvar errHostNotAllowed = errors.New(\"autotls: host not in allowlist\")\n\n// HostPolicy decides if a host is allowed for use with ACME, i.e. whether a\n// certificate will be returned if present and/or will be requested if not yet\n// present. Only hosts added with SetAllowedHostnames are allowed. During shutdown,\n// no new connections are allowed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype dirCache autocert.DirCache\n\nfunc (d dirCache) Delete(ctx context.Context, name string) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache delete result\", rerr, slog.String(\"name\", name))\n\t}()\n\terr := autocert.DirCache(d).Delete(ctx, name)\n\tif err != nil {\n\t\tlog.Errorx(\"deleting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert delete\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\nfunc (d dirCache) Get(ctx context.Context, name string) (rbuf []byte, rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache get result\", rerr, slog.String(\"name\", name))\n\t}()\n\tbuf, err := autocert.DirCache(d).Get(ctx, name)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\tlog.Infox(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if err != nil {\n\t\tlog.Errorx(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Debug(\"autotls cert get\", slog.String(\"name\", name))\n\t}\n\treturn buf, err\n}\n\nfunc (d dirCache) Put(ctx context.Context, name string, data []byte) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache put result\", rerr, slog.String(\"name\", name))\n\t}()\n\tmetricCertput.Inc()\n\terr := autocert.DirCache(d).Put(ctx, name, data)\n\tif err != nil {\n\t\tlog.Errorx(\"storing cert in dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert store\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\n```\n# Function Signature: func (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n# Natural Language Annotations: // SetAllowedHostnames sets a new list of allowed hostnames for automatic TLS.\n// After setting the host names, a goroutine is start to check that new host names\n// are fully served by publicIPs (only if non-empty and there is no unspecified\n// address in the list). If no, log an error with a warning that ACME validation\n// may fail.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n\t// Lock the manager to update the allowed hostnames\n\tm.Lock()\n\tm.hosts = hostnames\n\tm.Unlock()\n\n\t// Check if hostnames need to be checked against the public IPs\n\tif !checkHosts || len(publicIPs) == 0 {\n\t\treturn\n\t}\n\n\t// Convert string public IPs to net.IP for comparison\n\tvar publicNetIPs []net.IP\n\tfor _, ip := range publicIPs {\n\t\tpublicNetIPs = append(publicNetIPs, net.ParseIP(ip))\n\t}\n\n\t// Check in a separate goroutine\n\tgo func() {\n\t\tfor host := range hostnames {\n\t\t\taddresses, err := resolver.LookupIPAddr(context.Background(), host.String())\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorx(\"failed to resolve host\", err, slog.String(\"host\", host.String()))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfor _, addr := range addresses {\n\t\t\t\tif addr.IP.IsUnspecified() {\n\t\t\t\t\tlog.Warn(\"host resolves to unspecified IP\", slog.String(\"host\", host.String()))\n\t\t\t\t} else if len(publicNetIPs) > 0 {\n\t\t\t\t\tfound := false\n\t\t\t\t\tfor _, publicIP := range publicNetIPs {\n\t\t\t\t\t\tif addr.IP.Equal(publicIP) {\n\t\t\t\t\t\t\tfound = true\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif !found {\n\t\t\t\t\t\tlog.Warn(\"host is not fully served by public IPs; ACME validation may fail\", slog.String(\"host\", host.String()))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n}\n```"]}, "mox-autotls/autotls.go-HostPolicy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/autotls/autotls.go:\n```\n// Package autotls automatically configures TLS (for SMTP, IMAP, HTTP) by\n// requesting certificates with ACME, typically from Let's Encrypt.\npackage autotls\n\n// We do tls-alpn-01, and also http-01. For DNS we would need a third party tool\n// with an API that can make the DNS changes, as we don't want to link in dozens of\n// bespoke API's for DNS record manipulation into mox.\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto\"\n\t\"crypto/ecdsa\"\n\t\"crypto/elliptic\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/acme\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/autocert\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxvar\"\n)\n\nvar (\n\tmetricCertput = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_autotls_certput_total\",\n\t\t\tHelp: \"Number of certificate store puts.\",\n\t\t},\n\t)\n)\n\n// Manager is in charge of a single ACME identity, and automatically requests\n// certificates for allowlisted hosts.\ntype Manager struct {\n\tACMETLSConfig *tls.Config // For serving HTTPS on port 443, which is required for certificate requests to succeed.\n\tTLSConfig     *tls.Config // For all TLS servers not used for validating ACME requests. Like SMTP and IMAP (including with STARTTLS) and HTTPS on ports other than 443.\n\tManager       *autocert.Manager\n\n\tshutdown <-chan struct{}\n\n\tsync.Mutex\n\thosts map[dns.Domain]struct{}\n}\n\n// Load returns an initialized autotls manager for \"name\" (used for the ACME key\n// file and requested certs and their keys). All files are stored within acmeDir.\n//\n// contactEmail must be a valid email address to which notifications about ACME can\n// be sent. directoryURL is the ACME starting point.\n//\n// eabKeyID and eabKey are for external account binding when making a new account,\n// which some ACME providers require.\n//\n// getPrivateKey is called to get the private key for the host and key type. It\n// can be used to deliver a specific (e.g. always the same) private key for a\n// host, or a newly generated key.\n//\n// When shutdown is closed, no new TLS connections can be created.\nfunc Load(name, acmeDir, contactEmail, directoryURL string, eabKeyID string, eabKey []byte, getPrivateKey func(host string, keyType autocert.KeyType) (crypto.Signer, error), shutdown <-chan struct{}) (*Manager, error) {\n\tif directoryURL == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty ACME directory URL\")\n\t}\n\tif contactEmail == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty contact email\")\n\t}\n\n\t// Load identity key if it exists. Otherwise, create a new key.\n\tp := filepath.Join(acmeDir, name+\".key\")\n\tvar key crypto.Signer\n\tf, err := os.Open(p)\n\tif f != nil {\n\t\tdefer f.Close()\n\t}\n\tif err != nil && os.IsNotExist(err) {\n\t\tkey, err = ecdsa.GenerateKey(elliptic.P256(), cryptorand.Reader)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"generating ecdsa identity key: %s\", err)\n\t\t}\n\t\tder, err := x509.MarshalPKCS8PrivateKey(key)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"marshal identity key: %s\", err)\n\t\t}\n\t\tblock := &pem.Block{\n\t\t\tType: \"PRIVATE KEY\",\n\t\t\tHeaders: map[string]string{\n\t\t\t\t\"Note\": fmt.Sprintf(\"PEM PKCS8 ECDSA private key generated for ACME provider %s by mox\", name),\n\t\t\t},\n\t\t\tBytes: der,\n\t\t}\n\t\tb := &bytes.Buffer{}\n\t\tif err := pem.Encode(b, block); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"pem encode: %s\", err)\n\t\t} else if err := os.WriteFile(p, b.Bytes(), 0660); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"writing identity key: %s\", err)\n\t\t}\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"open identity key file: %s\", err)\n\t} else {\n\t\tvar privKey any\n\t\tif buf, err := io.ReadAll(f); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"reading identity key: %s\", err)\n\t\t} else if p, _ := pem.Decode(buf); p == nil {\n\t\t\treturn nil, fmt.Errorf(\"no pem data\")\n\t\t} else if p.Type != \"PRIVATE KEY\" {\n\t\t\treturn nil, fmt.Errorf(\"got PEM block %q, expected \\\"PRIVATE KEY\\\"\", p.Type)\n\t\t} else if privKey, err = x509.ParsePKCS8PrivateKey(p.Bytes); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"parsing PKCS8 private key: %s\", err)\n\t\t}\n\t\tswitch k := privKey.(type) {\n\t\tcase *ecdsa.PrivateKey:\n\t\t\tkey = k\n\t\tcase *rsa.PrivateKey:\n\t\t\tkey = k\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unsupported private key type %T\", key)\n\t\t}\n\t}\n\n\tm := &autocert.Manager{\n\t\tCache:  dirCache(filepath.Join(acmeDir, \"keycerts\", name)),\n\t\tPrompt: autocert.AcceptTOS,\n\t\tEmail:  contactEmail,\n\t\tClient: &acme.Client{\n\t\t\tDirectoryURL: directoryURL,\n\t\t\tKey:          key,\n\t\t\tUserAgent:    \"mox/\" + moxvar.Version,\n\t\t},\n\t\tGetPrivateKey: getPrivateKey,\n\t\t// HostPolicy set below.\n\t}\n\t// If external account binding key is provided, use it for registering a new account.\n\t// todo: ideally the key and its id are provided temporarily by the admin when registering a new account. but we don't do that interactive setup yet. in the future, an interactive setup/quickstart would ask for the key once to register a new acme account.\n\tif eabKeyID != \"\" {\n\t\tm.ExternalAccountBinding = &acme.ExternalAccountBinding{\n\t\t\tKID: eabKeyID,\n\t\t\tKey: eabKey,\n\t\t}\n\t}\n\n\tloggingGetCertificate := func(hello *tls.ClientHelloInfo) (*tls.Certificate, error) {\n\t\tlog := mlog.New(\"autotls\", nil).WithContext(hello.Context())\n\n\t\t// We handle missing invalid hostnames/ip's by returning a nil certificate and nil\n\t\t// error, which crypto/tls turns into a TLS alert \"unrecognized name\", which can be\n\t\t// interpreted by clients as a hint that they are using the wrong hostname, or a\n\t\t// certificate is missing.\n\n\t\t// Handle missing SNI to prevent logging an error below.\n\t\t// At startup, during config initialization, we already adjust the tls config to\n\t\t// inject the listener hostname if there isn't one in the TLS client hello. This is\n\t\t// common for SMTP STARTTLS connections, which often do not care about the\n\t\t// verification of the certificate.\n\t\tif hello.ServerName == \"\" {\n\t\t\tlog.Debug(\"tls request without sni servername, rejecting\", slog.Any(\"localaddr\", hello.Conn.LocalAddr()), slog.Any(\"supportedprotos\", hello.SupportedProtos))\n\t\t\treturn nil, nil\n\t\t}\n\n\t\tcert, err := m.GetCertificate(hello)\n\t\tif err != nil && errors.Is(err, errHostNotAllowed) {\n\t\t\tlog.Debugx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t\treturn nil, nil\n\t\t} else if err != nil {\n\t\t\tlog.Errorx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t}\n\t\treturn cert, err\n\t}\n\n\tacmeTLSConfig := *m.TLSConfig()\n\tacmeTLSConfig.GetCertificate = loggingGetCertificate\n\n\ttlsConfig := tls.Config{\n\t\tGetCertificate: loggingGetCertificate,\n\t}\n\n\ta := &Manager{\n\t\tACMETLSConfig: &acmeTLSConfig,\n\t\tTLSConfig:     &tlsConfig,\n\t\tManager:       m,\n\t\tshutdown:      shutdown,\n\t\thosts:         map[dns.Domain]struct{}{},\n\t}\n\tm.HostPolicy = a.HostPolicy\n\treturn a, nil\n}\n\n// CertAvailable checks whether a non-expired ECDSA certificate is available in the\n// cache for host. No other checks than expiration are done.\nfunc (m *Manager) CertAvailable(ctx context.Context, log mlog.Log, host dns.Domain) (bool, error) {\n\tck := host.ASCII // Would be \"+rsa\" for rsa keys.\n\tdata, err := m.Manager.Cache.Get(ctx, ck)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\treturn false, nil\n\t} else if err != nil {\n\t\treturn false, fmt.Errorf(\"attempt to get certificate from cache: %v\", err)\n\t}\n\n\t// The cached keycert is of the form: private key, leaf certificate, intermediate certificates...\n\tprivb, rem := pem.Decode(data)\n\tif privb == nil {\n\t\treturn false, fmt.Errorf(\"missing private key in cached keycert file\")\n\t}\n\tpubb, _ := pem.Decode(rem)\n\tif pubb == nil {\n\t\treturn false, fmt.Errorf(\"missing certificate in cached keycert file\")\n\t} else if pubb.Type != \"CERTIFICATE\" {\n\t\treturn false, fmt.Errorf(\"second pem block is %q, expected CERTIFICATE\", pubb.Type)\n\t}\n\tcert, err := x509.ParseCertificate(pubb.Bytes)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing certificate from cached keycert file: %v\", err)\n\t}\n\t// We assume the certificate has a matching hostname, and is properly CA-signed. We\n\t// only check the expiration time.\n\tif time.Until(cert.NotBefore) > 0 || time.Since(cert.NotAfter) > 0 {\n\t\treturn false, nil\n\t}\n\treturn true, nil\n}\n\n// SetAllowedHostnames sets a new list of allowed hostnames for automatic TLS.\n// After setting the host names, a goroutine is start to check that new host names\n// are fully served by publicIPs (only if non-empty and there is no unspecified\n// address in the list). If no, log an error with a warning that ACME validation\n// may fail.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Hostnames returns the allowed host names for use with ACME.\nfunc (m *Manager) Hostnames() []dns.Domain {\n\tm.Lock()\n\tdefer m.Unlock()\n\tvar l []dns.Domain\n\tfor h := range m.hosts {\n\t\tl = append(l, h)\n\t}\n\treturn l\n}\n\nvar errHostNotAllowed = errors.New(\"autotls: host not in allowlist\")\n\n// HostPolicy decides if a host is allowed for use with ACME, i.e. whether a\n// certificate will be returned if present and/or will be requested if not yet\n// present. Only hosts added with SetAllowedHostnames are allowed. During shutdown,\n// no new connections are allowed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype dirCache autocert.DirCache\n\nfunc (d dirCache) Delete(ctx context.Context, name string) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache delete result\", rerr, slog.String(\"name\", name))\n\t}()\n\terr := autocert.DirCache(d).Delete(ctx, name)\n\tif err != nil {\n\t\tlog.Errorx(\"deleting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert delete\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\nfunc (d dirCache) Get(ctx context.Context, name string) (rbuf []byte, rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache get result\", rerr, slog.String(\"name\", name))\n\t}()\n\tbuf, err := autocert.DirCache(d).Get(ctx, name)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\tlog.Infox(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if err != nil {\n\t\tlog.Errorx(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Debug(\"autotls cert get\", slog.String(\"name\", name))\n\t}\n\treturn buf, err\n}\n\nfunc (d dirCache) Put(ctx context.Context, name string, data []byte) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache put result\", rerr, slog.String(\"name\", name))\n\t}()\n\tmetricCertput.Inc()\n\terr := autocert.DirCache(d).Put(ctx, name, data)\n\tif err != nil {\n\t\tlog.Errorx(\"storing cert in dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert store\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\n```\n# Function Signature: func (m *Manager) HostPolicy(ctx context.Context, host string) (rerr error) {\n# Natural Language Annotations: // HostPolicy decides if a host is allowed for use with ACME, i.e. whether a\n// certificate will be returned if present and/or will be requested if not yet\n// present. Only hosts added with SetAllowedHostnames are allowed. During shutdown,\n// no new connections are allowed.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (m *Manager) HostPolicy(ctx context.Context, host string) error {\n\tm.Lock()\n\tdefer m.Unlock()\n\n\t// Check if shutdown signal is active, disallow new connections if so.\n\tselect {\n\tcase <-m.shutdown:\n\t\treturn fmt.Errorf(\"autotls: new connections disallowed during shutdown\")\n\tdefault:\n\t}\n\n\t// Verify if the host is in the allowlist.\n\tif _, allowed := m.hosts[dns.Domain(host)]; !allowed {\n\t\treturn errHostNotAllowed\n\t}\n\treturn nil\n}\n```"]}, "mox-imapserver/protocol.go-containsSeq": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/imapserver/protocol.go:\n```\npackage imapserver\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/store\"\n)\n\ntype numSet struct {\n\tsearchResult bool // \"$\"\n\tranges       []numRange\n}\n\ntype numRange struct {\n\tfirst setNumber\n\tlast  *setNumber // if nil, this numRange is just a setNumber in \"first\" and first.star will be false\n}\n\ntype setNumber struct {\n\tnumber uint32\n\tstar   bool // References last message (max sequence number/uid). ../rfc/9051:799\n}\n\n// containsSeq returns whether seq is in the numSet, given uids and (saved) searchResult.\n// uids and searchResult must be sorted. searchResult can have uids that are no longer in uids.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (ss numSet) containsUID(uid store.UID, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn uidSearch(searchResult, uid) > 0 && uidSearch(uids, uid) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif uid >= first && uid <= last && uidSearch(uids, uid) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// contains returns whether the numset contains the number.\n// only allowed on basic, strictly increasing numsets.\nfunc (ss numSet) contains(v uint32) bool {\n\tfor _, r := range ss.ranges {\n\t\tif r.first.number == v || r.last != nil && v > r.first.number && v <= r.last.number {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (ss numSet) empty() bool {\n\treturn !ss.searchResult && len(ss.ranges) == 0\n}\n\n// Strings returns the numset in zero or more strings of maxSize bytes. If\n// maxSize is <= 0, a single string is returned.\nfunc (ss numSet) Strings(maxSize int) []string {\n\tif ss.searchResult {\n\t\treturn []string{\"$\"}\n\t}\n\tvar l []string\n\tvar line string\n\tfor _, r := range ss.ranges {\n\t\ts := \"\"\n\t\tif r.first.star {\n\t\t\ts += \"*\"\n\t\t} else {\n\t\t\ts += fmt.Sprintf(\"%d\", r.first.number)\n\t\t}\n\t\tif r.last == nil {\n\t\t\tif r.first.star {\n\t\t\t\tpanic(\"invalid numSet range first star without last\")\n\t\t\t}\n\t\t} else {\n\t\t\ts += \":\"\n\t\t\tif r.last.star {\n\t\t\t\ts += \"*\"\n\t\t\t} else {\n\t\t\t\ts += fmt.Sprintf(\"%d\", r.last.number)\n\t\t\t}\n\t\t}\n\n\t\tnsize := len(line) + len(s)\n\t\tif line != \"\" {\n\t\t\tnsize++ // comma\n\t\t}\n\t\tif maxSize > 0 && nsize > maxSize {\n\t\t\tl = append(l, line)\n\t\t\tline = s\n\t\t\tcontinue\n\t\t}\n\t\tif line != \"\" {\n\t\t\tline += \",\"\n\t\t}\n\t\tline += s\n\t}\n\tif line != \"\" {\n\t\tl = append(l, line)\n\t}\n\treturn l\n}\n\nfunc (ss numSet) String() string {\n\tl := ss.Strings(0)\n\tif len(l) == 0 {\n\t\treturn \"\"\n\t}\n\treturn l[0]\n}\n\n// interpretStar returns a numset that interprets stars in a numset, returning a new\n// numset without stars with increasing first/last.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// whether numSet only has numbers (no star/search), and is strictly increasing.\nfunc (s *numSet) isBasicIncreasing() bool {\n\tif s.searchResult {\n\t\treturn false\n\t}\n\tvar last uint32\n\tfor _, r := range s.ranges {\n\t\tif r.first.star || r.first.number <= last || r.last != nil && (r.last.star || r.last.number < r.first.number) {\n\t\t\treturn false\n\t\t}\n\t\tlast = r.first.number\n\t\tif r.last != nil {\n\t\t\tlast = r.last.number\n\t\t}\n\t}\n\treturn true\n}\n\ntype numIter struct {\n\ts numSet\n\ti int\n\tr *rangeIter\n}\n\n// newIter must only be called on a numSet that is basic (no star/search) and ascending.\nfunc (s numSet) newIter() *numIter {\n\treturn &numIter{s: s}\n}\n\nfunc (i *numIter) Next() (uint32, bool) {\n\tif v, ok := i.r.Next(); ok {\n\t\treturn v, ok\n\t}\n\tif i.i >= len(i.s.ranges) {\n\t\treturn 0, false\n\t}\n\ti.r = i.s.ranges[i.i].newIter()\n\ti.i++\n\treturn i.r.Next()\n}\n\ntype rangeIter struct {\n\tr numRange\n\to int\n}\n\n// newIter must only be called on a range in a numSet that is basic (no star/search) and ascending.\nfunc (r numRange) newIter() *rangeIter {\n\treturn &rangeIter{r: r, o: 0}\n}\n\nfunc (r *rangeIter) Next() (uint32, bool) {\n\tif r == nil {\n\t\treturn 0, false\n\t}\n\tif r.o == 0 {\n\t\tr.o++\n\t\treturn r.r.first.number, true\n\t}\n\tif r.r.last == nil || r.r.first.number+uint32(r.o) > r.r.last.number {\n\t\treturn 0, false\n\t}\n\tv := r.r.first.number + uint32(r.o)\n\tr.o++\n\treturn v, true\n}\n\n// append adds a new number to the set, extending a range, or starting a new one (possibly the first).\n// can only be used on basic numsets, without star/searchResult.\nfunc (s *numSet) append(v uint32) {\n\tif len(s.ranges) == 0 {\n\t\ts.ranges = []numRange{{first: setNumber{number: v}}}\n\t\treturn\n\t}\n\tri := len(s.ranges) - 1\n\tr := s.ranges[ri]\n\tif v == r.first.number+1 && r.last == nil {\n\t\ts.ranges[ri].last = &setNumber{number: v}\n\t} else if r.last != nil && v == r.last.number+1 {\n\t\tr.last.number++\n\t} else {\n\t\ts.ranges = append(s.ranges, numRange{first: setNumber{number: v}})\n\t}\n}\n\ntype partial struct {\n\toffset uint32\n\tcount  uint32\n}\n\ntype sectionPart struct {\n\tpart []uint32\n\ttext *sectionText\n}\n\ntype sectionText struct {\n\tmime    bool // if \"MIME\"\n\tmsgtext *sectionMsgtext\n}\n\n// a non-nil *sectionSpec with nil msgtext & nil part means there were []'s, but nothing inside. e.g. \"BODY[]\".\ntype sectionSpec struct {\n\tmsgtext *sectionMsgtext\n\tpart    *sectionPart\n}\n\ntype sectionMsgtext struct {\n\ts       string   // \"HEADER\", \"HEADER.FIELDS\", \"HEADER.FIELDS.NOT\", \"TEXT\"\n\theaders []string // for \"HEADER.FIELDS\"*\n}\n\ntype fetchAtt struct {\n\tfield         string // uppercase, eg \"ENVELOPE\", \"BODY\". \".PEEK\" is removed.\n\tpeek          bool\n\tsection       *sectionSpec\n\tsectionBinary []uint32\n\tpartial       *partial\n}\n\ntype searchKey struct {\n\t// Only one of searchKeys, seqSet and op can be non-nil/non-empty.\n\tsearchKeys   []searchKey // In case of nested/multiple keys. Also for the top-level command.\n\tseqSet       *numSet     // In case of bare sequence set. For op UID, field uidSet contains the parameter.\n\top           string      // Determines which of the fields below are set.\n\theaderField  string\n\tastring      string\n\tdate         time.Time\n\tatom         string\n\tnumber       int64\n\tsearchKey    *searchKey\n\tsearchKey2   *searchKey\n\tuidSet       numSet\n\tclientModseq *int64\n}\n\nfunc compactUIDSet(l []store.UID) (r numSet) {\n\tfor len(l) > 0 {\n\t\te := 1\n\t\tfor ; e < len(l) && l[e] == l[e-1]+1; e++ {\n\t\t}\n\t\tfirst := setNumber{number: uint32(l[0])}\n\t\tvar last *setNumber\n\t\tif e > 1 {\n\t\t\tlast = &setNumber{number: uint32(l[e-1])}\n\t\t}\n\t\tr.ranges = append(r.ranges, numRange{first, last})\n\t\tl = l[e:]\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n# Natural Language Annotations: // containsSeq returns whether seq is in the numSet, given uids and (saved) searchResult.\n// uids and searchResult must be sorted. searchResult can have uids that are no longer in uids.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n    if len(uids) == 0 {\n        return false\n    }\n    if ss.searchResult {\n        return uidSearch(searchResult, seq) > 0 && uidSearch(uids, seq) > 0\n    }\n\n    for _, r := range ss.ranges {\n        var first, last uint32\n        \n        if r.first.star {\n            first = uids[len(uids)-1]\n        } else {\n            first = r.first.number\n        }\n        \n        last = first\n        if r.last != nil {\n            if r.last.star {\n                last = uids[len(uids)-1]\n            } else {\n                last = r.last.number\n            }\n        }\n        \n        if first > last {\n            first, last = last, first\n        }\n        \n        if uint32(seq) >= first && uint32(seq) <= last && uidSearch(uids, store.UID(seq)) > 0 {\n            return true\n        }\n    }\n    \n    return false\n}\n```\n\nHere, the function `containsSeq` checks if a given sequence number (`seq`) is in the `numSet`, considering both the list of UIDs and the search results. The `uids` and `searchResult` lists must be sorted, and `searchResult` can contain UIDs that might not be present in `uids`. The function first checks if the `numSet` is based on a search result and validates using `uidSearch` if the sequence exists in both `uids` and `searchResult`. If not, it then iterates over the ranges in `numSet`, calculates the correct boundaries considering possible star markers, and checks the sequence against these adjusted boundaries. If the sequence fits within any range and is found in `uids`, it returns true, otherwise false."]}, "mox-imapserver/protocol.go-interpretStar": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/imapserver/protocol.go:\n```\npackage imapserver\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/store\"\n)\n\ntype numSet struct {\n\tsearchResult bool // \"$\"\n\tranges       []numRange\n}\n\ntype numRange struct {\n\tfirst setNumber\n\tlast  *setNumber // if nil, this numRange is just a setNumber in \"first\" and first.star will be false\n}\n\ntype setNumber struct {\n\tnumber uint32\n\tstar   bool // References last message (max sequence number/uid). ../rfc/9051:799\n}\n\n// containsSeq returns whether seq is in the numSet, given uids and (saved) searchResult.\n// uids and searchResult must be sorted. searchResult can have uids that are no longer in uids.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (ss numSet) containsUID(uid store.UID, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn uidSearch(searchResult, uid) > 0 && uidSearch(uids, uid) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif uid >= first && uid <= last && uidSearch(uids, uid) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// contains returns whether the numset contains the number.\n// only allowed on basic, strictly increasing numsets.\nfunc (ss numSet) contains(v uint32) bool {\n\tfor _, r := range ss.ranges {\n\t\tif r.first.number == v || r.last != nil && v > r.first.number && v <= r.last.number {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (ss numSet) empty() bool {\n\treturn !ss.searchResult && len(ss.ranges) == 0\n}\n\n// Strings returns the numset in zero or more strings of maxSize bytes. If\n// maxSize is <= 0, a single string is returned.\nfunc (ss numSet) Strings(maxSize int) []string {\n\tif ss.searchResult {\n\t\treturn []string{\"$\"}\n\t}\n\tvar l []string\n\tvar line string\n\tfor _, r := range ss.ranges {\n\t\ts := \"\"\n\t\tif r.first.star {\n\t\t\ts += \"*\"\n\t\t} else {\n\t\t\ts += fmt.Sprintf(\"%d\", r.first.number)\n\t\t}\n\t\tif r.last == nil {\n\t\t\tif r.first.star {\n\t\t\t\tpanic(\"invalid numSet range first star without last\")\n\t\t\t}\n\t\t} else {\n\t\t\ts += \":\"\n\t\t\tif r.last.star {\n\t\t\t\ts += \"*\"\n\t\t\t} else {\n\t\t\t\ts += fmt.Sprintf(\"%d\", r.last.number)\n\t\t\t}\n\t\t}\n\n\t\tnsize := len(line) + len(s)\n\t\tif line != \"\" {\n\t\t\tnsize++ // comma\n\t\t}\n\t\tif maxSize > 0 && nsize > maxSize {\n\t\t\tl = append(l, line)\n\t\t\tline = s\n\t\t\tcontinue\n\t\t}\n\t\tif line != \"\" {\n\t\t\tline += \",\"\n\t\t}\n\t\tline += s\n\t}\n\tif line != \"\" {\n\t\tl = append(l, line)\n\t}\n\treturn l\n}\n\nfunc (ss numSet) String() string {\n\tl := ss.Strings(0)\n\tif len(l) == 0 {\n\t\treturn \"\"\n\t}\n\treturn l[0]\n}\n\n// interpretStar returns a numset that interprets stars in a numset, returning a new\n// numset without stars with increasing first/last.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// whether numSet only has numbers (no star/search), and is strictly increasing.\nfunc (s *numSet) isBasicIncreasing() bool {\n\tif s.searchResult {\n\t\treturn false\n\t}\n\tvar last uint32\n\tfor _, r := range s.ranges {\n\t\tif r.first.star || r.first.number <= last || r.last != nil && (r.last.star || r.last.number < r.first.number) {\n\t\t\treturn false\n\t\t}\n\t\tlast = r.first.number\n\t\tif r.last != nil {\n\t\t\tlast = r.last.number\n\t\t}\n\t}\n\treturn true\n}\n\ntype numIter struct {\n\ts numSet\n\ti int\n\tr *rangeIter\n}\n\n// newIter must only be called on a numSet that is basic (no star/search) and ascending.\nfunc (s numSet) newIter() *numIter {\n\treturn &numIter{s: s}\n}\n\nfunc (i *numIter) Next() (uint32, bool) {\n\tif v, ok := i.r.Next(); ok {\n\t\treturn v, ok\n\t}\n\tif i.i >= len(i.s.ranges) {\n\t\treturn 0, false\n\t}\n\ti.r = i.s.ranges[i.i].newIter()\n\ti.i++\n\treturn i.r.Next()\n}\n\ntype rangeIter struct {\n\tr numRange\n\to int\n}\n\n// newIter must only be called on a range in a numSet that is basic (no star/search) and ascending.\nfunc (r numRange) newIter() *rangeIter {\n\treturn &rangeIter{r: r, o: 0}\n}\n\nfunc (r *rangeIter) Next() (uint32, bool) {\n\tif r == nil {\n\t\treturn 0, false\n\t}\n\tif r.o == 0 {\n\t\tr.o++\n\t\treturn r.r.first.number, true\n\t}\n\tif r.r.last == nil || r.r.first.number+uint32(r.o) > r.r.last.number {\n\t\treturn 0, false\n\t}\n\tv := r.r.first.number + uint32(r.o)\n\tr.o++\n\treturn v, true\n}\n\n// append adds a new number to the set, extending a range, or starting a new one (possibly the first).\n// can only be used on basic numsets, without star/searchResult.\nfunc (s *numSet) append(v uint32) {\n\tif len(s.ranges) == 0 {\n\t\ts.ranges = []numRange{{first: setNumber{number: v}}}\n\t\treturn\n\t}\n\tri := len(s.ranges) - 1\n\tr := s.ranges[ri]\n\tif v == r.first.number+1 && r.last == nil {\n\t\ts.ranges[ri].last = &setNumber{number: v}\n\t} else if r.last != nil && v == r.last.number+1 {\n\t\tr.last.number++\n\t} else {\n\t\ts.ranges = append(s.ranges, numRange{first: setNumber{number: v}})\n\t}\n}\n\ntype partial struct {\n\toffset uint32\n\tcount  uint32\n}\n\ntype sectionPart struct {\n\tpart []uint32\n\ttext *sectionText\n}\n\ntype sectionText struct {\n\tmime    bool // if \"MIME\"\n\tmsgtext *sectionMsgtext\n}\n\n// a non-nil *sectionSpec with nil msgtext & nil part means there were []'s, but nothing inside. e.g. \"BODY[]\".\ntype sectionSpec struct {\n\tmsgtext *sectionMsgtext\n\tpart    *sectionPart\n}\n\ntype sectionMsgtext struct {\n\ts       string   // \"HEADER\", \"HEADER.FIELDS\", \"HEADER.FIELDS.NOT\", \"TEXT\"\n\theaders []string // for \"HEADER.FIELDS\"*\n}\n\ntype fetchAtt struct {\n\tfield         string // uppercase, eg \"ENVELOPE\", \"BODY\". \".PEEK\" is removed.\n\tpeek          bool\n\tsection       *sectionSpec\n\tsectionBinary []uint32\n\tpartial       *partial\n}\n\ntype searchKey struct {\n\t// Only one of searchKeys, seqSet and op can be non-nil/non-empty.\n\tsearchKeys   []searchKey // In case of nested/multiple keys. Also for the top-level command.\n\tseqSet       *numSet     // In case of bare sequence set. For op UID, field uidSet contains the parameter.\n\top           string      // Determines which of the fields below are set.\n\theaderField  string\n\tastring      string\n\tdate         time.Time\n\tatom         string\n\tnumber       int64\n\tsearchKey    *searchKey\n\tsearchKey2   *searchKey\n\tuidSet       numSet\n\tclientModseq *int64\n}\n\nfunc compactUIDSet(l []store.UID) (r numSet) {\n\tfor len(l) > 0 {\n\t\te := 1\n\t\tfor ; e < len(l) && l[e] == l[e-1]+1; e++ {\n\t\t}\n\t\tfirst := setNumber{number: uint32(l[0])}\n\t\tvar last *setNumber\n\t\tif e > 1 {\n\t\t\tlast = &setNumber{number: uint32(l[e-1])}\n\t\t}\n\t\tr.ranges = append(r.ranges, numRange{first, last})\n\t\tl = l[e:]\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (s numSet) interpretStar(uids []store.UID) numSet {\n# Natural Language Annotations: // interpretStar returns a numset that interprets stars in a numset, returning a new\n// numset without stars with increasing first/last.\nPlease return the generated code surrounded by ```\n", "response": ["To create the `interpretStar` function, we need to interpret the stars (`*`) in the `numSet`, translating them into actual numbers based on the UIDs list provided. The stars generally represent the highest valid UID in the context. The result will be a new `numSet` without any stars and with strictly increasing sequence numbers.\n\nHere's how the function can be implemented:\n\n```go\nfunc (s numSet) interpretStar(uids []store.UID) numSet {\n    if len(uids) == 0 {\n        return numSet{} // Return an empty numSet if no uids are provided.\n    }\n\n    // maxUID is the largest UID in the list provided.\n    maxUID := uids[len(uids)-1]\n    var newRanges []numRange\n\n    for _, r := range s.ranges {\n        first := r.first.number\n        if r.first.star {\n            first = uint32(maxUID)\n        }\n        \n        last := uint32(maxUID) // Default to maxUID if star is used.\n        if r.last != nil {\n            last = r.last.number\n            if r.last.star {\n                last = uint32(maxUID)\n            }\n        }\n\n        // Append the interpreted range after ensuring correct order.\n        if first <= last {\n            newRanges = append(newRanges, numRange{first: setNumber{number: first}, last: &setNumber{number: last}})\n        } else {\n            newRanges = append(newRanges, numRange{first: setNumber{number: last}, last: &setNumber{number: first}})\n        }\n    }\n\n    return numSet{ranges: newRanges}\n}\n```\n\nThis function ensures that any stars in the original `numSet` are replaced with the maximum UID available from the provided UID list, maintaining a strictly increasing order in the interpreted `numSet`."]}, "mox-http/web.go-ServeHTTP": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/http/web.go:\n```\n// Package http provides HTTP listeners/servers, for\n// autoconfiguration/autodiscovery, the account and admin web interface and\n// MTA-STS policies.\npackage http\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"fmt\"\n\t\"io\"\n\tgolog \"log\"\n\t\"log/slog\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t_ \"embed\"\n\t_ \"net/http/pprof\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\n\t\"github.com/mjl-/mox/autotls\"\n\t\"github.com/mjl-/mox/config\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/ratelimit\"\n\t\"github.com/mjl-/mox/webaccount\"\n\t\"github.com/mjl-/mox/webadmin\"\n\t\"github.com/mjl-/mox/webapisrv\"\n\t\"github.com/mjl-/mox/webmail\"\n)\n\nvar pkglog = mlog.New(\"http\", nil)\n\nvar (\n\t// metricRequest tracks performance (time to write response header) of server.\n\tmetricRequest = promauto.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"mox_httpserver_request_duration_seconds\",\n\t\t\tHelp:    \"HTTP(s) server request with handler name, protocol, method, result codes, and duration until response status code is written, in seconds.\",\n\t\t\tBuckets: []float64{0.001, 0.005, 0.01, 0.05, 0.100, 0.5, 1, 5, 10, 20, 30, 60, 120},\n\t\t},\n\t\t[]string{\n\t\t\t\"handler\", // Name from webhandler, can be empty.\n\t\t\t\"proto\",   // \"http\", \"https\", \"ws\", \"wss\"\n\t\t\t\"method\",  // \"(unknown)\" and otherwise only common verbs\n\t\t\t\"code\",\n\t\t},\n\t)\n\t// metricResponse tracks performance of entire request as experienced by users,\n\t// which also depends on their connection speed, so not necessarily something you\n\t// could act on.\n\tmetricResponse = promauto.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"mox_httpserver_response_duration_seconds\",\n\t\t\tHelp:    \"HTTP(s) server response with handler name, protocol, method, result codes, and duration of entire response, in seconds.\",\n\t\t\tBuckets: []float64{0.001, 0.005, 0.01, 0.05, 0.100, 0.5, 1, 5, 10, 20, 30, 60, 120},\n\t\t},\n\t\t[]string{\n\t\t\t\"handler\", // Name from webhandler, can be empty.\n\t\t\t\"proto\",   // \"http\", \"https\", \"ws\", \"wss\"\n\t\t\t\"method\",  // \"(unknown)\" and otherwise only common verbs\n\t\t\t\"code\",\n\t\t},\n\t)\n)\n\n// We serve a favicon when webaccount/webmail/webadmin/webapi for account-related\n// domains. They are configured as \"service handler\", which have a lower priority\n// than web handler. Admins can configure a custom /favicon.ico route to override\n// the builtin favicon. In the future, we may want to make it easier to customize\n// the favicon, possibly per client settings domain.\n//\n//go:embed favicon.ico\nvar faviconIco string\nvar faviconModTime = time.Now()\n\nfunc init() {\n\tp, err := os.Executable()\n\tif err == nil {\n\t\tif st, err := os.Stat(p); err == nil {\n\t\t\tfaviconModTime = st.ModTime()\n\t\t}\n\t}\n}\n\nfunc faviconHandle(w http.ResponseWriter, r *http.Request) {\n\thttp.ServeContent(w, r, \"favicon.ico\", faviconModTime, strings.NewReader(faviconIco))\n}\n\ntype responseWriterFlusher interface {\n\thttp.ResponseWriter\n\thttp.Flusher\n}\n\n// http.ResponseWriter that writes access log and tracks metrics at end of response.\ntype loggingWriter struct {\n\tW                responseWriterFlusher // Calls are forwarded.\n\tStart            time.Time\n\tR                *http.Request\n\tWebsocketRequest bool // Whether request from was websocket.\n\n\t// Set by router.\n\tHandler  string\n\tCompress bool\n\n\t// Set by handlers.\n\tStatusCode                   int\n\tSize                         int64        // Of data served to client, for non-websocket responses.\n\tUncompressedSize             int64        // Can be set by a handler that already serves compressed data, and we update it while compressing.\n\tGzip                         *gzip.Writer // Only set if we transparently compress within loggingWriter (static handlers handle compression themselves, with a cache).\n\tErr                          error\n\tWebsocketResponse            bool        // If this was a successful websocket connection with backend.\n\tSizeFromClient, SizeToClient int64       // Websocket data.\n\tAttrs                        []slog.Attr // Additional fields to log.\n}\n\nfunc (w *loggingWriter) AddAttr(a slog.Attr) {\n\tw.Attrs = append(w.Attrs, a)\n}\n\nfunc (w *loggingWriter) Flush() {\n\tw.W.Flush()\n}\n\nfunc (w *loggingWriter) Header() http.Header {\n\treturn w.W.Header()\n}\n\n// protocol, for logging.\nfunc (w *loggingWriter) proto(websocket bool) string {\n\tproto := \"http\"\n\tif websocket {\n\t\tproto = \"ws\"\n\t}\n\tif w.R.TLS != nil {\n\t\tproto += \"s\"\n\t}\n\treturn proto\n}\n\nfunc (w *loggingWriter) Write(buf []byte) (int, error) {\n\tif w.StatusCode == 0 {\n\t\tw.WriteHeader(http.StatusOK)\n\t}\n\n\tvar n int\n\tvar err error\n\tif w.Gzip == nil {\n\t\tn, err = w.W.Write(buf)\n\t\tif n > 0 {\n\t\t\tw.Size += int64(n)\n\t\t}\n\t} else {\n\t\t// We flush after each write. Probably takes a few more bytes, but prevents any\n\t\t// issues due to buffering.\n\t\t// w.Gzip.Write updates w.Size with the compressed byte count.\n\t\tn, err = w.Gzip.Write(buf)\n\t\tif err == nil {\n\t\t\terr = w.Gzip.Flush()\n\t\t}\n\t\tif n > 0 {\n\t\t\tw.UncompressedSize += int64(n)\n\t\t}\n\t}\n\tif err != nil {\n\t\tw.error(err)\n\t}\n\treturn n, err\n}\n\nfunc (w *loggingWriter) setStatusCode(statusCode int) {\n\tif w.StatusCode != 0 {\n\t\treturn\n\t}\n\n\tw.StatusCode = statusCode\n\tmethod := metricHTTPMethod(w.R.Method)\n\tmetricRequest.WithLabelValues(w.Handler, w.proto(w.WebsocketRequest), method, fmt.Sprintf(\"%d\", w.StatusCode)).Observe(float64(time.Since(w.Start)) / float64(time.Second))\n}\n\n// SetUncompressedSize is used through an interface by\n// ../webmail/webmail.go:/WriteHeader, preventing an import cycle.\nfunc (w *loggingWriter) SetUncompressedSize(origSize int64) {\n\tw.UncompressedSize = origSize\n}\n\nfunc (w *loggingWriter) WriteHeader(statusCode int) {\n\tif w.StatusCode != 0 {\n\t\treturn\n\t}\n\n\tw.setStatusCode(statusCode)\n\n\t// We transparently gzip-compress responses for requests under these conditions, all must apply:\n\t//\n\t// - Enabled for handler (static handlers make their own decisions).\n\t// - Not a websocket request.\n\t// - Regular success responses (not errors, or partial content or redirects or \"not modified\", etc).\n\t// - Not already compressed, or any other Content-Encoding header (including \"identity\").\n\t// - Client accepts gzip encoded responses.\n\t// - The response has a content-type that is compressible (text/*, */*+{json,xml}, and a few common files (e.g. json, xml, javascript).\n\tif w.Compress && !w.WebsocketRequest && statusCode == http.StatusOK && w.W.Header().Values(\"Content-Encoding\") == nil && acceptsGzip(w.R) && compressibleContentType(w.W.Header().Get(\"Content-Type\")) {\n\t\t// todo: we should gather the first kb of data, see if it is compressible. if not, just return original. should set timer so we flush if it takes too long to gather 1kb. for smaller data we shouldn't compress at all.\n\n\t\t// We track the gzipped output for the access log.\n\t\tcw := countWriter{Writer: w.W, Size: &w.Size}\n\t\tw.Gzip, _ = gzip.NewWriterLevel(cw, gzip.BestSpeed)\n\t\tw.W.Header().Set(\"Content-Encoding\", \"gzip\")\n\t\tw.W.Header().Del(\"Content-Length\") // No longer valid, set again for small responses by net/http.\n\t}\n\tw.W.WriteHeader(statusCode)\n}\n\nfunc acceptsGzip(r *http.Request) bool {\n\ts := r.Header.Get(\"Accept-Encoding\")\n\tt := strings.Split(s, \",\")\n\tfor _, e := range t {\n\t\te = strings.TrimSpace(e)\n\t\ttt := strings.Split(e, \";\")\n\t\tif len(tt) > 1 && t[1] == \"q=0\" {\n\t\t\tcontinue\n\t\t}\n\t\tif tt[0] == \"gzip\" {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nvar compressibleTypes = map[string]bool{\n\t\"application/csv\":          true,\n\t\"application/javascript\":   true,\n\t\"application/json\":         true,\n\t\"application/x-javascript\": true,\n\t\"application/xml\":          true,\n\t\"image/vnd.microsoft.icon\": true,\n\t\"image/x-icon\":             true,\n\t\"font/ttf\":                 true,\n\t\"font/eot\":                 true,\n\t\"font/otf\":                 true,\n\t\"font/opentype\":            true,\n}\n\nfunc compressibleContentType(ct string) bool {\n\tct = strings.SplitN(ct, \";\", 2)[0]\n\tct = strings.TrimSpace(ct)\n\tct = strings.ToLower(ct)\n\tif compressibleTypes[ct] {\n\t\treturn true\n\t}\n\tt, st, _ := strings.Cut(ct, \"/\")\n\treturn t == \"text\" || strings.HasSuffix(st, \"+json\") || strings.HasSuffix(st, \"+xml\")\n}\n\nfunc compressibleContent(f *os.File) bool {\n\t// We don't want to store many small files. They take up too much disk overhead.\n\tif fi, err := f.Stat(); err != nil || fi.Size() < 1024 || fi.Size() > 10*1024*1024 {\n\t\treturn false\n\t}\n\n\tbuf := make([]byte, 512)\n\tn, err := f.ReadAt(buf, 0)\n\tif err != nil && err != io.EOF {\n\t\treturn false\n\t}\n\tct := http.DetectContentType(buf[:n])\n\treturn compressibleContentType(ct)\n}\n\ntype countWriter struct {\n\tWriter io.Writer\n\tSize   *int64\n}\n\nfunc (w countWriter) Write(buf []byte) (int, error) {\n\tn, err := w.Writer.Write(buf)\n\tif n > 0 {\n\t\t*w.Size += int64(n)\n\t}\n\treturn n, err\n}\n\nvar tlsVersions = map[uint16]string{\n\ttls.VersionTLS10: \"tls1.0\",\n\ttls.VersionTLS11: \"tls1.1\",\n\ttls.VersionTLS12: \"tls1.2\",\n\ttls.VersionTLS13: \"tls1.3\",\n}\n\nfunc metricHTTPMethod(method string) string {\n\t// https://www.iana.org/assignments/http-methods/http-methods.xhtml\n\tmethod = strings.ToLower(method)\n\tswitch method {\n\tcase \"acl\", \"baseline-control\", \"bind\", \"checkin\", \"checkout\", \"connect\", \"copy\", \"delete\", \"get\", \"head\", \"label\", \"link\", \"lock\", \"merge\", \"mkactivity\", \"mkcalendar\", \"mkcol\", \"mkredirectref\", \"mkworkspace\", \"move\", \"options\", \"orderpatch\", \"patch\", \"post\", \"pri\", \"propfind\", \"proppatch\", \"put\", \"rebind\", \"report\", \"search\", \"trace\", \"unbind\", \"uncheckout\", \"unlink\", \"unlock\", \"update\", \"updateredirectref\", \"version-control\":\n\t\treturn method\n\t}\n\treturn \"(other)\"\n}\n\nfunc (w *loggingWriter) error(err error) {\n\tif w.Err == nil {\n\t\tw.Err = err\n\t}\n}\n\nfunc (w *loggingWriter) Done() {\n\tif w.Err == nil && w.Gzip != nil {\n\t\tif err := w.Gzip.Close(); err != nil {\n\t\t\tw.error(err)\n\t\t}\n\t}\n\n\tmethod := metricHTTPMethod(w.R.Method)\n\tmetricResponse.WithLabelValues(w.Handler, w.proto(w.WebsocketResponse), method, fmt.Sprintf(\"%d\", w.StatusCode)).Observe(float64(time.Since(w.Start)) / float64(time.Second))\n\n\ttlsinfo := \"plain\"\n\tif w.R.TLS != nil {\n\t\tif v, ok := tlsVersions[w.R.TLS.Version]; ok {\n\t\t\ttlsinfo = v\n\t\t} else {\n\t\t\ttlsinfo = \"(other)\"\n\t\t}\n\t}\n\terr := w.Err\n\tif err == nil {\n\t\terr = w.R.Context().Err()\n\t}\n\tattrs := []slog.Attr{\n\t\tslog.String(\"httpaccess\", \"\"),\n\t\tslog.String(\"handler\", w.Handler),\n\t\tslog.String(\"method\", method),\n\t\tslog.Any(\"url\", w.R.URL),\n\t\tslog.String(\"host\", w.R.Host),\n\t\tslog.Duration(\"duration\", time.Since(w.Start)),\n\t\tslog.Int(\"statuscode\", w.StatusCode),\n\t\tslog.String(\"proto\", strings.ToLower(w.R.Proto)),\n\t\tslog.Any(\"remoteaddr\", w.R.RemoteAddr),\n\t\tslog.String(\"tlsinfo\", tlsinfo),\n\t\tslog.String(\"useragent\", w.R.Header.Get(\"User-Agent\")),\n\t\tslog.String(\"referrr\", w.R.Header.Get(\"Referrer\")),\n\t}\n\tif w.WebsocketRequest {\n\t\tattrs = append(attrs,\n\t\t\tslog.Bool(\"websocketrequest\", true),\n\t\t)\n\t}\n\tif w.WebsocketResponse {\n\t\tattrs = append(attrs,\n\t\t\tslog.Bool(\"websocket\", true),\n\t\t\tslog.Int64(\"sizetoclient\", w.SizeToClient),\n\t\t\tslog.Int64(\"sizefromclient\", w.SizeFromClient),\n\t\t)\n\t} else if w.UncompressedSize > 0 {\n\t\tattrs = append(attrs,\n\t\t\tslog.Int64(\"size\", w.Size),\n\t\t\tslog.Int64(\"uncompressedsize\", w.UncompressedSize),\n\t\t)\n\t} else {\n\t\tattrs = append(attrs,\n\t\t\tslog.Int64(\"size\", w.Size),\n\t\t)\n\t}\n\tattrs = append(attrs, w.Attrs...)\n\tpkglog.WithContext(w.R.Context()).Debugx(\"http request\", err, attrs...)\n}\n\n// Built-in handlers, e.g. mta-sts and autoconfig.\ntype pathHandler struct {\n\tName      string                       // For logging/metrics.\n\tHostMatch func(host dns.IPDomain) bool // If not nil, called to see if domain of requests matches. Host can be zero value for invalid domain/ip.\n\tPath      string                       // Path to register, like on http.ServeMux.\n\tHandler   http.Handler\n}\ntype serve struct {\n\tKinds     []string // Type of handler and protocol (e.g. acme-tls-alpn-01, account-http, admin-https).\n\tTLSConfig *tls.Config\n\tFavicon   bool\n\n\t// SystemHandlers are for MTA-STS, autoconfig, ACME validation. They can't be\n\t// overridden by WebHandlers. WebHandlers are evaluated next, and the internal\n\t// service handlers from Listeners in mox.conf (for admin, account, webmail, webapi\n\t// interfaces) last. WebHandlers can also pass requests to the internal servers.\n\t// This order allows admins to serve other content on domains serving the mox.conf\n\t// internal services.\n\tSystemHandlers  []pathHandler // Sorted, longest first.\n\tWebserver       bool\n\tServiceHandlers []pathHandler // Sorted, longest first.\n}\n\n// SystemHandle registers a named system handler for a path and optional host. If\n// path ends with a slash, it is used as prefix match, otherwise a full path match\n// is required. If hostOpt is set, only requests to those host are handled by this\n// handler.\nfunc (s *serve) SystemHandle(name string, hostMatch func(dns.IPDomain) bool, path string, fn http.Handler) {\n\ts.SystemHandlers = append(s.SystemHandlers, pathHandler{name, hostMatch, path, fn})\n}\n\n// Like SystemHandle, but for internal services \"admin\", \"account\", \"webmail\",\n// \"webapi\" configured in the mox.conf Listener.\nfunc (s *serve) ServiceHandle(name string, hostMatch func(dns.IPDomain) bool, path string, fn http.Handler) {\n\ts.ServiceHandlers = append(s.ServiceHandlers, pathHandler{name, hostMatch, path, fn})\n}\n\nvar (\n\tlimiterConnectionrate = &ratelimit.Limiter{\n\t\tWindowLimits: []ratelimit.WindowLimit{\n\t\t\t{\n\t\t\t\tWindow: time.Minute,\n\t\t\t\tLimits: [...]int64{1000, 3000, 9000},\n\t\t\t},\n\t\t\t{\n\t\t\t\tWindow: time.Hour,\n\t\t\t\tLimits: [...]int64{5000, 15000, 45000},\n\t\t\t},\n\t\t},\n\t}\n)\n\n// ServeHTTP is the starting point for serving HTTP requests. It dispatches to the\n// right pathHandler or WebHandler, and it generates access logs and tracks\n// metrics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc redirectToTrailingSlash(srv *serve, hostMatch func(dns.IPDomain) bool, name, path string) {\n\t// Helpfully redirect user to version with ending slash.\n\tif path != \"/\" && strings.HasSuffix(path, \"/\") {\n\t\thandler := mox.SafeHeaders(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\t\thttp.Redirect(w, r, path, http.StatusSeeOther)\n\t\t}))\n\t\tsrv.ServiceHandle(name, hostMatch, path[:len(path)-1], handler)\n\t}\n}\n\n// Listen binds to sockets for HTTP listeners, including those required for ACME to\n// generate TLS certificates. It stores the listeners so Serve can start serving them.\nfunc Listen() {\n\t// Initialize listeners in deterministic order for the same potential error\n\t// messages.\n\tnames := maps.Keys(mox.Conf.Static.Listeners)\n\tsort.Strings(names)\n\tfor _, name := range names {\n\t\tl := mox.Conf.Static.Listeners[name]\n\t\tportServe := portServes(l)\n\n\t\tports := maps.Keys(portServe)\n\t\tsort.Ints(ports)\n\t\tfor _, port := range ports {\n\t\t\tsrv := portServe[port]\n\t\t\tfor _, ip := range l.IPs {\n\t\t\t\tlisten1(ip, port, srv.TLSConfig, name, srv.Kinds, srv)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc portServes(l config.Listener) map[int]*serve {\n\tportServe := map[int]*serve{}\n\n\t// For system/services, we serve on host localhost too, for ssh tunnel scenario's.\n\tlocalhost := dns.Domain{ASCII: \"localhost\"}\n\n\tldom := l.HostnameDomain\n\tif l.Hostname == \"\" {\n\t\tldom = mox.Conf.Static.HostnameDomain\n\t}\n\tlistenerHostMatch := func(host dns.IPDomain) bool {\n\t\tif host.IsIP() {\n\t\t\treturn true\n\t\t}\n\t\treturn host.Domain == ldom || host.Domain == localhost\n\t}\n\taccountHostMatch := func(host dns.IPDomain) bool {\n\t\tif listenerHostMatch(host) {\n\t\t\treturn true\n\t\t}\n\t\treturn mox.Conf.IsClientSettingsDomain(host.Domain)\n\t}\n\n\tvar ensureServe func(https bool, port int, kind string, favicon bool) *serve\n\tensureServe = func(https bool, port int, kind string, favicon bool) *serve {\n\t\ts := portServe[port]\n\t\tif s == nil {\n\t\t\ts = &serve{nil, nil, false, nil, false, nil}\n\t\t\tportServe[port] = s\n\t\t}\n\t\ts.Kinds = append(s.Kinds, kind)\n\t\tif favicon && !s.Favicon {\n\t\t\ts.ServiceHandle(\"favicon\", accountHostMatch, \"/favicon.ico\", mox.SafeHeaders(http.HandlerFunc(faviconHandle)))\n\t\t\ts.Favicon = true\n\t\t}\n\n\t\tif https && l.TLS.ACME != \"\" {\n\t\t\ts.TLSConfig = l.TLS.ACMEConfig\n\t\t} else if https {\n\t\t\ts.TLSConfig = l.TLS.Config\n\t\t\tif l.TLS.ACME != \"\" {\n\t\t\t\ttlsport := config.Port(mox.Conf.Static.ACME[l.TLS.ACME].Port, 443)\n\t\t\t\tensureServe(true, tlsport, \"acme-tls-alpn-01\", false)\n\t\t\t}\n\t\t}\n\t\treturn s\n\t}\n\n\tif l.TLS != nil && l.TLS.ACME != \"\" && (l.SMTP.Enabled && !l.SMTP.NoSTARTTLS || l.Submissions.Enabled || l.IMAPS.Enabled) {\n\t\tport := config.Port(mox.Conf.Static.ACME[l.TLS.ACME].Port, 443)\n\t\tensureServe(true, port, \"acme-tls-alpn-01\", false)\n\t}\n\n\tif l.AccountHTTP.Enabled {\n\t\tport := config.Port(l.AccountHTTP.Port, 80)\n\t\tpath := \"/\"\n\t\tif l.AccountHTTP.Path != \"\" {\n\t\t\tpath = l.AccountHTTP.Path\n\t\t}\n\t\tsrv := ensureServe(false, port, \"account-http at \"+path, true)\n\t\thandler := mox.SafeHeaders(http.StripPrefix(path[:len(path)-1], http.HandlerFunc(webaccount.Handler(path, l.AccountHTTP.Forwarded))))\n\t\tsrv.ServiceHandle(\"account\", accountHostMatch, path, handler)\n\t\tredirectToTrailingSlash(srv, accountHostMatch, \"account\", path)\n\t}\n\tif l.AccountHTTPS.Enabled {\n\t\tport := config.Port(l.AccountHTTPS.Port, 443)\n\t\tpath := \"/\"\n\t\tif l.AccountHTTPS.Path != \"\" {\n\t\t\tpath = l.AccountHTTPS.Path\n\t\t}\n\t\tsrv := ensureServe(true, port, \"account-https at \"+path, true)\n\t\thandler := mox.SafeHeaders(http.StripPrefix(path[:len(path)-1], http.HandlerFunc(webaccount.Handler(path, l.AccountHTTPS.Forwarded))))\n\t\tsrv.ServiceHandle(\"account\", accountHostMatch, path, handler)\n\t\tredirectToTrailingSlash(srv, accountHostMatch, \"account\", path)\n\t}\n\n\tif l.AdminHTTP.Enabled {\n\t\tport := config.Port(l.AdminHTTP.Port, 80)\n\t\tpath := \"/admin/\"\n\t\tif l.AdminHTTP.Path != \"\" {\n\t\t\tpath = l.AdminHTTP.Path\n\t\t}\n\t\tsrv := ensureServe(false, port, \"admin-http at \"+path, true)\n\t\thandler := mox.SafeHeaders(http.StripPrefix(path[:len(path)-1], http.HandlerFunc(webadmin.Handler(path, l.AdminHTTP.Forwarded))))\n\t\tsrv.ServiceHandle(\"admin\", listenerHostMatch, path, handler)\n\t\tredirectToTrailingSlash(srv, listenerHostMatch, \"admin\", path)\n\t}\n\tif l.AdminHTTPS.Enabled {\n\t\tport := config.Port(l.AdminHTTPS.Port, \n```\n# Function Signature: func (s *serve) ServeHTTP(xw http.ResponseWriter, r *http.Request) {\n# Natural Language Annotations: // ServeHTTP is the starting point for serving HTTP requests. It dispatches to the\n// right pathHandler or WebHandler, and it generates access logs and tracks\n// metrics.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *serve) ServeHTTP(xw http.ResponseWriter, r *http.Request) {\n\tstartTime := time.Now()\n\tlw := &loggingWriter{\n\t\tW:     xw.(responseWriterFlusher),\n\t\tStart: startTime,\n\t\tR:     r,\n\t}\n\n\tfor _, ph := range s.SystemHandlers {\n\t\tif ph.HostMatch != nil && !ph.HostMatch(dns.IPDomain{ASCII: r.Host}) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif handlerMatches(r.URL.Path, ph.Path) {\n\t\t\tlw.Handler = ph.Name\n\t\t\tph.Handler.ServeHTTP(lw, r)\n\t\t\tlw.Done()\n\t\t\treturn\n\t\t}\n\t}\n\n\tfor _, ph := range s.ServiceHandlers {\n\t\tif ph.HostMatch != nil && !ph.HostMatch(dns.IPDomain{ASCII: r.Host}) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif handlerMatches(r.URL.Path, ph.Path) {\n\t\t\tlw.Handler = ph.Name\n\t\t\tph.Handler.ServeHTTP(lw, r)\n\t\t\tlw.Done()\n\t\t\treturn\n\t\t}\n\t}\n\n\thttp.NotFound(lw, r)\n\tlw.Done()\n}\n\nfunc handlerMatches(requestPath, registeredPath string) bool {\n\tif strings.HasSuffix(registeredPath, \"/\") {\n\t\treturn strings.HasPrefix(requestPath, registeredPath)\n\t}\n\treturn requestPath == registeredPath\n}\n```"]}, "mox-moxio/bufpool.go-Readline": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/bufpool.go:\n```\npackage moxio\n\nimport (\n\t\"bufio\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// todo: instead of a bufpool, should maybe just make an alternative to bufio.Reader with a big enough buffer that we can fully use to read a line.\n\nvar ErrLineTooLong = errors.New(\"line from remote too long\") // Returned by Bufpool.Readline.\n\n// Bufpool caches byte slices for reuse during parsing of line-terminated commands.\ntype Bufpool struct {\n\tc    chan []byte\n\tsize int\n}\n\n// NewBufpool makes a new pool, initially empty, but holding at most \"max\" buffers of \"size\" bytes each.\nfunc NewBufpool(max, size int) *Bufpool {\n\treturn &Bufpool{\n\t\tc:    make(chan []byte, max),\n\t\tsize: size,\n\t}\n}\n\n// get returns a buffer from the pool if available, otherwise allocates a new buffer.\n// The buffer should be returned with a call to put.\nfunc (b *Bufpool) get() []byte {\n\tvar buf []byte\n\n\t// Attempt to get buffer from pool. Otherwise create new buffer.\n\tselect {\n\tcase buf = <-b.c:\n\tdefault:\n\t}\n\tif buf == nil {\n\t\tbuf = make([]byte, b.size)\n\t}\n\treturn buf\n}\n\n// put puts a \"buf\" back in the pool. Put clears the first \"n\" bytes, which should\n// be all the bytes that have been read in the buffer. If the pool is full, the\n// buffer is discarded, and will be cleaned up by the garbage collector.\n// The caller should no longer reference \"buf\" after a call to put.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Readline reads a \\n- or \\r\\n-terminated line. Line is returned without \\n or \\r\\n.\n// If the line was too long, ErrLineTooLong is returned.\n// If an EOF is encountered before a \\n, io.ErrUnexpectedEOF is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n# Natural Language Annotations: // Readline reads a \\n- or \\r\\n-terminated line. Line is returned without \\n or \\r\\n.\n// If the line was too long, ErrLineTooLong is returned.\n// If an EOF is encountered before a \\n, io.ErrUnexpectedEOF is returned.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n\tbuf := b.get()\n\tdefer func() {\n\t\t// We return the buffer to the pool, whether an error occurs or not\n\t\tb.put(buf)\n\t}()\n\n\tvar isPrefix bool\n\tfor {\n\t\tpart, prefix, err := r.ReadLine()\n\t\tisPrefix = prefix\n\n\t\tif err != nil {\n\t\t\tif err == io.EOF && len(part) > 0 {\n\t\t\t\t// EOF was hit before newline and there is content in part\n\t\t\t\trerr = io.ErrUnexpectedEOF\n\t\t\t} else {\n\t\t\t\trerr = err\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\tif len(buf)+len(part) > b.size {\n\t\t\trerr = ErrLineTooLong\n\t\t\treturn\n\t\t}\n\n\t\tbuf = append(buf, part...)\n\n\t\tif !isPrefix {\n\t\t\t// End of line\n\t\t\tbreak\n\t\t}\n\t}\n\n\tline = string(buf)\n\treturn\n}\n```"]}, "mox-moxio/linkcopy.go-LinkOrCopy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/linkcopy.go:\n```\npackage moxio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func LinkOrCopy(log mlog.Log, dst, src string, srcReaderOpt io.Reader, sync bool) (rerr error) {\n# Natural Language Annotations: // LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage moxio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nfunc LinkOrCopy(log mlog.Log, dst, src string, srcReaderOpt io.Reader, sync bool) (rerr error) {\n\t// First attempt to create a hardlink\n\tif err := os.Link(src, dst); err == nil {\n\t\tlog.Info(fmt.Sprintf(\"Hard link created from %s to %s\", src, dst))\n\t\treturn nil\n\t} else {\n\t\tlog.Warning(fmt.Sprintf(\"Failed to create hard link: %v. Attempting file copy.\", err))\n\t}\n\n\t// If hardlink failed, attempt to copy the file\n\tvar srcFile io.Reader\n\tvar err error\n\n\tif srcReaderOpt != nil {\n\t\tsrcFile = srcReaderOpt\n\t} else {\n\t\t// open the source file for reading\n\t\tfile, openErr := os.Open(src)\n\t\tif openErr != nil {\n\t\t\treturn openErr\n\t\t}\n\t\tdefer func() {\n\t\t\tif closeErr := file.Close(); closeErr != nil && rerr == nil {\n\t\t\t\trerr = closeErr\n\t\t\t}\n\t\t}()\n\t\tsrcFile = file\n\t}\n\n\t// create or truncate the destination file\n\tdstFile, createErr := os.Create(dst)\n\tif createErr != nil {\n\t\treturn createErr\n\t}\n\tdefer func() {\n\t\tif closeErr := dstFile.Close(); closeErr != nil && rerr == nil {\n\t\t\trerr = closeErr\n\t\t}\n\t\t// remove the destination file if there was an error\n\t\tif rerr != nil {\n\t\t\tif removeErr := os.Remove(dst); removeErr != nil {\n\t\t\t\tlog.Warning(fmt.Sprintf(\"Failed to remove %s after error: %v\", dst, removeErr))\n\t\t\t}\n\t\t}\n\t}()\n\n\tif _, err = io.Copy(dstFile, srcFile); err != nil {\n\t\treturn err\n\t}\n\n\t// If sync was requested, ensure the destination file is synced to disk\n\tif sync {\n\t\tif syncErr := dstFile.Sync(); syncErr != nil {\n\t\t\treturn syncErr\n\t\t}\n\t\tlog.Info(fmt.Sprintf(\"Synced %s to disk\", dst))\n\t}\n\n\tlog.Info(fmt.Sprintf(\"File copied from %s to %s\", src, dst))\n\treturn nil\n}\n```"]}, "mox-moxio/base64writer.go-Base64Writer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/base64writer.go:\n```\npackage moxio\n\nimport (\n\t\"encoding/base64\"\n\t\"io\"\n)\n\n// implement io.Closer\ntype closerFunc func() error\n\nfunc (f closerFunc) Close() error {\n\treturn f()\n}\n\n// Base64Writer turns a writer for data into one that writes base64 content on\n// \\r\\n separated lines of max 78+2 characters length.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype lineWrapper struct {\n\tw io.Writer\n\tn int // Written on current line.\n}\n\nfunc (lw *lineWrapper) Write(buf []byte) (int, error) {\n\twrote := 0\n\tfor len(buf) > 0 {\n\t\tn := 78 - lw.n\n\t\tif n > len(buf) {\n\t\t\tn = len(buf)\n\t\t}\n\t\tnn, err := lw.w.Write(buf[:n])\n\t\tif nn > 0 {\n\t\t\twrote += nn\n\t\t\tbuf = buf[nn:]\n\t\t}\n\t\tif err != nil {\n\t\t\treturn wrote, err\n\t\t}\n\t\tlw.n += nn\n\t\tif lw.n == 78 {\n\t\t\t_, err := lw.w.Write([]byte(\"\\r\\n\"))\n\t\t\tif err != nil {\n\t\t\t\treturn wrote, err\n\t\t\t}\n\t\t\tlw.n = 0\n\t\t}\n\t}\n\treturn wrote, nil\n}\n\nfunc (lw *lineWrapper) Close() error {\n\tif lw.n > 0 {\n\t\tlw.n = 0\n\t\t_, err := lw.w.Write([]byte(\"\\r\\n\"))\n\t\treturn err\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func Base64Writer(w io.Writer) io.WriteCloser {\n# Natural Language Annotations: // Base64Writer turns a writer for data into one that writes base64 content on\n// \\r\\n separated lines of max 78+2 characters length.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Base64Writer(w io.Writer) io.WriteCloser {\n\tbase64Encoder := base64.NewEncoder(base64.StdEncoding, &lineWrapper{w: w})\n\treturn struct {\n\t\tio.Writer\n\t\tio.Closer\n\t}{\n\t\tWriter: base64Encoder,\n\t\tCloser: closerFunc(func() error {\n\t\t\tif err := base64Encoder.Close(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tlw := &lineWrapper{w: w}\n\t\t\treturn lw.Close()\n\t\t}),\n\t}\n}\n```"]}, "mox-dkim/sig.go-Header": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dkim/sig.go:\n```\npackage dkim\n\nimport (\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// Sig is a DKIM-Signature header.\n//\n// String values must be compared case insensitively.\ntype Sig struct {\n\t// Required fields.\n\tVersion       int        // Version, 1. Field \"v\". Always the first field.\n\tAlgorithmSign string     // \"rsa\" or \"ed25519\". Field \"a\".\n\tAlgorithmHash string     // \"sha256\" or the deprecated \"sha1\" (deprecated). Field \"a\".\n\tSignature     []byte     // Field \"b\".\n\tBodyHash      []byte     // Field \"bh\".\n\tDomain        dns.Domain // Field \"d\".\n\tSignedHeaders []string   // Duplicates are meaningful. Field \"h\".\n\tSelector      dns.Domain // Selector, for looking DNS TXT record at <s>._domainkey.<domain>. Field \"s\".\n\n\t// Optional fields.\n\t// Canonicalization is the transformation of header and/or body before hashing. The\n\t// value is in original case, but must be compared case-insensitively. Normally two\n\t// slash-separated values: header canonicalization and body canonicalization. But\n\t// the \"simple\" means \"simple/simple\" and \"relaxed\" means \"relaxed/simple\". Field\n\t// \"c\".\n\tCanonicalization string\n\tLength           int64     // Body length to verify, default -1 for whole body. Field \"l\".\n\tIdentity         *Identity // AUID (agent/user id). If nil and an identity is needed, should be treated as an Identity without localpart and Domain from d= field. Field \"i\".\n\tQueryMethods     []string  // For public key, currently known value is \"dns/txt\" (should be compared case-insensitively). If empty, dns/txt must be assumed. Field \"q\".\n\tSignTime         int64     // Unix epoch. -1 if unset. Field \"t\".\n\tExpireTime       int64     // Unix epoch. -1 if unset. Field \"x\".\n\tCopiedHeaders    []string  // Copied header fields. Field \"z\".\n}\n\n// Identity is used for the optional i= field in a DKIM-Signature header. It uses\n// the syntax of an email address, but does not necessarily represent one.\ntype Identity struct {\n\tLocalpart *smtp.Localpart // Optional.\n\tDomain    dns.Domain\n}\n\n// String returns a value for use in the i= DKIM-Signature field.\nfunc (i Identity) String() string {\n\ts := \"@\" + i.Domain.ASCII\n\t// We need localpart as pointer to indicate it is missing because localparts can be\n\t// \"\" which we store (decoded) as empty string and we need to differentiate.\n\tif i.Localpart != nil {\n\t\ts = i.Localpart.String() + s\n\t}\n\treturn s\n}\n\nfunc newSigWithDefaults() *Sig {\n\treturn &Sig{\n\t\tCanonicalization: \"simple/simple\",\n\t\tLength:           -1,\n\t\tSignTime:         -1,\n\t\tExpireTime:       -1,\n\t}\n}\n\n// Algorithm returns an algorithm string for use in the \"a\" field. E.g.\n// \"ed25519-sha256\".\nfunc (s Sig) Algorithm() string {\n\treturn s.AlgorithmSign + \"-\" + s.AlgorithmHash\n}\n\n// Header returns the DKIM-Signature header in string form, to be prepended to a\n// message, including DKIM-Signature field name and trailing \\r\\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Like quoted printable, but with \"|\" encoded as well.\n// We also encode \":\" because it is used as separator in DKIM headers which can\n// cause trouble for \"q\", even though it is listed in dkim-safe-char,\n// ../rfc/6376:497.\nfunc packQpHdrValue(s string) string {\n\t// ../rfc/6376:474\n\tconst hex = \"0123456789ABCDEF\"\n\tvar r string\n\tfor _, b := range []byte(s) {\n\t\tif b > ' ' && b < 0x7f && b != ';' && b != '=' && b != '|' && b != ':' {\n\t\t\tr += string(b)\n\t\t} else {\n\t\t\tr += \"=\" + string(hex[b>>4]) + string(hex[(b>>0)&0xf])\n\t\t}\n\t}\n\treturn r\n}\n\nvar (\n\terrSigHeader         = errors.New(\"not DKIM-Signature header\")\n\terrSigDuplicateTag   = errors.New(\"duplicate tag\")\n\terrSigMissingCRLF    = errors.New(\"missing crlf at end\")\n\terrSigExpired        = errors.New(\"signature timestamp (t=) must be before signature expiration (x=)\")\n\terrSigIdentityDomain = errors.New(\"identity domain (i=) not under domain (d=)\")\n\terrSigMissingTag     = errors.New(\"missing required tag\")\n\terrSigUnknownVersion = errors.New(\"unknown version\")\n\terrSigBodyHash       = errors.New(\"bad body hash size given algorithm\")\n)\n\n// parseSignatures returns the parsed form of a DKIM-Signature header.\n//\n// buf must end in crlf, as it should have occurred in the mail message.\n//\n// The dkim signature with signature left empty (\"b=\") and without trailing\n// crlf is returned, for use in verification.\nfunc parseSignature(buf []byte, smtputf8 bool) (sig *Sig, verifySig []byte, err error) {\n\tdefer func() {\n\t\tif x := recover(); x == nil {\n\t\t\treturn\n\t\t} else if xerr, ok := x.(error); ok {\n\t\t\tsig = nil\n\t\t\tverifySig = nil\n\t\t\terr = xerr\n\t\t} else {\n\t\t\tpanic(x)\n\t\t}\n\t}()\n\n\txerrorf := func(format string, args ...any) {\n\t\tpanic(fmt.Errorf(format, args...))\n\t}\n\n\tif !bytes.HasSuffix(buf, []byte(\"\\r\\n\")) {\n\t\txerrorf(\"%w\", errSigMissingCRLF)\n\t}\n\tbuf = buf[:len(buf)-2]\n\n\tds := newSigWithDefaults()\n\tseen := map[string]struct{}{}\n\tp := parser{s: string(buf), smtputf8: smtputf8}\n\tname := p.xhdrName(false)\n\tif !strings.EqualFold(name, \"DKIM-Signature\") {\n\t\txerrorf(\"%w\", errSigHeader)\n\t}\n\tp.wsp()\n\tp.xtake(\":\")\n\tp.wsp()\n\t// ../rfc/6376:655\n\t// ../rfc/6376:656 ../rfc/6376-eid5070\n\t// ../rfc/6376:658 ../rfc/6376-eid5070\n\tfor {\n\t\tp.fws()\n\t\tk := p.xtagName()\n\t\tp.fws()\n\t\tp.xtake(\"=\")\n\t\t// Special case for \"b\", see below.\n\t\tif k != \"b\" {\n\t\t\tp.fws()\n\t\t}\n\t\t// Keys are case-sensitive: ../rfc/6376:679\n\t\tif _, ok := seen[k]; ok {\n\t\t\t// Duplicates not allowed: ../rfc/6376:683\n\t\t\txerrorf(\"%w: %q\", errSigDuplicateTag, k)\n\t\t\tbreak\n\t\t}\n\t\tseen[k] = struct{}{}\n\n\t\t// ../rfc/6376:1021\n\t\tswitch k {\n\t\tcase \"v\":\n\t\t\t// ../rfc/6376:1025\n\t\t\tds.Version = int(p.xnumber(10))\n\t\t\tif ds.Version != 1 {\n\t\t\t\txerrorf(\"%w: version %d\", errSigUnknownVersion, ds.Version)\n\t\t\t}\n\t\tcase \"a\":\n\t\t\t// ../rfc/6376:1038\n\t\t\tds.AlgorithmSign, ds.AlgorithmHash = p.xalgorithm()\n\t\tcase \"b\":\n\t\t\t// ../rfc/6376:1054\n\t\t\t// To calculate the hash, we have to feed the DKIM-Signature header to the hash\n\t\t\t// function, but with the value for \"b=\" (the signature) left out. The parser\n\t\t\t// tracks all data that is read, except when drop is true.\n\t\t\t// ../rfc/6376:997\n\t\t\t// Surrounding whitespace must be cleared as well. ../rfc/6376:1659\n\t\t\t// Note: The RFC says \"surrounding\" whitespace, but whitespace is only allowed\n\t\t\t// before the value as part of the ABNF production for \"b\". Presumably the\n\t\t\t// intention is to ignore the trailing \"[FWS]\" for the tag-spec production,\n\t\t\t// ../rfc/6376:656\n\t\t\t// Another indication is the term \"value portion\", ../rfc/6376:1667. It appears to\n\t\t\t// mean everything after the \"b=\" part, instead of the actual value (either encoded\n\t\t\t// or decoded).\n\t\t\tp.drop = true\n\t\t\tp.fws()\n\t\t\tds.Signature = p.xbase64()\n\t\t\tp.fws()\n\t\t\tp.drop = false\n\t\tcase \"bh\":\n\t\t\t// ../rfc/6376:1076\n\t\t\tds.BodyHash = p.xbase64()\n\t\tcase \"c\":\n\t\t\t// ../rfc/6376:1088\n\t\t\tds.Canonicalization = p.xcanonical()\n\t\t\t// ../rfc/6376:810\n\t\tcase \"d\":\n\t\t\t// ../rfc/6376:1105\n\t\t\tds.Domain = p.xdomain()\n\t\tcase \"h\":\n\t\t\t// ../rfc/6376:1134\n\t\t\tds.SignedHeaders = p.xsignedHeaderFields()\n\t\tcase \"i\":\n\t\t\t// ../rfc/6376:1171\n\t\t\tid := p.xauid()\n\t\t\tds.Identity = &id\n\t\tcase \"l\":\n\t\t\t// ../rfc/6376:1244\n\t\t\tds.Length = p.xbodyLength()\n\t\tcase \"q\":\n\t\t\t// ../rfc/6376:1268\n\t\t\tds.QueryMethods = p.xqueryMethods()\n\t\tcase \"s\":\n\t\t\t// ../rfc/6376:1300\n\t\t\tds.Selector = p.xselector()\n\t\tcase \"t\":\n\t\t\t// ../rfc/6376:1310\n\t\t\tds.SignTime = p.xtimestamp()\n\t\tcase \"x\":\n\t\t\t// ../rfc/6376:1327\n\t\t\tds.ExpireTime = p.xtimestamp()\n\t\tcase \"z\":\n\t\t\t// ../rfc/6376:1361\n\t\t\tds.CopiedHeaders = p.xcopiedHeaderFields()\n\t\tdefault:\n\t\t\t// We must ignore unknown fields. ../rfc/6376:692 ../rfc/6376:1022\n\t\t\tp.xchar() // ../rfc/6376-eid5070\n\t\t\tfor !p.empty() && !p.hasPrefix(\";\") {\n\t\t\t\tp.xchar()\n\t\t\t}\n\t\t}\n\t\tp.fws()\n\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t\tp.xtake(\";\")\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// ../rfc/6376:2532\n\trequired := []string{\"v\", \"a\", \"b\", \"bh\", \"d\", \"h\", \"s\"}\n\tfor _, req := range required {\n\t\tif _, ok := seen[req]; !ok {\n\t\t\txerrorf(\"%w: %q\", errSigMissingTag, req)\n\t\t}\n\t}\n\n\tif strings.EqualFold(ds.AlgorithmHash, \"sha1\") && len(ds.BodyHash) != 20 {\n\t\txerrorf(\"%w: got %d bytes, must be 20 for sha1\", errSigBodyHash, len(ds.BodyHash))\n\t} else if strings.EqualFold(ds.AlgorithmHash, \"sha256\") && len(ds.BodyHash) != 32 {\n\t\txerrorf(\"%w: got %d bytes, must be 32 for sha256\", errSigBodyHash, len(ds.BodyHash))\n\t}\n\n\t// ../rfc/6376:1337\n\tif ds.SignTime >= 0 && ds.ExpireTime >= 0 && ds.SignTime >= ds.ExpireTime {\n\t\txerrorf(\"%w\", errSigExpired)\n\t}\n\n\t// Default identity is \"@\" plus domain. We don't set this value because we want to\n\t// keep the distinction between absent value.\n\t// ../rfc/6376:1172 ../rfc/6376:2537 ../rfc/6376:2541\n\tif ds.Identity != nil && ds.Identity.Domain.ASCII != ds.Domain.ASCII && !strings.HasSuffix(ds.Identity.Domain.ASCII, \".\"+ds.Domain.ASCII) {\n\t\txerrorf(\"%w: identity domain %q not under domain %q\", errSigIdentityDomain, ds.Identity.Domain.ASCII, ds.Domain.ASCII)\n\t}\n\n\treturn ds, []byte(p.tracked), nil\n}\n\n```\n# Function Signature: func (s *Sig) Header() (string, error) {\n# Natural Language Annotations: // Header returns the DKIM-Signature header in string form, to be prepended to a\n// message, including DKIM-Signature field name and trailing \\r\\n.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *Sig) Header() (string, error) {\n\tvar headerBuilder strings.Builder\n\n\t// Ensure the version is correct\n\tif s.Version != 1 {\n\t\treturn \"\", errSigUnknownVersion\n\t}\n\n\t// Start with the DKIM-Signature field name\n\theaderBuilder.WriteString(\"DKIM-Signature: \")\n\n\t// Function to append a field to the header\n\tappendField := func(key string, value string) {\n\t\theaderBuilder.WriteString(key)\n\t\theaderBuilder.WriteString(\"=\")\n\t\theaderBuilder.WriteString(value)\n\t\theaderBuilder.WriteString(\"; \")\n\t}\n\n\tappendField(\"v\", \"1\")\n\n\t// Append signing algorithm\n\tappendField(\"a\", s.Algorithm())\n\n\t// Append canonicalization\n\tappendField(\"c\", s.Canonicalization)\n\n\t// Append signature\n\tappendField(\"b\", base64.StdEncoding.EncodeToString(s.Signature))\n\n\t// Append signed headers\n\tappendField(\"bh\", base64.StdEncoding.EncodeToString(s.BodyHash))\n\tappendField(\"d\", s.Domain.ASCII)\n\tsignedHeadersJoined := strings.Join(s.SignedHeaders, \":\")\n\tappendField(\"h\", signedHeadersJoined)\n\n\t// Append optional fields if they are set\n\tif s.Identity != nil {\n\t\tappendField(\"i\", s.Identity.String())\n\t}\n\tif s.Length >= 0 {\n\t\tappendField(\"l\", fmt.Sprintf(\"%d\", s.Length))\n\t}\n\tif s.QueryMethods != nil {\n\t\tappendField(\"q\", strings.Join(s.QueryMethods, \":\"))\n\t}\n\tif s.Selector.ASCII != \"\" {\n\t\tappendField(\"s\", s.Selector.ASCII)\n\t}\n\tif s.SignTime >= 0 {\n\t\tappendField(\"t\", fmt.Sprintf(\"%d\", s.SignTime))\n\t}\n\tif s.ExpireTime >= 0 {\n\t\tappendField(\"x\", fmt.Sprintf(\"%d\", s.ExpireTime))\n\t}\n\tif len(s.CopiedHeaders) > 0 {\n\t\tcopiedHeadersJoined := strings.Join(s.CopiedHeaders, \"|\")\n\t\tappendField(\"z\", copiedHeadersJoined)\n\t}\n\n\t// Remove the last \"; \" and add the trailing CRLF\n\theader := strings.TrimSuffix(headerBuilder.String(), \"; \") + \"\\r\\n\"\n\n\treturn header, nil\n}\n```"]}, "mox-dkim/txt.go-Record": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dkim/txt.go:\n```\npackage dkim\n\nimport (\n\t\"crypto/ed25519\"\n\t\"crypto/rsa\"\n\t\"crypto/x509\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n)\n\n// Record is a DKIM DNS record, served on <selector>._domainkey.<domain> for a\n// given selector and domain (s= and d= in the DKIM-Signature).\n//\n// The record is a semicolon-separated list of \"=\"-separated field value pairs.\n// Strings should be compared case-insensitively, e.g. k=ed25519 is equivalent to k=ED25519.\n//\n// Example:\n//\n//\tv=DKIM1;h=sha256;k=ed25519;p=ln5zd/JEX4Jy60WAhUOv33IYm2YZMyTQAdr9stML504=\ntype Record struct {\n\tVersion  string   // Version, fixed \"DKIM1\" (case sensitive). Field \"v\".\n\tHashes   []string // Acceptable hash algorithms, e.g. \"sha1\", \"sha256\". Optional, defaults to all algorithms. Field \"h\".\n\tKey      string   // Key type, \"rsa\" or \"ed25519\". Optional, default \"rsa\". Field \"k\".\n\tNotes    string   // Debug notes. Field \"n\".\n\tPubkey   []byte   // Public key, as base64 in record. If empty, the key has been revoked. Field \"p\".\n\tServices []string // Service types. Optional, default \"*\" for all services. Other values: \"email\". Field \"s\".\n\tFlags    []string // Flags, colon-separated. Optional, default is no flags. Other values: \"y\" for testing DKIM, \"s\" for \"i=\" must have same domain as \"d\" in signatures. Field \"t\".\n\n\tPublicKey any `json:\"-\"` // Parsed form of public key, an *rsa.PublicKey or ed25519.PublicKey.\n}\n\n// ../rfc/6376:1438\n\n// ServiceAllowed returns whether service s is allowed by this key.\n//\n// The optional field \"s\" can specify purposes for which the key can be used. If\n// value was specified, both \"*\" and \"email\" are enough for use with DKIM.\nfunc (r *Record) ServiceAllowed(s string) bool {\n\tif len(r.Services) == 0 {\n\t\treturn true\n\t}\n\tfor _, ss := range r.Services {\n\t\tif ss == \"*\" || strings.EqualFold(s, ss) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Record returns a DNS TXT record that should be served at\n// <selector>._domainkey.<domain>.\n//\n// Only values that are not the default values are included.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc qpSection(s string) string {\n\tconst hex = \"0123456789ABCDEF\"\n\n\t// ../rfc/2045:1260\n\tvar r string\n\tfor i, b := range []byte(s) {\n\t\tif i > 0 && (b == ' ' || b == '\\t') || b > ' ' && b < 0x7f && b != '=' {\n\t\t\tr += string(rune(b))\n\t\t} else {\n\t\t\tr += \"=\" + string(hex[b>>4]) + string(hex[(b>>0)&0xf])\n\t\t}\n\t}\n\treturn r\n}\n\nvar (\n\terrRecordDuplicateTag     = errors.New(\"duplicate tag\")\n\terrRecordMissingField     = errors.New(\"missing field\")\n\terrRecordBadPublicKey     = errors.New(\"bad public key\")\n\terrRecordUnknownAlgorithm = errors.New(\"unknown algorithm\")\n\terrRecordVersionFirst     = errors.New(\"first field must be version\")\n)\n\n// ParseRecord parses a DKIM DNS TXT record.\n//\n// If the record is a dkim record, but an error occurred, isdkim will be true and\n// err will be the error. Such errors must be treated differently from parse errors\n// where the record does not appear to be DKIM, which can happen with misconfigured\n// DNS (e.g. wildcard records).\nfunc ParseRecord(s string) (record *Record, isdkim bool, err error) {\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\tif xerr, ok := x.(error); ok {\n\t\t\trecord = nil\n\t\t\terr = xerr\n\t\t\treturn\n\t\t}\n\t\tpanic(x)\n\t}()\n\n\txerrorf := func(format string, args ...any) {\n\t\tpanic(fmt.Errorf(format, args...))\n\t}\n\n\trecord = &Record{\n\t\tVersion:  \"DKIM1\",\n\t\tKey:      \"rsa\",\n\t\tServices: []string{\"*\"},\n\t}\n\n\tp := parser{s: s, drop: true}\n\tseen := map[string]struct{}{}\n\t// ../rfc/6376:655\n\t// ../rfc/6376:656 ../rfc/6376-eid5070\n\t// ../rfc/6376:658 ../rfc/6376-eid5070\n\t// ../rfc/6376:1438\n\tfor {\n\t\tp.fws()\n\t\tk := p.xtagName()\n\t\tp.fws()\n\t\tp.xtake(\"=\")\n\t\tp.fws()\n\t\t// Keys are case-sensitive: ../rfc/6376:679\n\t\tif _, ok := seen[k]; ok {\n\t\t\t// Duplicates not allowed: ../rfc/6376:683\n\t\t\txerrorf(\"%w: %q\", errRecordDuplicateTag, k)\n\t\t\tbreak\n\t\t}\n\t\tseen[k] = struct{}{}\n\t\t// Version must be the first.\n\t\tswitch k {\n\t\tcase \"v\":\n\t\t\t// ../rfc/6376:1443\n\t\t\tv := p.xtake(\"DKIM1\")\n\t\t\t// Version being set is a signal this appears to be a valid record. We must not\n\t\t\t// treat e.g. DKIM1.1 as valid, so we explicitly check there is no more data before\n\t\t\t// we decide this record is DKIM.\n\t\t\tp.fws()\n\t\t\tif !p.empty() {\n\t\t\t\tp.xtake(\";\")\n\t\t\t}\n\t\t\trecord.Version = v\n\t\t\tif len(seen) != 1 {\n\t\t\t\t// If version is present, it must be the first.\n\t\t\t\txerrorf(\"%w\", errRecordVersionFirst)\n\t\t\t}\n\t\t\tisdkim = true\n\t\t\tif p.empty() {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tcontinue\n\n\t\tcase \"h\":\n\t\t\t// ../rfc/6376:1463\n\t\t\trecord.Hashes = []string{p.xhyphenatedWord()}\n\t\t\tfor p.peekfws(\":\") {\n\t\t\t\tp.fws()\n\t\t\t\tp.xtake(\":\")\n\t\t\t\tp.fws()\n\t\t\t\trecord.Hashes = append(record.Hashes, p.xhyphenatedWord())\n\t\t\t}\n\t\tcase \"k\":\n\t\t\t// ../rfc/6376:1478\n\t\t\trecord.Key = p.xhyphenatedWord()\n\t\tcase \"n\":\n\t\t\t// ../rfc/6376:1491\n\t\t\trecord.Notes = p.xqpSection()\n\t\tcase \"p\":\n\t\t\t// ../rfc/6376:1501\n\t\t\trecord.Pubkey = p.xbase64()\n\t\tcase \"s\":\n\t\t\t// ../rfc/6376:1533\n\t\t\trecord.Services = []string{p.xhyphenatedWord()}\n\t\t\tfor p.peekfws(\":\") {\n\t\t\t\tp.fws()\n\t\t\t\tp.xtake(\":\")\n\t\t\t\tp.fws()\n\t\t\t\trecord.Services = append(record.Services, p.xhyphenatedWord())\n\t\t\t}\n\t\tcase \"t\":\n\t\t\t// ../rfc/6376:1554\n\t\t\trecord.Flags = []string{p.xhyphenatedWord()}\n\t\t\tfor p.peekfws(\":\") {\n\t\t\t\tp.fws()\n\t\t\t\tp.xtake(\":\")\n\t\t\t\tp.fws()\n\t\t\t\trecord.Flags = append(record.Flags, p.xhyphenatedWord())\n\t\t\t}\n\t\tdefault:\n\t\t\t// We must ignore unknown fields. ../rfc/6376:692 ../rfc/6376:1439\n\t\t\tfor !p.empty() && !p.hasPrefix(\";\") {\n\t\t\t\tp.xchar()\n\t\t\t}\n\t\t}\n\n\t\tisdkim = true\n\t\tp.fws()\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t\tp.xtake(\";\")\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif _, ok := seen[\"p\"]; !ok {\n\t\txerrorf(\"%w: public key\", errRecordMissingField)\n\t}\n\n\tswitch strings.ToLower(record.Key) {\n\tcase \"\", \"rsa\":\n\t\tif len(record.Pubkey) == 0 {\n\t\t\t// Revoked key, nothing to do.\n\t\t} else if pk, err := x509.ParsePKIXPublicKey(record.Pubkey); err != nil {\n\t\t\txerrorf(\"%w: %s\", errRecordBadPublicKey, err)\n\t\t} else if _, ok := pk.(*rsa.PublicKey); !ok {\n\t\t\txerrorf(\"%w: got %T, need an RSA key\", errRecordBadPublicKey, record.PublicKey)\n\t\t} else {\n\t\t\trecord.PublicKey = pk\n\t\t}\n\tcase \"ed25519\":\n\t\tif len(record.Pubkey) == 0 {\n\t\t\t// Revoked key, nothing to do.\n\t\t} else if len(record.Pubkey) != ed25519.PublicKeySize {\n\t\t\txerrorf(\"%w: got %d bytes, need %d\", errRecordBadPublicKey, len(record.Pubkey), ed25519.PublicKeySize)\n\t\t} else {\n\t\t\trecord.PublicKey = ed25519.PublicKey(record.Pubkey)\n\t\t}\n\tdefault:\n\t\txerrorf(\"%w: %q\", errRecordUnknownAlgorithm, record.Key)\n\t}\n\n\treturn record, true, nil\n}\n\n```\n# Function Signature: func (r *Record) Record() (string, error) {\n# Natural Language Annotations: // Record returns a DNS TXT record that should be served at\n// <selector>._domainkey.<domain>.\n//\n// Only values that are not the default values are included.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (r *Record) Record() (string, error) {\n\tvar builder strings.Builder\n\n\t// Always include the version, which has no default.\n\tbuilder.WriteString(fmt.Sprintf(\"v=%s\", r.Version))\n\n\t// Include hash algorithms if they are specified\n\tif len(r.Hashes) > 0 {\n\t\tbuilder.WriteString(\";h=\")\n\t\tbuilder.WriteString(strings.Join(r.Hashes, \":\"))\n\t}\n\n\t// Only include the key type if it is not the default (\"rsa\")\n\tif strings.ToLower(r.Key) != \"rsa\" {\n\t\tbuilder.WriteString(\";k=\")\n\t\tbuilder.WriteString(r.Key)\n\t}\n\n\t// Include notes if they are present\n\tif r.Notes != \"\" {\n\t\tbuilder.WriteString(\";n=\")\n\t\tbuilder.WriteString(qpSection(r.Notes))\n\t}\n\n\t// Include pubkey only if it is present (cannot omit a revoked key representation)\n\tif len(r.Pubkey) > 0 {\n\t\tbuilder.WriteString(\";p=\")\n\t\tbuilder.WriteString(base64.StdEncoding.EncodeToString(r.Pubkey))\n\t}\n\n\t// Include services if they are specified and different from the default (\"*\")\n\tif len(r.Services) > 0 && !(len(r.Services) == 1 && r.Services[0] == \"*\") {\n\t\tbuilder.WriteString(\";s=\")\n\t\tbuilder.WriteString(strings.Join(r.Services, \":\"))\n\t}\n\n\t// Include flags if they are specified\n\tif len(r.Flags) > 0 {\n\t\tbuilder.WriteString(\";t=\")\n\t\tbuilder.WriteString(strings.Join(r.Flags, \":\"))\n\t}\n\n\treturn builder.String(), nil\n}\n```"]}}